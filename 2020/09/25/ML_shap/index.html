<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>2. shap 解释模型 | aigonna</title><meta name="keywords" content="LSTM,shap"><meta name="author" content="aigonna"><meta name="copyright" content="aigonna"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="2. shap 解释模型1. shap解释回归模型1234567891011121314import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.cm as cmimport matplotlib.pyplot as pltfrom sklearn.feature_extraction.text i">
<meta property="og:type" content="article">
<meta property="og:title" content="2. shap 解释模型">
<meta property="og:url" content="http://aigonna.com/2020/09/25/ML_shap/index.html">
<meta property="og:site_name" content="aigonna">
<meta property="og:description" content="2. shap 解释模型1. shap解释回归模型1234567891011121314import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.cm as cmimport matplotlib.pyplot as pltfrom sklearn.feature_extraction.text i">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://aigonna.com/img/imgs/10.jpg">
<meta property="article:published_time" content="2020-09-25T11:09:10.000Z">
<meta property="article:modified_time" content="2022-05-01T12:01:31.142Z">
<meta property="article:author" content="aigonna">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="shap">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://aigonna.com/img/imgs/10.jpg"><link rel="shortcut icon" href="/img/AI.png"><link rel="canonical" href="http://aigonna.com/2020/09/25/ML_shap/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-RMGH8E0YEQ"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RMGH8E0YEQ');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    jQuery: 'https://fastly.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://fastly.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://fastly.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-01 20:01:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/AI.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">211</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/imgs/10.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">aigonna</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">2. shap 解释模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2020-09-25T11:09:10.000Z" title="undefined 2020-09-25 19:09:10">2020-09-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>6分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="2-shap-解释模型"><a href="#2-shap-解释模型" class="headerlink" title="2. shap 解释模型"></a>2. shap 解释模型</h3><h4 id="1-shap解释回归模型"><a href="#1-shap解释回归模型" class="headerlink" title="1. shap解释回归模型"></a>1. shap解释回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> stop_words</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> string, re</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<p><strong>导入数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;https://query.data.world/s/yd24ckbjzyp7h6zp7bacafpv2lgfkh&quot;</span>, encoding=<span class="string">&quot;ISO-8859-1&quot;</span>)</span><br><span class="line">display(df.shape)</span><br><span class="line">display(df[<span class="string">&quot;relevance&quot;</span>].value_counts()/df.shape[<span class="number">0</span>])</span><br><span class="line">=================================================================================</span><br><span class="line">(<span class="number">8000</span>, <span class="number">15</span>)</span><br><span class="line">no          <span class="number">0.821375</span></span><br><span class="line">yes         <span class="number">0.177500</span></span><br><span class="line"><span class="keyword">not</span> sure    <span class="number">0.001125</span></span><br><span class="line">Name: relevance, dtype: float64</span><br></pre></td></tr></table></figure>
<p>去除not sure：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = df[df.relevance != <span class="string">&#x27;not sure&#x27;</span>]</span><br><span class="line">df.shape</span><br><span class="line">======================================================================</span><br><span class="line">(<span class="number">7991</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;relevance&#x27;</span>] = df.relevance.<span class="built_in">map</span>(&#123;<span class="string">&#x27;yes&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>:<span class="number">0</span>&#125;)</span><br><span class="line">df = df[[<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;relevance&#x27;</span>]]</span><br><span class="line">df.shape</span><br><span class="line">======================================================================</span><br><span class="line">(<span class="number">7991</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">stopwords = stop_words.ENGLISH_STOP_WORDS</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean</span>(<span class="params">doc</span>): <span class="comment">#doc is a string of text</span></span><br><span class="line">    doc = doc.replace(<span class="string">&quot;&lt;/br&gt;&quot;</span>, <span class="string">&quot; &quot;</span>) <span class="comment">#This text contains a lot of &lt;br/&gt; tags.</span></span><br><span class="line">    doc = <span class="string">&quot;&quot;</span>.join([char <span class="keyword">for</span> char <span class="keyword">in</span> doc <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation <span class="keyword">and</span> <span class="keyword">not</span> char.isdigit()])</span><br><span class="line">    doc = <span class="string">&quot; &quot;</span>.join([token <span class="keyword">for</span> token <span class="keyword">in</span> doc.split() <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> stopwords])</span><br><span class="line">    <span class="comment">#remove punctuation and numbers</span></span><br><span class="line">    <span class="keyword">return</span> doc</span><br><span class="line"></span><br><span class="line">x = df.text</span><br><span class="line">y = df.relevance</span><br><span class="line"><span class="built_in">print</span>(x.shape, y.shape)</span><br><span class="line">=========================================================================</span><br><span class="line">(<span class="number">7991</span>,) (<span class="number">7991</span>,)</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape)</span><br><span class="line">=========================================================================</span><br><span class="line">(<span class="number">5993</span>,) (<span class="number">5993</span>,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vect = TfidfVectorizer(min_df=<span class="number">5</span>)</span><br><span class="line">x_train_dtm = vect.fit_transform(x_train)</span><br><span class="line">x_test_dtm = vect.transform(x_test)</span><br><span class="line">model = LogisticRegression(class_weight=<span class="string">&#x27;balanced&#x27;</span>)</span><br><span class="line">model.fit(x_train_dtm, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_class = model.predict(x_test_dtm)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: &quot;</span>, accuracy_score(y_test, y_pred_class))</span><br><span class="line">==============================================================================</span><br><span class="line">Accuracy:  <span class="number">0.7382382382382382</span></span><br></pre></td></tr></table></figure>
<p><strong>shap 解释模型</strong></p>
<p>这里有个比较全面的实例 <a target="_blank" rel="noopener" href="https://mathpretty.com/10699.html">模型解释–SHAP Value的简单介绍</a> 。使用步骤：</p>
<ol>
<li>实例化线性解释器 <code>shap.LinearExplainer()</code> <ul>
<li>不同模型对应不同解释器：<ul>
<li><strong>TreeExplainer</strong> : Support XGBoost, LightGBM, CatBoost and scikit-learn models by Tree SHAP.</li>
<li><strong>DeepExplainer (DEEP SHAP)</strong> : Support TensorFlow and Keras models by using DeepLIFT and Shapley values.</li>
<li><strong>GradientExplainer</strong> : Support TensorFlow and Keras models.</li>
<li><strong>KernelExplainer (Kernel SHAP)</strong> : Applying to any models by using <strong>LIME and Shapley values</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>对数据进行解释</li>
<li>对结果进行可视化</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line">explainer = shap.LinearExplainer(model, x_train_dtm,  feature_perturbation=<span class="string">&quot;intervebtional&quot;</span>)</span><br><span class="line">shap_values = explainer.shap_values(x_test_dtm)</span><br><span class="line">x_test_array = x_test_dtm.toarray()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">pprint(df[<span class="string">&#x27;text&#x27;</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(dpi=<span class="number">120</span>)</span><br><span class="line">shap.initjs()</span><br><span class="line">shap.summary_plot(shap_values, x_test_array, feature_names=vect.get_feature_names())</span><br></pre></td></tr></table></figure>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205012001544.png" alt="Snipaste_2021-05-18_15-58-56" style="zoom:30%;" /></p>
<p>上图说明：</p>
<ol>
<li>特征重要性：变量重要程度由上往下递减，这里看到economy, 对于整个预测来说是比较重要的</li>
<li>水平方向是每个特征对于对应预测的影响</li>
<li>原始值：红色代表观察的数值大，蓝色代表观察的数值小</li>
<li>相关程度：dollar对于决策这篇文章是不是跟美国经济相关程度</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shap.initjs()</span><br><span class="line">shap.force_plot(</span><br><span class="line">    explainer.expected_value, shap_values[<span class="number">0</span>, :], x_test_array[<span class="number">0</span>, :],</span><br><span class="line">    feature_names=vect.get_feature_names()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205012001928.png" alt="Snipaste_2021-05-18_16-00-34" style="zoom:30%;" /></p>
<p>上图说明：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">原始论文</a> 说base_value是 <script type="math/tex">E(\hat y)</script>: the value that would be predicted if we did not know any features for the current output.可以理解为预测值或者预测期望。</li>
<li>红色和蓝色: 将预测值推高的特征值显示为红色，推低的显示为蓝色。</li>
<li>经济： 对该文章是否跟美国经济有关有积极影响，将预测值推向右边。</li>
</ol>
<h4 id="2-shap-解释lstm"><a href="#2-shap-解释lstm" class="headerlink" title="2. shap 解释lstm"></a>2. shap 解释lstm</h4><p>注： tf使用为1.15.2，1.14也行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> re, os, sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, GlobalMaxPooling1D,\</span><br><span class="line">                        Conv1D, MaxPooling1D, Embedding, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> Constant</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">max_seq_len = <span class="number">1000</span></span><br><span class="line">max_num_words = <span class="number">2000</span></span><br><span class="line">emb_dim = <span class="number">100</span></span><br><span class="line">valid_split = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">20000</span></span><br><span class="line">maxlen = <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p><strong>导入数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_directory_data</span>(<span class="params">directory</span>):</span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    data[<span class="string">&quot;sentence&quot;</span>] = []</span><br><span class="line">    data[<span class="string">&quot;sentiment&quot;</span>] = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> os.listdir(directory):</span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(os.path.join(directory, file_path), <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data[<span class="string">&quot;sentence&quot;</span>].append(f.read())</span><br><span class="line">            data[<span class="string">&quot;sentiment&quot;</span>].append(re.match(<span class="string">&quot;\d+_(\d+)\.txt&quot;</span>, file_path).group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame.from_dict(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge positive and negative examples, add a polarity column and shuffle.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>(<span class="params">directory</span>):</span><br><span class="line">    pos_df = load_directory_data(os.path.join(directory, <span class="string">&quot;pos&quot;</span>))</span><br><span class="line">    neg_df = load_directory_data(os.path.join(directory, <span class="string">&quot;neg&quot;</span>))</span><br><span class="line">    pos_df[<span class="string">&quot;polarity&quot;</span>] = <span class="number">1</span></span><br><span class="line">    neg_df[<span class="string">&quot;polarity&quot;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> pd.concat([pos_df, neg_df]).sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_and_load_datasets</span>(<span class="params">force_download=<span class="literal">False</span></span>):</span><br><span class="line">    dataset = tf.keras.utils.get_file(</span><br><span class="line">        fname=<span class="string">&quot;aclImdb.tar.gz&quot;</span>,</span><br><span class="line">        origin=<span class="string">&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot;</span>,</span><br><span class="line">        extract=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_df = load_dataset(os.path.join(os.path.dirname(dataset),</span><br><span class="line">                                         <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;train&quot;</span>))</span><br><span class="line">    test_df = load_dataset(os.path.join(os.path.dirname(dataset),</span><br><span class="line">                                        <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;test&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train, test = download_and_load_datasets()</span><br></pre></td></tr></table></figure>
<p><strong>数据预处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">train_texts = train[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line">train_labels = train[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line">test_texts = test[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line">test_labels = test[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">labels_index = &#123;<span class="string">&#x27;pos&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;neg&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words=max_num_words)</span><br><span class="line">tokenizer.fit_on_texts(train_texts)</span><br><span class="line">train_sequences = tokenizer.texts_to_sequences(train_texts)<span class="comment">#将文本转为词索引</span></span><br><span class="line">test_sequences = tokenizer.texts_to_sequences(test_texts)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Found %s unique tokens.&quot;</span>%<span class="built_in">len</span>(word_index))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将文本转为等长的向量</span></span><br><span class="line">train_valid_data = pad_sequences(train_sequences, maxlen=max_seq_len)</span><br><span class="line">test_data = pad_sequences(test_sequences, maxlen=max_seq_len)</span><br><span class="line">train_valid_labels = to_categorical(np.asarray(train_labels))</span><br><span class="line">test_labels = to_categorical(np.asarray(test_labels))</span><br><span class="line"></span><br><span class="line"><span class="comment">#划分数据集</span></span><br><span class="line">indices = np.arange(train_valid_data.shape[<span class="number">0</span>])</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line"></span><br><span class="line">train_valid_data = train_valid_data[indices]</span><br><span class="line">train_valid_labels = train_valid_labels[indices]</span><br><span class="line">num_valid_samples = <span class="built_in">int</span>(valid_split * train_valid_data.shape[<span class="number">0</span>])</span><br><span class="line">x_train = train_valid_data[:-num_valid_samples]</span><br><span class="line">y_train = train_valid_labels[:-num_valid_samples]</span><br><span class="line">x_val = train_valid_data[-num_valid_samples:]</span><br><span class="line">y_val = train_valid_labels[-num_valid_samples:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;划分数据集为训练测试集完毕！&quot;</span>)</span><br><span class="line">============================================================================</span><br><span class="line">Found <span class="number">88582</span> unique tokens.</span><br><span class="line">划分数据集为训练测试集完毕！</span><br></pre></td></tr></table></figure>
<p><strong>模型训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_features = vocab_size + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;定义训练LSTM模型: &quot;</span>)</span><br><span class="line">lstm = Sequential() <span class="comment">#shap只能用Sequential搭建deepnet模型</span></span><br><span class="line">lstm.add(Embedding(max_num_words, <span class="number">128</span>))</span><br><span class="line">lstm.add(LSTM(<span class="number">128</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">lstm.add(Dense(<span class="number">2</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">lstm.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">             optimizer=<span class="string">&#x27;Adam&#x27;</span>,</span><br><span class="line">             metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LSTM开始训练！&quot;</span>)</span><br><span class="line">lstm.fit(x_train, y_train,</span><br><span class="line">         batch_size=<span class="number">32</span>, epochs=<span class="number">2</span>,</span><br><span class="line">         validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<p><strong>shap 解释lstm</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"></span><br><span class="line">shap.initjs()</span><br><span class="line">explainer = shap.DeepExplainer(lstm, x_train[:<span class="number">20</span>])</span><br><span class="line"><span class="comment">#解释每个预测值要2*背景数据，下面解释10个</span></span><br><span class="line">shap_values = explainer.shap_values(x_val[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>获取验证集上前10个词对应的索引矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">words = imdb.get_word_index()</span><br><span class="line">num2word = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words.keys():</span><br><span class="line">    num2word[words[w]] = w</span><br><span class="line">x_val_words = np.stack([np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: num2word.get(x, <span class="string">&quot;NONE&quot;</span>), x_val[i]))) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shap.initjs()</span><br><span class="line"></span><br><span class="line">shap.force_plot(explainer.expected_value[<span class="number">0</span>], shap_values[<span class="number">0</span>][<span class="number">0</span>], x_val_words[<span class="number">0</span>],</span><br><span class="line">                text_rotation=<span class="number">30</span>,</span><br><span class="line">                matplotlib=<span class="literal">True</span>,</span><br><span class="line">                show=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261827484.png" alt="image-20210518202004105" style="zoom:30%;" /></p>
<p>补充材料：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1485897">如何解决机器学习树集成模型的解释性问题</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">aigonna</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://aigonna.com/2020/09/25/ML_shap/">http://aigonna.com/2020/09/25/ML_shap/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://aigonna.com" target="_blank">aigonna</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LSTM/">LSTM</a><a class="post-meta__tags" href="/tags/shap/">shap</a></div><div class="post_share"><div class="social-share" data-image="/img/imgs/10.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://fastly.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/10/02/NLP%20Paper%204.%20fasttext%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><img class="prev-cover" src="/img/imgs/3.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">4. Fasttext 分类器论文Bag of Tricks for Efficient Text Classification</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/24/ML_lime/"><img class="next-cover" src="/img/imgs/11.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">1. lime 解释LSTM模型</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/12/CS224N Lecture 6 Vanishing Gradients, Fancy RNNs, Seq2Seq/" title="CS224N Lecture 6 Vanishing Gradients, Fancy RNNs, Seq2Seq"><img class="cover" src="/img/imgs/7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-12</div><div class="title">CS224N Lecture 6 Vanishing Gradients, Fancy RNNs, Seq2Seq</div></div></a></div><div><a href="/2020/09/20/4.RNN,  LSTM , GRU 结构解释和其在Pytorch中的使用/" title="RNN， LSTM ， GRU 结构解释和其在Pytorch中的使用"><img class="cover" src="/img/imgs/7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-20</div><div class="title">RNN， LSTM ， GRU 结构解释和其在Pytorch中的使用</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81MDc2My8yNzI0NQ"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-shap-%E8%A7%A3%E9%87%8A%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. shap 解释模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-shap%E8%A7%A3%E9%87%8A%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. shap解释回归模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-shap-%E8%A7%A3%E9%87%8Alstm"><span class="toc-text">2. shap 解释lstm</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022&nbsp;<i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;"class="fas fa-heart"></i> By aigonna</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://fastly.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      //tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://fastly.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'forest',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>