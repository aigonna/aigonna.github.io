<!DOCTYPE html><html class="hide-aside" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>10. RoBERTa 论文笔记 | aigonna</title><meta name="keywords" content="RoBERTa,BERT,PTMs,BPE"><meta name="author" content="miller"><meta name="copyright" content="miller"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="10. RoBERTa 论文笔记RoBERTa是RoBERTa: A Robustly Optimized BERT Pretraining Approach的简称。本文主要是论文的阅读笔记。 Abstract预训练语言模型已经取得了显著的表现但是仔细比较不同方法是有挑战性的。在计算上训练非常昂贵，通常在不同大小的私有数据集上，并且超参数选择对最终结果有重大影响。我们提出一个BERT预训练的复制研">
<meta property="og:type" content="article">
<meta property="og:title" content="10. RoBERTa 论文笔记">
<meta property="og:url" content="http://aigonna.com/2020/12/19/NLP%20Paper%20%2010.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="aigonna">
<meta property="og:description" content="10. RoBERTa 论文笔记RoBERTa是RoBERTa: A Robustly Optimized BERT Pretraining Approach的简称。本文主要是论文的阅读笔记。 Abstract预训练语言模型已经取得了显著的表现但是仔细比较不同方法是有挑战性的。在计算上训练非常昂贵，通常在不同大小的私有数据集上，并且超参数选择对最终结果有重大影响。我们提出一个BERT预训练的复制研">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://aigonna.com/img/imgs/16.jpg">
<meta property="article:published_time" content="2020-12-19T13:49:39.000Z">
<meta property="article:modified_time" content="2021-07-20T14:57:37.662Z">
<meta property="article:author" content="miller">
<meta property="article:tag" content="RoBERTa">
<meta property="article:tag" content="BERT">
<meta property="article:tag" content="PTMs">
<meta property="article:tag" content="BPE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://aigonna.com/img/imgs/16.jpg"><link rel="shortcut icon" href="/img/AI.png"><link rel="canonical" href="http://aigonna.com/2020/12/19/NLP%20Paper%20%2010.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-07-20 22:57:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><link rel="stylesheet" href="/css/bg.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/AI.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">150</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/imgs/16.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">aigonna</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10. RoBERTa 论文笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2020-12-19T13:49:39.000Z" title="undefined 2020-12-19 21:49:39">2020-12-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-paper/">NLP paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="10-RoBERTa-论文笔记"><a href="#10-RoBERTa-论文笔记" class="headerlink" title="10. RoBERTa 论文笔记"></a>10. RoBERTa 论文笔记</h3><p>RoBERTa是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>的简称。本文主要是论文的阅读笔记。</p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>预训练语言模型已经取得了显著的表现但是仔细比较不同方法是有挑战性的。在计算上训练非常昂贵，通常在不同大小的私有数据集上，并且超参数选择对最终结果有重大影响。我们提出一个BERT预训练的复制研究，仔细衡量许多关键超参数和训练数据大小的影响。我们发现BERT明显训练不足，并且能匹配和超越在其之后发布的每个模型表现。我们最好的模型在GLUE， RACE 和SQuAD上获得了最佳成绩。这些结果强调之前忽略的设计方案，并提出了近期报告的改进的来源的问题。我们发布了我们的模型和代码。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><p>自训练方法如ELMo， GPT，BERT， XLM， XLNet已经带来显著的性能提升，但是它们都被质疑，能否准确算出哪些方法贡献最大。在计算上训练是昂贵的，限制了可以进行微调的数目，或者说经常在不同大小的私有数据集上进行，限制了衡量建模效果的能力的发展。</p>
<p>我们提出BERT预训练的复制研究，包含慎重的<strong>超参数微调</strong>和<strong>训练集大小</strong>的影响评估。我们发现BERT是显著地训练不足，并提出一个改进的用来训练BERT模型的方案，我们称之为RoBERTa, 其能匹配或超越所有post-BERT方法的性能。我们改进非常简单，它们包括:</p>
<ol>
<li>训练模型更长时间，更大batch_size, 更多的数据</li>
<li>移除next sentence预测任务</li>
<li>训练更长的句子</li>
<li>动态改变应用于训练数据的掩码模式</li>
</ol>
<p>我们也收集一个新的大型新闻数据集(CC-News) ，其大小类似于其它私人使用的数据集，以便更好地控制训练数据集大小的影响。</p>
<p>当控制训练数据时，我们改进的训练程序提升了已公布的BERT在GLUE和SQuAD两者上的的成绩。当在更多数据上进行更长时间的训练后，RoBERTa在公共GLUE排行榜上获得了88.5的分数，与Yang (Xlnet 2019) 88.4成绩相当。我们模型在4/9的GLUE任务上获得了最佳的成果: MNLI, QNLI, RTE 和STS-B。我们也在SQuAD和RACE上获得了相当的成绩。总的来说，我们重建了BERT的掩码语言模型训练方法和其它最近提出的有竞争性训练方法，如扰动自回归语言模型。</p>
<p>总的来说，本论文的贡献有:</p>
<ol>
<li>我们提出了一组的重要的BERT设计方案和训练策略，并引入更好的下游任务性能的替代方案；</li>
<li>我们使用一个小说数据集，CC-News，来确保使用更多数据进行预训练来进一步提升在下游任务上的表现；</li>
<li>我们训练提升表明掩码预训练语言模型，在正确的设计方案下，跟近期发布的所有其它方法都具有竞争力。</li>
</ol>
<p>我们发布了用Pytorch实现的模型，预训练和微调代码。</p>
<h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2.  Background"></a>2.  Background</h4><p>本节，简洁回顾BERT预训练方法和一些我们将在接下来小节中实验检查的训练方案。</p>
<h5 id="2-1-Setup"><a href="#2-1-Setup" class="headerlink" title="2.1 Setup"></a>2.1 Setup</h5><p>BERT 采取两段(字符序列)连接作为输入，<script type="math/tex">x_1, \cdots,x_N 和 y_1, \cdots, y_M</script>。字符段通常由多余一个自然语句构成。两个字符段表示为一个用特殊字符分割两者的单一输入序列给BERT:<script type="math/tex">[\text{CLS}], x_1, x_2, \cdots, x_N, [\text{SEP}], y_1, \cdots, y_M, [\text{EOS}]</script>。 其中<script type="math/tex">M, N</script>限制为<script type="math/tex">M + N \lt T</script>,而<script type="math/tex">T</script>是训练中控制最大序列长度的参数。</p>
<p>该模型是首次预训练在大规模无标签文本语料并随后使用最终任务的有标签数据微调。</p>
<h5 id="2-2-Architecture"><a href="#2-2-Architecture" class="headerlink" title="2.2 Architecture"></a>2.2 Architecture</h5><p>BERT使用现在十分普遍的transformer架构，其我们将不会回顾其细节。我们使用<script type="math/tex">L</script>层的transformer架构。每个块使用A个维度为H的隐藏层的自注意力头。</p>
<h5 id="2-3-Training-Objectives"><a href="#2-3-Training-Objectives" class="headerlink" title="2.3 Training Objectives"></a>2.3 Training Objectives</h5><p>在预训练期间，BERT使用两个方法:掩码语言模型MLM和下一句预测NSP.</p>
<p><strong>Masked Language Model (MLM)</strong> </p>
<p>在输入序列中随机采样选择字符并替换其为特殊的字符[MASK]. MLM目标函数是预测这些mask的 tokens的交叉熵损失。BERT 统一地将输入tokens按15%可能性作为被选择字符进行替换。被选择的80%用[MASK]替换， 10%不变，10%用词汇token中随机选择替换。</p>
<p>在原本实现中，随机选择掩码和替换只在开始执行一次，并将其保存在训练时用；然而在实际中，数据是复制的所以掩码对于每次训练的句子来说不总是一样的。</p>
<p><strong>Next Sentence Prediction(NSP)</strong> </p>
<p>NSP 是关于预测在原始文本中的两个字符段是否是跟在一起的二分类损失。正阳样本通过从文本语料库中抽取连续的句子创建。负样本从不同文档中通过取成对片段来创建。正负样本都以同样的概率采样。</p>
<p>NSP目标被设计为用来提升在下游任务的表现，像自然语言推断NLI，需要推理出成对句子间的关系。</p>
<h5 id="2-4-Optimization"><a href="#2-4-Optimization" class="headerlink" title="2.4 Optimization"></a>2.4 Optimization</h5><p>BERT 用Adam来优化，参数如下<script type="math/tex">\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 1e-6</script>,其<script type="math/tex">L_2</script>权重衰减率为<script type="math/tex">0.01</script>. 学习率预热在前10,000步达到峰值<script type="math/tex">1e-4</script>，然后线性衰减。BERT以0.1的dropout在所有层和注意力权重，以及高斯误差线性单元GELU激活函数训练。模型以最大长度为T=512字符序列，mini-batches为<script type="math/tex">B=256</script>预训练<script type="math/tex">S = 1,000, 000</script>步迭代。</p>
<h5 id="2-5-Data"><a href="#2-5-Data" class="headerlink" title="2.5 Data"></a>2.5 Data</h5><p>BERT 在BOOKCorpus+英文WIKIPEDIA组合文本上训练，其包含总共16GB压缩文本。</p>
<h4 id="3-Experimental-Setup"><a href="#3-Experimental-Setup" class="headerlink" title="3. Experimental Setup"></a>3. Experimental Setup</h4><p>在本节中，我们描述我们的BERT复制研究实验设置。</p>
<h5 id="3-1-Implementation"><a href="#3-1-Implementation" class="headerlink" title="3.1 Implementation"></a>3.1 Implementation</h5><p>我们在 <a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq">FAIRSEQ</a> 上再现BERT.基本上遵循原始的在小节2 中给定的BERT优化参数，除了峰值学习率lr和预热步数，它们都在每个设置中分开地第微调。 我们额外发现训练对Adam epsilon项非常敏感，并且在一些具体情况中我们在微调它后获得更好的表现或稳定性。类似地，我们发现设置<script type="math/tex">\beta_2 = 0.98</script>在大的batch size上提升了稳定性。</p>
<p>我们用最大为T=512字符的序列进行预训练。不同于BERT 2019，我们没有随机插入短序列，并且我们不在前90%更新步数中减少序列长度。我们仅训练完整长度的序列。</p>
<p>我们用混合精度的浮点运算在DGX-1机器上训练，每台机器有8x32GB NVIDIA V100 GPUs，其通过无限带宽互连。</p>
<h5 id="3-2-Data"><a href="#3-2-Data" class="headerlink" title="3.2 Data"></a>3.2 Data</h5><p>BERT-style 预训练严重依赖大量的文本。Baevski[2019 Cloze-driven Pretraining of Self-attention Networks] [完型填空驱动的预训练自注意力网络, 用类似英语完型填空的方法，对Transformer模型进行预训练] 论文证明增加数据集能提升在最终任务的性能。一些努力已经在比原始BERT更大更多样性的数据集上尝试了。不幸的是, 不是所有额外的数据集都会发布。对于我们研究而言，我们关注于为实验尽可能收集更多数据，允许我们为每次比较匹配整体数据质量和数量。</p>
<p>我们考虑5个不同大小和领域的英语语言语料库，总共超过160GB压缩文本。我们使用如下文本语料库:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/soskek/bookcorpus">BOOKCORPUS</a> 加 英文维基百科。这就是原始BERT使用的数据(16GB)。</li>
<li>CC-News, 本论文收集于CommonCrawl News英文部分数据集。其包含63百万爬取与2016年9月到2019年2月的新闻文章 (76GB 在过滤后)。</li>
<li>OPENWEBTEXT,  一个开源的娱乐网页文本语料库。文本提取于最少3个赞的Reddit分享链接(38GB)。</li>
<li>STORIES， 一个包含CommonCrawl 的子类，其过滤后跟<strong>威诺格拉德模式</strong>故事风格类似的数据(31GB)。</li>
</ul>
<h5 id="3-3-Evalution"><a href="#3-3-Evalution" class="headerlink" title="3.3 Evalution"></a>3.3 Evalution</h5><p>遵循之前的工作，我们在下游任务中使用以下3个基准来评估RoBERTa。</p>
<p><strong>GLUE</strong> 通用语言理解评估基准，是9个为评估自然语言理解系统数据集集合。任务被设定为单句分类或者句子对分类。GLUE组织者提供训练集和验证集数据划分，以及一个提交服务器，允许参与者在留出测试数据上评估和比较的排行榜。</p>
<p>对于在小节4中的复制研究，我们报告结果是在对应单一任务训练数据微调预训练模型后的验证集结果。微调流程遵循原始BERT论文。</p>
<p>在小节5，我们额外地报告了源于公共排行榜测试集的结果。这些结果依赖针对几个特点任务的修改版，如在5.1小节中描述的。</p>
<p><strong>SQuAD</strong> 斯坦福QA数据集提供一段文本和一个问题。任务是通过从上下文中提取相关范围内容来回答这个问题。我们评估两个版本的SQuAD：V1.1 和V2.0. 在V1.1版本中上下文只是包含答案，然而V2.0一些问题就不在提供的上下文中，这使得该任务更有挑战性。</p>
<p>对于SQuAD V1.1 我们像BERT采用一样范围的预测方式。对于V2.0，我们添加额外二值分类器来预测该问题是否有答案，我们通过将分类和范围跨度损失项相加来联合训练。在评估阶段，只预测哪些分类是有答案的成对的范围跨度索引。</p>
<p><strong>RACE</strong> 阅读理解数据集，是大规模阅读理解数据集，包含超过28,000文章和将近100,000问题。数据集从中国英语考试中收集，其被设计用来作为初中和高中学生考试题。在阅读理解数据集中，每篇文章和多个问题联系在一起。对于每个问题，任务是从四个选择中选择一个正确的。RACE有大量的比其它流行的阅读理解数据集长的上下文本，并且需要推理的问题占比是非常大的。</p>
<h4 id="4-Training-Procedure-Analysis"><a href="#4-Training-Procedure-Analysis" class="headerlink" title="4. Training Procedure Analysis"></a>4. Training Procedure Analysis</h4><p>本节探索和量化哪些方案对于成功预训练BERT模型是重要。我们保持模型架构是固定的。特别地，我们开始用如<script type="math/tex">\text{BERT}_{BASE}, \ L=12, H=768, A=12, 110M  \ 参数</script>同样配置来训练BERT模型。</p>
<h5 id="4-1-Static-vs-Daynamic-Masking"><a href="#4-1-Static-vs-Daynamic-Masking" class="headerlink" title="4.1 Static vs. Daynamic Masking"></a>4.1 Static vs. Daynamic Masking</h5><p>如小节2中讨论的，BERT依赖随机掩码来预测tokens。原始BERT实现是在数据预处理时执行掩码一次，造成单一的静态掩码。为了避免在每epoch的每个训练实例上使用同样的掩码，训练数据被复制10份以便每个序列在超过40轮训练中是由10种不同方式掩码而成的。因此，每个训练序列在训练期间看起来是由4次相同的掩码构成的。</p>
<p>我们和动态掩码比较该策略，动态掩码是每次“喂”序列到模型是生成掩码模式。这在对多步预训练或者使用大数据时变得至关重要。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210716171452.png" alt="image-20210716165610591" style="zoom:30%;" /></p>
<p><strong>结论</strong> 如上表1，和发布的<script type="math/tex">\text{BERT}_{BASE}</script> 结果相比，我们用静态和动态掩码复现该结果。我们发现用静态掩码复现的表现类似于原始的BERT模型，但动态掩码轻微好于静态掩码。</p>
<p>给出动态掩码的结果和额外的效率好处，我们在剩下的实验中使用动态掩码。</p>
<h5 id="4-2-模型输入格式和下一句预测"><a href="#4-2-模型输入格式和下一句预测" class="headerlink" title="4.2 模型输入格式和下一句预测"></a>4.2 模型输入格式和下一句预测</h5><p>在原始的BERT预训练流程中，模型观测两个文档片段的连接，这两个片段要么是连续从同一文档中采样(p=0.5)，要么是从不同文档中采样。加上掩码语言模型目标，该模型训练通过一个辅助的下一句预测损失来预测是否被观察的文档片段来自于同一或不同文档。</p>
<p>NSP 损失在训练原始BERT模型被假设为一个非常重要的因子。Devlin 观察移除NSP任务会影响表现，在QNLI，MNLI，和SQuAD 1.1上有显著的表现退化。然而，近期一些工作质疑NSP 损失的必要性 [Cross-lingual Language Model Pretraining——Lample 2019]。</p>
<p>为了更好理解该差异，我们比较几种可替换训练格式：</p>
<ul>
<li><strong>SEGMENT-PAIR + NSP </strong>：这个遵循原始BERT使用格式，采用NSP loss. 每个都是成对的片段输入，这样每个输入都包含多个自然语句，但总长度必须小于512。</li>
<li><strong>SENTENCE-PAIR + NSP</strong> ： 每个输入包含一个自然语言句子对，这个句子对要么从一个文档部分中连续采样而来，要么就采样自不同文本。因为这些输入明显小于512字符长度，我们增加batch size以便字符总数保留类似于SEGMENT-PAIR + NSP。训练还是用NSP loss。</li>
<li><strong>FULL-SENTENCES</strong> : 每个输入由连续采样自同一文档或多个文档完整句子打包而成，另外总长度最大为512字符。这个输入可能跨文档。当我们到文档结尾时，我们开始从下一个文档中采样，并在两个文档间添加额外的分割符。训练移除NSP loss。</li>
<li><strong>DOC-SENTENCES</strong> : 输入构建类似于FULL-SENTENCES，处理其不跨文本采样。输入采样接近文档末尾还小于512字符，那么我们动态地在这些实例中增加batch size来达到和FULL-SENTENCES总的字符数目是相似的。训练移除NSP loss。</li>
</ul>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210716203254.png" alt="image-20210716203223556" style="zoom:30%;" /></p>
<p><strong>结论</strong>    如上表2所示4种不同设置的结果。我们开始拿原始的SEGMENT-PAIR 输入格式和SENTENCE-PAIR格式比较，都保留NSP loss，但最终都使用单一句子。发现<strong>使用单独句子让其在下游任务中表现变差</strong>，这使得我们假设是因为<strong>模型无法学到长距离依赖信息</strong>。</p>
<p>我们接下来比较没有NSP loss和用单一文档(DOC-SENTENCES) 形成的文本块训练。发现该设置表现优于原始发布的<script type="math/tex">\text{BERT}_{BASE}</script>结果并且<strong>移除NSP 任务不太影响下游任务表现</strong>，这是跟BERT [2019]对比。这种现象可能是原始BERT实现可能<strong>仅仅移除了损失项却仍然保留SENTENCE-PAIR输入格式</strong>。</p>
<p>最后我们发现限制输入序列来自于单一文档(DOC-SENTENCES)表现轻微好于从多个文档打包形成的输入(FULL-SENTENCES)。然而，因为这些DOC-SENTENCES格式结构是变化的batch sizes，为了更容易与相关工作比较，我们使用FULL-SENTENCES在剩下的实验中。</p>
<h5 id="4-3-Training-with-large-batches"><a href="#4-3-Training-with-large-batches" class="headerlink" title="4.3 Training with large batches"></a>4.3 Training with large batches</h5><p>在神经网络翻译的过去工作中，表明用<strong>非常大的小批次mini-batches能提升优化速度和最终任务的表现</strong>，当然学习率要恰当地增长。近期工作表明BERT也服从大的batch 训练方式。</p>
<p><script type="math/tex">\text{BERT}_{BASE}</script> 原始训练1M 步， 每批为256的序列。在计算成本上相当于，通过梯度累积用每批为2K的序列训练125K 步或每批为8K的序列训练31K步。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210716210005.png" alt="image-20210716205954879" style="zoom:30%;" /></p>
<p>如上表3所示， 我们跟<script type="math/tex">\text{BERT}_{BASE}</script>比较困惑度和最终任务表现， 随着每批大小增加，就是控制流入的训练数据数目。观察到<strong>用大批次训练会提升掩码语言模型目标的困惑度，也会提升最终任务的准确率</strong>。大批次数据也更容易用分布式数据系统来并行化训练，在接下来的实验中，我们用每批8K的序列来训练。</p>
<p>尤其是 You [Reducing bert pre-training time from 3 days to 76 minutes] 用更大的批次数据训练BETR，达到32K序列。我们留给将来工作探索大批次训练的限制。</p>
<h5 id="4-4-Text-Encoding"><a href="#4-4-Text-Encoding" class="headerlink" title="4.4 Text Encoding"></a>4.4 Text Encoding</h5><p>Byte-Pair Encoding(BPE) 字节对编码是介于字符级和单词级的混合表示，可以处理大规模自然语言语料库中的常见的词汇。BPE依赖subwords子词单元而不是完整单词，子词单元可以用统计分析训练语料库得到。</p>
<p>BPE词汇表大小通常在10k-100K子词单元。然而，对大规模和多样语料库建模时，unicode字符占了大部分，例如 Radford 2019<a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2    Language Models are Unsupervised Multitask Learners</a> 考虑了其工作原因。GPT-2 引入了一个简单BPE实现，使用直接代替unicode 字符作为基本子词单元。使用自己使得其可能学到不太大(50 K units) 子词词汇表，其仍然能在不引入然后“未知”字符条件下编码任意输入文本。</p>
<p>原始BERT实现使用一个字符级别的30K的BPE词汇表，其是在用启发式字符化规则预处理输入后学习得到的。根据Radford 2019 GPT-2中，考虑用字节级别50K子词单元的BPE词汇表训练BERT，而不是用任何额外的预处理或字符化输入。对应着<script type="math/tex">\text{BERT}_{BASE}</script>和<script type="math/tex">\text{BERT}_{LARGE}</script>会增加大约15M和20M额外训练参数。</p>
<blockquote>
<p>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。<br>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</p>
<p>​                                                                                                        ——<a target="_blank" rel="noopener" href="https://blog.csdn.net/ljp1919/article/details/100666563">RoBERTa 笔记</a></p>
</blockquote>
<p>早期实验显示这些编码直接只有轻微差异，在Radford 2019 GPT-2 BPE的一些任务上得到更长的最终任务表现甚至稍差。尽管如此，我们相信统一编码方案的优势超过性能轻微下降。更多这些编码的细节比较将留给未来工作。</p>
<h4 id="5-RoBERTa"><a href="#5-RoBERTa" class="headerlink" title="5. RoBERTa"></a>5. RoBERTa</h4><p>在之前的小结中我们提出对BERT预训练流程进行修改来提升最终任务表现。现在合计这些改进和评估它们共同的影响。我们把这些配置叫做RoBERTa，即Robustly optimized BERT approach 强壮的BERT优化方法。特别地，RoBERTa用动态掩码(4.1节)训练， 没有NSP loss的FULL-SENTENCES 输入(4.2节)，大的mini-batches(4.3节)以及大规模的字节级编码BPE(4.4节)。</p>
<p>另外地，我们研究两个其它重要的因素，在之前工作中没有被强调的：</p>
<ol>
<li>用于预训练的数据</li>
<li>训练步数</li>
</ol>
<p>例如，近期提出的XLNet架构，预训练数据接近原始BERT的10倍。其用8倍的batch size， 优化步数减半训练，因此看起来是BERT4倍的预训练序列。</p>
<p>为了从其他模型方案中(如， 预训练目标)帮助理清这些因素的重要性，开始时使用<script type="math/tex">\text{BERT}_{LARGE}</script>架构的配置L=24， H=1024,A=16， 355M参数来训练RoBERTa。在一个类似的BOOKCORPUS+WIKIPEDIA数据集预训练100K步，这也用于BERT Devlin 2019 .整个预训练使用1024 块V100 GPUs 将近一天。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210717001058.png" alt="image-20210717001045090" style="zoom:25%;" /></p>
<p><strong>结论</strong> 如上表4所示， 当控制训练数据时，观察到RoBERTa相比原始报告<script type="math/tex">\text{BERT}_{LARGE}</script>的有大的提升，再次肯定了我们在小节4中提到的设计方案的重要性。</p>
<p>接着，联合3个小节3.2中附加的数据集和BOOK + WIKI数据。并用跟之前一样训练步数（100K）在联合数据上训练RoBERTa。总共预训练数据文本超过160GB。进一步观察其在所有下游任务的性能提升，验证数据大小和多样性在预训练中的重要性。</p>
<p>最后， 预训练RoBERTa 步数显著变长，预训练步数从100K到300K，最后进一步到500K。我们再次观察在下游任务上获得显著性能提升，在大部分任务上300K到500K步训练的模型性能优于<script type="math/tex">\text{XLNet}_{LARGE}</script>.注意到更长时间训练模型没有在我们的数据上出现过拟合，而从额外训练中获益。</p>
<p>在剩余的本文中，在3个不同基准上评估最好的RoBERTa模型，分别是GLUE， SQuAD和RACE。特别地，我们考虑在小节3.2中介绍的5个数据集上训练500K步。</p>
<h5 id="5-1-GLUE-Results"><a href="#5-1-GLUE-Results" class="headerlink" title="5.1 GLUE Results"></a>5.1 GLUE Results</h5><p>对于GLUE，我们考虑两个微调设置。第一个设置(单一任务， 验证)，对每个GLUE任务分别微调RoBERTa，这仅使用对应任务的训练数据。我们考虑限制超参数扫描每个任务，如batch size 为{16， 32}， lr为{1e-5, 2e-5, 3e-5}, 在开始的6%不是线性预热接着线性衰减到0.微调10轮，并基于每个任务验证集评估标准执行早停。剩下的超参数在预训练时保持不变。在该设置中，在每个任务上的5个随机初始化模型在验证集上结果的中位数作为报告，并且没有模型集成。</p>
<p>第二个设置(集成， test)， 通过GLUE排行榜和其它方法在测试集上比较RoBERTa。然而许多排行榜上结果依赖多任务微调，我们提交<strong>只依赖与单一任务微调</strong>。对于RTE，STS以及MRPC，发现其对微调帮助其余MNLI单一任务模型，而不是预训练RoBERTa的基线。我们探索一个稍宽的超参数空间，如附录，并在每个任务上组合5到7个模型。</p>
<p><strong>具体任务修改</strong>  两个GLUE任务需要特定任务的微调方法来达到有竞争性的排行榜结果。</p>
<p><strong>QNLI</strong> (Qusetion-answering NLI，问答自然语言推断)： 对于QNLI任务最近提交在GLUE排行榜采用成对的答案计算排名，就是候选答案挖掘于训练集并和另一个比较，一个单一的(问题， 候选答案)对被分类为正例。该计算公式明显地简化了盖伦肉，但不是直接对比于BERT。遵循最近工作，我们对测试提交采用该排名方法，但为了直接和BERT比较，报告中验证集结果基于纯粹的分类任务。</p>
<p><strong>WNLI</strong> (Winograd NLI，Winograd自然语言推断): 我们发现提供的NLI-format数据很难处理，反而我们使用从Super GLUE [Wang 2019 A stickier benchmark for general-purpose language understanding systems]中重新格式化后的WNLI数据，其表示查询词和所指对象的范围。我们使用边缘排名损失来微调RoBERTa,来自于[Kocijan 2019 A Surprisingly Robust Trick for the Winograd Schema Challenge]。我们使用spaCy来提取来自句子中的额外的候选名词词组并微调我们模型让其对正向相关词组比生成的任意的负向候选词组分配高分数。该计算方式不好的结果是我们只能利用正训练样本，这只包括超过一半提供的训练样本。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210717035407.png" alt="image-20210717035356897" style="zoom:25%;" /></p>
<p><strong>结论</strong>    如上表5所示，第一个设置(单一任务， 验证集)，RoBERTa在GLUE的9项任务中用验证集取得了最佳成绩。至关重要的是，RoBERTa使用跟<script type="math/tex">\text{BERT}_{LARGE}</script>一样的掩码语言建模预训练目标和架构，但始终优于<script type="math/tex">\text{BERT}_{LARGE}</script>和<script type="math/tex">\text{XLNet}_{LARGE}</script>.这引发了有关模型架构和预训练目标，和更平凡的细节如在本文中的研究的数据大小，训练时间哪个更重要的质疑。</p>
<p>第二个设置(集成， 测试集)， 我们将RoBERTa提交给GLUE排行榜，并取得了9个任务中4个最佳的成绩及迄今为止的最高平均分。这十分振奋人心，因为RoBERTa不依赖多任务的微调，不像大部分其它的高分提交。我们期望未来工作通过包含更多先决的多任务微调流程更进一步提升这些成绩。</p>
<h5 id="5-2-SQuAD-Results"><a href="#5-2-SQuAD-Results" class="headerlink" title="5.2 SQuAD Results"></a>5.2 SQuAD Results</h5><p>我们相比之前工作对SQuAD采用更简单的方法。特别是，BERT和XLNet通过额外的QA数据集来增加它们的训练数据，我们<strong>只用提供的SQuAD数据来微调RoBERTa</strong>。Yang 2019 还采用自定义的逐层计划学习率来微调XLNet，而我们在所有层使用同样的学习率。</p>
<p>对于SQuAD v1.1我们跟BERT使用同样的微调方案。对于V2.0， 我们添加给定一个问题是否可回答的分类任务；通过对该分类和预测范围的损失相加来联合训练分类器和范围预测。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210717042722.png" alt="image-20210717042714019" style="zoom:25%;" /></p>
<p><strong>结论</strong>    如上表6所示，在SQuAD v1.1 的验证集上，RoBERTa跟XLNet结果差不多。在V2.0验证集上，RoBERTa取得新的最佳成绩，比XLNet提升0.4个点(EM) 和 0.6个点 (F1)。</p>
<p>我们也提交RoBERTa到公共的SQuAD 2.0 排行榜，来评估其相对其它系统的表现。大部分顶层系统要么构建于BERT要么XLNet，两种都依赖于额外训练数据。相反，本文提交不使用任何额外的数据。</p>
<p>单一RoBERTa模型优于其它单一模型的提交，并且是这些模型之间不依赖数据增强的最高分。</p>
<h5 id="5-3-RACE-Results"><a href="#5-3-RACE-Results" class="headerlink" title="5.3 RACE Results"></a>5.3 RACE Results</h5><p>在RACE中，系统提供了一篇文章、一个相关问题和四个候选答案。系统需要区分四个候选答案哪个是正确的。</p>
<p>我们为该任务修改RoBERTa，将每个候选答案与其对应的问题和文章连接在一起。然后编码这四个序列和传递结果的[CLS]表示，流过全连接层，其用来预测正确答案。我们截断那些长于128个字符的问题-答案对，如果需要，截断文章使得总长度最长为512字符。</p>
<p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210717045602.png" alt="image-20210717045600233" style="zoom:25%;" /></p>
<p>结果如上表7，在RACE测试集上，RoBERTa在初中组和高中组都取得了最佳成绩。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>我们仔细评估了一些在预训练BERT模型时的设计方案。发现性能能大大地提升方案：</p>
<ol>
<li>训练更长时间</li>
<li>在更多数据上使用大的batch size</li>
<li>移除NSP目标</li>
<li>训练更长的序列</li>
<li>使用训练数据动态改变掩码模型</li>
</ol>
<p>本文的提升预训练方案，称作RoBERTa，在GLUE，RACE和SQuAD上取得了最佳成绩。</p>
<ul>
<li>在GLUE上没有多任务微调</li>
<li>在SQuAD上没有使用额外数据</li>
</ul>
<p>这些结果证明了之前整体设计方案的重要性和近期提出的可替代方案中建议BERT的预训练目标保留是有竞争力的。</p>
<p>添加的数据有一个<strong>小说数据集，CC-News</strong>。<a target="_blank" rel="noopener" href="https://github.com/pytorch/fairseq">预训练和微调的模型和代码地址</a>.</p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a target="_blank" rel="noopener" href="https://github.com/brightmart/roberta_zh">中文预训练RoBERTa模型</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/ljp1919/article/details/100666563">RoBERTa 笔记</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143064748">RoBERTa论文详解和代码实战</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">miller</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://aigonna.com/2020/12/19/NLP%20Paper%20%2010.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">http://aigonna.com/2020/12/19/NLP%20Paper%20%2010.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://aigonna.com" target="_blank">aigonna</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RoBERTa/">RoBERTa</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a><a class="post-meta__tags" href="/tags/PTMs/">PTMs</a><a class="post-meta__tags" href="/tags/BPE/">BPE</a></div><div class="post_share"><div class="social-share" data-image="/img/imgs/16.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/23/NLP%20Paper%2011.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><img class="prev-cover" src="/img/imgs/11.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">11. Reformer 论文翻译笔记</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/14/CS224N%20Lecture%207%20Machine%20Translation,%20Attention,%20Subword%20Models/"><img class="next-cover" src="/img/imgs/10.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CS224N Lecture 7 Machine Translation, Attention, Subword Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/11/29/NLP Paper 6. BERT 论文笔记/" title="6. BERT 论文笔记"><img class="cover" src="/img/imgs/16.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-29</div><div class="title">6. BERT 论文笔记</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-RoBERTa-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><span class="toc-text">10. RoBERTa 论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Background"><span class="toc-text">2.  Background</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-Setup"><span class="toc-text">2.1 Setup</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-Architecture"><span class="toc-text">2.2 Architecture</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-Training-Objectives"><span class="toc-text">2.3 Training Objectives</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-Optimization"><span class="toc-text">2.4 Optimization</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-5-Data"><span class="toc-text">2.5 Data</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Experimental-Setup"><span class="toc-text">3. Experimental Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-Implementation"><span class="toc-text">3.1 Implementation</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-Data"><span class="toc-text">3.2 Data</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-Evalution"><span class="toc-text">3.3 Evalution</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Training-Procedure-Analysis"><span class="toc-text">4. Training Procedure Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-Static-vs-Daynamic-Masking"><span class="toc-text">4.1 Static vs. Daynamic Masking</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F%E5%92%8C%E4%B8%8B%E4%B8%80%E5%8F%A5%E9%A2%84%E6%B5%8B"><span class="toc-text">4.2 模型输入格式和下一句预测</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-3-Training-with-large-batches"><span class="toc-text">4.3 Training with large batches</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-Text-Encoding"><span class="toc-text">4.4 Text Encoding</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-RoBERTa"><span class="toc-text">5. RoBERTa</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-GLUE-Results"><span class="toc-text">5.1 GLUE Results</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-2-SQuAD-Results"><span class="toc-text">5.2 SQuAD Results</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-3-RACE-Results"><span class="toc-text">5.3 RACE Results</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference"><span class="toc-text">Inference</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021&nbsp;<i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;"class="fas fa-heart"></i> By miller</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      //tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'forest',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: '5sVUTO1MTpgoo7pDynh6zYEM-MdYXbMMI',
      appKey: 'vPUvOT1iP6YqcPPtYSqf8F7A',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script><h4>AIgonna</h4></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script></div></body></html>