<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>16. Reformer 论文翻译笔记 | aigonna</title><meta name="keywords" content="Reformer,LSH,RevNet,Hash"><meta name="author" content="aigonna"><meta name="copyright" content="aigonna"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="16. Reformer 论文翻译笔记机构：Google Research 、U.C. Berkeley        作者：Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya        论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2001.04451.pdf        收录会议：ICLR2020        论文代码：https:&#x2F;&#x2F;git">
<meta property="og:type" content="article">
<meta property="og:title" content="16. Reformer 论文翻译笔记">
<meta property="og:url" content="http://aigonna.com/2020/12/23/NLP%20Paper%2016.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="aigonna">
<meta property="og:description" content="16. Reformer 论文翻译笔记机构：Google Research 、U.C. Berkeley        作者：Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya        论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2001.04451.pdf        收录会议：ICLR2020        论文代码：https:&#x2F;&#x2F;git">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://aigonna.com/img/imgs/3.jpg">
<meta property="article:published_time" content="2020-12-23T13:49:39.000Z">
<meta property="article:modified_time" content="2022-05-01T12:34:54.157Z">
<meta property="article:author" content="aigonna">
<meta property="article:tag" content="Reformer">
<meta property="article:tag" content="LSH">
<meta property="article:tag" content="RevNet">
<meta property="article:tag" content="Hash">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://aigonna.com/img/imgs/3.jpg"><link rel="shortcut icon" href="/img/AI.png"><link rel="canonical" href="http://aigonna.com/2020/12/23/NLP%20Paper%2016.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-RMGH8E0YEQ"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RMGH8E0YEQ');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-01 20:34:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><link rel="stylesheet" href="/css/bg.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/AI.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">207</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/imgs/3.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">aigonna</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">16. Reformer 论文翻译笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2020-12-23T13:49:39.000Z" title="undefined 2020-12-23 21:49:39">2020-12-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-paper/">NLP paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="16-Reformer-论文翻译笔记"><a href="#16-Reformer-论文翻译笔记" class="headerlink" title="16. Reformer 论文翻译笔记"></a>16. Reformer 论文翻译笔记</h3><p>机构：Google Research 、U.C. Berkeley<br>        作者：Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya<br>        论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.04451.pdf">https://arxiv.org/pdf/2001.04451.pdf</a><br>        收录会议：ICLR2020<br>        论文代码：<a target="_blank" rel="noopener" href="https://github.com/google/trax/tree/master/trax/models/reformer">https://github.com/google/trax/tree/master/trax/models/reformer</a></p>
<p>Reformer， 主要是对Transformer的计算复杂度和内存进行优化，最关键的两个点是:</p>
<ul>
<li>局部敏感哈希</li>
<li>可逆的残差连接</li>
</ul>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011955895.png" alt="image-20210804203759229" style="zoom: 33%;" /></p>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>大型Transformer模型常常在大量任务上取得最佳成绩，但是训练这些模型是非常昂贵的。本文引入两种计算来提升Transformer的效率。</p>
<ul>
<li>第一，用局部敏感哈希代替点乘式的attention，使其空间复杂度从<script type="math/tex">O(L^2)</script>降低到<script type="math/tex">O(L\log L)</script>​，其中L是文本序列的长度。</li>
<li>此外，用可逆的残差连接层代理标准残差, 这使得训练过程中只需要存储移除激活值， <script type="math/tex">N</script>是层数。</li>
</ul>
<p>最终结果表明Reformer性能与Transformer相当，同时在长序列上更具有高效内存和更快。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h4><p>在大型的Transformer中每层网络参数超过了0.5B，网络层数上升到64层如<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1808.04444">Al-Rfou 2018 Character-level language modeling with deeper self-attention</a>论文中。另外，Transformer模型还增大序列长度，在单一样例中，上升到1.1万文本字符长度。这导致Transformer模型只能在大型的工业研究实验室中训练，并且其并行化训练让其甚至无法在但GPU上微调。</p>
<p>作者假设一个5亿参数的Transformer层，因为是float，每个参数占4byte, 那么<script type="math/tex">5 \times 4 \times10^8 = 2 \times 10^9 \text{bytes}</script>, 转化为<script type="math/tex">2 \times 10^9 / 1024 (KB)/1024(MB)/1024(GB)  \approx 2GB</script>​​​。</p>
<p>对于64K字符的激活值，如果嵌入层尺寸为1024，batch size为8​，总共需要64K x 1K x 8 = 0.5B floats。又是2GB左右内存。</p>
<p>如果作者的内存使用仅仅是一层上述的网络的话，作者甚至能在单一GPU上轻松训练序列长度为64K的大型Transformer模型。此外，整个用来训练BERT的语料也只需要17GB存储内存。那么，为什么作者甚至无法在单一机器上微调模型呢?</p>
<p>因为上面只估计了一层网络的内存和输入激活层的内存消耗，并没有考虑以下Transformer中主要的内存资源占用。</p>
<ul>
<li>N层模型的内存占用是单层的N倍大，因为实际上在反向传播过程中需要存储每层的激活值。</li>
<li>因为中间的前馈层大小<script type="math/tex">d_{ff}</script>​是远远大于注意力激活层<script type="math/tex">d_{model}</script>​​的​，它会占用大部分内存。</li>
<li>长度为<script type="math/tex">L</script>的序列注意力计算的时间和空间复杂度都是<script type="math/tex">O(L^2 )</script>​，甚至单一长度为64K的字符序列​就能耗尽GPU内存。</li>
</ul>
<p>引入Reformer模型，用以下技术能解决这些问题:</p>
<ul>
<li>可逆层，首次介绍是Gomez 2017，能只存储整个模型的一个激活值的复制，这样N倍问题消失了</li>
<li>在前馈层分开激活和分块处理，消除了<script type="math/tex">d_{ff}</script>因素影响，降低了内存占用。</li>
<li>基于局部敏感哈希的近似注意力计算，让计算复杂度从<script type="math/tex">O(L^2 )</script>降为<script type="math/tex">O(L\log L )</script>​​,这样就能处理长序列了​</li>
</ul>
<p>跟标准Transformer相比，训练流程影响微不足道。分开激活只在实现上有影响，其数值上还等于以前Transformer一样。使用可逆的残差连接代替标准的不更笨模型，但在作者实验的所有配置上有轻微影响。最重要的，注意力中的局部敏感哈希是最大的改变，这能影响训练变化，依赖于使用共存哈希的数目。</p>
<p>作者实验在人工任务上，文本任务(enwik8), 使用长为64K的序列；和图像生成任务(Imagenet-64 生成)，使用长为12K的训练。结果表明Reformer结果跟Transformer差不多，尤其是文本任务，有数量级级别的更高效内存效率。</p>
<h4 id="2-局部敏感哈希注意力"><a href="#2-局部敏感哈希注意力" class="headerlink" title="2. 局部敏感哈希注意力"></a>2. 局部敏感哈希注意力</h4><p><strong>Dot-product attention</strong>  Transformer中叫做放缩点乘注意力。输入是维度为<script type="math/tex">d_k</script>​的queries查询值和keys键值向量，以及维度<script type="math/tex">d_v</script>​的value值向量。所有的query和keys点乘，除以<script type="math/tex">\sqrt{d_k}</script>​，然后通过softmax函数获取values的权重。实际上，用如下公式进行矩阵计算:</p>
<script type="math/tex; mode=display">
\text{Attention } (Q, K, V) = \text{softmax } (\frac{QK^T}{\sqrt d_k}) V \tag{1}</script><p><strong>多头注意力</strong>  在Transformer中，采用h次不同的线性投影queries， keys和values 来代替一个<script type="math/tex">d_{model}</script>​维度的keys， values和queries注意力函数，对应地学习得到线性投影的<script type="math/tex">d_k, d_k, d_v</script>​​注意力。注意力被并行地用于这些queries, keys和values投影子空间，得到<script type="math/tex">d_v</script>​维度的输出。它们被拼接起来，再次投影得到最终结果values。这就是多头注意力。</p>
<p><strong>高效内存注意力 </strong> 为了计算注意力机制的内存使用，作者把注意力集中于公式1的计算。假设Q, K, V的大小为[batch_size, length, d_model]。</p>
<p>主要的问题在于<script type="math/tex">QK^T</script>项，其大小为[batch_size, length, d_model。在实验部分，作者在序列长度为64K上训练模型，在该示例中，即使batch size为1，这个64K x 64K的矩阵，32位float也要16GB内存空间。这是不明智的并阻碍了在长序列上使用Transformer。值得注意的是<script type="math/tex">QK^T</script>矩阵不需要在内存中完全实现。而是可以对每个query <script type="math/tex">q_i</script>​​​​​​分开地计算注意力，这样只要在内存中计算公式1​一次，然后当需要梯度时在反向传播中再计算一次。这种计算注意的方式可能低效，但是使用内存只跟length成正比。在实验部分，作者使用高效的注意力实现来跑完整的注意力并将其作为基线。</p>
<p><strong>Q, K, V来自哪？</strong></p>
<p>前面描述的多头注意力，就是在keys, queries和values上操作，但是作者只给一个激活值A的tensor，其形状为[batch_size, length, d_model​​​​​]——来自于将句子里的字符token嵌入到向量中。为了从A中构建Q, K, 和V, Transformer用3个不同参数的线性层将A投影成Q，K和V。（就是实现多头注意力中的3个线性层如下：）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">		</span><br><span class="line">    	self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>) </span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">==================================================================    </span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  </span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  </span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>对于局部敏感哈希注意力，作者希望queries和keys相同(Q和K)。这很容易实现，就是使用同样的层从A投影到Q和K，另外使用不同的投影层得到V。作者调用该模型就像共享-QK的Transformer.在第5小节实验中，作者证明共享QK不影响表现，甚至作者加上用K的长度归一化项也没有影响。</p>
<p><strong>哈希注意力</strong>  对于局部敏感哈希注意力，作者开始用两个tensor，Q=K和V，其形状是[batch_size, length, d_model]。作者保持多头注意力机制完整和集中于公式1中注意力计算。正如已提到的，主要的问题在<script type="math/tex">QK^T</script>项,其大小为[batch_size, length, d_model]。但要注意的是作者实际上只对<script type="math/tex">\text{softmax }QK^T</script>。<strong>因为softmax取决于值最大的部分，那么只要集中于K中与每个<script type="math/tex">q_i</script>中最近的部分就可以了</strong>。(两个向量点乘最大值)。例如，如果K长度为64K，对于每个<script type="math/tex">q_i</script>只要考虑其中一个小的子集，就是说，只需要32或者64个最近的keys.这就更高效了，但是作者如何找到其中最近的keys呢?</p>
<p><strong>局部敏感哈希(LSH)</strong>  在高维空间中快速地寻找最近的点可以用局部敏感哈希LSH。如果按邻近的向量能以高概率获得相同的哈希值，远的则不能的哈希方案分配每个向量<script type="math/tex">x</script>给哈希函数<script type="math/tex">h(x)</script>​​​​，这就叫局部敏感。作者实例中，实际只要邻近向量以高概率得到相同的哈希值，并且哈希桶也以高概率具有相同大小。</p>
<p>作者用如图1所示的随机投影方法，得到b的哈希值，作者首先固定一个随机矩阵R，大小为[<script type="math/tex">d_k, b/2</script>​],来得到b。然后定义<script type="math/tex">h(x) = \text{argmax }\ ( [xR; -xR])</script>​,其中[u; v]表示拼接两个向量。该方法被称为LSH方案，其很容易实现并应用于批次的向量。</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956090.png" alt="image-20210803153858209" style="zoom:35%;" /></p>
<p>如下图所示，图来自于Inference 1.就是key用不同的随机投影会得到一个值，将这些值组合起来就是对应的哈希桶，这只要保证投影时相近的点投影后在哈希桶是一样的就可以了。</p>
<p><img src="https://miro.medium.com/max/1392/1*fN4ck7Jd0gDilFeAZhowpA.gif" alt="img" style="zoom:60%;" /></p>
<p><strong>LSH注意力</strong>  了解了本文的LSH方案和通常的哈希注意力想法后，现在开始形式化本文的LSH注意力。首先，作者重写标准注意力的公式1，对于一个第i个query有:</p>
<script type="math/tex; mode=display">
o_i = \sum_{j \in \mathcal{P}_i} \exp (q_i \cdot k_j -z(i, \mathcal{P}_i))v_j \quad \quad \text{其中} \mathcal{P}_i = \{j:i\ge j\} \tag{2}</script><p>这里，引入<script type="math/tex">\mathcal{P}_i</script>表示位置i处的query，z表示配分函数(就像softmax的归一化项)。为了表达清晰，忽略放缩项<script type="math/tex">\sqrt{d}_k</script>。</p>
<p>出于批次目的，作者通常在更大的<script type="math/tex">\widetilde{\mathcal{P}}_{i}=\{0,1, \ldots, l\} \supseteq \mathcal{P}_{i}</script>集合上用注意力，同时掩码不在<script type="math/tex">\mathcal{P}_i</script>​中的元素:</p>
<script type="math/tex; mode=display">
o_i = \sum_{j \in \mathcal{P}_i} \exp (q_i \cdot k_j -m(j, \mathcal{P}_i)-z(i, \mathcal{P}_i))v_j \quad \text{其中} m\left(j, \mathcal{P}_{i}\right)=\left\{\begin{array}{ll}
\infty & \text { if } j \notin \mathcal{P}_{i} \\
0 & \text { otherwise }
\end{array}\right. \tag{3}</script><p>上面公式的意思是，对于不能attention的位置，<script type="math/tex">m(j, \mathcal{P}_i)</script>为正无穷，那么<script type="math/tex">q_i \cdot k_j</script>​是正无穷，最后还是0。</p>
<p>现在作者转向LSH attention，query中位置i所能够注意到的集合<script type="math/tex">\mathcal{P}_i</script>​​​​被限制到一个哈希桶中。</p>
<script type="math/tex; mode=display">
\mathcal{P}_i = \{ j: h(q_i) = h(k_j)\} \tag{4}</script><p>如下图2(a-b)部分是完整的attention和哈希变种attention的对比简图。（a）部分描述完整的注意力矩阵通常是稀疏的，但在计算上稀疏是没有优势的。(b)部分中,queries和keys已经按照它们的哈希桶排序了。因为相似项会以高概率掉入同样的哈希桶中，完整的attention模式可以通过只让注意力在每个桶内来近似。</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011957364.png" alt="image-20210803221257158" style="zoom:30%;" /></p>
<p>这个公式中哈希桶的大小往往不均匀，这使得跨桶批处理变得困难。并且，实际过程中桶内的queries和keys的数目也可能不一样，可能一个桶中含有大量queries但没有keys。</p>
<p>为了缓解这个问题，作者首先通过设置<script type="math/tex">k_j = \frac{q_j}{||q_j||}</script>​​来确保<script type="math/tex">h(k_j) = h(q_j)</script>​​。接下来，作者先按照桶号对桶排序，在每个桶内按照序列的位置排序，这得到一个新的排序后的序列<script type="math/tex">i \mapsto s_j</script>​​。</p>
<p>(如图d中序列 <script type="math/tex">[q_1, q_2, q_3, q_3,q_6,q_5]</script>​  到 <script type="math/tex">[q_1, q_2, q_4, q_3, q_6, q_5]</script>​​) 在这个排序的注意力矩阵中，一对来自同一桶中将聚类到对角线附近(如上图c部分).作者能遵循批量方法，将queries分词m块(排序后)，每块注意到自己和前一个块(如上图d)。按照之前记号，对应着下面设置:</p>
<script type="math/tex; mode=display">
\widetilde{\mathcal{P}}_{i}=\left\{j:\left\lfloor\frac{s_{i}}{m}\right\rfloor-1 \leq\left\lfloor\frac{s_{j}}{m}\right\rfloor \leq\left\lfloor\frac{s_{i}}{m}\right\rfloor\right\} \tag{5}</script><p>如果<script type="math/tex">\max_i |\mathcal{P}_i| \lt m</script>,那么<script type="math/tex">\mathcal{P}_{i}\supseteq \widetilde{\mathcal{P}}_{i}</script>.实际上，作者设置<script type="math/tex">m = \frac{2l}{n_{\text{ buckets}}}</script>（l是queries序列长度）。平均每个桶的大小为<script type="math/tex">\frac{l}{n_{\text{ buckets}}}</script>​​,并且假定一个桶增长到两倍大小的概率足够低。LSH注意力整体流程如上图2所示。</p>
<blockquote>
<p>总结来说，整个过程就如左半边图：</p>
<ul>
<li><p>首先作者令输入序列的queries = keys</p>
</li>
<li><p>然后作者对其做LSH bucketing，得到每个query和key都在各自的bucket中（不同颜色表示）</p>
</li>
<li><p>作者跟根据bucket对query进行排序，同个bucket中，按照query原本的position进行排序。</p>
</li>
<li><p>在之后作者对于每个排序后的新序列，进行chunk 拆分</p>
</li>
<li><p>最后作者对于每个query只管制自己以及自己之前的chunk，对于这些候选集中相同bucket的key进行attend。</p>
<p>​                                                                                                                                                                                                    ——<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/105123890">Reformer详解</a></p>
</li>
</ul>
</blockquote>
<p><strong>多轮局部敏感哈希注意力</strong> 用hash总会有很小的概率让相似的项掉落不同的桶中。这个概率可以用<script type="math/tex">n_{\text{rounds}}</script>轮不同的哈希函数<script type="math/tex">\{h^{(1)}, h^{(2)}, \cdots\}</script>​​来减小如：</p>
<script type="math/tex; mode=display">
\mathcal{P}_{i}=\bigcup_{r=1}^{n_{\text {rounds }}} \mathcal{P}_{i}^{(r)} \quad \text { where } \mathcal{P}_{i}^{(r)}=\left\{j: h^{(r)}\left(q_{i}\right)=h^{(r)}\left(q_{j}\right)\right\} \tag{6}</script><p>多轮实例本质上是并行执行LSH注意力<script type="math/tex">n_{\text{rounds}}</script>轮；具体细节流程描述在附录A。</p>
<p><strong>共享QK注意力的原因掩码</strong>  在Transformer解码器中，掩码(如公式3表示的<script type="math/tex">m(j, \mathcal{P}_i )</script>​​​)被用来阻止注意到未来的位置。为了实现LSH注意力,作者将每个query/key向量和一个位置索引相连，使用用于对query/key向量进行排序的相同排列再次对位置索引排序，然后用比较操作来计算掩码。</p>
<p>然而注意到未来是不允许的，通常Transformer实现方法是这样做，允许注意到自身。这种做法在共享-QK公式中是不行的因为query点乘自身，总是好于点乘其它的向量。因此修改掩码来禁止token注意到自身，除非token没有其它有效的注意目标(如序列中第一个token).</p>
<h5 id="2-1-综合任务分析"><a href="#2-1-综合任务分析" class="headerlink" title="2.1 综合任务分析"></a>2.1 综合任务分析</h5><p>为了验证LSH注意力的表现和研究其方法，作者开始用一个综合任务:复制一个符号序列。在该任务中，每一个训练和测试样本形如<script type="math/tex">0w0w 其中w \in \{1, \cdots, N \}^*</script>​，是一个从1到N的符号序列(实验中N=127).w长度为3的示例如下：</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956929.png" alt="image-20210804151253082" style="zoom:33%;" /></p>
<p>为了研究LSH注意力,作者在样本形如0w0w，其中w长度为511上训练(整个输入0w0w长为1024). 由于这是一个语言建模任务，作者总是在给定之前所有字符条件下预测下一个字符，但是作者掩码损失和准确了却只考虑了输入后半部分的位置，即那些实际可被预测到的位置。</p>
<p>上述任务能被一层Transformer模型完美解决(准确率100%, loss=0)。但需要注意的是，它需要非局部注意力查找(non-local attention lookups),因此依赖于有限跨度的的稀疏注意力模型都无法解决该问题。为了让其变得容易和快速训练但类似于NLP中使用的模型，作者使用一层Transformer: <script type="math/tex">d_{\text{model}}=d_{ff}=256</script>, 4个头。用4种不同设置训练模型150K步：完整的注意力， <script type="math/tex">n_{\text{rounds}}</script>​​分别为1， 2， 4的LSH注意力。</p>
<p>结果总结如下表2，完整注意力模型可以立即被LSH注意力模型使用，但准确率会损失一些。当从头开始训练LSH注意力，用4轮哈希的模型几乎达到完美的准确率。有趣的是，用8轮哈希来评估就是完美的。1轮或者2轮都会下降一些。<strong>模型在越少轮哈希上训练结果越差，但即使只用1轮哈希序列，用8轮哈希来评估也几乎完美。</strong></p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956492.png" alt="image-20210804154335045" style="zoom:33%;" /></p>
<h4 id="3-可逆的Transformer"><a href="#3-可逆的Transformer" class="headerlink" title="3. 可逆的Transformer"></a>3. 可逆的Transformer</h4><p>如上面部分所示，用一种近似的方法将注意力的复杂度从长度的平方降到线性，这是可以接受。但是如下表1清楚展示，每一栏都以<script type="math/tex">b \cdot n_h \cdot l</script>项开始: <script type="math/tex">b \cdot n_h \cdot l \cdot d_k</script>或者被<script type="math/tex">b  \cdot l \cdot d_{\text{model}}</script>替换，这种内存占用代价是不可避免的。真正地,在每层之前的激活函数已经是<script type="math/tex">b  \cdot l \cdot d_{\text{model}}</script>了， 所以内存占用上<script type="math/tex">n_l</script>层模型至少要<script type="math/tex">b  \cdot l \cdot d_{\text{model}} \cdot n_l</script>。甚至更差：Transformer内部的前馈层会达到<script type="math/tex">b  \cdot l \cdot d_{\text{ff}} \cdot n_l</script>。在大型的Transformer中通常设置<script type="math/tex">d_{ff}=4K, n_l=16, l=64K</script>​，不切实际地，这会达到16GB内存占用。</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956272.png" alt="image-20210804155653491" style="zoom:33%;" /></p>
<p>在这部分，作者将展示如何减少这种内存占用，首先用可逆层来处理<script type="math/tex">n_l</script>项部分，然后展示如何分块来处理<script type="math/tex">d_{ff}</script>​问题。每个方法在内存和时间复杂度上的效果总结和下表3。</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956935.png" alt="image-20210804162632125" style="zoom:33%;" /></p>
<p><strong>RevNets</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1707.04585.pdf">可逆残差网络</a>论文提出用于解决图片问题的ResNets中，如下图所示。</p>
<p><img src="https://aigonna.oss-cn-shenzhen.aliyuncs.com/blog/202205011956259.png" alt="image-20210804202919592" style="zoom:33%;" /></p>
<p>主要想法是每一层的activations可以根据下一层的activations推出来，这样就不需要存储中间的activations。普通的残差形式为<script type="math/tex">y = x + F(x)</script>，那么一对可逆层作用域一对输入有输出如:<script type="math/tex">(x_1, x_2) \mapsto (y_1, y_2)</script>，就有如下等式:</p>
<script type="math/tex; mode=display">
y_{1}=x_{1}+F\left(x_{2}\right) \quad \quad y_{2}=x_{2}+G\left(y_{1}\right) \tag{7}</script><p>那么可以推导得到:</p>
<script type="math/tex; mode=display">
x_2 = y_2 -G(y_1) \quad \quad x_1 = y_1 - F(x_2) \tag{8}</script><p><strong>可逆Transformer</strong>   应用RevNet想法于Transformer，原本sub-encoder block中的注意力层和前馈层是通过ResNet连接的，将其转换为RevNet。就是说把F变成注意力层，G变成前馈层。注意Layer Normalization 是移到残差块里面的:</p>
<script type="math/tex; mode=display">
Y_1 = X_1 + \text{Attention}(X_2) \quad Y_2 = X_2 + \text{FeedForward}(Y_1) \tag{9}</script><p>可逆Transformer不需要存储每层的激活值就去掉了<script type="math/tex">n_l</script>项。在第5部分，展示了其表现和同样参数的普通Transformer一样，就是让<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>都是<script type="math/tex">d_{model}</script>​​大小来实现。</p>
<p><strong>Chunking</strong>  尽管可逆性消掉了<script type="math/tex">n_l</script>项，变薄的网络层仍然要大量的内存。前馈层的维度会非常大如<script type="math/tex">d_{ff}=4K</script>甚至更大。但是，跨位置序列里FFN计算是完全独立的，完整的FFN计算可分成<script type="math/tex">c</script>块:</p>
<script type="math/tex; mode=display">
Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right] \tag{10}</script><p>这层通常通过对所有位置并行执行来批量化，但一次一块可以减少内存。可逆计算如公式8以及其反向传播也是分块的。除了前馈层，还对大词汇表的模型(超过<script type="math/tex">d_{model}</script>​单词类型)在输出时的对数概率计算分块，并一次性计算序列各部分的损失。</p>
<p><strong>Chunking， 大批次和参数复用</strong>  用分块和可逆层，作者在整个网络中activations占用内存与网络层数无关、尽管参数的数量随着层数的增加而增加，但参数并非如此。这个问题得到解决，因为作者可以在该层不计算时让CPU与内存交换该层的参数。在标准Transformer里，这是非常低效的因为内存转移到CPU中非常慢。然而，Reformer中batch size乘以长度非常大，因此用参数完成计算分摊了转换的成本。</p>
<p>后面部分就是些实验设置和结果了，就不翻译了。</p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0">Illustrating the Reformer</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/ljp1919/article/details/106061012">论文阅读笔记reformer</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/105123890">Reformer 详解</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">aigonna</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://aigonna.com/2020/12/23/NLP%20Paper%2016.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">http://aigonna.com/2020/12/23/NLP Paper 16. Reformer 论文笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://aigonna.com" target="_blank">aigonna</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Reformer/">Reformer</a><a class="post-meta__tags" href="/tags/LSH/">LSH</a><a class="post-meta__tags" href="/tags/RevNet/">RevNet</a><a class="post-meta__tags" href="/tags/Hash/">Hash</a></div><div class="post_share"><div class="social-share" data-image="/img/imgs/3.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/01/06/%E6%8A%80%E8%83%BD_1.%20%E5%AD%90%E5%9B%BE%E7%BB%98%E5%88%B6/"><img class="prev-cover" src="/img/imgs/14.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">1.子图绘制</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/19/NLP%20Paper%2015.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><img class="next-cover" src="/img/imgs/16.jpg" onerror="onerror=null;src='/img/imgs/0.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">15. RoBERTa 论文笔记</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81MDc2My8yNzI0NQ"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Reformer-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E7%AC%94%E8%AE%B0"><span class="toc-text">16. Reformer 论文翻译笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B1%80%E9%83%A8%E6%95%8F%E6%84%9F%E5%93%88%E5%B8%8C%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">2. 局部敏感哈希注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-%E7%BB%BC%E5%90%88%E4%BB%BB%E5%8A%A1%E5%88%86%E6%9E%90"><span class="toc-text">2.1 综合任务分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8F%AF%E9%80%86%E7%9A%84Transformer"><span class="toc-text">3. 可逆的Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference"><span class="toc-text">Inference</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022&nbsp;<i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;"class="fas fa-heart"></i> By aigonna</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      //tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'forest',
      })
      false && mermaid.init()
    })
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script><h4>AIgonna</h4></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script></div></body></html>