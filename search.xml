<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS224W  3. Node Embeddings</title>
      <link href="2022/03/03/CS224W_3.%20Node%20Embeddings/"/>
      <url>2022/03/03/CS224W_3.%20Node%20Embeddings/</url>
      
        <content type="html"><![CDATA[<h3 id="3-Node-Embeddings"><a href="#3-Node-Embeddings" class="headerlink" title="3. Node Embeddings"></a>3. Node Embeddings</h3><h4 id="1-Recap-Traditioal-ML-for-Graphs"><a href="#1-Recap-Traditioal-ML-for-Graphs" class="headerlink" title="1. Recap: Traditioal ML for Graphs"></a>1. Recap: Traditioal ML for Graphs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724755.png" alt="image-20220323151923339" style="zoom:25%;" /></p><p><strong>传统的图机器学习</strong>：</p><ul><li>给定一个<strong>输入图</strong>，<strong>抽取节点、边和图级别特征</strong>, 学习一个能<strong>将这些特征映射到标签的模型</strong>(像SVM， NN…)</li><li>这个抽取不同级别特征的过程，就是<strong>特征工程feature engineering</strong></li><li>学习到模型后拿来预测就是具体的<strong>下游预测任务</strong></li></ul><h5 id="1-Graph-representation-learning"><a href="#1-Graph-representation-learning" class="headerlink" title="1. Graph representation learning"></a>1. Graph representation learning</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724757.png" alt="image-20220323152545815" style="zoom:25%;" /></p><p><strong>图的表征学习</strong>：</p><p>目标：图机器学习的高效的<strong>任务无关特征</strong>学习。对于图，要抽取到任务无关的的特征，这样下游任务不同，也能用。</p><p>如上图，就是把节点u，拿函数<script type="math/tex">f: u \to \mathbb{R}^d</script>， 映射成向量v，<script type="math/tex">\mathbf{v} \in \mathbb{R}^d</script>. 这就是<strong>特征表示</strong>或者更具体地说<strong>embedding</strong>.</p><h5 id="2-Why-Embedding？"><a href="#2-Why-Embedding？" class="headerlink" title="2. Why Embedding？"></a>2. Why Embedding？</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724758.png" alt="image-20220323154010440" style="zoom:25%;" /></p><ul><li><strong>任务：映射节点到一个embedding 空间</strong><ul><li>节点间embeddings的相似性表示它们在网络中的相似性。如节点彼此接近(被同一条边连接)，其embedding相似度要高。</li><li>编码网络信息</li><li>有潜力为许多下游任务做预测</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724759.png" alt="image-20220323161300277" style="zoom:25%;" /></p><p><strong>Node embedding 例子</strong>：</p><p>下图是<a href="https://arxiv.org/pdf/1403.6652.pdf"> DeepWalk: Online Learning of Social Representations</a>原图。Karate Club数据集在2D的投影，图中不同颜色的节点在embedding后也是相距比较远的，但同一颜色都是比较近的。(甚至有一种一一对应的映射感觉, 可能效果比较好)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724760.png" alt="image-20220323160938534" style="zoom:25%;" /></p><h4 id="2-Node-embedding-Encoder-and-Decoder"><a href="#2-Node-embedding-Encoder-and-Decoder" class="headerlink" title="2. Node embedding: Encoder and Decoder"></a>2. Node embedding: Encoder and Decoder</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724761.png" alt="image-20220323162445828" style="zoom:25%;" /></p><p><strong>设置</strong></p><ul><li>假定有一个图G:<ul><li><script type="math/tex">\mathbf{V}</script>是顶点集</li><li><script type="math/tex">\mathbf{A}</script>是邻接矩阵(假定是二值化的)</li><li>为了简单起见，不使用节点特征和额外的信息</li></ul></li></ul><h5 id="1-Embedding-Nodes"><a href="#1-Embedding-Nodes" class="headerlink" title="1. Embedding Nodes"></a>1. Embedding Nodes</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724762.png" alt="image-20220323223431397" style="zoom:25%;" /></p><p>目标是编码节点，以便embedding space中的相似度近似于图中的相似的。</p><p>如上图中，原始的网络中的邻居节点u、v ，编码到embedding空间后，对应的<script type="math/tex">z_\mathbf{u}, z_\mathbf{v}</script>也要相近。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724763.png" alt="image-20220323224409924" style="zoom:25%;" /></p><p>节点的相似度计算：</p><script type="math/tex; mode=display">\text{similarity}(u, v) \approx \mathbf{z}_v^T \mathbf{z}_u \tag{1}</script><p>其中, <script type="math/tex">\text{similarity}</script>就是接下来要讲的相似度计算函数，也是这里<script type="math/tex">\text{ENC}</script>表示的编码过程。</p><h5 id="2-Learning-Node-Embeddings"><a href="#2-Learning-Node-Embeddings" class="headerlink" title="2. Learning Node Embeddings"></a>2. Learning Node Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724764.png" alt="image-20220324123920503" style="zoom:25%;" /></p><ol><li>Encoder 将节点映射到embeddings</li><li>定义节点相似度函数(就是一个衡量原始网络中节点相似度的函数，如余弦相似度)</li><li>Decoder 记为DEC将embeddings映射为相似度分数</li><li>优化encoder参数使得式1成立。</li></ol><h5 id="3-Two-Key-Components"><a href="#3-Two-Key-Components" class="headerlink" title="3. Two Key Components"></a>3. Two Key Components</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724765.png" alt="image-20220324132755170" style="zoom:25%;" /></p><ul><li><strong>Encoder</strong>: 映射每个节点到低维度向量。</li></ul><script type="math/tex; mode=display">\text{ENC}(v) = \mathbf{z}_v \tag{2}</script><ul><li>相似度函数: 明确编码后向量空间和原始网络是怎样的映射关系，如上图，就是原始网络中u和v的相似度和这两个节点embedding后的点积是对应的。</li></ul><h5 id="4-“Shallow”-Encoding"><a href="#4-“Shallow”-Encoding" class="headerlink" title="4. “Shallow” Encoding"></a>4. “Shallow” Encoding</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724766.png" alt="image-20220324171439630" style="zoom:25%;" /></p><p>最简单的编码方法: 编码器仅仅是一个embedding-lookup.</p><p>对于式2继续完善下:</p><script type="math/tex; mode=display">\text{ENC}(v) = \mathbf{z}_v = \mathbf{Z} \cdot v\tag{3}</script><p>其中,<script type="math/tex">\mathbf{Z} \in \mathbb{R}^{d\times |\mathcal{V|}}</script>, 这是一个<script type="math/tex">d \times |\mathcal{V}|</script>的矩阵，每一列都是一个节点的embedding[通过学习/优化得到的]。怎么得到后面再说，要明确的是， 这是由<script type="math/tex">d</script>个节点的<script type="math/tex">\mathcal{V}</script>维embedding构成的矩阵。</p><p>而<script type="math/tex">v \in \mathbb{I}^{|\mathcal{V}|}</script>是一个指示向量，除了对应节点<script type="math/tex">\mathbf{v}</script>那一列是1外其他都是0（one-hot向量）。这样从矩阵乘法来说，式3相乘的过程就是一个查表的过程。这也是为什么说这是一个embedding-lookup。下图图示就非常清楚了，注意一下这个矩阵<script type="math/tex">\mathbf{Z}</script>名字叫embedding matrix(NLP 同学相对很容易理解)。</p><p><strong>学习或优化得到embedding matrix</strong>的<strong>方法</strong>有: <strong>DeepWalk, node2vec</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724767.png" alt="image-20220324172927542" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724768.png" alt="image-20220324173143547" style="zoom:25%;" /></p><h5 id="5-Framework-Summary"><a href="#5-Framework-Summary" class="headerlink" title="5. Framework Summary"></a>5. Framework Summary</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724770.png" alt="image-20220324173626155" style="zoom:25%;" /></p><p><strong>Encoder+Decoder Framework</strong></p><ul><li>浅层的编码器: embedding lookup</li><li>参数优化: <script type="math/tex">\mathbf{Z}</script> 包含<script type="math/tex">u \in \mathbf{V}</script>即图中所有节点embedding  <script type="math/tex">\mathbf{z}_u</script>.</li><li>深层的编码器GNNs将会在Lecture 6讲到。</li></ul><ul><li>Decoder：基于节点相似度</li><li>目标：最大化相似节点对(u, v)的<script type="math/tex">\mathbf{z}_v^T \mathbf{z}_u</script>.</li></ul><h5 id="6-How-to-Define-Node-Similarity"><a href="#6-How-to-Define-Node-Similarity" class="headerlink" title="6. How to Define Node Similarity?"></a>6. How to Define Node Similarity?</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724771.png" alt="image-20220324175805592" style="zoom:25%;" /></p><ul><li>上述方法的关键点是怎么定义<strong>节点相似度</strong> ？</li><li>两个节点应该有相似的embedding如果它们…<ul><li>是邻接的</li><li>有共同邻居节点</li><li>有相似的结构特征</li></ul></li><li>接下来将用<strong>随机游走</strong>来学习获得节点相似度，和怎样为该相似度指标优化embedding。</li></ul><h5 id="7-Note-on-Node-Embeddings"><a href="#7-Note-on-Node-Embeddings" class="headerlink" title="7. Note on Node Embeddings"></a>7. Note on Node Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724772.png" alt="image-20220324180525403" style="zoom:25%;" /></p><ul><li>监督学习和非监督学习方法都可以用来学习节点的embedding<ul><li><strong>不利用</strong>节点标签</li><li><strong>不利用</strong>节点特征</li><li>目标是直接估计一组节点坐标(embedding)，以便保留网络某些结构</li></ul></li><li>embeddings都是任务无关的<ul><li>不为特定任务训练但可以在任何任务上使用</li></ul></li></ul><h4 id="3-Random-Walk-Approaches-for-Node-Embeddings"><a href="#3-Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="3. Random Walk Approaches for Node Embeddings"></a>3. Random Walk Approaches for Node Embeddings</h4><h5 id="1-Notation"><a href="#1-Notation" class="headerlink" title="1. Notation"></a>1. Notation</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724773.png" alt="image-20220324212650526" style="zoom:25%;" /></p><p><strong>记号</strong>：</p><ul><li>向量<script type="math/tex">\mathbf{z}_u</script>: 节点u的embedding (我们要学习到的)</li><li>概率<script type="math/tex">P(\mathcal{v}|\mathbf{z}_u)</script>: 基于<script type="math/tex">\mathbf{z}_u</script>的预测概率，即从节点u开始随机游走到节点v的概率</li></ul><p><strong>非线性函数用来生成预测概率</strong>：</p><ol><li><strong>Softmax函数</strong>：将K实数向量转化为总和为1的K概率值，Softmax函数分子就是将原值<script type="math/tex">\mathbf{z}_i</script>转化为e的幂次，分母表示归一化因子来确保和为1</li><li><strong>Sigmoid函数</strong>：S-shape函数将实数值转化为(0, 1)之间的值</li></ol><h5 id="2-Random-Walk"><a href="#2-Random-Walk" class="headerlink" title="2. Random Walk"></a>2. Random Walk</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724774.png" alt="image-20220325000320160" style="zoom:25%;" /></p><p>给定一个图和一个起点，我们可以随机选取一个邻居作为下一个游走的点，接下来我们继续选取下一个邻居游走，重复。这个访问的节点随机序列就是<strong>图的随机游走</strong>。</p><h5 id="3-Random-Walk-Embeddings"><a href="#3-Random-Walk-Embeddings" class="headerlink" title="3. Random-Walk Embeddings"></a>3. Random-Walk Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724775.png" alt="image-20220325001649371" style="zoom:25%;" /></p><p>这里<script type="math/tex">\mathbf{z}_u^T\mathbf{z}_v</script>表示<script type="math/tex">\mathbf{u}, \ \mathbf{v}</script>在图中一次随机游走时共同出现的概率。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724776.png" alt="image-20220325002437153" style="zoom:25%;" /></p><ol><li>估算使用策略R的随机游走从节点u访问到节点v的概率</li><li>这个概率就是节点u和v的相似度，我们可以根据这个概率来优化embedding，在embedding空间的相似度，这里<script type="math/tex">\text{dot product}=\cos(\theta)</script>,就是编码的随机游走相似度。</li></ol><h5 id="4-Why-Random-Walks"><a href="#4-Why-Random-Walks" class="headerlink" title="4. Why Random Walks?"></a>4. Why Random Walks?</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724777.png" alt="image-20220325003657831" style="zoom:25%;" /></p><ul><li><strong>可解释性</strong>：灵活的随机的节点相似度定义，随机游走每步的游走表示了节点的局部信息，游走路径序列表示了全局信息，这样包含了局部和高阶的邻域信息。<ul><li><strong>想法</strong>:如果以较高的概率随机从节点u访问节点v，那么u和v是相似的(高阶，多跳的信息)。</li></ul></li><li>高效性：不需要在训练时考虑所有节点对；仅仅只要考虑随机游走时的共现对，节省了计算量</li></ul><h5 id="5-Unsupervised-Feature-Learning"><a href="#5-Unsupervised-Feature-Learning" class="headerlink" title="5. Unsupervised Feature Learning"></a>5. Unsupervised Feature Learning</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724778.png" alt="image-20220325151321054" style="zoom:25%;" /></p><ul><li><strong>直觉</strong>: 找到节点在保留相似度的d维空间的embedding。</li><li><strong>想法</strong>:学习节点的embedding使得网络中邻近的节点的embedding是相近的，就是如果在网络中邻近的节点，embedding后相似度要高，这样就将图结构的网络空间信息embedding到embedding space。</li><li>给定节点u，怎么定义邻近节点？<ul><li><script type="math/tex">N_R(u)</script>表示某一随机游走策略R中获得的节点u的邻居节点。</li></ul></li></ul><h5 id="6-Feature-Learning-as-Optimization"><a href="#6-Feature-Learning-as-Optimization" class="headerlink" title="6. Feature Learning as Optimization"></a>6. Feature Learning as Optimization</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724779.png" alt="image-20220325155505388" style="zoom:25%;" /></p><p>如果设置优化问题？</p><ul><li>给定图<script type="math/tex">G=(V, E)</script></li><li>目标是学习一个映射<script type="math/tex">f: u \to \mathbb{R}^d \ : f(u)=\mathbf{z}_u</script></li><li>对数似然目标函数：</li></ul><script type="math/tex; mode=display">\max_{f} \sum_{u\in V} \log \ P(N_R(u)|\mathbf{Z}_u) \tag{3}</script><p>其中，<script type="math/tex">N_R(u)</script> 是某一策略R随机游走中节点u的邻居节点</p><p>解释下式3，对于节点u和其邻居节点，<script type="math/tex">P(N_R(u)|\mathbf{Z}_u)</script>表示给定节点u的情况下，让其邻居节点出现的概率最大。总的来说就是让图中所有节点，在映射函数f的作用下，使得每个节点u的邻居节点出现的概率的对数和最大。</p><ul><li>给定节点u，我们希望学习特征表示是对出现节点u随机游走邻域节点<script type="math/tex">N_R(u)</script>的预测。</li></ul><h5 id="7-Random-Walk-Optimization"><a href="#7-Random-Walk-Optimization" class="headerlink" title="7. Random Walk Optimization"></a>7. Random Walk Optimization</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724780.png" alt="image-20220325172104022" style="zoom:25%;" /></p><p><strong>怎么去做？</strong></p><ol><li>使用固定长度的游走策略R从图中每个节点u进行随机游走</li><li>对每个节点u记录其<script type="math/tex">N_R(u)</script>，就是从u出发随机游走得到的节点集合(这是个multiset，因为有重复元素,随机游走会重复访问多次)</li><li>按照给定节点u预测其邻域节点<script type="math/tex">N_R(u)</script></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724781.png" alt="image-20220325173006647" style="zoom:25%;" /></p><p>等价地，</p><script type="math/tex; mode=display">\mathcal{L} = \sum_{u \in V} \sum_{v \in N_R(u)} - \log (P(v|\mathbf{z}_u)) \tag{4}</script><p>这里把<script type="math/tex">N_R(u)</script>用v来表示了下。</p><ul><li><strong>直觉</strong>: 优化embeddings <script type="math/tex">\mathbf{z}_u</script>来最大随机游走共现的似然概率。</li><li>对<script type="math/tex">P(v|\mathbf{z}_u)</script>使用softmax有：</li></ul><script type="math/tex; mode=display">P(v|\mathbf{z}_u) = \frac{\exp(\mathbf{z}_u^T\mathbf{z}_v)}{\sum_{n\in V} \exp(\mathbf{z}_u^T\mathbf{z}_n)} \tag{5}</script><p>为什么使用softmax? 因为节点v的和节点u相似度最大的，经过<script type="math/tex">f(x) = e^x</script>处理后能更加区分出来。分母只是因为要归一化，概率和为1。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724782.png" alt="image-20220325174228884" style="zoom:25%;" /></p><p>将式4和式5<strong>结合起来</strong>：</p><script type="math/tex; mode=display">\mathcal{L} = \sum_{u \in V} \sum_{v \in N_R(u)} - \log (\frac{\exp(\mathbf{z}_u^T\mathbf{z}_v)}{\sum_{n\in V }  \exp(\mathbf{z}_u^T\mathbf{z}_n)}) \tag{6}</script><ol><li>第一个求和是对图中所有节点u，要整体最大不能是单一节点</li><li>第二个求和是对从u到v的所有随机游走中出现的节点v求和</li><li>最后黄色部分是预测在随机游走中u和v共现的概率</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724784.png" alt="image-20220325174939286" style="zoom:25%;" /></p><p>但实际做起来计算代价非常高，是图节点数的平方次复杂度。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724785.png" alt="image-20220325175101141" style="zoom:25%;" /></p><p>那么我们能不能优化式6呢？</p><p>分母的归一化部分，有<script type="math/tex">|V|</script>的复杂度，如果能优化就会极大降低计算复杂度。</p><h5 id="8-Negative-Sampling"><a href="#8-Negative-Sampling" class="headerlink" title="8.Negative Sampling"></a>8.Negative Sampling</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724786.png" alt="image-20220325180542372" style="zoom:25%;" /></p><p><strong>解决方法</strong>： 负采样。</p><p>这里将softmax替换为sigmoid, 近似得到:</p><script type="math/tex; mode=display">\log (\frac{\exp(\mathbf{z}_u^T\mathbf{z}_v)}{\sum_{n\in V }  \exp(\mathbf{z}_u^T\mathbf{z}_n)})\approx \log(\sigma(\mathbf{z}_u^T\mathbf{z}_v)) - \sum_{i=1}^k \log(\mathbf{z}_u^T\mathbf{z}_{n_i})),\  其中\ n_i \sim P_V\tag{7}</script><p>负采样就是将式7分母归一化部分中所有节点<script type="math/tex">n</script>替换为，随机采样得到的k个<strong>负样本</strong><script type="math/tex">n_i</script>，即不在random walk上的样本。</p><p>负采样使得对数部分计算非常迅速。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724787.png" alt="image-20220325212208362" style="zoom:25%;" /></p><ul><li>以节点度对应的概率来采样k个负样本节点</li><li>两个决定k的因素<ol><li>更高的k对应更可靠的估计值</li><li>更高的k对应着更多的负样本带来的更高的偏差</li></ol></li></ul><p>实际上,<script type="math/tex">k=5-20</script>。那么能不能采样任何节点获取负样本，或者只采样不在random walk上的节点？</p><p>答案是：实际操作中，对任何节点采样来获得负样本，高效性。正确的方式是只采样不在random wall上的节点。</p><h5 id="9-Stochastic-Gradient-Descent"><a href="#9-Stochastic-Gradient-Descent" class="headerlink" title="9. Stochastic Gradient Descent"></a>9. Stochastic Gradient Descent</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724788.png" alt="image-20220325220304225" style="zoom:25%;" /></p><ul><li>获得目标函数后，我们怎么最小化它呢？</li><li>梯度下降来最小化损失函数<script type="math/tex">\mathcal{L}</script>:<ul><li>在所有节点u中随机选取一个初始化，得到<script type="math/tex">\mathbf{z}_u</script></li><li>迭代直到收敛<ul><li>对所有节点u，计算导数<script type="math/tex">\frac{\partial \mathcal{L}}{\partial \mathbf{z}_u}</script></li><li>对所有节点u，计算以学习率为<script type="math/tex">\eta</script>更新迭代后的导数: <script type="math/tex">\mathbf{z}_u \leftarrow \mathbf{z}_u - \eta \frac{\partial \mathcal{L}}{\partial \mathcal{z}_u}</script></li></ul></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724789.png" alt="image-20220325223809441" style="zoom:25%;" /></p><p><strong>Stochastic Gradient Descent</strong> 随机梯度下降就是把所有节点替换为一批独立的训练样本节点。</p><h5 id="10-Random-Walks-Summary"><a href="#10-Random-Walks-Summary" class="headerlink" title="10. Random Walks: Summary"></a>10. Random Walks: Summary</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724790.png" alt="image-20220325224026963" style="zoom:25%;" /></p><ol><li>从图中每一个节点进行短径固定长度的随机游走</li><li>对每一个节点u记录其邻域节点<script type="math/tex">N_R(u)</script>， 这是一个从u开始随机游走得到节点的multiset，有重复节点</li><li>使用随机梯度下降算法来优化embedding，使得损失函数最小</li></ol><h5 id="11-How-should-we-randomly-walk？"><a href="#11-How-should-we-randomly-walk？" class="headerlink" title="11. How should we randomly walk？"></a>11. How should we randomly walk？</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724791.png" alt="image-20220325224527820" style="zoom:25%;" /></p><ul><li><p>前面已经讲了怎么在给定策略R的情况下优化embeddings</p></li><li><p>那么我们应该使用什么策略进行随机游走呢？最简单的想法：直接固定长度，每个节点都无偏的随机游走。</p><p>但问题是这样的策略使得相似度有非常大的局限性。</p></li><li><p>我们怎么将其泛化呢？</p></li></ul><h5 id="12-Node2Vec"><a href="#12-Node2Vec" class="headerlink" title="12. Node2Vec"></a>12. Node2Vec</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724792.png" alt="image-20220325225110352" style="zoom:25%;" /></p><ol><li><strong>Overview of node2vec</strong></li></ol><p>node2vec 来自于论文 <a href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf">node2vec: Scalable Feature Learning for Networks</a>，想详细了解可以阅读下。</p><ul><li>目标： 在特征空间嵌入相似性来表示邻近节点</li><li>将这个目标表示为最大似然优化问题，对下游预测任务独立。</li><li><p>关键点：节点u的邻域节点<script type="math/tex">N_R(u)</script>灵活性能丰富的节点embeddings</p></li><li><p>进一步的有偏的2阶(2阶就是走2步)随机游走R可以生成节点u的邻域节点<script type="math/tex">N_R(u)</script>的邻近节点</p></li></ul><ol><li><strong>node2vec: Bias Walks</strong></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724793.png" alt="image-20220325235431558" style="zoom:25%;" /></p><p><strong>想法</strong>：使用灵活，有偏的随机游走能平衡网络局部和全局信息。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724794.png" alt="image-20220326001102908" style="zoom:25%;" /></p><p>两种典型的策略来定义给定节点u的邻域节点<script type="math/tex">N_R(u)</script>:BFS 和DFS。</p><p>这是两种常见的图搜索算法，</p><ul><li><strong>BFS广度优先搜索</strong>: 以某节点为起点搜索其所有的邻居节点，局部视角</li><li>DFS<strong>深度优先搜索</strong>: 以某节点为起点直到搜索到终点为止，全局视角</li></ul><p>下图是二者比较：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261724795.png" alt="image-20220326002815491" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261657119.png" alt="202203260030604" style="zoom:25%;" /></p><p><strong>插入BFS和DFS</strong>：</p><p>有偏的固定长度随机游走R，在给定节点u的情况下能得到邻域节点<script type="math/tex">N_R(u)</script></p><ul><li>两个超参数<ol><li><strong>Return parameter p(返回参数)</strong>，用来控制是否返回以前的节点。</li><li><strong>In-out parameter q(出入参数)</strong>, 表示在起点附近游走inwards（类似BFS）和向更远处游走outwards（类似DFS）的比例，直觉来说就是BFS和DFS的比率。</li></ol></li></ul><h5 id="13-Biased-Random-Walks"><a href="#13-Biased-Random-Walks" class="headerlink" title="13. Biased Random Walks"></a>13. Biased Random Walks</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261955634.png" alt="image-20220326195545312" style="zoom:25%;" /></p><p><strong>有偏的2阶随机游走探索网络邻居节点：</strong></p><ul><li>假设通过边<script type="math/tex">(s_1, w)</script>到达w节点</li><li>那么现在w节点的邻居节点可能是<script type="math/tex">s1, s2, s_3</script>。这跟最开始的起点<script type="math/tex">s_1</script>的意义是不一样的，具体如上图所示。</li></ul><p><strong>想法</strong>：记住从哪来</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224422.png" alt="image-20220326203131058" style="zoom: 25%;" /></p><ul><li><p>通过边<script type="math/tex">(s_1, w)</script>走到w节点，下一步去哪？</p><p>这里，如上图所示，走到下一个节点的的概率是不一样的。(注意，<script type="math/tex">\frac{1}{p}, \frac{1}{q}, 1</script>不是归一化的概率，加起来和不为1)</p></li><li><p>p, q是 模型的转移概率。</p></li></ul><p>如上图所示，前一步通过边<script type="math/tex">(s_1, w)</script>到达节点w。现在走到的节点w，邻居节点有<script type="math/tex">s_1, s_2, s_3, s_4</script>。具体怎么设计p, q概率呢？</p><p>根据上一跳节点和下一跳节点的距离设计，说到距离，就要明确这两点在哪？上一跳节点为<script type="math/tex">s_1</script>，下一跳为<script type="math/tex">s_1, s_2, s_3, s_4</script>。</p><p>论文 <a href="https://arxiv.org/pdf/1607.00653.pdf">node2vec: Scalable Feature Learning for Networks</a> <strong>3.2.2 Search bias α</strong>部分提到：</p><script type="math/tex; mode=display">\alpha_{p q}(t, x)=\left\{\begin{array}{ll}\frac{1}{p} & \text { if } d_{t x}=0 \\1 & \text { if } d_{t x}=1 \\\frac{1}{q} & \text { if } d_{t x}=2\end{array}\right. \tag{7}</script><p>这里<script type="math/tex">t, x</script>分别表示上一跳节点，和未来要走到的节点。<script type="math/tex">d_{tx}</script>表示距离。</p><p>上图中，<script type="math/tex">s_1</script>到<script type="math/tex">s_1</script>的距离为0，所以为<script type="math/tex">\frac{1}{p}</script>。</p><p>还有可能跳到节点<script type="math/tex">s_2, s_2, s_3</script>到<script type="math/tex">s_1</script>的距离分别为1， 2， 2，就得到如图示的转移概率。这时的概率还没有归一化，最后要归一化。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224424.png" alt="image-20220326214838642" style="zoom:25%;" /></p><ul><li>类似BFS的游走：较小的p值，那么<script type="math/tex">\frac{1}{p}</script>就较大，会回到上一跳节点。</li><li>类似DFS的游走: 较小的q值，会往跟上一跳较远的距离跳</li></ul><h5 id="14-node2vec-algorithm"><a href="#14-node2vec-algorithm" class="headerlink" title="14. node2vec algorithm"></a>14. node2vec algorithm</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224425.png" alt="image-20220326215424958" style="zoom:25%;" /></p><ol><li>计算随机游走概率</li><li>从每个节点u开始模拟策略r距离为l的随机游走</li><li>用随机梯度下降优化node2vec 目标</li></ol><p>优点：线性时间复杂度(节点邻居树是固定的)，三步都是独立的可以并行化。</p><p>其它随机游走想法：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224426.png" alt="image-20220326220933815" style="zoom:25%;" /></p><h5 id="15-Summary-so-far"><a href="#15-Summary-so-far" class="headerlink" title="15. Summary so far"></a>15. Summary so far</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224427.png" alt="image-20220326221024967" style="zoom:25%;" /></p><ul><li>核心想法： 嵌入节点使得<strong>嵌入空间的距离</strong>反映<strong>原网络的节点相似度</strong></li><li>不同的节点相似度表示<ul><li>简单的： 如果两个节点相连就相似</li><li>邻居的重叠度(第二节谈到)</li><li>随机游走方法(本节内容)</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262224428.png" alt="image-20220326221609150" style="zoom:25%;" /></p><p>那我们应该用什么方法？</p><ul><li>没有一种方法是万能的，如node2vec在节点分类表现好，然而其他方法在边预测上好</li><li>随机游走方法通常比较高效</li><li>总的来说，要选择节点相似度符合你的任务。</li></ul><h4 id="4-Embedding-Entire-Graphs"><a href="#4-Embedding-Entire-Graphs" class="headerlink" title="4. Embedding Entire Graphs"></a>4. Embedding Entire Graphs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262233638.png" alt="image-20220326223331824" style="zoom:25%;" /></p><ul><li>目标：想要把一个子图或者一张图嵌入到特征空间去，得到图的embedding:<script type="math/tex">\mathbf{z}_G</script></li><li>具体任务：<ul><li>对有毒和无毒分子分类</li><li>鉴别异常的图</li></ul></li></ul><h5 id="1-方法1"><a href="#1-方法1" class="headerlink" title="1. 方法1"></a>1. 方法1</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262259730.png" alt="image-20220326225911614" style="zoom:25%;" /></p><p><strong>简单但有效的方法1</strong>：</p><ol><li>对子图和整张图进行标准的node embedding</li><li>然后对图(或子图)中所有节点的embedding直接求和平均：</li></ol><script type="math/tex; mode=display">\mathbf{z}_G = \sum_{v \in G} \mathbf{z}_v \tag{8}</script><h5 id="2-方法2"><a href="#2-方法2" class="headerlink" title="2. 方法2"></a>2. 方法2</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262323420.png" alt="image-20220326232345958" style="zoom:25%;" /></p><p>方法2：引入一个”虚拟节点“来表示子图，然后再对其用node embedding。</p><h5 id="3-方法3：-Anonymous-Walk-Embeddings"><a href="#3-方法3：-Anonymous-Walk-Embeddings" class="headerlink" title="3. 方法3： Anonymous Walk Embeddings"></a>3. 方法3： Anonymous Walk Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262326285.png" alt="image-20220326232624070" style="zoom:25%;" /></p><p>来源于 论文<a href="https://arxiv.org/pdf/1805.11921.pdf">Anonymous Walk Embeddings</a>。</p><blockquote><p>匿名游走的节点如果在先前出现过，则将此时的index设置为它第一次出现时的index，习惯性从1开始计数，每当遇到一个新的没有遇到过的节点，则自增state（index），即state的最大值等于这一条walk中存在的unique的节点数目。</p><p>​                                                                                                                                                        —— <a href="https://blog.csdn.net/pku_langzi/article/details/121797407">Machine Learning with Graphs 之 Anonymous Walk Embeddings</a></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262336202.png" alt="image-20220326233623495" style="zoom:25%;" /></p><p>对于例子：Random walk <script type="math/tex">w_1</script>整条walk的序列为<script type="math/tex">1 \to 2 \to 3 \to 2\to 3</script>.</p><p>而Random walk <script type="math/tex">w_2</script>的序列为<script type="math/tex">1\to 2\to 3\to 2\to 3</script>。它们有一样的anonymous walk.</p><h5 id="4-Number-of-Walks-Grows"><a href="#4-Number-of-Walks-Grows" class="headerlink" title="4. Number of Walks Grows"></a>4. Number of Walks Grows</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203262351544.png" alt="image-20220326235135712" style="zoom:25%;" /></p><p>对于一个给定的图G，匿名游走得到的pattern数目跟路径长度l成指数成长。</p><p>路径长度为3的匿名游走pattern有5个。</p><h5 id="5-Simple-Use-of-Anonymous-Walks"><a href="#5-Simple-Use-of-Anonymous-Walks" class="headerlink" title="5. Simple Use of Anonymous Walks"></a>5. Simple Use of Anonymous Walks</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203270016445.png" alt="image-20220327001544263" style="zoom:25%;" /></p><p>那么我们怎么利用匿名游走来嵌入整张图呢？</p><ul><li>记录所有的长度l的匿名游走<script type="math/tex">w_i</script></li><li>将图表示为这些匿名游走的分布</li></ul><p>具体来说，就像上节中的Graphlet Degree Vector(GDV)，我们先统计得到所有长度为l的匿名游走pattern，然后将图表示为获得pattern的频数或者概率。</p><p>举例来说,</p><ol><li>设定l=3</li><li>将图表示为5维的向量<ul><li>因为长度为3的匿名游走有5种pattern</li><li><script type="math/tex">\mathbf{z}_G[i]=</script> 图中出现匿名游走pattern i的概率.</li></ul></li></ol><h5 id="6-Sampling-Anonymous-Walks"><a href="#6-Sampling-Anonymous-Walks" class="headerlink" title="6. Sampling Anonymous Walks"></a>6. Sampling Anonymous Walks</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203270031605.png" alt="image-20220327003147367" style="zoom:25%;" /></p><p>当l=12时，匿名游走w数目有4M。实际操作中，我们用采样来降低计算复杂度。</p><ul><li>独立地采样m条<strong>随机游走</strong>路径来近似<strong>匿名游走</strong>pattern</li><li>用这些采样得到的随机游走路径的概率分布来表示图</li></ul><p>那么我们的m取多大？可用下式来估算：</p><script type="math/tex; mode=display">m=\left[\frac{2}{\varepsilon^{2}}\left(\log \left(2^{\eta}-2\right)-\log (\delta)\right)\right] \tag{9}</script><p>其中，<script type="math/tex">\eta</script>是长度为<script type="math/tex">l</script>的匿名游走的总数。计算下上图例子，</p><p>当<script type="math/tex">\epsilon = 0.1, \delta=0.01</script>时,  <script type="math/tex">m = 200 \times 612.5=122,500</script></p><h5 id="7-New-idea-Learn-Walk-Embeddings"><a href="#7-New-idea-Learn-Walk-Embeddings" class="headerlink" title="7. New idea: Learn Walk Embeddings"></a>7. New idea: Learn Walk Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203270053332.png" alt="image-20220327005327087" style="zoom:25%;" /></p><p>不简单地用每次游走出现的次数的分数来表示匿名游走<script type="math/tex">w_i</script>,而是学习匿名游走<script type="math/tex">w_i</script>的embedding <script type="math/tex">\mathbf{z}</script>。</p><p>具体来说，(这跟NLP的doc2vec类似，后面再说)</p><ul><li>就是将所有的匿名游走的embedding <script type="math/tex">\mathbf{z}_i</script>加上图的embedding <script type="math/tex">\mathbf{z}_G</script>一起输入学习，其中<script type="math/tex">\mathbf{Z}={\mathbf{z}_i: i=1 \cdots\eta}</script>。</li></ul><p>这里，<script type="math/tex">\eta</script> 是采样的匿名游走数目。</p><p><strong>那么怎么embed walks？</strong></p><ul><li><strong>想法</strong>：跟node2vec类似，通过预测下一跳周围的walk来学习embed walks。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271156925.png" alt="image-20220327115103172" style="zoom:25%;" /></p><p><strong>实际流程如下</strong>：</p><ul><li>输出：也就是预测，输入图的embeding <script type="math/tex">\mathbf{z}_G</script> 。这个<script type="math/tex">\mathbf{z}_G</script> 也是要学习的目标。</li><li>从节点1开始，采样获得匿名随机游走，如上图所示<script type="math/tex">w_1, w_2, w_3, w_4</script>。</li><li>学习预测 walks 共现在<script type="math/tex">\Delta-\text{size}</script>窗口内的概率，比如，当窗口为2时，给定<script type="math/tex">w_1, w_2</script>去预测<script type="math/tex">w_3</script></li></ul><p>这样整个目标函数为：</p><script type="math/tex; mode=display">\max_{\mathbf{z}_G } \sum_{t=\Delta+1}^T \log P(w_t| w_{t-\Delta}, \cdots, w_{t-1}, \mathbf{z}_G) \tag{10}</script><p>其中，<script type="math/tex">T</script>是所有的walks数目。</p><p>对于式10，</p><ul><li>对数里面，表示给定<strong>图</strong>的embedding <script type="math/tex">\mathbf{z}_G</script> 和一定范围窗口内的<strong>采样匿名随机游走</strong>情况下，下一个walk出现的概率。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271226264.png" alt="image-20220327122634050" style="zoom:25%;" /></p><p>每个步骤具体来说，</p><ul><li>从节点u获取长度为l的T条不同的随机walks</li><li>学习预测将会出现在共现<script type="math/tex">\Delta-\text{size}</script>窗口内的walks</li><li>评估匿名游走<script type="math/tex">w_i</script>的embedding <script type="math/tex">\mathbf{z}_i</script>。</li></ul><p>目标函数为：</p><script type="math/tex; mode=display">\max _{\mathbf{z}_{i}, \mathbf{z}_{\boldsymbol{G}}} \frac{1}{T} \sum_{t=\Delta}^{T} \log P\left(w_{t} \mid\left\{w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{\boldsymbol{G}}\right\}\right) \tag{11}</script><p>这里，<script type="math/tex">\mathbf{z}_{i}</script>是负采样得到所有匿名游走的embedding，<script type="math/tex">\mathbf{z}_{\boldsymbol{G}}</script>是图的embedding.</p><script type="math/tex; mode=display">P\left(w_{t} \mid\left\{w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{\boldsymbol{G}}\right\}\right) =\frac{\exp(y(w_t))}{\sum^{\Delta}_{i=1}\exp(y(w_i))} \tag{12}</script><p>式12，表示当前walk t共现的可能性，分母是归一化因子，为了确保概率和为1。</p><p>其中<script type="math/tex">y(w_t)</script>计算如下：</p><script type="math/tex; mode=display">y(w_t) = b+U \cdot (\text{cat}(\frac{1}{\Delta}\sum_{i=1}^\Delta \mathbf{z}_i, \mathbf{z}_\boldsymbol{G})) \tag{13}</script><p>其中，<script type="math/tex">\text{cat}(\frac{1}{\Delta}\sum_{i=1}^\Delta \mathbf{z}_i, \mathbf{z}_\boldsymbol{G})</script>表示：</p><ul><li>将所有匿名walk的embedding加起来再求平均，从<script type="math/tex">t-\Delta</script>到<script type="math/tex">t-1</script>总共<script type="math/tex">\Delta</script>个embeddings，就除以<script type="math/tex">\Delta</script></li><li>然后将平均的embeddings和图的embedding <script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>拼接起来</li></ul><p>拼接后的embeddings一起输入到一个线性投影层，这就是整个式13的意义。</p><p>这里，<script type="math/tex">b \in \mathbb{R}, U\in \mathbb{R}^D</script>是可学习的参数。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271348844.png" alt="image-20220327133811721" style="zoom:25%;" /></p><p><strong>Optimization：</strong></p><ul><li>通过优化可以学习到图的embedding <script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>表示<ul><li><script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>是不是简单地对walk embedding <script type="math/tex">\mathbf{z}_i</script>求和？<script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>是不是下一个<script type="math/tex">\mathbf{z}_i</script>的残差？</li><li>论文中，<script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>是一个独立优化的向量参数，跟<script type="math/tex">\mathbf{z}_i</script>一样</li></ul></li><li>用<script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>来做预测，如图分类。<ul><li>选择1： 内积和<script type="math/tex">\mathbf{z}_\boldsymbol{G_1}^T\mathbf{z}_\boldsymbol{G_2}</script>(小节2中提到的)</li><li>选择2：把<script type="math/tex">\mathbf{z}_\boldsymbol{G}</script>输入到神经网络来对图分类</li></ul></li></ul><p>接下来看看Anonymous Walk Embeddings 跟 doc2vec(来自 <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Distributed Representations of Sentences and Documents</a>)类似之处。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271350355.png" alt="image-20220327135053181" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271351796.png" alt="image-20220327135156697" style="zoom:35%;" /></p><p>比较两幅整体结构图，</p><ul><li>Graph类似paragraph或document</li><li>Anonymous walk类似于 word</li></ul><p>更多展开分析就不进行了(如公式)，只是说下很多研究都是借鉴前人或其它方向的思想。</p><h5 id="8-Summary"><a href="#8-Summary" class="headerlink" title="8. Summary"></a>8. Summary</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271358803.png" alt="image-20220327135800593" style="zoom:25%;" /></p><p>总结：</p><p>3种graph embedding方法：</p><ol><li>方法一：简单粗暴直接embed node求和平均它们</li><li>方法2：创建super-node映射子图，然后embed 这个super-node</li><li>方法3： Anonymous Walk Embeddings</li></ol><h5 id="9-Preview-Hierarchical-Embeddings"><a href="#9-Preview-Hierarchical-Embeddings" class="headerlink" title="9. Preview:Hierarchical Embeddings"></a>9. Preview:Hierarchical Embeddings</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271402456.png" alt="image-20220327140224220" style="zoom:25%;" /></p><ul><li>Lecture 8 将学习更先进的方法来获取graph embeddings</li><li>在图中使用层次聚类节点，然后对这些聚类进行求和/平均得到节点embeddings</li></ul><h5 id="10-How-to-Use-Embeddings"><a href="#10-How-to-Use-Embeddings" class="headerlink" title="10. How to Use Embeddings?"></a>10. How to Use Embeddings?</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271406612.png" alt="image-20220327140607370" style="zoom:25%;" /></p><p>怎么使用节点的embeddings <script type="math/tex">\mathbf{z}_i</script>：</p><ul><li>聚类/社区发现: 聚类<script type="math/tex">\mathbf{z}_i</script>点</li><li>节点分类：基于<script type="math/tex">\mathbf{z}_i</script>预测节点i的标签</li><li>边预测： 基于<script type="math/tex">(\mathbf{z}_i, \mathbf{z}_j)</script>预测边<script type="math/tex">(i, j)</script>, 可以通过平均，平均，哈达玛积(Hadamard), 求embeddings的差值</li><li>图预测：通过聚合节点embeddings或匿名随机游走获得的图embedding <script type="math/tex">\mathbf{z}_G</script>来预测图的标签</li></ul><h4 id="本节总结"><a href="#本节总结" class="headerlink" title="本节总结"></a>本节总结</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/pic/202203271407273.png" alt="image-20220327140726056" style="zoom:25%;" /></p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://wandb.ai/syllogismos/machine-learning-with-graphs/reports/7-Graph-Representation-Learning--VmlldzozNzcwMDk">7.Graph Representation Learning</a></p><p>[2] <a href="https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node-representation-learning">Node Representation Learning</a></p><p>[3] <a href="http://snap.stanford.edu/proj/embeddings-www/files/nrltutorial-part1-embeddings.pdf">Embeddings</a></p><p>[4] <a href="https://chhzh123.github.io/blogs/2020-02-06-graph-embedding/">图表示学习（1）- 图嵌入</a></p><p>[5] <a href="https://leovan.me/cn/2020/04/graph-embedding-and-gnn/">图嵌入 (Graph Embedding) 和图神经网络 (Graph Neural Network)</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224W </tag>
            
            <tag> Node embedding </tag>
            
            <tag> DeepWalk </tag>
            
            <tag> node2vec </tag>
            
            <tag> Random Walk </tag>
            
            <tag> Random-Walk Embeddings </tag>
            
            <tag> Negative Sampling </tag>
            
            <tag> Biased Random Walks </tag>
            
            <tag> Anonymous Walk Embeddings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224W 2. Traditional Methods for ML on Graphs</title>
      <link href="2022/02/27/CS224W_2.%20Traditional%20Methods%20for%20ML%20on%20Graphs/"/>
      <url>2022/02/27/CS224W_2.%20Traditional%20Methods%20for%20ML%20on%20Graphs/</url>
      
        <content type="html"><![CDATA[<h3 id="2-Traditional-Methods-for-ML-on-Graphs"><a href="#2-Traditional-Methods-for-ML-on-Graphs" class="headerlink" title="2. Traditional Methods for ML on Graphs"></a>2. Traditional Methods for ML on Graphs</h3><h4 id="1-机器学习任务"><a href="#1-机器学习任务" class="headerlink" title="1. 机器学习任务"></a>1. 机器学习任务</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720356.png" alt="image-20220317152027098" style="zoom:25%;" /></p><p>传统机器学习在图网络中的任务分为:</p><ol><li>节点级别预测</li><li>边级别预测</li><li>图级别预测</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720358.png" alt="image-20220317152323487" style="zoom:25%;" /></p><p>传统机器学习中的Pipeline:</p><ul><li>设计节点/边/图相应特征</li><li>从训练数据中获取特征</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720359.png" alt="image-20220317152528358" style="zoom:25%;" /></p><p>具体来说，就是用：</p><ul><li>随机森林</li><li>SVM</li><li>NN等等</li></ul><p>模型在给定新的节点/边/图的情况下用获得的特征来做预测。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720360.png" alt="image-20220317152808839" style="zoom:25%;" /></p><p>在图上使用有效特征是模型取得好的表现的关键。传统ML pipeline使用手工设计的特征。本节，我们会回顾传统特征用来:</p><ul><li>节点级别预测</li><li>边级别预测</li><li>图级别预测</li></ul><p>简单来说，本节聚焦于无向图。</p><h4 id="2-图机器学习"><a href="#2-图机器学习" class="headerlink" title="2. 图机器学习"></a>2. 图机器学习</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720361.png" alt="image-20220317154234084" style="zoom:25%;" /></p><p>目标是:对一系列对象做预测。</p><p>设计方案:</p><ul><li>特征: d维的向量</li><li>对象: 节点、边、一系列节点、整张图</li><li>对象函数: 我们要解决什么任务?</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720362.png" alt="image-20220317154600149" style="zoom:25%;" /></p><p>例如，节点级别预测，</p><ul><li>给定<script type="math/tex">G = (V, E)</script></li><li>学习一个函数<script type="math/tex">f: V \to \mathbb{R}</script></li></ul><p>这样我们的问题就是怎样学到这个函数了,接下来会逐步阐述这个问题。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720363.png" alt="image-20220317154943092" style="zoom:25%;" /></p><p>例如上面例子中，绿色的度为2，红色为1，那么我们就可以拿节点的度来做分类，这就是一个节点分类任务。这样我们可以看出<strong>机器学习需要特征</strong>。接下来我们看看到底有哪些节点特征?</p><h4 id="3-节点级别特征概述"><a href="#3-节点级别特征概述" class="headerlink" title="3.节点级别特征概述"></a>3.节点级别特征概述</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720364.png" alt="image-20220317155203008" style="zoom:25%;" /></p><p>目标:特征化结构与节点在网络中的位置,</p><ul><li>节点的度</li><li>节点的中心性</li><li>聚类系数</li><li><strong>graphlets</strong>，不同构子图。</li></ul><h5 id="1-节点特征-节点中心性-centrality"><a href="#1-节点特征-节点中心性-centrality" class="headerlink" title="1. 节点特征: 节点中心性 centrality"></a>1. 节点特征: 节点中心性 centrality</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720365.png" alt="image-20220317160611241" style="zoom:25%;" /></p><ul><li>节点度不能衡量邻近节点的<strong>重要性</strong>。</li><li>节点中心性<script type="math/tex">c_v</script>：考虑节点在图中的<strong>重要性</strong>。</li></ul><p>例如，A微博粉丝100个，B微博粉丝30个，但B微博粉丝有10个大V，A微博没有。我们考虑节点重要性，在微博粉丝关系图谱中，B节点要重要些。</p><p>建模重要性的不同衡量指标:</p><ul><li>特征值中心性 Eigenvector centrality</li><li>中介中心性 Betweenness centrality</li><li>接近中心性 Closeness centrality</li></ul><p>注: <a href="https://web.stanford.edu/~jacksonm/netbook.pdf">Social and Economical Networks</a> 中第二章Centrality部分更详细地介绍了中心性。</p><h5 id="2-特征值中心性-Eigenvector-centrality"><a href="#2-特征值中心性-Eigenvector-centrality" class="headerlink" title="2. 特征值中心性 Eigenvector centrality"></a>2. 特征值中心性 Eigenvector centrality</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720367.png" alt="image-20220317175244195" style="zoom:25%;" /></p><p><strong>Eigenvector centrality</strong> :</p><ul><li><p>节点<script type="math/tex">v</script>的中心性由其周围的邻近节点<script type="math/tex">u</script>的重要性决定<script type="math/tex">u \in N(v)</script>。</p></li><li><p>对节点<script type="math/tex">v</script>的中心性，就是对周围邻近节点的中心性求和，即</p><script type="math/tex; mode=display">c_v = \frac{1}{\lambda} \sum_{u \in N(v)} c_u \tag{1}</script><p>式1是用递归的方法构建公式对中心性衡量。那么怎么求解呢?</p><p>注: <script type="math/tex">\lambda</script>是归一化因子。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720368.png" alt="image-20220317181332150" style="zoom:25%;" /></p><p>改写式1易得:</p><script type="math/tex; mode=display">\lambda \mathbf{c} = \mathbf{A}\mathbf{c} \tag{2}</script><p>其中：</p><ul><li><script type="math/tex">A</script>是邻接矩阵, 表示节点之间的邻居关系</li><li><script type="math/tex">\mathbf{c}</script>是中心性向量</li><li><script type="math/tex">\lambda</script>是特征值</li></ul><p>式2表明中心性<script type="math/tex">\mathbf{c}</script>就是<script type="math/tex">\mathbf{A}</script>的特征向量，特征值最大<script type="math/tex">\lambda_{max}</script>总是正的并且唯一，最大的特征向量<script type="math/tex">\mathbf{c}_{max}</script>对应最大的特征值<script type="math/tex">\lambda_{max}</script>，这个<script type="math/tex">\lambda</script>作为特征中心性来使用。</p><blockquote><p>若一个<script type="math/tex">n \times n</script>的矩阵各个元素非负，那它有一个非负特征值<script type="math/tex">\lambda</script>严格大于其他所有特征值，其对应的特征向量也非负。特别的，如果该矩阵是不可约的，那么有<script type="math/tex">\lambda</script>的重数(multiplicity)为1，且对应特征向量为正的。</p></blockquote></li></ul><h5 id="3-介数中心性-Betweenness-centrality"><a href="#3-介数中心性-Betweenness-centrality" class="headerlink" title="3. 介数中心性 Betweenness centrality"></a>3. 介数中心性 Betweenness centrality</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720369.png" alt="image-20220317182958823" style="zoom:25%;" /></p><p><a href="https://zh.wikipedia.org/wiki/%E4%BB%8B%E6%95%B0%E4%B8%AD%E5%BF%83%E6%80%A7">介数中心性</a>：</p><p>被计算节点处在在任意节点对最短路径的比例，可以理解为对于计算节点，作为被经过的节点组成最短路径的数目与所有路径的比。代表最短路径是否经过该节点。</p><h5 id="4-接近中心性-Closeness-centrality"><a href="#4-接近中心性-Closeness-centrality" class="headerlink" title="4. 接近中心性 Closeness centrality"></a>4. 接近中心性 Closeness centrality</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720370.png" alt="image-20220317213625370" style="zoom:25%;" /></p><p>Closeness centrality：如果一个节点是重要的话，那么它到达其它节点的最短路径的总和长度应该是够小的。即:</p><script type="math/tex; mode=display">c_{v} = \frac{1}{\sum_{u \neq v} \text{v到u的最短路径}} \tag{3}</script><p>如上图例子中,<script type="math/tex">c_A = 1 / (A到C最短路径+ A到B最短路径+...)=1/(1+2+2+3)=1/8</script></p><h5 id="5-节点特征-聚类系数-Cluster-Coefficient"><a href="#5-节点特征-聚类系数-Cluster-Coefficient" class="headerlink" title="5. 节点特征: 聚类系数 Cluster Coefficient"></a>5. 节点特征: 聚类系数 Cluster Coefficient</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720371.png" alt="image-20220317214710121" style="zoom:25%;" /></p><p>聚类系数: 表示邻居的聚集程度，这个可以一定程度表示图的稀疏程度。</p><p><strong>实际上就是算节点v与邻居构成实际组成的三角形数除上最大可能三角形个数</strong>。</p><ul><li>邻居连接多少边就是多少三角形，这条边连上个自身节点v，就是一个三角形。</li><li>随机取两点加上一个不共线的的自身节点v，就是所有可能的三角形。</li></ul><script type="math/tex; mode=display">e_v = \frac{邻居节点连接边的数目}{C^2_{\text{节点v的邻居数目}}}= \frac{2邻居节点连接边的数目}{\text{节点v的邻居数目}*(\text{节点v的邻居数目}-1)}\tag{4}</script><p>根据式4，</p><ul><li>上图中左图<script type="math/tex">e_v = \frac{2\times6}{4\times(4-1)}=1</script>. 节点v的邻居们连接用了6条边，邻居个数为4. 注意，节点v底下两条边，少了这两条计算结果就是2/3。</li></ul><p>下面有一些计算例子，可以试着计算下。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720372.png" alt="image-20220318152846057" style="zoom:25%;" /></p><h5 id="6-Graphlets-图元"><a href="#6-Graphlets-图元" class="headerlink" title="6. Graphlets 图元"></a>6. Graphlets 图元</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720373.png" alt="image-20220318154301233" style="zoom:25%;" /></p><p><strong>Graphlets</strong>: 聚类系数是对ego-network 中三角网络的计算。(这就是为什么解释聚类系数本质在计算实际三角形数目与所有三角形之比。)</p><blockquote><p>所谓的ego network，它的节点是由唯一的一个中心节点(ego)，以及这个节点的邻居(alters)组成的，它的边只包括了ego和alter之间，以及alter与alter之间的边。</p></blockquote><p>我们就可以用一些预指定的子图来替换三角形来计算，聚类系数计算的是三角形网络，我们拓展为图元。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720374.png" alt="image-20220318163100647" style="zoom:25%;" /></p><p>目标: 描述节点u周围的网络结构。</p><ul><li>图元是小的子图，描述节点u的邻居节点的结构。</li></ul><p>类比：</p><ol><li>度：衡量多少节点接触</li><li>聚类系数： 衡量一个节点接触多少三角形网络</li><li>Graphlet Degree Vector(GDV)：节点的基于图元特征。GDV是衡量节点接触的图元数目。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720375.png" alt="image-20220318182348051" style="zoom:25%;" /></p><ul><li>考虑2-5个节点的图元可以获得73个坐标向量: 就是节点记号来描述邻居节点的拓扑</li><li>GDV图元度向量可用来一种衡量节点局部网络拓扑结构。<ul><li>相比节点度或聚类系数，比较两个节点的向量(GDV)是一种更详细衡量局部拓扑结构相似度的方法。</li></ul></li></ul><h5 id="7-导出子图和同构"><a href="#7-导出子图和同构" class="headerlink" title="7. 导出子图和同构"></a>7. 导出子图和同构</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720376.png" alt="image-20220318184042399" style="zoom:25%;" /></p><p><strong>Induced subgraph</strong>: 导出子图是另外一种图，由<strong>顶点子集和连接该子集中顶点的所有边</strong>组成。</p><p>上图左边是导出子图，右边不是。是因为左边不包含子集中顶点的所有的边。</p><p><strong>Graph Isomorphism</strong>，图的同构。同构这个概念来自于群论(主要有个双射概念难理解)，这里定义为<strong>两个图包含同样数量的节点，连接方式也一样就说是同构</strong>。</p><p><a href="https://www.zhihu.com/question/326620873/answer/1063169941">知乎回答——怎么理解图的同构?怎么判断两个图是否同构？</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720377.png" alt="image-20220318230007472" style="zoom:25%;" /></p><p>图元是一个有根连接的异构子图。图中标的数字代表根节点可能的位置。例如对于G0，两个节点是等价，这样就一个标号。</p><p>到5个节点一共能产生如图所示73种graphlet。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720378.png" alt="image-20220318232019780" style="zoom:25%;" /></p><p><strong>Graphlet Degree Vector (GDV)</strong>:  以给定节点为根的图元计数向量。</p><p>如上图，给定u节点的可能有图元a， b, c, d.拿到原图(红色u节点图)去匹配得出，a有2，b有1，c有0，d有2.这样节点u的GDV就是[2, 1, 0, 2].</p><h5 id="8-节点级别特征总结"><a href="#8-节点级别特征总结" class="headerlink" title="8.节点级别特征总结"></a>8.节点级别特征总结</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720379.png" alt="image-20220319163520125" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720380.png" alt="image-20220319164217665" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720381.png" alt="image-20220319164620273" style="zoom:25%;" /></p><ul><li>基于重要性的特征: 获取图中节点的重要性<ul><li>节点度: 简单统计邻居节点数目</li><li>不同节点中心性衡量: <ul><li>衡量图中<strong>邻居节点重要性</strong></li><li>不同衡量选择：特征值中心性，中介中心性，接近中心性</li></ul></li></ul></li><li>基于结构的特征<ul><li>节点度: 简单统计邻居节点数目</li><li>聚类系数: 衡量跟邻居节点怎么连接</li><li>图元计数向量：统计不同图元出现的频率</li></ul></li></ul><h4 id="4-Link-level-prediction"><a href="#4-Link-level-prediction" class="headerlink" title="4. Link-level prediction"></a>4. Link-level prediction</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720382.png" alt="image-20220319173332975" style="zoom:25%;" /></p><p>边预测任务两种公式化：</p><ol><li>随机缺失边<ul><li>随机覆盖一些边，然后预测两个节点间是否会连接，2分类问题。</li></ul></li><li>边随时间演化<ul><li>给定<script type="math/tex">G[t_0, t^{\prime}_0]</script>, 即给定<script type="math/tex">t_0 \ 到 \ t^{\prime}_0</script>的边连接确定的图，预测未来<script type="math/tex">G[t_1, t^{\prime}_1]</script>边连接情况，输出即一个排序的list L. 如推荐中，我们用1月份用户A关注的数据预测2月份关注情况，来进行推荐。</li><li>评估：<ul><li><script type="math/tex">n=|E_{\text{new}}|</script>: 测试时期<script type="math/tex">[t_1, t^{\prime}_1]</script>新边出现的次数。</li><li>取输出list L 的前n个元素，并对正确预测的边计数。</li></ul></li></ul></li></ol><h5 id="1-Link-Prediction-via-Proximity"><a href="#1-Link-Prediction-via-Proximity" class="headerlink" title="1. Link Prediction via Proximity"></a>1. Link Prediction via Proximity</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720383.png" alt="image-20220319213930151" style="zoom:25%;" /></p><ul><li>方法<ul><li>对于每个节点对<script type="math/tex">(x, y)</script> 就是那分数<script type="math/tex">c(x, y)</script><ul><li>例如， <script type="math/tex">c(x, y)</script> 是<script type="math/tex">x , \ y</script> 共同的邻居数目</li><li>按照<script type="math/tex">c(x, y)</script>分数来对<script type="math/tex">(x, y)</script>对排序</li><li>预测前n对作为新的连接</li><li>评估时看哪些边真正在<script type="math/tex">G[t_1, t_1^{\prime}]</script>时出现。</li></ul></li></ul></li></ul><h5 id="2-边级别特征：概述"><a href="#2-边级别特征：概述" class="headerlink" title="2. 边级别特征：概述"></a>2. 边级别特征：概述</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720384.png" alt="image-20220319230231743" style="zoom:25%;" /></p><ul><li>基于距离的特征</li><li>局部邻居重叠</li><li>全局邻居重叠</li></ul><p><strong>基于距离特征的不合理之处：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720385.png" alt="image-20220319230748558" style="zoom:25%;" /></p><p>如上图中，BH直接最短距离为2，BE也为2.但BH有2个共享的邻居节点，BE只有一个。不能表现图结构的具体信息。因此我们就引入neighborhood overlap。</p><h5 id="3-局部邻居节点重叠"><a href="#3-局部邻居节点重叠" class="headerlink" title="3. 局部邻居节点重叠"></a>3. 局部邻居节点重叠</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720386.png" alt="image-20220320002454324" style="zoom:25%;" /></p><ol><li>直接看共同邻居节点:</li></ol><script type="math/tex; mode=display">f(v_1, v_2) = |N(v_1)\cap N(v_2)| \tag{5}</script><ol><li>Jaccard’s coefficient</li></ol><script type="math/tex; mode=display">f(v_1, v_2) = \frac{ |N(v_1)\cap N(v_2)|}{ |N(v_1)\cup N(v_2)|} \tag{6}</script><ol><li>Adamic-Adar index：<script type="math/tex; mode=display">\sum_{u\in N(v_1) \cap N(v_2)} \frac{1}{\log(k_u)} \tag{7}</script>其中<script type="math/tex">\log(k_u) 表示\ v1, \ v_2</script>公共的邻居节点的度，即所有邻居节点的度的对数的倒数的和。</li></ol><h5 id="4-全局邻居节点重叠"><a href="#4-全局邻居节点重叠" class="headerlink" title="4. 全局邻居节点重叠"></a>4. 全局邻居节点重叠</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720387.png" alt="image-20220320140601515" style="zoom:25%;" /></p><ul><li><strong>Local neighborhood features</strong> 的限制<ul><li>评价指标总是0如果两个节点没有任何共同的邻居节点</li><li>然而，两个节点可能在将来会连接</li></ul></li><li><strong>Global neighborhood overlap</strong> 评价指标考虑整个图能解决这个限制</li></ul><h5 id="5-全局邻居节点重叠指标"><a href="#5-全局邻居节点重叠指标" class="headerlink" title="5.全局邻居节点重叠指标"></a>5.全局邻居节点重叠指标</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720388.png" alt="image-20220320143201799" style="zoom:25%;" /></p><ul><li><p><strong>Katz index</strong>: 最基本的全局指标，给定一对节点，对其走过的所有路径长度计算，即计算节点对的各个路径长度下的路径数量。<strong>katz指标易受到节点度数的影响，度数越高的节点，其路径越多，取值越大</strong>。</p></li><li><p>那么怎么计算？——使用邻接矩阵计算。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720389.png" alt="image-20220320144810016" style="zoom:25%;" /></p><p><strong>直觉</strong>：利用邻接矩阵的幂。</p><ul><li>Recall: 对应<script type="math/tex">\mathbf{A}_{uv}=1, \ 其中 \mathbf{u} 是 \mathbf{v}的邻居节点, 即\text{if} \ \mathbf{u}\in N(\mathbf{v})</script>。相连为1嘛。</li><li>记<script type="math/tex">\mathbf{P}_{uv}^{(K)}</script>为u和v直接的路径长度。</li><li>接下来我们证明<script type="math/tex">\mathbf{P}^{(K)}_{uv} = \mathbf{A}^k</script>。</li><li>因为<script type="math/tex">\mathbf{P}^1_{uv}</script>表示u和v之间的路径长度为1，即直接的邻居节点。这时候，就变成了<script type="math/tex">\mathbf{P}^1_{uv}=\mathbf{A}_{uv}</script>,简单来说就是对应的邻接矩阵。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720390.png" alt="image-20220320182053717" style="zoom:25%;" /></p><p>那么怎么计算<script type="math/tex">P^{(2)}_{\mathbf{u}\mathbf{v}}</script> ?（我们要计算2步从u跳到v的所有路径，先跳1步再怎么1步跳到v。）</p><ol><li><strong>Step 1</strong>: 计算每个<script type="math/tex">\mathbf{u}的邻居节点和\ \mathbf{v}</script>之间的所有长度为1的路径数</li><li><strong>Step 2</strong>: 对经过<script type="math/tex">\mathbf{u}</script>的邻居节点路径求和</li><li>写成公式就是<script type="math/tex">P^{(2)}_{\mathbf{u}\mathbf{v}} = \sum_i \mathbf{A}_{ui} * \mathbf{P}^{(1)}_{iv}</script>，这里i代表u的邻居节点，再回过去看1,2步公式。最后可得就是邻接矩阵的幂次运算，其中幂次代表长度。</li></ol><h5 id="6-Katz-index"><a href="#6-Katz-index" class="headerlink" title="6. Katz index"></a>6. Katz index</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720391.png" alt="image-20220320232921148" style="zoom:25%;" /></p><p><strong>Katz index</strong>：对节点对之间所有长度的路径数量进行了统计计数。</p><p>那么怎么计算两个节点间路径?</p><ul><li><p>邻接矩阵的幂</p><ul><li><script type="math/tex">\mathbf{A}_{uv}</script>表示u和v之间长度为1的路径，直接的邻居节点。(跳1步)</li><li><script type="math/tex">\mathbf{A}_{uv}^2</script> 表示 u和v之间长度为2的路径，邻居的邻居。(跳2步)</li><li>那么<script type="math/tex">\mathbf{A}_{uv}^l</script>就是长度为l的路径。(跳l步)</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720392.png" alt="image-20220321001117436" style="zoom:25%;" /></p></li></ul><p>公式: <script type="math/tex">v_1 \ 和 v_2</script>之间所有长度的路径求和。</p><script type="math/tex; mode=display">S_{v_1v_2} = \sum_{l=1}^{\infty} \beta^l \mathbf{A}_{v_1v_2}^l \ \ \ 其中, 0 \lt \beta \lt 1\tag{8}</script><p>其中<script type="math/tex">0 \lt \beta \lt 1</script>为自定义权重衰减因子，邻居节点赋予不同的权重, 对于短路径赋予较大的权重, 而长路径赋予较小的权重。</p><p>证明: 简单地展开有:</p><script type="math/tex; mode=display">S=\beta A + \beta^2 A^2 + \cdots + \beta^n A^n \tag{9}</script><p>转换下形式推导:</p><script type="math/tex; mode=display">\begin{align}&(I-\beta A)(I+S)\\&=(I-\beta A)(I + (\beta A + \beta^2 A^2 + \cdots + \beta^n A^n ))\\&=I - \beta A  + (\beta A + \beta^2 A^2 + \cdots + \beta^n A^n ) - \beta A (\beta A + \beta^2 A^2 + \cdots + \beta^n A^n )\\&= I +(\beta^2 A^2 + \cdots + \beta^n A^n) -(\beta^2 A^2 + \cdots + \beta^n A^n + \beta^{n+1} A^{n+1})\\&= I \end{align}\tag{10}</script><p>由式10易得:</p><script type="math/tex; mode=display">S = (I-\beta A) ^{-1} - I \tag{11}</script><h5 id="7-边级别特征：总结"><a href="#7-边级别特征：总结" class="headerlink" title="7. 边级别特征：总结"></a>7. 边级别特征：总结</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720393.png" alt="image-20220321002807322" style="zoom:25%;" /></p><ul><li>基于距离的特征<ul><li>用两个节点间最短的路径，但不能获取邻居节点是怎么重叠</li></ul></li><li>局部邻居节点重叠<ul><li>获取两个节点间多少公共的邻居节点</li><li>容易变成0，没有公共邻居节点共享</li></ul></li><li>全局邻居节点重叠<ul><li>用全局图结构来对两个节点打分</li><li>Katz index: 对两个节点间所有路径长度计数(就把所有路径长度一起求和)</li></ul></li></ul><h4 id="5-图级别特征"><a href="#5-图级别特征" class="headerlink" title="5. 图级别特征"></a>5. 图级别特征</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720394.png" alt="image-20220321110728476" style="zoom:25%;" /></p><ul><li>目标: 表征整个图结构</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720396.png" alt="image-20220321112557118" style="zoom:25%;" /></p><p><strong>背景: Kernel Methods</strong></p><ul><li>Kernel method 在图级别预测的传统机器学习中广发运用</li><li><strong>Idea</strong>: 设计kernel来替换掉特征向量，跟机器学习中的核方法作用一样，比如SVM中核技巧</li><li>Kernel简介:<ul><li>Kernel <script type="math/tex">K(G, G’) \in \mathbb{R}</script> 衡量 b/w 数据的相似性</li><li>Kernel matrix <script type="math/tex">K = (K(G, G^\prime))_{G, G^{\prime}}</script>, 必须是半正定, 要其有正的特征值。</li><li>存在特征表达式<script type="math/tex">\phi(\cdot)</script>，使得<script type="math/tex">K(G, G^\prime) = \phi(G)^T \phi(G^{\prime})</script></li><li>kernel 一旦定义，就可以使用现成的ML模型如核SVM，进行预测。</li></ul></li></ul><h5 id="1-图级别特征：概述"><a href="#1-图级别特征：概述" class="headerlink" title="1. 图级别特征：概述"></a>1. 图级别特征：概述</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720397.png" alt="image-20220321141154532" style="zoom:25%;" /></p><ul><li>Graph Kernels： 衡量两个图的相似度<ul><li>Graphlet Kernel</li><li>Weisfeiler-Lehman Kernel</li><li>其它的kernel: Random-walk kernel, Shortest-path graph kernel…</li></ul></li></ul><h5 id="2-Graph-Kernel-Key-Idea"><a href="#2-Graph-Kernel-Key-Idea" class="headerlink" title="2. Graph Kernel : Key Idea"></a>2. Graph Kernel : Key Idea</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720398.png" alt="image-20220321142245206" style="zoom:25%;" /></p><ul><li><strong>目标</strong>：设计图特征向量<script type="math/tex">\phi(G)</script>.</li><li><strong>Key idea</strong>： 图的Bag-of-Words(BoW) <ul><li><strong>Recall</strong>: BoW简单利用词的数目作为文档特征(不考虑顺序)。</li><li>简单拓展到图: 将每个节点看作BoW中的词。</li><li>像如果两个4红色节点的图，经过同一<script type="math/tex">\phi</script>处理后提取的特征向量一样。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720399.png" alt="image-20220321170109031" style="zoom:25%;" /></p><p>上面已经说到用指定数目的节点的图过核函数<script type="math/tex">\phi</script>来获取特征向量，再进一步将其转换为节点的度。就变成了：我们使用<strong>节点度的“Bag”</strong>.</p><p>向上图中，我们得到的两个4节点的图经过核函数后得到的不同特征向量。这样是不行的。</p><ul><li>所有的<strong>Graphlet Kernel</strong> 和 <strong>Weisfeiler-Lehman(WL)</strong> 使用<strong>图的Bag-of- *</strong>表征，这里星号是比节点度更广泛的东西。</li></ul><h5 id="3-Graphlet-feature"><a href="#3-Graphlet-feature" class="headerlink" title="3. Graphlet feature"></a>3. Graphlet feature</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720400.png" alt="image-20220321171525867" style="zoom:25%;" /></p><p><strong>关键点</strong>：就是拿图元作为模板(pattern),去匹配图中出现对应图元对应数目。</p><p>注：这里定义的图元跟节点级别的特征有2点区别。</p><ol><li>这里图元中节点不需要被连接，可以是孤立的点</li><li>这里的图元没有根</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720401.png" alt="image-20220321174121813" style="zoom:25%;" /></p><p>令<script type="math/tex">G_k = (g_1, \cdots, g_{n_k})</script>，表示k个节点内的图元的列表。</p><p>例如，k=3，就因为可以是孤立的点，图元数目比节点图元数多。同样也没有根这个概念了。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720402.png" alt="image-20220321174858994" style="zoom:25%;" /></p><p>那么就有如下定义：给定图<script type="math/tex">G</script>,以及图元列表<script type="math/tex">\mathcal{G}_k = (g_1, g_2, \cdots, g_{n_k})</script>.那么图元数目向量<script type="math/tex">\mathcal{f}_G \ \mathbb{R}^{n_k}</script>如上图所示, 表示：</p><script type="math/tex; mode=display">(\mathcal{f}_G)_i$$等于属于G的图元$$g_i$$的数目，这样就构成一个向量。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720403.png" alt="image-20220321180458265" style="zoom:25%;" />如上所示例子中，图元$$g_1$$在图中匹配一个，其它依次匹配为3， 6， 0，所以该向量为$$\mathcal{f}_G = (1, 3, 6, 0)^T$$。##### 4. Graphlet Kernel<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720404.png" alt="image-20220321180757475" style="zoom:25%;" />- 给定两个图$$G, \ G^\prime$$，graphlet kernel 计算为:</script><p>  K(G, G^{\prime}) = \mathbf{f}<em>G\ ^T \mathbf{f}</em>{G^\prime} \tag{12}</p><script type="math/tex; mode=display">  - 问题:如果两个图size差非常多，那么将会导致乘积也非常大。  - **Solution**：归一化所有向量。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720405.png" alt="image-20220321223918765" style="zoom:25%;" />**限制**：枚举图元是非常昂贵的！- 对n大小的图枚举k大小的图元，需要$$n^k$$次- 这将会不可避免最坏的情况出现，因为子图同构测试(判断一个图和另外一个图是否同构)是NP-hard问题。- 如果一个图节点度被限制为d，存在复杂度为$$O(nd^{k-1})$$统计所有size为k的图元的算法.那我们能设计一个更有效的graph Kernel吗？##### 5. Weisfeiler-Lehman Kernel<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720406.png" alt="image-20220321225746090" style="zoom:25%;" />- 目标:设计高效的图特征描述器$$\phi(G) $$.- **想法**: 利用邻居节点结构来迭代丰富节点词汇表  - **Bag of node degrees**的广义版本，因为节点度只是one-hop邻居信息，这可以包括更多跳(路径长度更长)信息。- 算法的实现**颜色细化(Color refinement)**。算法也称为**Weisfeiler-Lehman graph isomorphism test(Weisfeiler-Lehman图同构检验)。**##### 6. Color Refinement<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720407.png" alt="image-20220321234711813" style="zoom:25%;" />- 给定一个图G，其有一系列节点V。  1. 分配每个节点v一个初始颜色$$c^{(0)}(v)</script><ol><li><p>按以下公式迭代改进节点颜色</p><script type="math/tex; mode=display">c^{(k+1)}(v) = \text{HASH}( \{c^{k}(v), \{c^{(k)}(u)\}\}_{u \in N(v)}) \tag{13}</script><p>HASH表示哈希函数，将不同输入映射到不同颜色。</p></li><li><p>在K步后，分配的颜色包含了<script type="math/tex">c^{(K)}(v)</script> K阶近邻的信息。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720408.png" alt="image-20220322002055956" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720409.png" alt="image-20220322002203246" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720410.png" alt="image-20220322002229317" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720411.png" alt="image-20220322002301971" style="zoom:25%;" /></p><p><strong>给定两个图来进行color refinement：</strong></p><ol><li>先开始对所有节点赋值为1,</li><li>aggregate邻居节点信息，具体做法是，第一个G的左上角节点，其有3个为1的的邻居节点，记为(1, 111)。同理有右上角为(1, 11).</li><li>按照第一个Hash表赋值，得到聚合后的信息</li><li>然后在对其聚合邻居信息, ，第一个G的左上角节点，变成(4, 345)</li><li>再按第二个hash表赋值，就行成了最后的节点信息。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720412.png" alt="image-20220322003506905" style="zoom:25%;" /></p><p>经过<strong>color refinement</strong> 3次后， WL kernel 如上所示。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720413.png" alt="image-20220322003803894" style="zoom:25%;" /></p><p>对其进行内积运算后得到36+4+1+4+1+2+1=49.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720414.png" alt="image-20220322003951695" style="zoom:25%;" /></p><p><strong>Weisferiler-Lehman Kernel</strong></p><ul><li>WL 核计算高效<ul><li>color refinement的时间复杂度每步都是线性的，因为其包含聚合邻居颜色</li></ul></li><li>当计算kernel值时，仅仅只有出现在两个图中的颜色需要记录<ul><li>因此，颜色最多只能是所有节点数目</li></ul></li><li>统计颜色的复杂度跟节点数成线性关系</li><li>总之，时间复杂度更变的数目呈线性关系</li></ul><h5 id="7-图级别特征：总结"><a href="#7-图级别特征：总结" class="headerlink" title="7. 图级别特征：总结"></a>7. 图级别特征：总结</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261720415.png" alt="image-20220322004406444" style="zoom:25%;" /></p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p> [1] <a href="https://blog.csdn.net/PolarisRisingWar/article/details/117336622">cs224w（图机器学习）2021冬季课程学习笔记2</a></p><p> [2] <a href="https://zhuanlan.zhihu.com/p/459473365">【CS224W学习笔记 day01】 图传统机器学习</a></p><p> [3] <a href="https://zhuanlan.zhihu.com/p/57544108">图神经网络的数学基础 一: 线性代数和矩阵</a></p><p> [4] <a href="https://www.zhihu.com/question/22610633/answer/155958595">如何简单地理解中心度</a></p><p> [5] <a href="https://jmlr.csail.mit.edu/papers/v12/shervashidze11a.html">Weisfeiler-Lehman Graph Kernels</a></p><p> [6] <a href="https://zhuanlan.zhihu.com/p/415174490">传统图机器学习特征提取方法 — 基于链接水平的特征</a></p><p> [7] <a href="https://publikasiilmiah.ums.ac.id/bitstream/handle/11617/9659/Irma%20Yuliana.pdf?sequence=1&amp;isAllowed=y">A Comparison of Community Clustering Techniques: Fruchterman-Reingold and Wakita-Tsurumi </a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224W </tag>
            
            <tag> centrality </tag>
            
            <tag> Cluster Coefficient </tag>
            
            <tag> Graphlet </tag>
            
            <tag> Isomorphism </tag>
            
            <tag> Katz index </tag>
            
            <tag> Graphlet Kernel </tag>
            
            <tag> Weisfeiler-Lehman Kernel </tag>
            
            <tag> Color Refinement </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224W 1. Introduction; Machine Learning for Graphs</title>
      <link href="2022/02/23/CS224W_1.%20Introduction;%20Machine%20Learning%20for%20Graphs/"/>
      <url>2022/02/23/CS224W_1.%20Introduction;%20Machine%20Learning%20for%20Graphs/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Introduction-Machine-Learning-for-Graphs"><a href="#1-Introduction-Machine-Learning-for-Graphs" class="headerlink" title="1. Introduction: Machine Learning for Graphs"></a>1. Introduction: Machine Learning for Graphs</h3><p>所有笔记参照于<a href="http://web.stanford.edu/class/cs224w/">slide</a>。</p><h4 id="1-无向图和有向图——有无方向-有无箭头"><a href="#1-无向图和有向图——有无方向-有无箭头" class="headerlink" title="1. 无向图和有向图——有无方向(有无箭头)"></a>1. 无向图和有向图——有无方向(有无箭头)</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261715672.png" alt="image-20220316174141245" style="zoom:25%;" /></p><h4 id="2-各种各样的图"><a href="#2-各种各样的图" class="headerlink" title="2. 各种各样的图"></a>2. 各种各样的图</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716507.png" alt="image-20220316175422417" style="zoom: 25%;" /></p><p>图的表示<script type="math/tex">G = (V, E, R, T)</script>.其中：</p><ol><li>V：节点的集合<script type="math/tex">v_i \in V</script>.</li><li>E: 表示节点间关系<script type="math/tex">(v_i, r, v_j) \in E</script>，即这个三元组构成边，或者说边表示了这三者关系</li><li><script type="math/tex">T(v_I)</script>: 节点类型</li><li>关系类型<script type="math/tex">r \in R</script></li></ol><p>下面这两个例子就很清楚绘制了图网络是怎么表示各种节点、节点类型以及边代表的关系。如Causes, pub Year.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716508.png" alt="image-20220316180008737" style="zoom:25%;" /></p><h4 id="3-节点的度"><a href="#3-节点的度" class="headerlink" title="3. 节点的度"></a>3. 节点的度</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716509.png" alt="image-20220316180240142" style="zoom:25%;" /></p><ol><li>无向图: 直接看该节点的连接边数目，如上图中，A的度就是4. 平均的度等于所有节点的度的平均值，也等于边数的2倍除以节点数<script type="math/tex">\frac{2E}{N}</script>。</li><li>有向图: <ul><li>对于节点:分为出度和入度，就看箭头指向，指向该节点是入度，从该节点出发是出度。上图中C的入度是2，出度是1.度为3。</li><li>对于图: <strong>有向图</strong>平均度为<script type="math/tex">\frac{E}{N}</script>,并且对于整个图来说，平均出度和入度应该相等。其用来: 衡量图的稠密性</li></ul></li></ol><p>注:</p><ul><li>Source: 节点的入度为0，就没有箭头指向该节点，(万物源于此)，这种节点叫做起始点。</li><li>Sink: 节点的出度为0，该节点没有箭头出来，终止了，这种节点叫做终止点。</li><li>总结:起始点（Source Node）入度为0、终止点（Sink Node）出度为0。</li></ul><h4 id="4-二分图"><a href="#4-二分图" class="headerlink" title="4. 二分图"></a>4. 二分图</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716510.png" alt="image-20220316212410545" style="zoom:25%;" /></p><p>二分图是一种内部节点可分为没有交集的集合U和集合V，使得每个边都连接一个U中的节点和集合V的节点。这样U和V都是独立的集合。(如上图，集合U中的节点是没有边的，集合V中的节点也是没有边的，这样的集合就是独立的)。</p><p><strong>折叠和投影二分图</strong></p><p>如果集合U和集合V共享至少一个共同的邻居，就可以通过在独立集合中创建边来折叠(Folded)二分图。如下图，集合U中的节点至少共享一个集合V中的邻居节点，集合U中的节点连接形成投影U，同样获得投影V。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716511.png" alt="image-20220316232238892" style="zoom:25%;" /></p><h4 id="5-邻接矩阵"><a href="#5-邻接矩阵" class="headerlink" title="5. 邻接矩阵"></a>5. 邻接矩阵</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716512.png" alt="image-20220316233317356" style="zoom:25%;" /></p><p>就是拿一个矩阵来表示图中每个节点的连接关系。其中，</p><script type="math/tex; mode=display">A_{ij}=1$$ 表示从节点i到节点j有节点相连，而等于0就是不相连。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716513.png" alt="image-20220316234135326" style="zoom:25%;" />1. 无向图: $$A_{ij} = A_{ji}, \ A_{ii}=0$$，无向图的邻接矩阵是一个对称矩阵。2. 有向图:非对称矩阵。绝大多数情况下，邻接矩阵是稀疏的。( $$|E| \ll E_{m a x} \ \text{or }  \bar{k} \ll N-1$$)。结果导致邻接矩阵被大量的零填充(不期望的属性)。为了缓解此问题，我们可以将图形表示为一组边(边链表)。这虽然使边缘查找更加困难，但是节省了内存。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716514.png" alt="image-20220317004552887" style="zoom:25%;" />#### 6. 图表示： 边列表<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716515.png" alt="image-20220317005013669" style="zoom:25%;" />就把连接关系拿一个元组表示，再组成一个列表(字典)。#### 7. 权重图——边是有权重的<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716516.png" alt="image-20220317005126013" style="zoom:25%;" />#### 8. 自循环和多图<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716517.png" alt="image-20220317005632181" style="zoom:25%;" />1. **完全图**（complete graph）: 任意两点都有边相连。2. **自环图**（Self-edges (self-loops)）:  自己与自己相连，邻居矩阵的对角线不为0.3. **多重图** （ Multigraph ）: 存在两点之间大于一条边。#### 9. 无向图的连通性任意两节点存在直接或间接的连接关系，就认为其是连通的。Bride edge(桥边)是指删除后能使图变成非连通图的边。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716518.png" alt="image-20220317005901725" style="zoom:25%;" />#### 10. 有向图的连通性1. 强连通: 对于两个任意节点，存在来回路径，两个节点互相都能到达。2. 弱连通: 忽略方向是才连通的。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716519.png" alt="image-20220317005944842" style="zoom:25%;" />强连通分量 SGCs:可以看作是一组节点组成了强连通。如$$G[A, B, C]$$就形成了一个SGC，$$G[E, F, G]$$也一样。<img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716520.png" alt="image-20220317010024979" style="zoom:25%;" />数据结构里面关于图的知识点，可以跳过。> 图：$$G=（V,E）\Longrightarrow \text{Graph} = (\text{Vertex}, \text{Edge})</script><p>&gt;</p><blockquote><p>其中，</p><ul><li><p>V是顶点(数据元素)的<strong>有穷非空</strong>集合；</p></li><li><p>E是边的又穷集合。</p></li><li><p><strong>无向图</strong>：每条边都是无方向的</p></li><li><p><strong>有向图</strong>：每条边都是有方向的。</p></li><li><p><strong>完全图</strong>：图中的任意两个点都有一条边相连。</p><p>​    对于n个顶点，</p><p>​    无向完全图有<script type="math/tex">n(n-1)/2</script>条边，有向完全图有<script type="math/tex">2C_n^2 = n(n-1)</script>.</p></li><li><p><strong>稀疏图</strong>：有很少边或弧的图<script type="math/tex">e \lt n\log n</script>.弧就是有向图的叫法。</p></li><li><p><strong>稠密图</strong>：有较多的边或弧的图。</p></li><li><p><strong>网</strong>： 边/弧带全的图。</p></li><li><p><strong>邻接</strong>：有边/弧相连的两个顶点之间的关系，</p><ul><li>存在<script type="math/tex">(v_i, v_j)</script>,则称<script type="math/tex">v_i, v_j</script>为邻接点。圆括号表示不区分先后顺序。</li><li>存在<script type="math/tex"><v_i, v_j></script>，则称<script type="math/tex">v_i</script><strong>邻接到</strong><script type="math/tex">v_j</script>,而<script type="math/tex">v_j</script><strong>邻接于</strong><script type="math/tex">v_i</script>。尖括号表示有先后顺序。</li></ul></li><li><p>关联（依附）：边、弧与顶点间的关系</p><p>​                存在<script type="math/tex">(v_i, v_j)/<v_i, v_j></script>,则称该边、弧关联与<script type="math/tex">v_i</script>和<script type="math/tex">v_j</script>。</p></li><li><p><strong>顶点的度</strong>：与该顶点相关联的边的数目，记为TD(v)。</p><ul><li>在<strong>有向图</strong>中，顶点的度等于该顶点的<strong>入度</strong>和<strong>出度</strong>之和。</li><li>顶点v的入度是以v为终点的有向边的条数，记为ID(V)。</li><li>顶点v的出度是以v为始点的有向边的条数，记为OD(V)。</li></ul><p>如下图例子所示，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716521.png" alt="image-20210606215235961" style="zoom:25%;" /></p></li></ul><p>跟<strong>路径</strong>有关的定义：</p><ol><li><strong>路径</strong>：接续的边构成的顶点序列。</li><li><strong>路径长度</strong>：路径上边或弧的数目/权值之和。</li><li><strong>回路(环)</strong>：第一个顶点和最后一个顶点相同路径。</li><li><strong>简单路径</strong>：除路径起点和终点可以相同外，其余顶点均不相同的路径。</li><li><strong>简单回路(简单环)</strong>：除路径起点和终点相同外，其余顶点均不相同的路径。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716522.png" alt="image-20210606220017356" style="zoom:25%;" /></p><ol><li><strong>连通图</strong>(强连通图)：在无/有向图<script type="math/tex">G=(V, {E})</script>中，若对任何两个顶点v、u都存在从v到u的路径，则称G是连通图。有向图满足这个条件就是强连通图。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261716524.png" alt="image-20210606220526307" style="zoom:25%;" /></p><ol><li><p><strong>权</strong>：图中的边或弧所具有的相关数为权。表明一个顶点到另一个顶点的距离和耗费。</p><p>​                                                                                                                                                                ——<a href="https://www.bilibili.com/video/BV1nJ411V7bd/">王卓 数据结构与算法</a></p></li></ol></blockquote><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://yasoz.github.io/cs224w-zh/#/Introduction-and-Graph-Structure">CS224W-notes</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224W </tag>
            
            <tag> Graph </tag>
            
            <tag> Node </tag>
            
            <tag> Edge </tag>
            
            <tag> Degree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.Seaborn notes</title>
      <link href="2021/11/22/%E6%8A%80%E8%83%BD_3.seaborn%20notes/"/>
      <url>2021/11/22/%E6%8A%80%E8%83%BD_3.seaborn%20notes/</url>
      
        <content type="html"><![CDATA[<h3 id="3-Seaborn-notes"><a href="#3-Seaborn-notes" class="headerlink" title="3.Seaborn notes"></a>3.Seaborn notes</h3><p>Seaborn 绘制图形技巧主要在于数据思维难，如：怎么选择字段，怎么选择预处理数据，绘制什么图像，以及要在图像上体现数据统计量及其变化。我们应该去逐一理解数据，选择合适的技巧。下面举例了8个常用的绘制函数。每个函数调用时，针对DataFrame这种数据类型，要注意打算绘制哪些字段(哪些列)，用什么绘制函数，输入参数有哪些，主要注意x,y选择的字段。</p><h4 id="1-lineplot"><a href="#1-lineplot" class="headerlink" title="1.lineplot"></a>1.lineplot</h4><p>绘制折线图命令：<code>seaborn.lineplot(x, y, data, hue, legend)</code>，基本上了解这些参数就可以了。</p><ol><li><p><code>x,y</code>就是数据的不同列会作为x轴，y轴的输入数据。如果具体指明对应的列，data参数可以不要。</p><p>如示例中可以改为 <code>sns.lineplot(x=fmri[&#39;timepoint&#39;], y=fmri[&#39;signal&#39;], hue=fmri[&#39;event&#39;])</code>。</p></li><li><p><code>data</code>可以是df, ndarray等。一般是df，这时上面的x,y参数可以只写列名。</p></li><li><p><code>hue</code>利用颜色来表示类型变量， 如例子fmri中绘制不同event对应的折线</p></li><li><p><code>legend</code> 图例，默认“auto”。可以选full, 也可以具体制定。如例子fmri右上角event的图例。</p></li></ol><p><a href="https://www.kaggle.com/saurav9786/seaborn-tutorial">具体查看官方解释</a>。</p><p>拿fmri示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fmri = sns.load_dataset(<span class="string">&quot;fmri&quot;</span>)</span><br><span class="line">fmri.head()</span><br><span class="line">=======================================</span><br><span class="line">subjecttimepointeventregionsignal</span><br><span class="line"><span class="number">0</span>s13  <span class="number">18</span>stimparietal-<span class="number">0.017552</span></span><br><span class="line"><span class="number">1</span>s5  <span class="number">14</span>stimparietal-<span class="number">0.080883</span></span><br><span class="line"><span class="number">2</span>s12  <span class="number">18</span>stimparietal-<span class="number">0.081033</span></span><br><span class="line"><span class="number">3</span>s11  <span class="number">18</span>stimparietal-<span class="number">0.046134</span></span><br><span class="line"><span class="number">4</span>s10  <span class="number">18</span>stimparietal-<span class="number">0.037970</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>), dpi=<span class="number">300</span>)</span><br><span class="line">sns.lineplot(data=fmri, x=<span class="string">&#x27;timepoint&#x27;</span>, y=<span class="string">&#x27;signal&#x27;</span>, hue=<span class="string">&#x27;event&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913530.png" alt="image-20211122162234712" style="zoom:20%;" /></p><p>我们来进一步设置，让图更好看一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">auto = pd.read_csv(<span class="string">&quot;Automobile_data.csv&quot;</span>)</span><br><span class="line">auto.head().T</span><br></pre></td></tr></table></figure><p><code>auto</code>信息如下:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913532.png" alt="image-20211122162403286" style="zoom:20%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>), dpi=<span class="number">500</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(rc=&#123;</span><br><span class="line">    <span class="string">&quot;axes.facecolor&quot;</span>: <span class="string">&quot;#283747&quot;</span>, <span class="comment">#背景色</span></span><br><span class="line">    <span class="string">&quot;axes.grid&quot;</span>: <span class="literal">False</span>, <span class="comment">#不要网格</span></span><br><span class="line">    <span class="string">&#x27;xtick.labelsize&#x27;</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;ytick.labelsize&#x27;</span>:<span class="number">10</span>,</span><br><span class="line">&#125;)</span><br><span class="line">plt.title(<span class="string">&quot;Engine size&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">sns.lineplot(x=auto[<span class="string">&#x27;engine-size&#x27;</span>], y=auto[<span class="string">&#x27;wheel-base&#x27;</span>].index.values, color=<span class="string">&#x27;#ffd700&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913534.png" alt="image-20211122162458464" style="zoom:20%;" /></p><h4 id="2-Scatterplots"><a href="#2-Scatterplots" class="headerlink" title="2.Scatterplots"></a>2.Scatterplots</h4><p>绘制散点图用 <code>sns.scatterplot</code>，其中:</p><ol><li><code>x, y, data</code>用法类似折线图</li><li><code>hue</code>用颜色表示类型字段信息。</li><li><code>size</code>用点的大小表示类型字段信息。</li></ol><p>还是 <code>auto = pd.read_csv(&quot;Automobile_data.csv&quot;)</code>这个df，我们探索 <code>&#39;engine-size&#39;</code>和 <code>&#39;wheel-base&#39;</code>轴距之间的关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>), dpi=<span class="number">500</span>)</span><br><span class="line">sns.<span class="built_in">set</span>(rc=&#123;</span><br><span class="line">    <span class="string">&quot;axes.facecolor&quot;</span>: <span class="string">&quot;#ECECFF&quot;</span>, <span class="comment">#背景色</span></span><br><span class="line">    <span class="string">&quot;axes.grid&quot;</span>: <span class="literal">False</span>, <span class="comment">#不要网格</span></span><br><span class="line">    <span class="string">&#x27;xtick.labelsize&#x27;</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;ytick.labelsize&#x27;</span>:<span class="number">10</span>,</span><br><span class="line">&#125;)</span><br><span class="line">sns.scatterplot(x=<span class="string">&#x27;engine-size&#x27;</span>, y=<span class="string">&#x27;wheel-base&#x27;</span>, data=auto)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913535.png" alt="image-20211122172246677" style="zoom:20%;" /></p><p>如果我们还要增加 <code>fuel-type</code>字段和 <code>city-mpg</code>字段信息，并只绘制 <code>(20, 200)</code>city-mpg数据，可以用hue颜色表示fuel-type，size大小表示city-mpg。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">12</span>), dpi=<span class="number">500</span>)</span><br><span class="line"><span class="comment"># plt.xlim([50, 200]) #x轴限制在50, 200</span></span><br><span class="line"><span class="comment"># sns.set(rc=&#123;</span></span><br><span class="line"><span class="comment">#     &quot;axes.facecolor&quot;: &quot;#ECECFF&quot;, #背景色</span></span><br><span class="line"><span class="comment">#     &quot;axes.grid&quot;: False, #不要网格</span></span><br><span class="line"><span class="comment">#     &#x27;xtick.labelsize&#x27;:10,</span></span><br><span class="line"><span class="comment">#     &#x27;ytick.labelsize&#x27;:10,</span></span><br><span class="line"><span class="comment"># &#125;)</span></span><br><span class="line">sns.scatterplot(x=<span class="string">&#x27;engine-size&#x27;</span>, y=<span class="string">&#x27;wheel-base&#x27;</span>, data=auto, hue=<span class="string">&#x27;fuel-type&#x27;</span>, size=<span class="string">&#x27;city-mpg&#x27;</span>, sizes=(<span class="number">20</span>, <span class="number">200</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913536.png" alt="image-20211122173458993" style="zoom:20%;" /></p><h4 id="3-relplot"><a href="#3-relplot" class="headerlink" title="3. relplot"></a>3. relplot</h4><p>relplot和scatterplot最主要区别是其有个kind参数可以选择scatter还是line。基本可以看做lineplot,scatterplot合体。</p><p><a href="https://seaborn.pydata.org/generated/seaborn.relplot.html">relplot 官方文档</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;savefig.dpi&#x27;</span>] = <span class="number">300</span> <span class="comment">#图片分辨率</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.dpi&#x27;</span>] = <span class="number">300</span> <span class="comment">#显示分辨率</span></span><br><span class="line"></span><br><span class="line">sns.relplot(x=<span class="string">&#x27;wheel-base&#x27;</span>,</span><br><span class="line">            y=<span class="string">&#x27;engine-size&#x27;</span>,</span><br><span class="line">            hue=<span class="string">&#x27;fuel-type&#x27;</span>,</span><br><span class="line">            data=auto,</span><br><span class="line">            kind=<span class="string">&#x27;scatter&#x27;</span>,</span><br><span class="line">            palette=[<span class="string">&#x27;#FF3333&#x27;</span>, <span class="string">&quot;#00CC00&quot;</span>], <span class="comment">#指定颜色 两种&#x27;fuel-type&#x27;</span></span><br><span class="line">            height=<span class="number">8.5</span>, <span class="comment">#图像高</span></span><br><span class="line">            aspect=<span class="number">1</span>,<span class="comment">#图像纵横比</span></span><br><span class="line">            )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913537.png" alt="image-20211122175310664" style="zoom:20%;" /></p><h4 id="4-barplot"><a href="#4-barplot" class="headerlink" title="4.barplot"></a>4.barplot</h4><p><strong>barplot</strong>主要绘制数字变量和类别变量关系。示例中的<code>city-mpg</code>和 <code>body-style</code>中的关系。可以将x，y变量调换让其横过来。其中:</p><ol><li><code>order</code>是排序索引</li><li><code>capsize</code>指bar的帽子宽度，如示例中两黑色横线。 <code>errwidth</code>指bar的线宽。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">order = auto.groupby([<span class="string">&#x27;body-style&#x27;</span>]).mean().sort_values(by=<span class="string">&#x27;city-mpg&#x27;</span>, ascending=<span class="literal">False</span>).index</span><br><span class="line">sns.barplot(auto[<span class="string">&#x27;body-style&#x27;</span>],</span><br><span class="line">            auto[<span class="string">&#x27;city-mpg&#x27;</span>],</span><br><span class="line">            order=order,</span><br><span class="line">            errwidth=<span class="number">1.0</span>, <span class="comment">#error bar线宽</span></span><br><span class="line">            capsize=<span class="number">0.1</span>, <span class="comment">#error bar 帽子宽度</span></span><br><span class="line">            )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913538.png" alt="image-20211122184507858" style="zoom:20%;" /></p><p>将其x,y调换将图像横过来，并随便指定个顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">order = [<span class="string">&#x27;hatchback&#x27;</span>, <span class="string">&#x27;convertible&#x27;</span>, <span class="string">&#x27;sedan&#x27;</span>, <span class="string">&#x27;wagon&#x27;</span>, <span class="string">&#x27;hardtop&#x27;</span>, ]</span><br><span class="line">sns.barplot(auto[<span class="string">&#x27;city-mpg&#x27;</span>],</span><br><span class="line">            auto[<span class="string">&#x27;body-style&#x27;</span>],</span><br><span class="line">            order=order,</span><br><span class="line">            errwidth=<span class="number">1.0</span>, <span class="comment">#error bar线宽</span></span><br><span class="line">            capsize=<span class="number">0.1</span>, <span class="comment">#error bar 帽子宽度</span></span><br><span class="line">            )</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913539.png" alt="image-20211122185222708" style="zoom:20%;" /></p><h4 id="5-countplot"><a href="#5-countplot" class="headerlink" title="5.countplot"></a>5.countplot</h4><p><code>countplt</code>用来绘制某个字段的数量分布信息.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">sns.<span class="built_in">set</span>(rc=&#123;</span><br><span class="line">        <span class="string">&quot;axes.facecolor&quot;</span>:<span class="string">&quot;#6C6C6C&quot;</span>, <span class="comment">#背景色</span></span><br><span class="line">        <span class="string">&quot;axes.grid&quot;</span>:<span class="literal">False</span>,</span><br><span class="line">        <span class="string">&#x27;xtick.labelsize&#x27;</span>:<span class="number">14</span>,</span><br><span class="line">        <span class="string">&#x27;ytick.labelsize&#x27;</span>:<span class="number">14</span></span><br><span class="line">&#125;)</span><br><span class="line">sns.countplot(x=<span class="string">&#x27;body-style&#x27;</span>,</span><br><span class="line">            data=auto,</span><br><span class="line">            order=auto[<span class="string">&#x27;body-style&#x27;</span>].value_counts().index,</span><br><span class="line">            edgecolor=<span class="string">&#x27;#ffd700&#x27;</span>,</span><br><span class="line">            <span class="comment"># facecolor=(0,0,0,0),</span></span><br><span class="line">            <span class="comment"># edgecolor=sns.color_palette(&quot;dark&quot;,3)</span></span><br><span class="line">            linewidth=<span class="number">1</span>,)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913540.png" alt="image-20211122205909878" style="zoom:20%;" /></p><h4 id="6-catplot"><a href="#6-catplot" class="headerlink" title="6. catplot"></a>6. catplot</h4><p><code>catplot</code>本身是绘制类别分布情况，但是其可以绘制柱状图和<code>violin</code>,这个参数是<code>kind</code>。并且<code>violin</code>可以表示双变量。关于<a href="https://waynestalk.com/python-box-violin-plot/">小提琴图和box图</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sns.<span class="built_in">set</span>(rc=&#123;</span><br><span class="line">        <span class="string">&quot;axes.facecolor&quot;</span>:<span class="string">&quot;#ECFFFF&quot;</span>, <span class="comment">#背景色</span></span><br><span class="line">        <span class="string">&quot;axes.grid&quot;</span>:<span class="literal">False</span>,</span><br><span class="line">        <span class="string">&#x27;xtick.labelsize&#x27;</span>:<span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;ytick.labelsize&#x27;</span>:<span class="number">8</span></span><br><span class="line">&#125;)</span><br><span class="line">sns.catplot(x=<span class="string">&#x27;city-mpg&#x27;</span>, y=<span class="string">&#x27;body-style&#x27;</span>,  data=auto, kind=<span class="string">&#x27;violin&#x27;</span>, height=<span class="number">5</span>, aspect=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913541.png" alt="image-20211122211452472" style="zoom:20%;" /></p><h4 id="7-Dist-plot"><a href="#7-Dist-plot" class="headerlink" title="7.Dist plot"></a>7.Dist plot</h4><p>这部分主要是绘制histogram及核密度分布kernel density estimate(KDE)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sns.<span class="built_in">set</span>(rc=&#123;</span><br><span class="line">        <span class="string">&quot;axes.facecolor&quot;</span>:<span class="string">&quot;#ECFFFF&quot;</span>, <span class="comment">#背景色</span></span><br><span class="line">        <span class="string">&quot;axes.grid&quot;</span>:<span class="literal">False</span>,</span><br><span class="line">        <span class="string">&#x27;xtick.labelsize&#x27;</span>:<span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;ytick.labelsize&#x27;</span>:<span class="number">8</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">#rug就是观测值 图上小柱子</span></span><br><span class="line">sns.distplot(auto[<span class="string">&#x27;engine-size&#x27;</span>], kde=<span class="literal">True</span>, rug=<span class="literal">True</span>, hist=<span class="literal">True</span>, vertical=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913542.png" alt="image-20211122213105405" style="zoom:25%;" /></p><p>我们可以用kdeplot只绘制kde图, <a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html">cmap 查询页</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.kdeplot(auto[<span class="string">&#x27;city-mpg&#x27;</span>], auto[<span class="string">&#x27;engine-size&#x27;</span>], shade=<span class="literal">True</span>, cmap=<span class="string">&#x27;rainbow&#x27;</span>, shade_lowest=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913543.png" alt="image-20211122214202818" style="zoom:20%;" /></p><h4 id="8-heatmap"><a href="#8-heatmap" class="headerlink" title="8. heatmap"></a>8. heatmap</h4><p>一般用到热力图用来看相关性。参数主要有:</p><ul><li><code>vmax</code>：设置颜色带的最大值</li><li><code>vmin</code>：设置颜色带的最小值</li><li><code>center</code>：设置颜色带的分界线</li><li><code>annot=True</code>：小格子里填不填注释</li><li><code>fmt</code>：小格子里数据格式</li><li><code>linewidths</code>:小格子间间隔线宽度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">corr = auto.corr()</span><br><span class="line">sns.heatmap(corr, cmap=<span class="string">&#x27;Reds&#x27;</span>, center=<span class="number">0.5</span>, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, annot=<span class="literal">True</span>, fmt=<span class="string">&quot;.1f&quot;</span>, linewidths=<span class="number">0.1</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.yticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261913544.png" alt="image-20211122220753662" style="zoom:25%;" /></p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://www.kaggle.com/saurav9786/seaborn-tutorial">Seaborn Tutorial</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code snippet </tag>
            
            <tag> seaborn </tag>
            
            <tag> heatmap </tag>
            
            <tag> plot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Kaggle比赛 Optiver Realized Volatility Prediction</title>
      <link href="2021/08/26/Kaggle%20Kaggle%E6%AF%94%E8%B5%9B%20Optiver%20Realized%20Volatility%20Prediction/"/>
      <url>2021/08/26/Kaggle%20Kaggle%E6%AF%94%E8%B5%9B%20Optiver%20Realized%20Volatility%20Prediction/</url>
      
        <content type="html"><![CDATA[<h3 id="2-Kaggle比赛-Optiver-Realized-Volatility-Prediction-1"><a href="#2-Kaggle比赛-Optiver-Realized-Volatility-Prediction-1" class="headerlink" title="2.Kaggle比赛 Optiver Realized Volatility Prediction 1"></a>2.Kaggle比赛 Optiver Realized Volatility Prediction 1</h3><h4 id="1-比赛数据解释"><a href="#1-比赛数据解释" class="headerlink" title="1. 比赛数据解释"></a>1. 比赛数据解释</h4><p>举办方 Optiver 提供了一个notebook来介绍数据和股票交易数据<a href="https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data#Order-book-statistics">Introduction to financial concepts and data</a>。</p><p>这里只介绍给的几个公式：</p><ol><li><strong>BidAskSpread</strong>， 反映该股票流通性，</li></ol><script type="math/tex; mode=display">\text{BidAskSpread} = \text{BestOffer}/\text{BestBid} -1 \tag{1}</script><ol><li><strong>Weighted averaged price</strong>, 表示股票估值，</li></ol><script type="math/tex; mode=display">\text{WAP} = \frac{\text{BidPrice}_{1}*\text{AskSize}_{1} + \text{AskPrice}_{1}*\text{BidSize}_{1}}{\text{BidSize}_{1} + \text{AskSize}_{1}} \tag{2}</script><ol><li><strong>log returns</strong>， 计算该股票某个时间段的差值,</li></ol><script type="math/tex; mode=display">r_{t_1, t_2} = \log \left( \frac{S_{t_2}}{S_{t_1}} \right) \tag{3}</script><p>其中,<script type="math/tex">t_1, t_2</script>代表两个时间点，如看作昨天和今天两个点。</p><ol><li><strong>Realized volatility</strong>，实际波动率,</li></ol><script type="math/tex; mode=display">\sigma = \sqrt{\sum_{t}r_{t-1, t}^2} \tag{4}</script><p><strong>In this competition, you will be given 10 minutes of book data and we ask you to predict what the volatility will be in the following 10 minutes.</strong> </p><p>接下来我会按照 <code>alexioslyon</code>的<a href="https://www.kaggle.com/alexioslyon/lgbm-baseline/notebook">lgbm-baseline</a>来解释下baseline。所有代码，按照其调用顺序解释，这会跟源notebook代码顺序有些差别。</p><p>另外df的展示基本是部分的，特征非常多。</p><h4 id="2-book，trade数据特征工程"><a href="#2-book，trade数据特征工程" class="headerlink" title="2. book，trade数据特征工程"></a>2. book，trade数据特征工程</h4><h5 id="1-读取train-test-data"><a href="#1-读取train-test-data" class="headerlink" title="1. 读取train/test data"></a>1. 读取train/test data</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_train_test_data</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取数据&quot;&quot;&quot;</span></span><br><span class="line">    train = pd.read_csv(data_dir + <span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">    test = pd.read_csv(data_dir + <span class="string">&#x27;test.csv&#x27;</span>)</span><br><span class="line">    <span class="comment">#构建新的特征列row_id</span></span><br><span class="line">    train[<span class="string">&#x27;row_id&#x27;</span>] = train[<span class="string">&#x27;stock_id&#x27;</span>].astype(<span class="built_in">str</span>) + <span class="string">&#x27;-&#x27;</span> + train[<span class="string">&#x27;time_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    test[<span class="string">&#x27;row_id&#x27;</span>] = test[<span class="string">&#x27;stock_id&#x27;</span>].astype(<span class="built_in">str</span>) + <span class="string">&#x27;-&#x27;</span> + test[<span class="string">&#x27;time_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Out training set has <span class="subst">&#123;train.shape[<span class="number">0</span>]&#125;</span> rows&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> train, test</span><br></pre></td></tr></table></figure><p>这里主要就读取train, test，和构建新的<code>row_id</code>.形成如下df.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823148.png" alt="image-20210902122324707" style="zoom: 50%;" /></p><p>现在我们可以通过 <code>train_stock_id = train[&#39;stock_id&#39;].unique()</code>得到唯一的 <code>stock_id</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823150.png" alt="image-20210902122505596" style="zoom:50%;" /></p><h5 id="2-预处理核心方法"><a href="#2-预处理核心方法" class="headerlink" title="2. 预处理核心方法"></a>2. 预处理核心方法</h5><p>像如下调用中，使用下面 <code>preprocessor(list_stock_ids, is_train=True)</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_stock_id = train[<span class="string">&#x27;stock_id&#x27;</span>].unique()</span><br><span class="line">train_ = preprocessor(train_stock_id, is_train=<span class="literal">True</span>)</span><br><span class="line">train = train.merge(train_, on=[<span class="string">&#x27;row_id&#x27;</span>], how=<span class="string">&#x27;left&#x27;</span>)</span><br></pre></td></tr></table></figure><p><code>file_path_book</code>对应路径如下图:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823151.png" alt="image-20210902120102383" style="zoom:50%;" /></p><p>现在我们只读到如下 13行<code>book_preprocessor(file_path_book), trade_preprocessor(file_path_trade)</code>这部分代码。现在要看 <code>book_preprocessor</code>这些函数具体处理细节了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessor</span>(<span class="params">list_stock_ids, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="comment">#并行化处理stock</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">for_joblib</span>(<span class="params">stock_id</span>):</span></span><br><span class="line">        <span class="keyword">if</span> is_train:</span><br><span class="line">            <span class="comment">#训练数据路径</span></span><br><span class="line">            file_path_book = data_dir + <span class="string">r&#x27;book_train.parquet/stock_id=&#x27;</span> + <span class="built_in">str</span>(stock_id)</span><br><span class="line">            file_path_trade = data_dir + <span class="string">r&#x27;trade_train.parquet/stock_id=&#x27;</span> + <span class="built_in">str</span>(stock_id)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            file_path_book = data_dir + <span class="string">r&#x27;book_test.parquet/stock_id=&#x27;</span> + <span class="built_in">str</span>(stock_id)</span><br><span class="line">            file_path_trade = data_dir + <span class="string">r&#x27;trade_test.parquet/stock_id=&#x27;</span> + <span class="built_in">str</span>(stock_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#预处理book和trade数据并merge</span></span><br><span class="line">        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on=<span class="string">&#x27;row_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> df_tmp</span><br><span class="line"></span><br><span class="line">    <span class="comment">#并行计算for_joblib stock_id看作输入变量</span></span><br><span class="line">    df = Parallel(n_jobs=-<span class="number">1</span>, verbose=<span class="number">1</span>)(delayed(for_joblib)(stock_id) <span class="keyword">for</span> stock_id <span class="keyword">in</span> list_stock_ids)</span><br><span class="line">    df = pd.concat(df, ignore_index=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h5 id="3-book-preprocessor逐行分析"><a href="#3-book-preprocessor逐行分析" class="headerlink" title="3. book_preprocessor逐行分析"></a>3. book_preprocessor逐行分析</h5><p>在 <code>book_preprocessor</code>中读取得到df如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = pd.read_parquet(<span class="string">&#x27;/data/book_train.parquet/stock_id=0&#x27;</span>)</span><br><span class="line">df1.head() </span><br></pre></td></tr></table></figure></p><p>对应着类似第6行中 <code>file_path_book</code>。我们看看在<code>book_preprocessor</code>未处理的df。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823153.png" alt="image-20210902122946901" style="zoom:50%;" /></p><p>如上图，我们看到 <code>book_train.parquet/stock_id=0</code>得到的df有  <code>&#39;time_id&#39;</code>, <code>&#39;ask_price1&#39;</code> 等等特征列。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">book_preprocessor</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;book数据预处理:</span></span><br><span class="line"><span class="string">    处理得到各个特征</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    df = pd.read_parquet(file_path)</span><br><span class="line">    df[<span class="string">&#x27;wap1&#x27;</span>] = calc_wap1(df)</span><br><span class="line">    df[<span class="string">&#x27;wap2&#x27;</span>] = calc_wap2(df)</span><br><span class="line">    df[<span class="string">&#x27;wap3&#x27;</span>] = calc_wap3(df)</span><br><span class="line">    df[<span class="string">&#x27;wap4&#x27;</span>] = calc_wap4(df)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算log 返回值, 即按照time_id来计算wap1的log return</span></span><br><span class="line">    df[<span class="string">&#x27;log_return1&#x27;</span>] = df.groupby([<span class="string">&#x27;time_id&#x27;</span>])[<span class="string">&#x27;wap1&#x27;</span>].apply(log_return)</span><br><span class="line">    df[<span class="string">&#x27;log_return2&#x27;</span>] = df.groupby([<span class="string">&#x27;time_id&#x27;</span>])[<span class="string">&#x27;wap2&#x27;</span>].apply(log_return)</span><br><span class="line">    df[<span class="string">&#x27;log_return3&#x27;</span>] = df.groupby([<span class="string">&#x27;time_id&#x27;</span>])[<span class="string">&#x27;wap3&#x27;</span>].apply(log_return)</span><br><span class="line">    df[<span class="string">&#x27;log_return4&#x27;</span>] = df.groupby([<span class="string">&#x27;time_id&#x27;</span>])[<span class="string">&#x27;wap4&#x27;</span>].apply(log_return)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算wap balance 资产权重平均价格</span></span><br><span class="line">    df[<span class="string">&#x27;wap_balance&#x27;</span>] = <span class="built_in">abs</span>(df[<span class="string">&#x27;wap1&#x27;</span>] - df[<span class="string">&#x27;wap2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#price_spread = 2(ask_p1 - bid_p2)/(ask_p2+bid_p2)</span></span><br><span class="line">    df[<span class="string">&#x27;price_spread&#x27;</span>] = (df[<span class="string">&#x27;ask_price1&#x27;</span>] - df[<span class="string">&#x27;bid_price1&#x27;</span>]) / ((df[<span class="string">&#x27;ask_price1&#x27;</span>] + df[<span class="string">&#x27;bid_price1&#x27;</span>]) / <span class="number">2</span>)</span><br><span class="line">    df[<span class="string">&#x27;price_spread2&#x27;</span>] = (df[<span class="string">&#x27;ask_price2&#x27;</span>] - df[<span class="string">&#x27;bid_price2&#x27;</span>]) / ((df[<span class="string">&#x27;ask_price2&#x27;</span>] + df[<span class="string">&#x27;bid_price2&#x27;</span>]) / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#bid ask 波动 price1 - price2</span></span><br><span class="line">    df[<span class="string">&#x27;bid_spread&#x27;</span>] = df[<span class="string">&#x27;bid_price1&#x27;</span>] - df[<span class="string">&#x27;bid_price2&#x27;</span>]</span><br><span class="line">    df[<span class="string">&#x27;ask_spread&#x27;</span>] = df[<span class="string">&#x27;ask_price1&#x27;</span>] - df[<span class="string">&#x27;ask_price2&#x27;</span>]</span><br><span class="line">    df[<span class="string">&#x27;bid_ask_spread&#x27;</span>] = <span class="built_in">abs</span>(df[<span class="string">&#x27;bid_spread&#x27;</span>] - df[<span class="string">&#x27;ask_spread&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#总交易量 ask/bid size1/2</span></span><br><span class="line">    df[<span class="string">&#x27;total_volume&#x27;</span>] = (df[<span class="string">&#x27;ask_size1&#x27;</span>] + df[<span class="string">&#x27;ask_size2&#x27;</span>]) + (df[<span class="string">&#x27;bid_size1&#x27;</span>] + df[<span class="string">&#x27;bid_size2&#x27;</span>])</span><br><span class="line">    df[<span class="string">&#x27;volume_imbalance&#x27;</span>] = <span class="built_in">abs</span>((df[<span class="string">&#x27;ask_size1&#x27;</span>] + df[<span class="string">&#x27;ask_size2&#x27;</span>]) - (df[<span class="string">&#x27;bid_size1&#x27;</span>] + df[<span class="string">&#x27;bid_size2&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#构建特征字典 都以其sum std等作为特征</span></span><br><span class="line">    create_feature_dict = &#123;</span><br><span class="line">        <span class="string">&#x27;wap1&#x27;</span>:[np.<span class="built_in">sum</span>, np.std],</span><br><span class="line">        <span class="string">&#x27;wap2&#x27;</span>:[np.<span class="built_in">sum</span>, np.std],</span><br><span class="line">        <span class="string">&#x27;wap3&#x27;</span>:[np.<span class="built_in">sum</span>, np.std],</span><br><span class="line">        <span class="string">&#x27;wap4&#x27;</span>:[np.<span class="built_in">sum</span>, np.std],</span><br><span class="line">        <span class="string">&#x27;log_return1&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return2&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return3&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return4&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;wap_balance&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;price_spread&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;price_spread2&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;bid_spread&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;ask_spread&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;total_volume&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;volume_imbalance&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;bid_ask_spread&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#时刻特征字典</span></span><br><span class="line">    create_feature_dict_time =&#123;</span><br><span class="line">        <span class="string">&#x27;log_return1&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return2&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return3&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;log_return4&#x27;</span>: [realized_volatility],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stats_window</span>(<span class="params">fe_dict, seconds_in_bucket, add_suffix=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;按时间段以及time_id聚合特征,bucket:时间段&quot;&quot;&quot;</span></span><br><span class="line">        df_feature = df[df[<span class="string">&#x27;seconds_in_bucket&#x27;</span>] &gt;= seconds_in_bucket]\</span><br><span class="line">            .groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(fe_dict).reset_index()</span><br><span class="line"></span><br><span class="line">        df_feature.columns =[<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df_feature.columns]<span class="comment">#将所有特征列连起来</span></span><br><span class="line">        <span class="keyword">if</span> add_suffix:</span><br><span class="line">            df_feature = df_feature.add_suffix(<span class="string">&#x27;_&#x27;</span> + <span class="built_in">str</span>(seconds_in_bucket))</span><br><span class="line">        <span class="keyword">return</span> df_feature</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取不同时间窗口的统计信息</span></span><br><span class="line">    df_feature = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">0</span>, add_suffix=<span class="literal">False</span>)</span><br><span class="line">    df_feature_500 = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">500</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_400 = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">400</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_300 = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">300</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_200 = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">200</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_100 = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">100</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#合并不同时间特征</span></span><br><span class="line">    df_feature = df_feature.merge(df_feature_500, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__500&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_400, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__400&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_300, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__300&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_200, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__200&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_100, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__100&#x27;</span>)</span><br><span class="line">    <span class="comment">#丢掉合并中产生的不需要的特征列</span></span><br><span class="line">    df_feature.drop([<span class="string">&#x27;time_id__500&#x27;</span>, <span class="string">&#x27;time_id__400&#x27;</span>, <span class="string">&#x27;time_id__300&#x27;</span>, <span class="string">&#x27;time_id__200&#x27;</span>, <span class="string">&#x27;time_id__100&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    stock_id = file_path.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">#&#x27;row_id&#x27;格式改变为stock_id-[time_id_]值这种形式</span></span><br><span class="line">    df_feature[<span class="string">&#x27;row_id&#x27;</span>] = df_feature[<span class="string">&#x27;time_id_&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">f&#x27;<span class="subst">&#123;stock_id&#125;</span>-<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment">#丢掉&#x27;time_id_&#x27;</span></span><br><span class="line">    df_feature.drop([<span class="string">&#x27;time_id_&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df_feature</span><br></pre></td></tr></table></figure></p><p>我们逐一来看特征构建过程：</p><ol><li><code>df[&#39;wap1&#39;]</code>这类就是计算wap，都利用的是公式2。对应着如下4个计算函数(这里贴出所有特征处理函数)：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_wap1</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Weighted averaged price权重平均买卖价1&quot;&quot;&quot;</span></span><br><span class="line">    wap1 = (df[<span class="string">&#x27;bid_price1&#x27;</span>] * df[<span class="string">&#x27;ask_size1&#x27;</span>] + df[<span class="string">&#x27;ask_price1&#x27;</span>] * df[<span class="string">&#x27;bid_size1&#x27;</span>]) \</span><br><span class="line">            / (df[<span class="string">&#x27;bid_size1&#x27;</span>] + df[<span class="string">&#x27;ask_size1&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> wap1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_wap2</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;权重平均买卖价2&quot;&quot;&quot;</span></span><br><span class="line">    wap2 = (df[<span class="string">&#x27;bid_price2&#x27;</span>] * df[<span class="string">&#x27;ask_size2&#x27;</span>] + df[<span class="string">&#x27;ask_price2&#x27;</span>] * df[<span class="string">&#x27;bid_size2&#x27;</span>]) \</span><br><span class="line">            / (df[<span class="string">&#x27;bid_size2&#x27;</span>] + df[<span class="string">&#x27;ask_size2&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> wap2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_wap3</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;权重平均买卖价2&quot;&quot;&quot;</span></span><br><span class="line">    wap3 = (df[<span class="string">&#x27;bid_price1&#x27;</span>] * df[<span class="string">&#x27;bid_size1&#x27;</span>] + df[<span class="string">&#x27;ask_price1&#x27;</span>] * df[<span class="string">&#x27;ask_size1&#x27;</span>]) \</span><br><span class="line">            / (df[<span class="string">&#x27;bid_size1&#x27;</span>] + df[<span class="string">&#x27;ask_size1&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> wap3</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_wap4</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;权重平均买卖价2&quot;&quot;&quot;</span></span><br><span class="line">    wap4 = (df[<span class="string">&#x27;bid_price2&#x27;</span>] * df[<span class="string">&#x27;bid_size2&#x27;</span>] + df[<span class="string">&#x27;ask_price2&#x27;</span>] * df[<span class="string">&#x27;ask_size2&#x27;</span>]) \</span><br><span class="line">            / (df[<span class="string">&#x27;bid_size2&#x27;</span>] + df[<span class="string">&#x27;ask_size2&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> wap4</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_return</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将输入变为log后对于某一axis寻找差值 log(x/y) = log(x) - log(y)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.log(series).diff()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">realized_volatility</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算实际波动率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>(series**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_unique</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算series中unique数目&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(np.unique(series))</span><br></pre></td></tr></table></figure><p>实际上如上面df1计算 <code>calc_wap1(df1)</code>后得到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>         <span class="number">1.001434</span></span><br><span class="line"><span class="number">1</span>         <span class="number">1.001448</span></span><br><span class="line"><span class="number">2</span>         <span class="number">1.001448</span></span><br><span class="line">            ...   </span><br><span class="line"><span class="number">917552</span>    <span class="number">0.998517</span></span><br><span class="line">Length: <span class="number">917553</span>, dtype: float32</span><br></pre></td></tr></table></figure><p>添加四个特征后：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823154.png" alt="image-20210902162425023" style="zoom:50%;" /></p><ol><li><code>df[&#39;log_return1&#39;]</code>这类特征也是4个，计算利用的公式3.对上一步得到的特征按 <code>&#39;time_id&#39;</code>分组后应用下面的 <code>log_return</code>计算 <code>&#39;wap1&#39;</code>。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_return</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将输入变为log后对于某一axis寻找差值 log(x/y) = log(x) - log(y)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.log(series).diff()</span><br></pre></td></tr></table></figure><p>如df1计算第一个后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df1.groupby([<span class="string">&#x27;time_id&#x27;</span>])[<span class="string">&#x27;wap1&#x27;</span>].apply(log_return)</span><br><span class="line">====================================================</span><br><span class="line"><span class="number">0</span>              NaN</span><br><span class="line"><span class="number">1</span>         <span class="number">0.000014</span></span><br><span class="line"><span class="number">2</span>         <span class="number">0.000000</span></span><br><span class="line"><span class="number">3</span>        -<span class="number">0.000005</span></span><br><span class="line">            ...   </span><br><span class="line"><span class="number">917551</span>    <span class="number">0.000001</span></span><br><span class="line"><span class="number">917552</span>    <span class="number">0.000000</span></span><br><span class="line">Name: wap1, Length: <span class="number">917553</span>, dtype: float32</span><br></pre></td></tr></table></figure><p>这时df1又会得到四个特征<code>log_return</code></p><p>在后面代码中我们还会对这4个特征求<code>[realized_volatility]</code>来构建特征（利用公式4）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">realized_volatility</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算实际波动率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>(series**<span class="number">2</span>))</span><br></pre></td></tr></table></figure><ol><li>剩下的如 <code>&#39;wap_balance&#39;</code>、<code>&#39;price_spread&#39;</code>等8个特征也如源码计算。这样处理后，df1.head()后面特征如下：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823155.png" alt="image-20210902163229024" style="zoom:50%;" /></p><ol><li>接下来构建统计信息的特征字典， <code>np.sum, np.std</code>. 注意与后面一个以时间构建实际波动率的字典<code>create_feature_dict_time</code>区分。</li></ol><ol><li>然后是<code>get_stats_window(*fe_dict*, *seconds_in_bucket*, *add_suffix*=False)</code>方法会将4中处理得到的特征字典作为 <code>fe_dict</code>，按照 <code>&#39;seconds_in_bucket&#39;</code>区间大于0， 500…时间段内合计对应 <code>fe_dict</code>信息后 <code>reset_index()</code>。<br>实际上像 <code>df[&#39;seconds_in_bucket&#39;] &gt;= seconds_in_bucket]</code>如果 <code>seconds_in_bucket=0</code>，那么就是选取df1中<code>df[&#39;seconds_in_bucket&#39;]&gt;0</code>部分来按照 <code>create_feature_dict</code>agg各特征的统计信息如：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823156.png" alt="image-20210902180440815" style="zoom:50%;" /></p><p>将上面列名整理下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df1_feature.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df1_feature.columns]</span><br><span class="line">df1_feature.columns</span><br><span class="line">====================================================</span><br><span class="line">[<span class="string">&#x27;wap1_sum&#x27;</span>, <span class="string">&#x27;wap1_std&#x27;</span>, <span class="string">&#x27;wap2_sum&#x27;</span>, <span class="string">&#x27;wap2_std&#x27;</span>, <span class="string">&#x27;wap3_sum&#x27;</span>, <span class="string">&#x27;wap3_std&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;wap4_sum&#x27;</span>, <span class="string">&#x27;wap4_std&#x27;</span>, <span class="string">&#x27;log_return1_realized_volatility&#x27;</span>,]</span><br></pre></td></tr></table></figure><p>再加上需要后缀就是 <code>get_stats_window</code>作用了。</p><p>如果是 <code>df1[&#39;seconds_in_bucket&#39;]&gt;500</code> 那么使用 <code>create_feature_dict_time</code>构建特征。一样操作，形成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1_feature = df1[df1[<span class="string">&#x27;seconds_in_bucket&#x27;</span>]&gt;<span class="number">500</span>].groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(create_feature_dict_time).head()</span><br><span class="line">df1_feature</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823157.png" alt="image-20210902182051187" style="zoom: 40%;" /></p><p>最后形成时间段波动率如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1_feature_500 = get_stats_window(create_feature_dict_time, <span class="number">500</span>, <span class="literal">True</span>)</span><br><span class="line">df1_feature_500.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823158.png" alt="image-20210902182406120" style="zoom:40%;" /></p><ol><li>接下来就是简单的合并特征丢掉不需要的特征和构建新 <code>&#39;row_id&#39;</code>，最后返回整个特征df。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1_feature = df1_feature.merge(df1_feature_500, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__500&#x27;</span>)</span><br><span class="line">df1_feature.head()</span><br></pre></td></tr></table></figure><p>就以<code>df1_feature</code>有的行拼接 <code>df1_feature_500</code>, 左边以 <code>&#39;time_id_&#39;</code>为index， 右边以 <code>&#39;time_id__500&#39;</code>，逐一合并。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823159.png" alt="image-20210902213721906" style="zoom: 50%;" /></p><p>特征列完整后接：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823160.png" alt="image-20210902213904689" style="zoom:40%;" /></p><p>接下来就是踢掉不必要的列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1_feature.drop([<span class="string">&#x27;time_id__500&#x27;</span>, <span class="string">&#x27;time_id__400&#x27;</span>, <span class="string">&#x27;time_id__300&#x27;</span>, <span class="string">&#x27;time_id__200&#x27;</span>, <span class="string">&#x27;time_id__100&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df1_feature.head()</span><br></pre></td></tr></table></figure><p>至于获取<code>stock_id</code>, 就很简单了，如 <code>stock_id = str(&quot;/content/work/data/book_train.parquet/stock_id=0&quot;).split(&#39;=&#39;)[1]</code>，这里就是0.</p><p>那么，<code>df1_feature[&#39;row_id&#39;]</code>应该是这个样子的 <code>0-5， 0-11</code>,就是<code>stock_id-time_id_</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1_feature[<span class="string">&#x27;row_id&#x27;</span>] = df1_feature[<span class="string">&#x27;time_id_&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">f&#x27;<span class="subst">&#123;stock_id&#125;</span>-<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">df1_feature.head()</span><br></pre></td></tr></table></figure><p>最后返回的特征df尾部就像这样:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823161.png" alt="image-20210902215049442" style="zoom:50%;" /></p><h5 id="4-trade-preprocessor逐行"><a href="#4-trade-preprocessor逐行" class="headerlink" title="4. trade_preprocessor逐行"></a>4. trade_preprocessor逐行</h5><p>第一句同样是 <code>df = pd.read_parquet(file_path)</code>，<code>test</code>只有<code>stock_id=0</code>我们看看其信息，比较下跟book的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2 = pd.read_parquet(<span class="string">&#x27;/content/work/data/trade_train.parquet/stock_id=0&#x27;</span>)</span><br><span class="line">df2.head() </span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823162.png" alt="image-20210902215322350" style="zoom:40%;" /></p><p>下面是整个 <code>trade_preprocessor</code>，后面会逐行解释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trade_preprocessor</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    df = pd.read_parquet(file_path)</span><br><span class="line">    df[<span class="string">&#x27;log_return&#x27;</span>] = df.groupby(<span class="string">&#x27;time_id&#x27;</span>)[<span class="string">&#x27;price&#x27;</span>].apply(log_return)</span><br><span class="line">    df[<span class="string">&#x27;amount&#x27;</span>] = df[<span class="string">&#x27;price&#x27;</span>] * df[<span class="string">&#x27;size&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#dict for aggregations</span></span><br><span class="line">    create_feature_dict = &#123;</span><br><span class="line">        <span class="string">&#x27;log_return&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;seconds_in_bucket&#x27;</span>: [count_unique],</span><br><span class="line">        <span class="string">&#x27;size&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>, np.<span class="built_in">min</span>],</span><br><span class="line">        <span class="string">&#x27;order_count&#x27;</span>: [np.<span class="built_in">sum</span>, np.<span class="built_in">max</span>],</span><br><span class="line">        <span class="string">&#x27;amount&#x27;</span>: [np.<span class="built_in">max</span>, np.<span class="built_in">max</span>, np.<span class="built_in">min</span>],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    create_feature_dict_time = &#123;</span><br><span class="line">        <span class="string">&#x27;log_return&#x27;</span>: [realized_volatility],</span><br><span class="line">        <span class="string">&#x27;seconds_in_bucket&#x27;</span>: [count_unique],</span><br><span class="line">        <span class="string">&#x27;size&#x27;</span>: [np.<span class="built_in">sum</span>],</span><br><span class="line">        <span class="string">&#x27;order_count&#x27;</span>: [np.<span class="built_in">sum</span>],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_stats_window</span>(<span class="params">fe_dict, seconds_in_bucket, add_suffix=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;按时间段以及time_id聚合特征,bucket:时间段&quot;&quot;&quot;</span></span><br><span class="line">        df_feature = df[df[<span class="string">&#x27;seconds_in_bucket&#x27;</span>] &gt;= seconds_in_bucket] \</span><br><span class="line">            .groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(fe_dict).reset_index()</span><br><span class="line"></span><br><span class="line">        df_feature.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df_feature.columns]  <span class="comment"># 将所有特征列连起来</span></span><br><span class="line">        <span class="keyword">if</span> add_suffix:</span><br><span class="line">            df_feature = df_feature.add_suffix(<span class="string">&#x27;_&#x27;</span> + <span class="built_in">str</span>(seconds_in_bucket))</span><br><span class="line">        <span class="keyword">return</span> df_feature</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取不同时间窗口的统计信息</span></span><br><span class="line">    df_feature = get_stats_window(create_feature_dict, seconds_in_bucket=<span class="number">0</span>, add_suffix=<span class="literal">False</span>)</span><br><span class="line">    df_feature_500 = get_stats_window(create_feature_dict_time, seconds_in_bucket=<span class="number">500</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_400 = get_stats_window(create_feature_dict_time, seconds_in_bucket=<span class="number">400</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_300 = get_stats_window(create_feature_dict_time, seconds_in_bucket=<span class="number">300</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_200 = get_stats_window(create_feature_dict_time, seconds_in_bucket=<span class="number">200</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line">    df_feature_100 = get_stats_window(create_feature_dict_time, seconds_in_bucket=<span class="number">100</span>, add_suffix=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tendency</span>(<span class="params">price, vol</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;趋势计算&quot;&quot;&quot;</span></span><br><span class="line">        df_diff = np.diff(price) <span class="comment">#价差</span></span><br><span class="line">        val = (df_diff / price[<span class="number">1</span>:]) * <span class="number">100</span></span><br><span class="line">        power = np.<span class="built_in">sum</span>(val * vol[<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> (power)</span><br><span class="line"></span><br><span class="line">    lis = []</span><br><span class="line">    <span class="keyword">for</span> n_time_id <span class="keyword">in</span> df[<span class="string">&#x27;time_id&#x27;</span>].unique():</span><br><span class="line">        <span class="comment">#按唯一time_id计算tendency..</span></span><br><span class="line">        df_id = df[df[<span class="string">&#x27;time_id&#x27;</span>]==n_time_id]</span><br><span class="line"></span><br><span class="line">        tendencyV = tendency(df_id[<span class="string">&#x27;price&#x27;</span>].values, df_id[<span class="string">&#x27;size&#x27;</span>].values)</span><br><span class="line">        <span class="comment">#计算f_max = price &gt; mean</span></span><br><span class="line">        f_max = np.<span class="built_in">sum</span>(df_id[<span class="string">&#x27;price&#x27;</span>].values &gt; np.mean(df_id[<span class="string">&#x27;price&#x27;</span>].values))</span><br><span class="line">        f_min = np.<span class="built_in">sum</span>(df_id[<span class="string">&#x27;price&#x27;</span>].values &lt; np.mean(df_id[<span class="string">&#x27;price&#x27;</span>].values))</span><br><span class="line"></span><br><span class="line">        df_max = np.<span class="built_in">sum</span>(np.diff(df_id[<span class="string">&#x27;price&#x27;</span>].values) &gt; <span class="number">0</span>)</span><br><span class="line">        df_min = np.<span class="built_in">sum</span>(np.diff(df_id[<span class="string">&#x27;price&#x27;</span>].values) &lt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#新构建特征 # abs(&#x27;price&#x27;-mean(&#x27;price&#x27;))</span></span><br><span class="line">        abs_diff = np.median(np.<span class="built_in">abs</span>(df_id[<span class="string">&#x27;price&#x27;</span>].values - np.mean(df_id[<span class="string">&#x27;price&#x27;</span>].values)))</span><br><span class="line">        energy = np.mean(df_id[<span class="string">&#x27;price&#x27;</span>].values**<span class="number">2</span>)</span><br><span class="line">        <span class="comment">#&#x27;price&#x27;值的75分位 - 25分位</span></span><br><span class="line">        iqr_p = np.percentile(df_id[<span class="string">&#x27;price&#x27;</span>].values, <span class="number">75</span>) - np.percentile(df_id[<span class="string">&#x27;price&#x27;</span>].values, <span class="number">25</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#vol var 数量变化特征构建</span></span><br><span class="line">        abs_diff_v = np.median(np.<span class="built_in">abs</span>(df_id[<span class="string">&#x27;size&#x27;</span>].values - np.mean(df_id[<span class="string">&#x27;size&#x27;</span>].values)))</span><br><span class="line">        energy_v = np.<span class="built_in">sum</span>(df_id[<span class="string">&#x27;size&#x27;</span>].values ** <span class="number">2</span>)</span><br><span class="line">        iqr_p_v = np.percentile(df_id[<span class="string">&#x27;size&#x27;</span>].values, <span class="number">75</span>) - np.percentile(df_id[<span class="string">&#x27;size&#x27;</span>].values, <span class="number">25</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#将上面构建的特征形成新的字典添加到lis</span></span><br><span class="line">        lis.append(&#123;<span class="string">&#x27;time_id&#x27;</span>: n_time_id, <span class="string">&#x27;tendency&#x27;</span>: tendencyV, <span class="string">&#x27;f_max&#x27;</span>: f_max,\</span><br><span class="line">                    <span class="string">&#x27;f_min&#x27;</span>: f_min, <span class="string">&#x27;df_max&#x27;</span>: df_max, <span class="string">&#x27;df_min&#x27;</span>: df_min,\</span><br><span class="line">                    <span class="string">&#x27;abs_diff&#x27;</span>: abs_diff, <span class="string">&#x27;energy&#x27;</span>: energy, <span class="string">&#x27;iqr_p&#x27;</span>: iqr_p,\</span><br><span class="line">                    <span class="string">&#x27;abs_diff_v&#x27;</span>: abs_diff_v, <span class="string">&#x27;energy_v&#x27;</span>: energy_v, <span class="string">&#x27;iqr_p_v&#x27;</span>: iqr_p_v&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将按时间id构建所有特征列表赋值给df_lr df</span></span><br><span class="line">    df_lr = pd.DataFrame(lis)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#跟df_lr合并</span></span><br><span class="line">    df_feature = df_feature.merge(df_lr, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id&#x27;</span>)</span><br><span class="line">    <span class="comment">#合并所有特征</span></span><br><span class="line">    df_feature = df_feature.merge(df_feature_500, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__500&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_400, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__400&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_300, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__300&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_200, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__200&#x27;</span>)</span><br><span class="line">    df_feature = df_feature.merge(df_feature_100, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id__100&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    df_feature.drop([<span class="string">&#x27;time_id__500&#x27;</span>, <span class="string">&#x27;time_id__400&#x27;</span>, <span class="string">&#x27;time_id__300&#x27;</span>, <span class="string">&#x27;time_id__200&#x27;</span>,\</span><br><span class="line">                     <span class="string">&#x27;time_id&#x27;</span>, <span class="string">&#x27;time_id__100&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#添加前缀</span></span><br><span class="line">    df_feature = df_feature.add_prefix(<span class="string">&#x27;trade_&#x27;</span>)</span><br><span class="line">    stock_id = file_path.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># &#x27;row_id&#x27;格式改变为stock_id-[trade_time_id_]值这种形式</span></span><br><span class="line">    df_feature[<span class="string">&#x27;row_id&#x27;</span>] = df_feature[<span class="string">&#x27;trade_time_id_&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="string">f&#x27;<span class="subst">&#123;stock_id&#125;</span>-<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 丢掉&#x27;trade_time_id_&#x27;</span></span><br><span class="line">    df_feature.drop([<span class="string">&#x27;trade_time_id_&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df_feature</span><br></pre></td></tr></table></figure><ol><li>先开始构建两个新特征:   <code>&#39;log_return&#39;</code>按<code>time_id</code>计算价格的<code>log_return</code>,<code>&#39;amount&#39;</code>.这时，df如下所示：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823163.png" alt="image-20210902220526461" style="zoom:40%;" /></p><ol><li>跟<code>book_preprocessor</code>类似地构建两个特征字典：<code>create_feature_dict</code>， <code>create_feature_dict_time</code>.其中第二个字典中 <code>count_unique</code>计算方法如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_unique</span>(<span class="params">series</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算series中unique数目&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(np.unique(series))</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2_feature = df2[df2[<span class="string">&#x27;seconds_in_bucket&#x27;</span>] &gt;= <span class="number">0</span>].groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(create_feature_dict).reset_index()</span><br><span class="line">df2_feature.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823164.png" alt="image-20210903153110099" style="zoom:40%;" /><br>然后就是连接特征列名字和添加<code>_seconds_in_bucket</code>后缀。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df2_feature.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df2_feature.columns]</span><br><span class="line"><span class="comment">#df2_feature = df2_feature.add_suffix(&#x27;_&#x27; + str(0)) </span></span><br><span class="line">df2_feature.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823165.png" alt="image-20210903154200983" style="zoom:40%;" /></p><ol><li>接下来也跟<code>book_preprocessor</code>中一样构建窗口特征信息。如：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2_feature_500 = get_stats_window(create_feature_dict_time, <span class="number">500</span>, <span class="literal">True</span>)</span><br><span class="line">df2_feature_500.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823166.png" alt="image-20210903154459440" style="zoom:40%;" /></p><ol><li>用价格和数量计算 <code>tendency(price, vol)</code>。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tendency</span>(<span class="params">price, vol</span>):</span>    </span><br><span class="line">        df_diff = np.diff(price)</span><br><span class="line">        val = (df_diff/price[<span class="number">1</span>:])*<span class="number">100</span></span><br><span class="line">        power = np.<span class="built_in">sum</span>(val*vol[<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span>(power)</span><br></pre></td></tr></table></figure><p>  就是求价格变化百分比乘以数量累加值。</p><ol><li>按时间id计算trade特征，如 <code>tendencyV</code>等，作为字典都添加到lis列表中。然后得到 <code>df_lr = pd.DataFrame(lis)</code>这个新的df。<br>具体来说，如 <code>df_id</code>一个特例信息如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[i <span class="keyword">for</span> i <span class="keyword">in</span> df2[<span class="string">&#x27;time_id&#x27;</span>].unique()][:<span class="number">5</span>]</span><br><span class="line">===========================================</span><br><span class="line">[<span class="number">5</span>, <span class="number">11</span>, <span class="number">16</span>, <span class="number">31</span>, <span class="number">62</span>]</span><br><span class="line"></span><br><span class="line">df2_id = df2[df2[<span class="string">&#x27;time_id&#x27;</span>] == <span class="number">5</span>]</span><br><span class="line">df2_id.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823167.png" alt="image-20210903161530862" style="zoom:40%;" /></p><p><code>tendencyV = tendency(df_id[&#39;price&#39;].values, df_id[&#39;size&#39;].values)</code>这个特征构建中，就是取<code>price, size</code>的值，经过<code>tendency</code>方法计算与前一个 <code>seconds_in_bucket</code>差值后得到其差值百分比后乘以对应数量，最后得到变化总和。其它特征也类似构建，最后形成<code>lis</code>列表，转换成新特征 <code>df_lr</code>。</p><p>你可以将整个这部分转换为一个方法，df作为输入，看看<code>df_lr</code>具体情况:（为了简洁将中间部分省去了。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lis_df_lr</span>(<span class="params">df</span>):</span></span><br><span class="line">    lis = []</span><br><span class="line">    <span class="keyword">for</span> n_time_id <span class="keyword">in</span> df[<span class="string">&#x27;time_id&#x27;</span>].unique():    </span><br><span class="line">    ...</span><br><span class="line">    df_lr = pd.DataFrame(lis)</span><br><span class="line">    <span class="keyword">return</span> df_lr</span><br><span class="line">df2_lr = lis_df_lr(df2)</span><br><span class="line">df2_lr.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823168.png" alt="image-20210903163049610" style="zoom:40%;" /></p><ol><li>先跟<code>df_lr</code>合并再合并所有窗口特征信息 <code>df_feature_500</code>…。<br> 后面就非常类似book数据处理了，只贴个处理后的部分特征信息：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2_feature = df2_feature.merge(df2_lr, how=<span class="string">&#x27;left&#x27;</span>, left_on=<span class="string">&#x27;time_id_&#x27;</span>, right_on=<span class="string">&#x27;time_id&#x27;</span>)</span><br><span class="line">df2_feature.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823169.png" alt="image-20210903163600676" style="zoom:40%;" /></p><ol><li>最后就是特征添加前缀 <code>&#39;trade_&#39;</code>, 构建新的 <code>&#39;row_id&#39;</code>,最后返回<code>df_feture</code>.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df2_feature = df2_feature.add_prefix(<span class="string">&#x27;trade_&#x27;</span>)</span><br><span class="line">df2_feature[<span class="string">&#x27;row_id&#x27;</span>] = df2_feature[<span class="string">&#x27;trade_time_id_&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(<span class="number">0</span>)&#125;</span>-<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line">df2_feature.drop([<span class="string">&#x27;trade_time_id_&#x27;</span>], axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">df2_feature.head()</span><br></pre></td></tr></table></figure><p>   最后特征df部分信息如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823170.png" alt="image-20210903164231324" style="zoom:40%;" /></p><h5 id="5-get-time-stock"><a href="#5-get-time-stock" class="headerlink" title="5.get_time_stock"></a>5.get_time_stock</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_stock</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    df中实际波动率realized_volatility相关列分别按</span></span><br><span class="line"><span class="string">    stock_id/time_id分组后聚合求统计指标</span></span><br><span class="line"><span class="string">   然后跟原df合并</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    vol_cols = [<span class="string">&#x27;log_return1_realized_volatility&#x27;</span>, <span class="string">&#x27;log_return2_realized_volatility&#x27;</span>,\</span><br><span class="line">                <span class="string">&#x27;log_return1_realized_volatility_400&#x27;</span>, <span class="string">&#x27;log_return2_realized_volatility_400&#x27;</span>,\</span><br><span class="line">                <span class="string">&#x27;log_return1_realized_volatility_300&#x27;</span>, <span class="string">&#x27;log_return2_realized_volatility_300&#x27;</span>,\</span><br><span class="line">                <span class="string">&#x27;log_return1_realized_volatility_200&#x27;</span>, <span class="string">&#x27;log_return2_realized_volatility_200&#x27;</span>,\</span><br><span class="line">                <span class="string">&#x27;trade_log_return_realized_volatility&#x27;</span>, <span class="string">&#x27;trade_log_return_realized_volatility_400&#x27;</span>,\</span><br><span class="line">                <span class="string">&#x27;trade_log_return_realized_volatility_300&#x27;</span>, <span class="string">&#x27;trade_log_return_realized_volatility_200&#x27;</span>,</span><br><span class="line">                ]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#按stock_id分组对vol_cols对应聚合求mean...</span></span><br><span class="line">    df_stock_id = df.groupby([<span class="string">&#x27;stock_id&#x27;</span>])[vol_cols].agg([<span class="string">&#x27;mean&#x27;</span>, <span class="string">&#x27;std&#x27;</span>, <span class="string">&#x27;max&#x27;</span>, <span class="string">&#x27;min&#x27;</span>, ]).reset_index()</span><br><span class="line">    <span class="comment"># Rename columns joining suffix</span></span><br><span class="line">    df_stock_id.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df_stock_id.columns]</span><br><span class="line">    df_stock_id = df_stock_id.add_suffix(<span class="string">&#x27;_&#x27;</span> + <span class="string">&#x27;stock&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按time_id分组对vol_cols对应聚合求mean...</span></span><br><span class="line">    df_time_id = df.groupby([<span class="string">&#x27;time_id&#x27;</span>])[vol_cols].agg([<span class="string">&#x27;mean&#x27;</span>, <span class="string">&#x27;std&#x27;</span>, <span class="string">&#x27;max&#x27;</span>, <span class="string">&#x27;min&#x27;</span>, ]).reset_index()</span><br><span class="line">    <span class="comment"># Rename columns joining suffix</span></span><br><span class="line">    df_time_id.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df_time_id.columns]</span><br><span class="line">    df_time_id = df_time_id.add_suffix(<span class="string">&#x27;_&#x27;</span> + <span class="string">&#x27;time&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 跟原df合并</span></span><br><span class="line">    df = df.merge(df_stock_id, how=<span class="string">&#x27;left&#x27;</span>, left_on=[<span class="string">&#x27;stock_id&#x27;</span>], right_on=[<span class="string">&#x27;stock_id__stock&#x27;</span>])</span><br><span class="line">    df = df.merge(df_time_id, how=<span class="string">&#x27;left&#x27;</span>, left_on=[<span class="string">&#x27;time_id&#x27;</span>], right_on=[<span class="string">&#x27;time_id__time&#x27;</span>])</span><br><span class="line">    df.drop([<span class="string">&#x27;stock_id__stock&#x27;</span>, <span class="string">&#x27;time_id__time&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><p>实际调用过程中是这部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train, test = read_train_test_data()</span><br><span class="line"><span class="comment">#get unique stock ids</span></span><br><span class="line">train_stock_id = train[<span class="string">&#x27;stock_id&#x27;</span>].unique()</span><br><span class="line">train_ = preprocessor(train_stock_id, is_train=<span class="literal">True</span>)</span><br><span class="line">train = train.merge(train_, on=[<span class="string">&#x27;row_id&#x27;</span>], how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#get unique stock ids</span></span><br><span class="line">test_stock_id = test[<span class="string">&#x27;stock_id&#x27;</span>].unique()</span><br><span class="line">test_ = preprocessor(train_stock_id, is_train=<span class="literal">True</span>)</span><br><span class="line">test = test.merge(test_, on=[<span class="string">&#x27;row_id&#x27;</span>], how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train = get_time_stock(train)</span><br><span class="line">test = get_time_stock(test)</span><br></pre></td></tr></table></figure><p>这里是得到train, test csv数据后，按 <code>&#39;stock_id&#39;</code>处理得到 <code>train_</code>等进一步merge得到一个我们在第3、4小节构建的整个新特征df。</p><p>接下来我们按照<code>vol_cols</code>列表中的特征列(这些特征都是df中表示 <code>realized_volatility</code>的，我也不知道为什么会扔掉部分，如return2这种，可能是已经怕特征冗余，或者太多特征处理时间过长)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_stock_id = train.groupby([<span class="string">&#x27;stock_id&#x27;</span>])[vol_cols].agg([<span class="string">&#x27;mean&#x27;</span>, <span class="string">&#x27;std&#x27;</span>, <span class="string">&#x27;max&#x27;</span>, <span class="string">&#x27;min&#x27;</span>]).reset_index()</span><br><span class="line">df_stock_id.head()</span><br></pre></td></tr></table></figure><p>这时候<code>df_stock_id</code>变成：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823171.png" alt="image-20210903185915539" style="zoom:40%;" /></p><p>将index连起来添加个后缀：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_stock_id.columns = [<span class="string">&#x27;_&#x27;</span>.join(col) <span class="keyword">for</span> col <span class="keyword">in</span> df_stock_id.columns]</span><br><span class="line">df_stock_id = df_stock_id.add_suffix(<span class="string">&#x27;_&#x27;</span> + <span class="string">&#x27;stock&#x27;</span>)</span><br><span class="line">df_stock_id.head()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823172.png" alt="image-20210903190402234" style="zoom:40%;" /></p><p>同样按<code>time_id</code>处理得到 <code>df_time_id</code>:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823173.png" alt="image-20210903190549023" style="zoom:40%;" /></p><p>接下来就是合并得到整个df了，如<code>train.head()</code>:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823174.png" alt="image-20210903201615083" style="zoom:40%;" /></p><p><code>test.head()</code>如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823175.png" alt="image-20210903201741354" style="zoom:40%;" /></p><p>接下来是利用 <code>&#39;trade_seconds_in_bucket_count_unique&#39;</code>继续构建新的特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 计算train test size_tau</span></span><br><span class="line">train[<span class="string">&#x27;size_tau&#x27;</span>] = np.sqrt(<span class="number">1</span>/train[<span class="string">&#x27;trade_seconds_in_bucket_count_unique&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau&#x27;</span>] = np.sqrt(<span class="number">1</span>/test[<span class="string">&#x27;trade_seconds_in_bucket_count_unique&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau_400&#x27;</span>] = np.sqrt(<span class="number">1</span>/train[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_400&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau_400&#x27;</span>] = np.sqrt(<span class="number">1</span>/test[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_400&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau_300&#x27;</span>] = np.sqrt(<span class="number">1</span>/train[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_300&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau_300&#x27;</span>] = np.sqrt(<span class="number">1</span>/test[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_300&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau_200&#x27;</span>] = np.sqrt(<span class="number">1</span>/train[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_200&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau_200&#x27;</span>] = np.sqrt(<span class="number">1</span>/test[<span class="string">&#x27;trade_seconds_in_bucket_count_unique_200&#x27;</span>])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823176.png" alt="image-20210903203324850" style="zoom:40%;" /></p><p>同样利用 <code>&#39;trade_order_count_sum&#39;</code>构建新特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## size_tau2</span></span><br><span class="line">train[<span class="string">&#x27;size_tau2&#x27;</span>] = np.sqrt(<span class="number">1</span>/train[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau2&#x27;</span>] = np.sqrt(<span class="number">1</span>/test[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau2_400&#x27;</span>] = np.sqrt(<span class="number">0.33</span>/train[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau2_400&#x27;</span>] = np.sqrt(<span class="number">0.33</span>/test[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau2_300&#x27;</span>] = np.sqrt(<span class="number">0.5</span>/train[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau2_300&#x27;</span>] = np.sqrt(<span class="number">0.5</span>/test[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;size_tau2_200&#x27;</span>] = np.sqrt(<span class="number">0.66</span>/train[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line">test[<span class="string">&#x27;size_tau2_200&#x27;</span>] = np.sqrt(<span class="number">0.66</span>/test[<span class="string">&#x27;trade_order_count_sum&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#delta tau</span></span><br><span class="line">train[<span class="string">&#x27;size_tau2_d&#x27;</span>] = train[<span class="string">&#x27;size_tau2_400&#x27;</span>] - train[<span class="string">&#x27;size_tau2&#x27;</span>]</span><br><span class="line">test[<span class="string">&#x27;size_tau2_d&#x27;</span>] = test[<span class="string">&#x27;size_tau2_400&#x27;</span>] - test[<span class="string">&#x27;size_tau2&#x27;</span>]</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823177.png" alt="image-20210903203711575" style="zoom:40%;" /></p><p>现在特征列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">colNames = [col <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">list</span>(train.columns)</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> &#123;<span class="string">&quot;stock_id&quot;</span>, <span class="string">&quot;time_id&quot;</span>, <span class="string">&quot;target&quot;</span>, <span class="string">&quot;row_id&quot;</span>&#125;]</span><br><span class="line"><span class="built_in">len</span>(colNames)<span class="comment">#194</span></span><br></pre></td></tr></table></figure><h4 id="3-Kmeans聚类构建特征"><a href="#3-Kmeans聚类构建特征" class="headerlink" title="3.Kmeans聚类构建特征"></a>3.Kmeans聚类构建特征</h4><p>在第一篇中，我们得到了特征工程后的<code>train, test df</code>。现在我们使用Kmeans聚类获取新的特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">train_p = pd.read_csv(data_dir + <span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line"><span class="comment">#重构train_p 以&#x27;time_id&#x27;为index</span></span><br><span class="line">train_p = train_p.pivot(index=<span class="string">&#x27;time_id&#x27;</span>, columns=<span class="string">&#x27;stock_id&#x27;</span>, values=<span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">corr = train_p.corr() <span class="comment">#pearson 相关系数</span></span><br><span class="line"></span><br><span class="line">ids = corr.index</span><br><span class="line"></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">7</span>, random_state=SEED).fit(corr.values)</span><br><span class="line"><span class="built_in">print</span>(kmeans.labels_)</span><br><span class="line">==========================================================================</span><br><span class="line">[<span class="number">0</span> <span class="number">5</span> <span class="number">3</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">5</span> <span class="number">3</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">5</span> <span class="number">0</span> <span class="number">0</span> <span class="number">6</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3</span> <span class="number">6</span> <span class="number">3</span> <span class="number">0</span> <span class="number">3</span> <span class="number">3</span> <span class="number">0</span> <span class="number">6</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"> <span class="number">6</span> <span class="number">3</span> <span class="number">0</span> <span class="number">3</span> <span class="number">0</span> <span class="number">3</span> <span class="number">3</span> <span class="number">0</span> <span class="number">5</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">6</span> <span class="number">2</span> <span class="number">5</span> <span class="number">2</span> <span class="number">3</span> <span class="number">0</span> <span class="number">3</span> <span class="number">3</span> <span class="number">0</span> <span class="number">3</span> <span class="number">0</span> <span class="number">5</span> <span class="number">6</span> <span class="number">2</span> <span class="number">5</span> <span class="number">6</span> <span class="number">5</span> <span class="number">1</span> <span class="number">4</span> <span class="number">6</span> <span class="number">5</span></span><br><span class="line"> <span class="number">5</span> <span class="number">0</span> <span class="number">2</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">5</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">5</span> <span class="number">2</span> <span class="number">6</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">6</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">0</span> <span class="number">5</span> <span class="number">0</span> <span class="number">3</span> <span class="number">0</span> <span class="number">5</span> <span class="number">3</span> <span class="number">5</span> <span class="number">3</span></span><br><span class="line"> <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">l = [] <span class="comment">#各类别对应索引列表 l[1]:[8, 80]</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">7</span>):</span><br><span class="line">    l.append([(x-<span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> ((ids+<span class="number">1</span>) * (kmeans.labels_==n)) <span class="keyword">if</span> x &gt; <span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>这里依次用<code>train</code>的透视表重构特征后的相关系数值进行聚类，然后用<code>((ids+1) * (kmeans.labels_==n))</code>得到各类别对应索引的列表l。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拿相关系数索引进一步构建新的特征df列表分别为mat, matTest</span></span><br><span class="line">mat = []</span><br><span class="line">matTest = []</span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> ind <span class="keyword">in</span> l:</span><br><span class="line">    <span class="built_in">print</span>(ind)</span><br><span class="line">    <span class="comment">#选取stock_id是ind的df</span></span><br><span class="line">    newDf = train.loc[train[<span class="string">&#x27;stock_id&#x27;</span>].isin(ind)]</span><br><span class="line">    newDf = newDf.groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(np.nanmean)</span><br><span class="line">    newDf.loc[:, <span class="string">&#x27;stock_id&#x27;</span>] = <span class="built_in">str</span>(n) + <span class="string">&#x27;c1&#x27;</span><span class="comment">#将stock_id这列赋值为str(n)+&#x27;c1&#x27;如0c1</span></span><br><span class="line">    mat.append(newDf)</span><br><span class="line"></span><br><span class="line">    newDf = test.loc[test[<span class="string">&#x27;stock_id&#x27;</span>].isin(ind)]</span><br><span class="line">    newDf = newDf.groupby([<span class="string">&#x27;time_id&#x27;</span>]).agg(np.nanmean)</span><br><span class="line">    newDf.loc[:, <span class="string">&#x27;stock_id&#x27;</span>] = <span class="built_in">str</span>(n) + <span class="string">&#x27;c1&#x27;</span></span><br><span class="line">    matTest.append(newDf)</span><br><span class="line">    n += <span class="number">1</span> <span class="comment">#总共7类</span></span><br><span class="line">=======================================================================================</span><br><span class="line">[<span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">23</span>, <span class="number">26</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">33</span>, <span class="number">36</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">48</span>, <span class="number">66</span>, <span class="number">69</span>, <span class="number">72</span>, <span class="number">85</span>, <span class="number">94</span>, <span class="number">95</span>, <span class="number">100</span>, <span class="number">102</span>, <span class="number">108</span>, <span class="number">109</span>, <span class="number">111</span>, <span class="number">113</span>, <span class="number">115</span>, <span class="number">118</span>, <span class="number">120</span>]</span><br><span class="line">[<span class="number">8</span>, <span class="number">80</span>]</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">18</span>, <span class="number">61</span>, <span class="number">63</span>, <span class="number">75</span>, <span class="number">86</span>, <span class="number">97</span>]</span><br><span class="line">[<span class="number">2</span>, <span class="number">7</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">30</span>, <span class="number">32</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">39</span>, <span class="number">41</span>, <span class="number">43</span>, <span class="number">46</span>, <span class="number">47</span>, <span class="number">51</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">64</span>, <span class="number">67</span>, <span class="number">68</span>, <span class="number">70</span>, <span class="number">93</span>, <span class="number">103</span>, <span class="number">104</span>, <span class="number">105</span>, <span class="number">107</span>, <span class="number">114</span>, <span class="number">119</span>, <span class="number">123</span>, <span class="number">125</span>]</span><br><span class="line">[<span class="number">81</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">56</span>, <span class="number">62</span>, <span class="number">73</span>, <span class="number">76</span>, <span class="number">78</span>, <span class="number">83</span>, <span class="number">84</span>, <span class="number">87</span>, <span class="number">90</span>, <span class="number">96</span>, <span class="number">101</span>, <span class="number">112</span>, <span class="number">116</span>, <span class="number">122</span>, <span class="number">124</span>, <span class="number">126</span>]</span><br><span class="line">[<span class="number">27</span>, <span class="number">31</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">40</span>, <span class="number">58</span>, <span class="number">59</span>, <span class="number">60</span>, <span class="number">74</span>, <span class="number">77</span>, <span class="number">82</span>, <span class="number">88</span>, <span class="number">89</span>, <span class="number">98</span>, <span class="number">99</span>, <span class="number">110</span>]</span><br></pre></td></tr></table></figure><p>接下来是对生成的matrix进行转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转为pd</span></span><br><span class="line">mat1 = pd.concat(mat).reset_index()</span><br><span class="line">mat1.drop(columns=[<span class="string">&#x27;target&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mat2 = pd.concat(matTest).reset_index()</span><br><span class="line"><span class="comment">#不太清楚这个test生成df为什么拼接mat1中time_id==5的数据</span></span><br><span class="line">mat2 = pd.concat([mat2, mat1.loc[mat1.time_id==<span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line">mat1 = mat1.pivot(index=<span class="string">&#x27;time_id&#x27;</span>, columns=<span class="string">&#x27;stock_id&#x27;</span>)</span><br><span class="line">mat1.columns = [<span class="string">&#x27;_&#x27;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> mat1.columns.ravel()]<span class="comment">#将列名连起来</span></span><br><span class="line">mat1.reset_index(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">mat2 = mat2.pivot(index=<span class="string">&#x27;time_id&#x27;</span>, columns=<span class="string">&#x27;stock_id&#x27;</span>)</span><br><span class="line">mat2.columns = [<span class="string">&#x27;_&#x27;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> mat2.columns.ravel()]</span><br><span class="line">mat2.reset_index(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>上面代码如<code>mat1 = mat1.pivot(index=&#39;time_id&#39;, columns=&#39;stock_id&#39;)</code>中<code>mat1</code>如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823178.png" alt="image-20210904151553980" style="zoom:40%;" /></p><p>拼接下列名 <code>mat1.columns = [&#39;_&#39;.join(x) for x in mat1.columns.ravel()]</code>：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261823179.png" alt="image-20210904151803379" style="zoom:40%;" /></p><p>然后就是选取一些特征继续合并：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.merge(train, mat1[nnn], how=<span class="string">&#x27;left&#x27;</span>, on=<span class="string">&#x27;time_id&#x27;</span>)<span class="comment">#nnn就是选取的特征列</span></span><br><span class="line">test = pd.merge(test, mat2[nnn], how=<span class="string">&#x27;left&#x27;</span>, on=<span class="string">&#x27;time_id&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这就是所有特征工程的内容了，真的服了<code>alexioslyon</code>大神的能力。接下来可以使用LGBM和NN、CNN等模型来预测<code>target</code>。你可以把<code>test,train</code> 用<code>train.to_pickle(‘path’)</code>保存下来，方便下次使用。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://www.kaggle.com/c/optiver-realized-volatility-prediction/">optiver-realized-volatility-prediction</a></p><p>[2] <a href="https://www.zhihu.com/question/28572693">股市交易的撮合机制究竟是怎么运行的？</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/397399668">Optiver Realized Volatility Prediction</a></p>]]></content>
      
      
      <categories>
          
          <category> Competition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kaggle </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>25. NEZHA 论文笔记</title>
      <link href="2021/08/23/NLP%20Paper%2025.%20NEZHA%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2021/08/23/NLP%20Paper%2025.%20NEZHA%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="25-NEZHA-论文笔记"><a href="#25-NEZHA-论文笔记" class="headerlink" title="25. NEZHA 论文笔记"></a>25. NEZHA 论文笔记</h3><p>本文是<a href="https://arxiv.org/pdf/1909.00204.pdf">NEZHA: NEURAL CONTEXTUALIZED REPRESENTATION FOR CHINESE LANGUAGE UNDERSTANDING</a>的笔记。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847057.png" alt="image-20211024030736310" style="zoom:33%;" /></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>预训练语言模型已经在大量NLU任务上取得巨大成功，这要归功于其在大规模语料上预训练获取的深层上下文信息的能力。本文提出了作者预训练语言模型NEZHA(NEural contextualiZed representation for CHinese lAnguage understanding),其在中文语料上训练然后为中文NLU任务微调。当前版本的NEZHA基于BERT，使用了4项改进措施。包括：</p><ol><li><strong>Functional Relative Positional Encoding</strong> 作为有效的位置编码方案</li><li><strong>Whole Word Masking</strong> 掩码策略</li><li><strong>Mixed Precision Training</strong> 混合精度训练</li><li><strong>LAMB Optimizer</strong> 优化器</li></ol><p>实验结果表明NEZHA在以下中文任务上微调后取得了最佳的表现：</p><ol><li>命名实体识别(People’s Daily NER)</li><li>句子匹配(LCQMC)</li><li>中文情感分析(ChnSenti)</li><li>自然语言推断(XNLI)</li></ol><h4 id="2-Pre-training-NEZHA-Models"><a href="#2-Pre-training-NEZHA-Models" class="headerlink" title="2. Pre-training NEZHA Models"></a>2. Pre-training NEZHA Models</h4><h5 id="2-1-Preliminaries-BERT-Model-amp-Positional-Encoding"><a href="#2-1-Preliminaries-BERT-Model-amp-Positional-Encoding" class="headerlink" title="2.1 Preliminaries: BERT Model &amp; Positional Encoding"></a>2.1 Preliminaries: BERT Model &amp; Positional Encoding</h5><p>BERT(Bidirectional Encoder Representations from Transformers) 是一个预训练模型，就是堆叠Transformer encoder。使用残差连接来连接各个子层，紧接一个normalization。BERT可视为一个去噪自编码器，因为MASK后的MLM任务就是将添加MASK后的词预测出来，即将添加噪声的的data恢复。</p><p>来看看Transformer中的具体实现过程，<a href="https://aigonna.com/2020/11/18/NLP%20Paper%2010.1%20Transformer%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">详细过程看transformer笔记</a>.</p><p>每个注意力头是对一个token序列如<script type="math/tex">x = (x_1, x_2, \cdots, x_n), 其中x_i \in \mathbb{R}^{d_x}</script>.（这里<script type="math/tex">x_i</script>是一个个的词向量,长为<script type="math/tex">d_x</script>）处理后得到一个输出序列<script type="math/tex">z = (z_1, z_2, \cdots, z_n), 其中z_i \in \mathbb{R}^{d_z}</script>.每个注意力有3个参数矩阵(训练得到):<script type="math/tex">W^K, W^Q, W^V \in \mathbb{R}^{d_x \times d_z}</script>。有这些参数，输出<script type="math/tex">z_i</script>可计算为:</p><script type="math/tex; mode=display">z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}\right) \tag{1}</script><p>这里要注意注意力分数<script type="math/tex">\alpha_{ij}</script>指位置i相对位置j的隐藏状态过softmax得到即:</p><script type="math/tex; mode=display">\alpha_{i j}=\frac{\exp e_{i j}}{\sum_{k} \exp e_{i k}} \tag{2}</script><p>对于<script type="math/tex">e_{ij}</script>，它是输入元素的线性变换的点积再放缩下得到的:</p><script type="math/tex; mode=display">e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}} \tag{3}</script><p>因为多头注意力是一种恒定的排列，其对词序不敏感。因此<strong>Transformer</strong>使用了<strong>函数式位置编码</strong>，<strong>BERT</strong>使用了<strong>参数式位置编码</strong>直接添加到embedding上。这里使用相对位置编码方案，注意力分数计算要包含两个位置相对距离参数嵌入，这里也是将其直接加载embedding上。这样，式1和3中的<script type="math/tex">z_i, e_{ij}</script>计算就变成了:</p><script type="math/tex; mode=display">z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V} + a_{ij}^V\right) \tag{4}</script><p>以及：</p><script type="math/tex; mode=display">e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}+a_{ij}^K\right)^{T} + }{\sqrt{d_{z}}} \tag{5}</script><p>其中<script type="math/tex">a_{ij}^V,a_{ij}^K \in \mathbb{R}^{d_z}</script>是两个向量，表示相对于位置i和位置j的编码，其跟所有注意力头共享。这里跟Transformer-XL和XLNet有所区别。接下来讲下其是怎么计算的。</p><h5 id="2-2-Functional-Relative-Positional-Encoding"><a href="#2-2-Functional-Relative-Positional-Encoding" class="headerlink" title="2.2 Functional Relative Positional Encoding"></a>2.2 Functional Relative Positional Encoding</h5><p>在当前版本的NEZHA中，使用函数式相对位置编码，还是由正余弦函数得来，并且模型训练时固定。如果<script type="math/tex">a_{ij}</script>的维度为<script type="math/tex">2 \cdot k</script>和 <script type="math/tex">2 \cdot k + 1</script>，那么,</p><script type="math/tex; mode=display">\begin{align}a_{ij}[2k] &= \sin((j-i)/(10000^{\frac{2\cdot k}{d_z}}))\\\\a_{ij}[2k+1] &= \cos((j-i)/(10000^{\frac{2\cdot k}{d_z}})) \end{align}\tag{6}</script><p>每个维度的位置编码对应着一个余弦，不同维度的有着不同的波长。(周期不同)。<script type="math/tex">d_z</script>是跟模型相关的，就是hidden size/number of heads。波长从<script type="math/tex">2\pi</script>到<script type="math/tex">10000 \cdot 2\pi</script>。<strong>选择固定的余弦函数主要是因为它可以使得模型推断比训练时遇到的句子更长的序列</strong>。</p><h5 id="2-3-Whole-Word-Masking"><a href="#2-3-Whole-Word-Masking" class="headerlink" title="2.3 Whole Word Masking"></a>2.3 Whole Word Masking</h5><p>全词掩码是 <a href="https://arxiv.org/pdf/1906.08101.pdf">Pre-Training with Whole Word Masking for Chinese BERT</a>提出来的，WWM能有效提高预训练语言模型。</p><blockquote><p>WWM:原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。</p></blockquote><h5 id="2-4-Mixed-Precision-Training"><a href="#2-4-Mixed-Precision-Training" class="headerlink" title="2.4 Mixed Precision Training"></a>2.4 Mixed Precision Training</h5><p>使用混合精度训练，能提升2-3倍效率，并较少模型空间消耗，根据其结果，大的batch size可以利用它。</p><p>为了方便，整个网络使用FP32浮点数，包括训练时模型参数、梯度。混合精度在训练时使用，具体来说，其维持一个单精度的Master Weights的复制(就是模型的权重)。在每轮训练中，将Master Weights精简到FP16,并且前向和反向传播时将其使用在权重，激活和梯度部分。最后将梯度转换成FP32格式，使用FP32梯度更新到Master Weights。</p><h5 id="2-5-LAMB-Optimizer"><a href="#2-5-LAMB-Optimizer" class="headerlink" title="2.5 LAMB Optimizer"></a>2.5 LAMB Optimizer</h5><p>在大的batch size中训练模型会给模型的泛化能力带来负面影响，而LAMB优化器通过自适应方式为每个参数调整lr，能够在大batch size下训练，并且能提高训练速度。</p><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h4><p>预训练语料：</p><ul><li>中文维基百科: 用WikiExtractor并清洗后大约有202M字符</li><li>百度百科: 清洗后有4,734M字符</li><li>中文新闻：多个新闻网站，如新浪新闻，5,600M字符</li></ul><p>预训练细节：</p><p>10台华为云服务，每台8块32GB的V100GPU。</p><ul><li>NEZHA base模型，最大学习率为1.8e-4(1800预热，线性衰减).总共batch size为180 <em> 8 </em> 10 = 14400，每个GPU 180 batch size. </li><li>NEZHA large模型,每个GPU batch size设置为64.总共batch size为64 <em> 8 </em> 10 = 5120.</li></ul><p>下面2张分别表示：</p><ol><li>预训练的语料和模型配置情况</li><li>预训练任务、训练精度、优化器选择、位置编码</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847059.png" alt="image-20211024025833351" style="zoom:30%;" /></p><h5 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h5><p>主要在以下任务上微调：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847060.png" alt="image-20211024030312229" style="zoom:33%;" /></p><p>实验结果：有所提升，但大部分任务干不过ERNIE。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847061.png" alt="image-20211024030410322" style="zoom:33%;" /></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NEZHA </tag>
            
            <tag> FRPE </tag>
            
            <tag> WWM </tag>
            
            <tag> Mixed Precision Training </tag>
            
            <tag> LAMB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3. LightAutoML 使用</title>
      <link href="2021/08/21/ML_3.lightAutoML/"/>
      <url>2021/08/21/ML_3.lightAutoML/</url>
      
        <content type="html"><![CDATA[<h3 id="3-LightAutoML-使用"><a href="#3-LightAutoML-使用" class="headerlink" title="3. LightAutoML 使用"></a>3. LightAutoML 使用</h3><p><a href="https://github.com/sberbank-ai-lab/LightAutoML">LightAutoML</a>是2020年新开源的自动机器学习框架，适用于图表数据集的多种任务，如二分类、多分类以及回归任务。包括不同的特征:数字，类别，日期，文本等。安装非常简单一条命令<code>pip install -U lightautoml</code>。</p><p>本文是 <a href="https://alexmryzhkov.medium.com/?p=2cce7da6f936">LightAutoML vs Titanic: 80% accuracy in several lines of code</a>简单翻译。<a href="https://github.com/aigonna/ML_Skills/blob/main/1_LightAutoML_%E4%BD%BF%E7%94%A8.ipynb">代码完整链接</a>。</p><p>拿Titanic数据集看看具体怎么使用，正题在第4部分，前面可以忽略。</p><h4 id="Step-1-导入包"><a href="#Step-1-导入包" class="headerlink" title="Step 1: 导入包"></a>Step 1: 导入包</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Standard python libraries</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Installed libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imports from LightAutoML package</span></span><br><span class="line"><span class="comment">#这两者基本用法相同后面更强大</span></span><br><span class="line"><span class="keyword">from</span> lightautoml.automl.presets.tabular_presets <span class="keyword">import</span> TabularAutoML, TabularUtilizedAutoML </span><br><span class="line"><span class="keyword">from</span> lightautoml.tasks <span class="keyword">import</span> Task</span><br></pre></td></tr></table></figure><h4 id="Step-2：导入数据"><a href="#Step-2：导入数据" class="headerlink" title="Step 2：导入数据"></a>Step 2：导入数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line">data_dir = Path(<span class="string">&quot;/data&quot;</span>)</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(data_dir/<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(data_dir/<span class="string">&#x27;test.csv&#x27;</span>)</span><br><span class="line">sample = pd.read_csv(data_dir/<span class="string">&#x27;gender_submission.csv&#x27;</span>)</span><br><span class="line">train_data.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210823151544.png" alt="image-20210823151527688"></p><ul><li>具体字段表示为：<ul><li>Age 年龄</li><li>Cabin 船舱号</li><li>Embarked 登船港口</li><li>Fare 票价</li><li>Name 乘客姓名</li><li>Parch 不同代直系亲属人数</li><li>SibSp 同代直系亲属人数</li><li>PassengerId 乘客ID</li><li>Pclass 客舱等级</li><li>Sex 性别</li><li>Ticket 船票编号</li><li>Survived 存活情况</li></ul></li></ul><h4 id="Step-3-清理数据得到新特征"><a href="#Step-3-清理数据得到新特征" class="headerlink" title="Step 3: 清理数据得到新特征"></a>Step 3: 清理数据得到新特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_title</span>(<span class="params">name</span>):</span></span><br><span class="line">    title_search = re.search(<span class="string">&#x27; ([A-Za-z]+)\.&#x27;</span>, name)<span class="comment">#搜索字母后接.像Mr.这种敬称</span></span><br><span class="line">    <span class="comment"># If the title exists, extract and return it.</span></span><br><span class="line">    <span class="keyword">if</span> title_search:</span><br><span class="line">        <span class="keyword">return</span> title_search.group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_extra_features</span>(<span class="params">data</span>):</span></span><br><span class="line">    data[<span class="string">&#x27;Ticket_type&#x27;</span>] = data[<span class="string">&#x27;Ticket&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">0</span>:<span class="number">3</span>]) <span class="comment">#Ticket_type只要Ticket前3字符</span></span><br><span class="line">    data[<span class="string">&#x27;Name_Words_Count&#x27;</span>] = data[<span class="string">&#x27;Name&#x27;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split())) <span class="comment">#name 词数</span></span><br><span class="line">    <span class="comment">#如果不nan就是有Cabin,转换为1,0int型数据</span></span><br><span class="line">    data[<span class="string">&#x27;Has_Cabin&#x27;</span>] = data[<span class="string">&quot;Cabin&quot;</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> - <span class="built_in">int</span>(<span class="built_in">type</span>(x) == <span class="built_in">float</span>)) </span><br><span class="line">    data[<span class="string">&#x27;FamilySize&#x27;</span>] = data[<span class="string">&#x27;SibSp&#x27;</span>] + data[<span class="string">&#x27;Parch&#x27;</span>] + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#根据票价的频率来选择箱子的均匀间隔，即每个箱子中含有的票价的数量是相同的</span></span><br><span class="line">    data[<span class="string">&#x27;CategoricalFare&#x27;</span>] = pd.qcut(data[<span class="string">&#x27;Fare&#x27;</span>], <span class="number">5</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="comment">#cut将年龄本身来选择箱子均匀间隔，即每个箱子的间距都是相同的</span></span><br><span class="line">    data[<span class="string">&#x27;CategoricalAge&#x27;</span>] = pd.cut(data[<span class="string">&#x27;Age&#x27;</span>], <span class="number">5</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    </span><br><span class="line">    data[<span class="string">&#x27;Title&#x27;</span>] = data[<span class="string">&#x27;Name&#x27;</span>].apply(get_title).replace([<span class="string">&#x27;Lady&#x27;</span>, <span class="string">&#x27;Countess&#x27;</span>,<span class="string">&#x27;Capt&#x27;</span>, <span class="string">&#x27;Col&#x27;</span>,<span class="string">&#x27;Don&#x27;</span>, <span class="string">&#x27;Dr&#x27;</span>, <span class="string">&#x27;Major&#x27;</span>, <span class="string">&#x27;Rev&#x27;</span>, <span class="string">&#x27;Sir&#x27;</span>, <span class="string">&#x27;Jonkheer&#x27;</span>, <span class="string">&#x27;Dona&#x27;</span>], <span class="string">&#x27;Rare&#x27;</span>)</span><br><span class="line">    data[<span class="string">&#x27;Title&#x27;</span>] = data[<span class="string">&#x27;Title&#x27;</span>].replace(<span class="string">&#x27;Mlle&#x27;</span>, <span class="string">&#x27;Miss&#x27;</span>)</span><br><span class="line">    data[<span class="string">&#x27;Title&#x27;</span>] = data[<span class="string">&#x27;Title&#x27;</span>].replace(<span class="string">&#x27;Ms&#x27;</span>, <span class="string">&#x27;Miss&#x27;</span>)</span><br><span class="line">    data[<span class="string">&#x27;Title&#x27;</span>] = data[<span class="string">&#x27;Title&#x27;</span>].replace(<span class="string">&#x27;Mme&#x27;</span>, <span class="string">&#x27;Mrs&#x27;</span>)</span><br><span class="line">    data[<span class="string">&#x27;Title&#x27;</span>] = data[<span class="string">&#x27;Title&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&quot;Mr&quot;</span>: <span class="number">1</span>, <span class="string">&quot;Miss&quot;</span>: <span class="number">2</span>, <span class="string">&quot;Mrs&quot;</span>: <span class="number">3</span>, <span class="string">&quot;Master&quot;</span>: <span class="number">4</span>, <span class="string">&quot;Rare&quot;</span>: <span class="number">5</span>&#125;).fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_data = create_extra_features(train_data)</span><br><span class="line">test_data = create_extra_features(test_data)</span><br><span class="line"></span><br><span class="line">tr_data, valid_data = train_test_split(train_data, </span><br><span class="line">                    test_size=<span class="number">0.2</span>, </span><br><span class="line">                    stratify=train_data[<span class="string">&quot;Survived&quot;</span>], </span><br><span class="line">                    random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h4 id="Step-4-LightAutoML-preset-usage"><a href="#Step-4-LightAutoML-preset-usage" class="headerlink" title="Step 4: LightAutoML preset usage"></a>Step 4: LightAutoML preset usage</h4><h5 id="1-创建Task对象"><a href="#1-创建Task对象" class="headerlink" title="1. 创建Task对象"></a>1. 创建Task对象</h5><p>这要你根据任务来选择，对应任务类型、<code>metric</code>两个参数。具体使用如：<code>task = Task(&#39;reg&#39;, )</code>，参数解释如下：</p><ul><li><code>&#39;reg&#39;</code>：表示回归问题</li><li><code>&#39;binary&#39;</code>: 表示二分类问题</li><li><code>&#39;multiclass&#39;</code>：表示多分类问题</li></ul><p>而<code>metric</code>参数是用来监测模型预测效果的,比如你要用<code>F1 metric</code>,你就定义一个：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1_metric</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> f1_score(y_true, (y_pred &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>))</span><br><span class="line"></span><br><span class="line">task = Task(<span class="string">&#x27;binary&#x27;</span>, metric = f1_metric)</span><br></pre></td></tr></table></figure><h5 id="2-设置columns-roles"><a href="#2-设置columns-roles" class="headerlink" title="2. 设置columns roles"></a>2. 设置columns roles</h5><p>类似于设置哪些特征列是target，哪些不要进入训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">roles = &#123;<span class="string">&#x27;target&#x27;</span>: <span class="string">&#x27;Survived&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;drop&#x27;</span>: [<span class="string">&#x27;PassengerId&#x27;</span>, <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Ticket&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure><h5 id="3-从preset创建AutoML-模型"><a href="#3-从preset创建AutoML-模型" class="headerlink" title="3. 从preset创建AutoML 模型"></a>3. 从preset创建AutoML 模型</h5><p><img src="https://github.com/sberbank-ai-lab/LightAutoML/raw/master/imgs/tutorial_blackbox_pipeline.png" alt="LightAutoML** " style="zoom:25%;" /></p><p>我们可以用 <code>TabularAutoML</code>创建一个自动调优的模型,然后根据LightAutoML输出调整参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">automl = TabularAutoML(task = task, </span><br><span class="line">            timeout = <span class="number">600</span>, <span class="comment"># 训练最大时间600 seconds = 10 minutes</span></span><br><span class="line">            cpu_limit = <span class="number">4</span>, <span class="comment"># 使用cpu核心数Optimal for Kaggle kernels</span></span><br><span class="line">            general_params = &#123;<span class="string">&#x27;use_algos&#x27;</span>: [[<span class="string">&#x27;lgb&#x27;</span>, <span class="string">&#x27;lgb_tuned&#x27;</span>, <span class="string">&#x27;cb&#x27;</span>]]&#125;)</span><br></pre></td></tr></table></figure><p>基本的算法现在是在<code>general_params</code>中的 <code>&#39;use_algos&#39;</code>设置，可以设置的参数有:</p><ul><li>线性模型: <code>&#39;linear_l2&#39;</code></li><li>基于数据集带实验参数的LightGBM模型： <code>&#39;lgb&#39;</code></li><li>用Optuna自动调参微调参数的LightGBM模型： <code>&#39;lgb_tuned&#39;</code></li><li>带实验参数的CatBoost模型: <code>&#39;cb&#39;</code></li><li>Optuna调整参数的CatBoost模型: <code>&#39;cb_tuned&#39;</code></li></ul><p>因为想Stacking算法它们是由两个Level训练得到结果的，所以 <code>&#39;use_algos&#39;: [[&#39;linear_l2&#39;, &#39;lgb&#39;, &#39;lgb_tuned&#39;], [&#39;lgb_tuned&#39;, &#39;cb&#39;]]</code>代表着第一Level用3个对应的算法调，第二个Level用两个算法，如上图。(LightAutoML叫layer)在第二个Level全部训练完后，从这两个算法的权重平均预测值构建最终预测结果。实际上，我们可以根据<a href="https://github.com/sberbank-ai-lab/LightAutoML/blob/master/lightautoml/automl/presets/tabular_config.yml">YAML config</a> 来定制整个参数集。</p><p>再举一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">automl = TabularAutoML(task=task,</span><br><span class="line">            timeout=TIMEOUT,</span><br><span class="line">            cpu_limit = N_THREADS,</span><br><span class="line">            reader_params=&#123;<span class="string">&#x27;n_jobs&#x27;</span>: N_THREADS, <span class="string">&#x27;cv&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;random_state&#x27;</span>: RANDOM_SEED&#125;,</span><br><span class="line">            general_params=&#123;<span class="string">&#x27;use_algos&#x27;</span>: [[<span class="string">&#x27;lgb&#x27;</span>, <span class="string">&#x27;cb&#x27;</span>]], <span class="comment">#使用算法</span></span><br><span class="line">                            <span class="string">&#x27;return_all_predictions&#x27;</span>: <span class="literal">True</span>, <span class="comment">##返回在blending阶段前的所有层的预测值</span></span><br><span class="line">                            <span class="string">&#x27;weighted_blender_max_nonzero_coef&#x27;</span>: <span class="number">0.0</span>&#125;, <span class="comment">##在blending阶段所有算法不drop</span></span><br><span class="line">            verbose=<span class="number">2</span>  <span class="comment"># Available values: 0,1,2,3 (from less detailed to more) 输出信息级别 3最详细</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure><h5 id="4-预测"><a href="#4-预测" class="headerlink" title="4. 预测"></a>4. 预测</h5><p>用Hold-out data 和valid_data预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oof_pred = automl.fit_predict(tr_data, roles = roles)</span><br><span class="line">valid_pred = automl.predict(valid_data)</span><br></pre></td></tr></table></figure><p><code>Automl preset training completed in 67.70 seconds.</code> ，一分钟左右时间，看看分数就<code>oof acc：0.85 ， val acc: 0.83</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc_score</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> accuracy_score(y_true, (y_pred &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;OOF acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(acc_score(tr_data[<span class="string">&#x27;Survived&#x27;</span>].values,      oof_pred.data[:, <span class="number">0</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;VAL acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(acc_score(valid_data[<span class="string">&#x27;Survived&#x27;</span>].values, valid_pred.data[:, <span class="number">0</span>])))</span><br><span class="line">========================================================================================</span><br><span class="line">OOF acc: <span class="number">0.851123595505618</span></span><br><span class="line">VAL acc: <span class="number">0.8324022346368715</span></span><br></pre></td></tr></table></figure><h5 id="5-用TabularUtilizedAutoML创建AutoML"><a href="#5-用TabularUtilizedAutoML创建AutoML" class="headerlink" title="5. 用TabularUtilizedAutoML创建AutoML"></a>5. 用TabularUtilizedAutoML创建AutoML</h5><p><code>TabularUtilizedAutoML</code>跟 <code>TabularAutoML</code>最大区别在于timeout 利用它的time utilization是尽可能最大化的，感觉上它的config也细节多一些。#TODO</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">automl = TabularUtilizedAutoML(task = task, </span><br><span class="line">            timeout = <span class="number">600</span>, <span class="comment"># 600 seconds = 10 minutes</span></span><br><span class="line">            cpu_limit = <span class="number">4</span>, <span class="comment"># Optimal for Kaggle kernels</span></span><br><span class="line">            general_params = &#123;<span class="string">&#x27;use_algos&#x27;</span>: [[<span class="string">&#x27;lgb&#x27;</span>, <span class="string">&#x27;lgb_tuned&#x27;</span>, <span class="string">&#x27;cb&#x27;</span>]],&#125;)</span><br><span class="line">oof_pred = automl.fit_predict(tr_data, roles = roles)</span><br></pre></td></tr></table></figure><p>看看得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">valid_pred = automl.predict(valid_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;OOF acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(acc_score(tr_data[<span class="string">&#x27;Survived&#x27;</span>].values, oof_pred.data[:, <span class="number">0</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;VAL acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(acc_score(valid_data[<span class="string">&#x27;Survived&#x27;</span>].values, valid_pred.data[:, <span class="number">0</span>])))</span><br><span class="line">===========================================================================================</span><br><span class="line">OOF acc: <span class="number">0.8693820224719101</span></span><br><span class="line">VAL acc: <span class="number">0.8212290502793296</span></span><br></pre></td></tr></table></figure><h5 id="6-用完整数据集训练"><a href="#6-用完整数据集训练" class="headerlink" title="6.用完整数据集训练"></a>6.用完整数据集训练</h5><p>用得到的参数训练全部数据，并预测提交，最后结果0.79665。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">automl = TabularUtilizedAutoML(task = task, </span><br><span class="line">                timeout = <span class="number">600</span>, <span class="comment"># 600 seconds = 10 minutes</span></span><br><span class="line">                cpu_limit = <span class="number">4</span>, <span class="comment"># Optimal for Kaggle kernels</span></span><br><span class="line">                general_params = &#123;<span class="string">&#x27;use_algos&#x27;</span>: [[<span class="string">&#x27;lgb&#x27;</span>, <span class="string">&#x27;lgb_tuned&#x27;</span>, <span class="string">&#x27;cb&#x27;</span>]],&#125;)</span><br><span class="line">oof_pred = automl.fit_predict(train_data, roles = roles)</span><br><span class="line">test_pred = automl.predict(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成submission 最后0.79665</span></span><br><span class="line">sample[<span class="string">&#x27;Survived&#x27;</span>] = (test_pred.data[:, <span class="number">0</span>] &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">sample.to_csv(<span class="string">&#x27;automl_utilized_600_f1_score.csv&#x27;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p><a href="https://www.kaggle.com/alexryzhkov/aug21-lightautoml-starter">还可以看看这个</a>，学习进一步使用。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LightAutoML </tag>
            
            <tag> Titanic dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tabular Playground Series 比赛   去噪和quantile normalization</title>
      <link href="2021/08/20/Kaggle%20kaggle%20%20Tabular%20Playground%20Series%20%E6%AF%94%E8%B5%9B/"/>
      <url>2021/08/20/Kaggle%20kaggle%20%20Tabular%20Playground%20Series%20%E6%AF%94%E8%B5%9B/</url>
      
        <content type="html"><![CDATA[<h3 id="Tabular-Playground-Series-比赛-去噪和quantile-normalization"><a href="#Tabular-Playground-Series-比赛-去噪和quantile-normalization" class="headerlink" title="Tabular Playground Series 比赛   去噪和quantile normalization"></a>Tabular Playground Series 比赛   去噪和quantile normalization</h3><p><a href="https://www.kaggle.com/pourchot/in-python-tabular-denoising-residual-network/output">原文</a>，利用自编码器去噪。</p><h4 id="1-导入包和数据"><a href="#1-导入包和数据" class="headerlink" title="1. 导入包和数据"></a>1. 导入包和数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random, math, sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, initializers, regularizers, activations, callbacks</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> tensorflow_addons <span class="keyword">as</span> tfa</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, scale</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.simplefilter(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure><p>使用版本信息:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"><span class="built_in">print</span>(keras.__version__)</span><br><span class="line">===========================</span><br><span class="line"><span class="number">2.4</span><span class="number">.1</span></span><br><span class="line"><span class="number">2.4</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><p><strong>导入数据</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data_dir = Path(<span class="string">&quot;/data&quot;</span>)</span><br><span class="line">train = pd.read_csv(data_dir/<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(data_dir/<span class="string">&#x27;test.csv&#x27;</span>)</span><br><span class="line">sample = pd.read_csv(data_dir/<span class="string">&#x27;sample_submission.csv&#x27;</span>)</span><br><span class="line">train.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">y = np.array(train[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">X = train.drop([<span class="string">&#x27;loss&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">xall = pd.concat([X, test], axis=<span class="number">0</span>, copy=<span class="literal">False</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#print(xall.head())</span></span><br><span class="line"><span class="built_in">print</span>(X.shape, y.shape, test.shape, xall.shape)</span><br><span class="line">==================================================</span><br><span class="line">(<span class="number">250000</span>, <span class="number">100</span>) (<span class="number">250000</span>,) (<span class="number">150000</span>, <span class="number">100</span>) (<span class="number">400000</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-quantile-normalize"><a href="#2-quantile-normalize" class="headerlink" title="2. quantile normalize"></a>2. quantile normalize</h4><p><a href="https://en.wikipedia.org/wiki/Quantile_normalization">Quantile_normalization</a>解释，用它处理数据来得到相同分布。具体来说：</p><ol><li>先分级，如例子中基因的数目为4就分为4个级别。</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">A</span>    <span class="number">5</span>    <span class="number">4</span>    <span class="number">3</span></span><br><span class="line"><span class="selector-tag">B</span>    <span class="number">2</span>    <span class="number">1</span>    <span class="number">4</span></span><br><span class="line">C    <span class="number">3</span>    <span class="number">4</span>    <span class="number">6</span></span><br><span class="line">D    <span class="number">4</span>    <span class="number">2</span>    <span class="number">8</span></span><br></pre></td></tr></table></figure><p>对应分配级别<code>i-iv</code>，记下对应的<code>rank</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">A</span>    iv    iii   <span class="selector-tag">i</span></span><br><span class="line"><span class="selector-tag">B</span>    <span class="selector-tag">i</span>     <span class="selector-tag">i</span>     ii</span><br><span class="line">C    ii    iii   iii</span><br><span class="line">D    iii   ii    iv</span><br></pre></td></tr></table></figure><ol><li>对原始数据的按特征列排序。</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">A</span>    <span class="number">5</span>    <span class="number">4</span>    <span class="number">3</span>    becomes <span class="selector-tag">A</span> <span class="number">2</span> <span class="number">1</span> <span class="number">3</span></span><br><span class="line"><span class="selector-tag">B</span>    <span class="number">2</span>    <span class="number">1</span>    <span class="number">4</span>    becomes <span class="selector-tag">B</span> <span class="number">3</span> <span class="number">2</span> <span class="number">4</span></span><br><span class="line">C    <span class="number">3</span>    <span class="number">4</span>    <span class="number">6</span>    becomes C <span class="number">4</span> <span class="number">4</span> <span class="number">6</span></span><br><span class="line">D    <span class="number">4</span>    <span class="number">2</span>    <span class="number">8</span>    becomes D <span class="number">5</span> <span class="number">4</span> <span class="number">8</span></span><br></pre></td></tr></table></figure><ol><li>对排序后的数据进行每行求均值</li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A (2 + 1 + 3)/3 = 2.00 = rank i</span><br><span class="line">B (3 + 2 + 4)/3 = 3.00 = rank ii</span><br><span class="line">C (4 + 4 + 6)/3 = 4.67 = rank iii</span><br><span class="line">D (5 + 4 + 8)/3 = 5.67 = rank iv</span><br></pre></td></tr></table></figure><ol><li>我们再按1中的对应的rank填入对应值, 到这儿就是quantile normalize算法过程。</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">A</span>    <span class="number">5.67</span>    <span class="number">4.67</span>    <span class="number">2.00</span></span><br><span class="line"><span class="selector-tag">B</span>    <span class="number">2.00</span>    <span class="number">2.00</span>    <span class="number">3.00</span></span><br><span class="line">C    <span class="number">3.00</span>    <span class="number">4.67</span>    <span class="number">4.67</span></span><br><span class="line">D    <span class="number">4.67</span>    <span class="number">3.00</span>    <span class="number">5.67</span></span><br></pre></td></tr></table></figure><p>实际过程中，步骤4中的第二列，有两个一样值4.67，这是要避免的，你要把每列都同分布嘛，就取和最靠近的哪个rank对应值的平均值。这就是具体quantile normalize过程，最后结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">A</span>    <span class="number">5.67</span>    <span class="number">5.17</span>    <span class="number">2.00</span></span><br><span class="line"><span class="selector-tag">B</span>    <span class="number">2.00</span>    <span class="number">2.00</span>    <span class="number">3.00</span></span><br><span class="line">C    <span class="number">3.00</span>    <span class="number">5.17</span>    <span class="number">4.67</span></span><br><span class="line">D    <span class="number">4.67</span>    <span class="number">3.00</span>    <span class="number">5.67</span></span><br></pre></td></tr></table></figure><p>在该数据集中，我们直接先对原始数据进行类似最大最小化的75分位数25分位处理，来避免分位数归一化中的同数情况。#TODO<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">xmedian = pd.DataFrame.median(xall, <span class="number">0</span>)</span><br><span class="line">x25quan = xall.quantile(<span class="number">0.25</span>, <span class="number">0</span>)</span><br><span class="line">x75quan = xall.quantile(<span class="number">0.75</span>, <span class="number">0</span>)</span><br><span class="line">xall = (xall- xmedian)/(x75quan-x25quan)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantile_norm</span>(<span class="params">df_input</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;分位数标准化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#对df_input按每行值进行升序排序</span></span><br><span class="line">    sorted_df = pd.DataFrame(np.sort(df_input.values, axis=<span class="number">0</span>), index=df_input.index, columns=df_input.columns)</span><br><span class="line">    mean_df = sorted_df.mean(axis=<span class="number">1</span>)<span class="comment">#排序均值</span></span><br><span class="line">    mean_df.index = np.arange(<span class="number">1</span>, <span class="built_in">len</span>(mean_df) + <span class="number">1</span>)<span class="comment">#分级</span></span><br><span class="line">    quantile_df = df_input.rank(axis=<span class="number">0</span>, method=<span class="string">&#x27;min&#x27;</span>).stack().astype(<span class="string">&#x27;int&#x27;</span>).<span class="built_in">map</span>(mean_df).unstack()</span><br><span class="line">    <span class="keyword">return</span> (quantile_df)</span><br><span class="line"></span><br><span class="line">qall = np.array(quantile_norm(xall))<span class="comment">#将xall进行分位数标准化后作为array</span></span><br><span class="line">qlabeled = qall[:<span class="built_in">len</span>(train), :] <span class="comment">#取train长度行作为标记的data</span></span><br><span class="line">qunlabled = qall[<span class="built_in">len</span>(train):, :]</span><br></pre></td></tr></table></figure></p><p>拿 <code>quantile_norm(df_input)</code>实验一下wiki例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xall = pd.DataFrame([[<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>]])</span><br><span class="line">qall = np.array(quantile_norm(xall))</span><br><span class="line"><span class="built_in">print</span>(qall)</span><br><span class="line">=============================================</span><br><span class="line">[[<span class="number">5.66666667</span> <span class="number">4.66666667</span> <span class="number">2.</span>        ]</span><br><span class="line"> [<span class="number">2.</span>         <span class="number">2.</span>         <span class="number">3.</span>        ]</span><br><span class="line"> [<span class="number">3.</span>         <span class="number">4.66666667</span> <span class="number">4.66666667</span>]</span><br><span class="line"> [<span class="number">4.66666667</span> <span class="number">3.</span>         <span class="number">5.66666667</span>]]</span><br></pre></td></tr></table></figure><h4 id="3-creation-bins"><a href="#3-creation-bins" class="headerlink" title="3. creation bins"></a>3. creation bins</h4><p>创建箱分位数据来进一步增加数据输入多样性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ball = np.zeros((qall.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>])) <span class="comment">#创建qall.shape行数 X.shape列数的全0矩阵来存储</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">    <span class="comment">#取第i列，按照X.shape[1]分箱 无标签 重复出现丢掉</span></span><br><span class="line">    ball[:, i] = pd.qcut(qall[:, i], X.shape[<span class="number">1</span>], labels=<span class="literal">False</span>, duplicates=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line">blabled = ball[:X.shape[<span class="number">0</span>], :]</span><br><span class="line">bunlabled = ball[X.shape[<span class="number">0</span>]:, :]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>例如拿wiki例子中数据，就是上面 <code>[5.66666667 4.66666667 2.        ]</code>这个矩阵， 来处理下有:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ball = np.zeros((qall.shape[<span class="number">0</span>], qall.shape[<span class="number">1</span>]))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(qall.shape[<span class="number">1</span>]):</span><br><span class="line">    ball[:, i] = pd.qcut(qall[:, i], qall.shape[<span class="number">1</span>], labels=<span class="literal">False</span>, duplicates=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(ball)</span><br><span class="line">===================================</span><br><span class="line">[[<span class="number">2.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">2.</span>]]</span><br></pre></td></tr></table></figure><h4 id="4-denoise-Autoencoder"><a href="#4-denoise-Autoencoder" class="headerlink" title="4. denoise Autoencoder"></a>4. denoise Autoencoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#噪声生成为0, 0.1的正态分布，size指定</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">.1</span>, (qall.shape[<span class="number">0</span>], qall.shape[<span class="number">1</span>]))</span><br><span class="line">qall = np.array(qall)</span><br><span class="line">xnoisy = qall + noise</span><br><span class="line">limit = np.<span class="built_in">int</span>(<span class="number">0.8</span> * qall.shape[<span class="number">0</span>])<span class="comment">#用来划分训练集和验证集</span></span><br><span class="line">xtrain = xnoisy[<span class="number">0</span>:limit, :]</span><br><span class="line">ytrain = qall[<span class="number">0</span>:limit, ]</span><br><span class="line">xval = xnoisy[limit:qall.shape[<span class="number">0</span>], :]</span><br><span class="line">yval = qall[limit:qall.shape[<span class="number">0</span>], :]</span><br><span class="line"><span class="built_in">print</span>(xtrain.shape, ytrain.shape, xval.shape, yval.shape)</span><br><span class="line">======================================================</span><br><span class="line">(<span class="number">320000</span>, <span class="number">100</span>) (<span class="number">320000</span>, <span class="number">100</span>) (<span class="number">80000</span>, <span class="number">100</span>) (<span class="number">80000</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#早停和衰减策略</span></span><br><span class="line">es = tf.keras.callbacks.EarlyStopping(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    min_delta=<span class="number">1e-9</span>, <span class="comment">#最小变化阈值</span></span><br><span class="line">    patience=<span class="number">20</span>,</span><br><span class="line">    verbose=<span class="number">0</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    baseline=<span class="literal">None</span>,</span><br><span class="line">    restore_best_weights=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plateau = tf.keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    factor=<span class="number">0.8</span>,</span><br><span class="line">    patience=<span class="number">4</span>,</span><br><span class="line">    verbose=<span class="number">0.</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    loss = K.mean(K.square(y_pred - y_true))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoencoded</span>():</span></span><br><span class="line">    ae_input = layers.Input(shape=(qall.shape[<span class="number">1</span>]))</span><br><span class="line">    ae_encoded = layers.Dense(units=qall.shape[<span class="number">1</span>], activation=<span class="string">&#x27;elu&#x27;</span>)(ae_input)</span><br><span class="line">    ae_encoded = layers.Dense(units=qall.shape[<span class="number">1</span>]*<span class="number">3</span>, activation=<span class="string">&#x27;elu&#x27;</span>)(ae_encoded)</span><br><span class="line">    ae_decoded = layers.Dense(units=qall.shape[<span class="number">1</span>], activation=<span class="string">&#x27;elu&#x27;</span>)(ae_encoded)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Model(ae_input, ae_decoded), Model(ae_input, ae_encoded)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">autoencoder, encoder = autoencoded()</span><br><span class="line">autoencoder.<span class="built_in">compile</span>(loss=custom_loss,</span><br><span class="line">            optimizer=keras.optimizers.Adam(lr=<span class="number">5e-3</span>))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>自编码器的总结:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.summary()</span><br><span class="line">===========================</span><br><span class="line">Model: <span class="string">&quot;model&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(<span class="literal">None</span>, <span class="number">100</span>)]             <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (<span class="literal">None</span>, <span class="number">100</span>)               <span class="number">10100</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (<span class="literal">None</span>, <span class="number">300</span>)               <span class="number">30300</span>     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (<span class="literal">None</span>, <span class="number">100</span>)               <span class="number">30100</span>     </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">70</span>,<span class="number">500</span></span><br><span class="line">Trainable params: <span class="number">70</span>,<span class="number">500</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>训练自编码器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">history = autoencoder.fit(xtrain, ytrain,</span><br><span class="line">            epochs=<span class="number">200</span>,</span><br><span class="line">            batch_size=<span class="number">512</span>,</span><br><span class="line">            verbose=<span class="number">0</span>,</span><br><span class="line">            validation_data=(xval, yval),</span><br><span class="line">            callbacks=[es, plateau])</span><br><span class="line">            </span><br><span class="line">eall = encoder.predict(qall) <span class="comment">#data 用去噪编码器编a码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;max encoded value = &quot;</span>, np.<span class="built_in">max</span>(eall))<span class="comment">#max encoded value =  10.96393   </span></span><br><span class="line"><span class="built_in">print</span>(eall.shape)<span class="comment"># (400000, 300)   </span></span><br></pre></td></tr></table></figure></p><h4 id="5-根据方差阈值特征编码选择特征"><a href="#5-根据方差阈值特征编码选择特征" class="headerlink" title="5. 根据方差阈值特征编码选择特征"></a>5. 根据方差阈值特征编码选择特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">evar = np.var(eall, axis=<span class="number">0</span>, ddof=<span class="number">1</span>) <span class="comment">#ddof=1计算总体方差时除以n-1,0时除以n</span></span><br><span class="line">evar1 = evar &gt; <span class="number">0.8</span></span><br><span class="line">a = np.where(evar1 == <span class="literal">False</span>, evar1, <span class="number">1</span>)<span class="comment">#满足evar1 == False为真，a=evar1/0, 反之为1</span></span><br><span class="line">nb_col = a.<span class="built_in">sum</span>()<span class="comment">#100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建3倍特征列df， 列名变为f&#x27;col_&#123;i&#125;&#x27;</span></span><br><span class="line">eall_1 = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(qall.shape[<span class="number">1</span>] * <span class="number">3</span>):</span><br><span class="line">    <span class="keyword">if</span> evar1[i] == <span class="literal">True</span>:</span><br><span class="line">        colname = <span class="string">f&#x27;col_<span class="subst">&#123;i&#125;</span>&#x27;</span></span><br><span class="line">        eall_1[colname] = eall[:, i]</span><br><span class="line">        </span><br><span class="line">eall_1 = np.array(eall_1)</span><br><span class="line">elabeled = eall_1[:<span class="built_in">len</span>(train), :]</span><br><span class="line">eunlabeled = eall_1[<span class="built_in">len</span>(train):, :]</span><br><span class="line">elabeled.shape, eunlabeled.shape</span><br><span class="line">=================================</span><br><span class="line">((<span class="number">250000</span>, <span class="number">100</span>), (<span class="number">150000</span>, <span class="number">100</span>))</span><br></pre></td></tr></table></figure><h4 id="6-对编码特征进行PCA降维再标准化"><a href="#6-对编码特征进行PCA降维再标准化" class="headerlink" title="6. 对编码特征进行PCA降维再标准化"></a>6. 对编码特征进行PCA降维再标准化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">15</span>)</span><br><span class="line">pall = pca.fit_transform(eall)</span><br><span class="line">sc_pca = StandardScaler()</span><br><span class="line">pall = sc_pca.fit_transform(pall)</span><br></pre></td></tr></table></figure><h4 id="7-合并PCA降维后特征和编码特征-方差阈值之后"><a href="#7-合并PCA降维后特征和编码特征-方差阈值之后" class="headerlink" title="7.合并PCA降维后特征和编码特征(方差阈值之后)"></a>7.合并PCA降维后特征和编码特征(方差阈值之后)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plabled = pall[:<span class="built_in">len</span>(train), :]</span><br><span class="line">punlabled = pall[<span class="built_in">len</span>(train):, :]</span><br><span class="line">elabeled = np.hstack((elabeled, plabled)) <span class="comment">#水平拼接特征形成elabeled:plabled</span></span><br><span class="line">eunlabeled = np.hstack((eunlabeled, punlabled))</span><br></pre></td></tr></table></figure><h4 id="8-构建残差模型来解决任务"><a href="#8-构建残差模型来解决任务" class="headerlink" title="8. 构建残差模型来解决任务"></a>8. 构建残差模型来解决任务</h4><p>这块很有意思搭建多残差网络，根据图来理解</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261814708.png" alt="Snipaste_2021-08-20_18-59-23" style="zoom:35%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NN Model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_res_model</span>():</span></span><br><span class="line">    <span class="comment">#分位数标准化输入</span></span><br><span class="line">    inputQ = layers.Input(shape=(qall.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment">#encoded+PCA后输入</span></span><br><span class="line">    inputE = layers.Input(shape=(elabeled.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment">#箱型输入</span></span><br><span class="line">    inputB = layers.Input(shape=(blabled.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第一路 前馈网络 先合并QE在dropout</span></span><br><span class="line">    denseQE = layers.Dropout(<span class="number">0.3</span>)(layers.Concatenate()([inputQ, inputE]))</span><br><span class="line">    denseQE = tfa.layers.WeightNormalization(layers.Dense(</span><br><span class="line">        units=<span class="number">300</span>,</span><br><span class="line">        activation=<span class="string">&#x27;elu&#x27;</span>,</span><br><span class="line">        kernel_initializer=<span class="string">&#x27;lecun_normal&#x27;</span></span><br><span class="line">    ))(denseQE)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第二路: enbedding + conv block 先embedding</span></span><br><span class="line">    embedB = layers.Embedding(input_dim=blabled.shape[<span class="number">1</span>] + <span class="number">1</span>,</span><br><span class="line">                              output_dim=<span class="number">6</span>,</span><br><span class="line">                              embeddings_regularizer=<span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">                              embeddings_initializer=<span class="string">&#x27;lecun_uniform&#x27;</span>)(inputB)</span><br><span class="line"></span><br><span class="line">    embedB = layers.Dropout(<span class="number">0.3</span>)(embedB)</span><br><span class="line">    embedB = layers.Conv1D(<span class="number">6</span>, <span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(embedB)</span><br><span class="line">    embedB = layers.Flatten()(embedB)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 残差网络结构， 先合并两路输出</span></span><br><span class="line">    hidden = layers.Dropout(<span class="number">0.3</span>)(layers.Concatenate()([denseQE, embedB]))</span><br><span class="line">    hidden = tfa.layers.WeightNormalization(layers.Dense(</span><br><span class="line">        units=<span class="number">64</span>, activation=<span class="string">&#x27;elu&#x27;</span>, kernel_initializer=<span class="string">&#x27;lecun_normal&#x27;</span></span><br><span class="line">    ))(hidden)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#输出合并第一次</span></span><br><span class="line">    output = layers.Dropout(<span class="number">0.3</span>)(layers.Concatenate()([embedB, hidden]))</span><br><span class="line">    output = tfa.layers.WeightNormalization(</span><br><span class="line">        layers.Dense(units=<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_initializer=<span class="string">&#x27;lecun_normal&#x27;</span>)</span><br><span class="line">    )(output)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#输出再合并</span></span><br><span class="line">    output = layers.Dropout(<span class="number">0.4</span>)(layers.Concatenate()([embedB, hidden, output]))</span><br><span class="line">    output = tfa.layers.WeightNormalization(layers.Dense(</span><br><span class="line">        units=<span class="number">32</span>, activation=<span class="string">&#x27;selu&#x27;</span>, kernel_initializer=<span class="string">&#x27;lecun_normal&#x27;</span></span><br><span class="line">    ))(output)</span><br><span class="line">    output = layers.Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;selu&#x27;</span>,\</span><br><span class="line">                          kernel_initializer=<span class="string">&#x27;lecun_normal&#x27;</span>)(output)</span><br><span class="line">    </span><br><span class="line">    model = Model([inputQ, inputE, inputB], output)</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">                  metrics=[tf.keras.metrics.RootMeanSquaredError()],</span><br><span class="line">                  optimizer=keras.optimizers.Adam(lr=<span class="number">0.005</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="comment"># nn_model = get_res_model()</span></span><br><span class="line"><span class="comment"># tf.keras.utils.plot_model(nn_model, to_file=&#x27;./dense.png&#x27;, show_shapes=False, show_layer_names=True)</span></span><br></pre></td></tr></table></figure><h4 id="9-训练模型"><a href="#9-训练模型" class="headerlink" title="9. 训练模型"></a>9. 训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">N_FOLDS = <span class="number">10</span></span><br><span class="line">SEED = <span class="number">1</span></span><br><span class="line">EPOCH = <span class="number">100</span></span><br><span class="line">N_ROUND = <span class="number">5</span></span><br><span class="line">oof = np.zeros((y.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">pred = np.zeros((test.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_ROUND):</span><br><span class="line">    oof_round = np.zeros((y.shape[<span class="number">0</span>], <span class="number">1</span>)) <span class="comment">#构建每轮</span></span><br><span class="line">    skf = StratifiedKFold(n_splits=N_FOLDS,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          random_state=SEED*i)</span><br><span class="line">    <span class="keyword">for</span> fold, (tr_idx, ts_idx) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf.split(X, y)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n ---------Training Round <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> Fold <span class="subst">&#123;fold+<span class="number">1</span>&#125;</span> ----\n&quot;</span>)</span><br><span class="line">        <span class="comment">#标准化</span></span><br><span class="line">        qtrain = qlabeled[tr_idx]</span><br><span class="line">        qval = qlabeled[ts_idx]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Binned</span></span><br><span class="line">        btrain = blabled[tr_idx]</span><br><span class="line">        bval= blabled[ts_idx]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#encoded</span></span><br><span class="line">        etrain = elabeled[tr_idx]</span><br><span class="line">        <span class="built_in">eval</span> = elabeled[ts_idx]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#target</span></span><br><span class="line">        ytrain = y[tr_idx]</span><br><span class="line">        yval = y[ts_idx]</span><br><span class="line">        </span><br><span class="line">        K.clear_session()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#模型训练</span></span><br><span class="line">        nn_model = get_res_model()</span><br><span class="line">        nn_model.fit([qtrain, etrain, btrain],</span><br><span class="line">                  ytrain,</span><br><span class="line">                  batch_size=<span class="number">2048</span>,</span><br><span class="line">                  epochs=EPOCH,</span><br><span class="line">                  validation_data=([qval, <span class="built_in">eval</span>, bval], yval),</span><br><span class="line">                  callbacks=[es, plateau],</span><br><span class="line">                  verbose=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        nn_model.save(data_dir/<span class="string">f&#x27;model<span class="subst">&#123;i&#125;</span>.ckpt&#x27;</span>)</span><br><span class="line">        <span class="comment">#预测</span></span><br><span class="line">        pred_round = nn_model.predict([qval, <span class="built_in">eval</span>, bval])</span><br><span class="line">        oof[ts_idx] += pred_round / N_ROUND<span class="comment">#预测值/轮数</span></span><br><span class="line">        oof_round[ts_idx] += pred_round</span><br><span class="line">        </span><br><span class="line">        pred += nn_model.predict([qunlabled, eunlabeled, bunlabled]) / (N_FOLDS * N_ROUND)</span><br><span class="line">    score_round = math.sqrt(mean_squared_error(y, oof_round))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;第<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>轮分数<span class="subst">&#123;score_round&#125;</span>=====\n&quot;</span>)</span><br><span class="line">score_round = math.sqrt(mean_squared_error(y, oof))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终分数<span class="subst">&#123;score_round&#125;</span>=====\n&quot;</span>)</span><br></pre></td></tr></table></figure><h6 id="10-预测"><a href="#10-预测" class="headerlink" title="10. 预测"></a>10. 预测</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sample_submission = pd.read_csv(data_dir/<span class="string">&#x27;sample_submission.csv&#x27;</span>)</span><br><span class="line">sample_submission[<span class="string">&#x27;pred&#x27;</span>] = pred</span><br><span class="line">pd.DataFrame(oof).to_csv(<span class="string">&#x27;oof.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">sample_submission.to_csv(<span class="string">&#x27;sb1.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">display(pd.read_csv(<span class="string">&#x27;sb1.csv&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kaggle </tag>
            
            <tag> Autoencoder </tag>
            
            <tag> denoise </tag>
            
            <tag> PCA </tag>
            
            <tag> quantile normalize </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. XGboost 上:原理</title>
      <link href="2021/08/19/ML_1.%20XGboost%20%E4%B8%8A%E5%8E%9F%E7%90%86/"/>
      <url>2021/08/19/ML_1.%20XGboost%20%E4%B8%8A%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="1-XGboost-上-原理"><a href="#1-XGboost-上-原理" class="headerlink" title="1. XGboost 上:原理"></a>1. XGboost 上:原理</h3><h4 id="1-XGboost-目标函数"><a href="#1-XGboost-目标函数" class="headerlink" title="1. XGboost 目标函数"></a>1. XGboost 目标函数</h4><p><a href="https://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf">XGboost</a>的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，并且适合多平台和gpu计算。  XGBoost的目标函数由训练损失和正则项构成，定义如下：</p><script type="math/tex; mode=display">\text{Obj}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k=1}^{K} \Omega\left(f_{k}\right) \tag{1}</script><p>其中，<script type="math/tex">\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)</script>是训练损失， <script type="math/tex">\sum_{k=1}^{K} \Omega\left(f_{k}\right)</script>是正则项。</p><p>这里，我们只要理解3个名词，1.<strong>训练损失</strong>，2. <strong><script type="math/tex">f_k</script>​所代表的的每颗树的函数表示</strong>, 3.<strong>每颗树的复杂度总和代表的正则项部分</strong>。</p><ol><li>对于机器学习算法来说，任何模型预测值和真实值之间的差值要有一个衡量的指标——损失函数(loss function 或 cost function)，这个差值可以通过损失函数计算得来的。拿什么做损失函数是可以选择、试验的，如回归问题用<code>均方根预测rmse:root mean squared error</code> 或者 分类问题用<code>Logistic  Loss</code>  <script type="math/tex">l\left(y_{i}, \hat{y}_{i}\right)=y_{i} \ln \left(1+e^{-\hat{y}_{i}}\right)+\left(1-y_{i}\right) \ln \left(1+e^{\hat{y}_{i}}\right)</script>.</li><li>对于每个样本<script type="math/tex">(x_i, y_i)</script>​​, 通过每颗树预测后可表示为<script type="math/tex">f_k(x_i)</script>​​。由于XGboost是一个加法模型，最终预测值可以表示为每颗树的累加之和。</li></ol><script type="math/tex; mode=display">\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), \quad f_{k} \in \mathcal{F} \tag{2}</script><ol><li>对于每个棵树的其复杂度一般是要限制的，如果每棵树过于复杂，就是每个弱分类器过于复杂，极易导致过拟合，机器学习中常用正则项(regularization term)惩罚过于复杂的模型。复杂度表示为<script type="math/tex">\Omega\left(f_{k}\right)</script>​,累加后表示为：</li></ol><script type="math/tex; mode=display">\sum_{k=1}^{K} \Omega\left(f_{k}\right) \tag{3}</script><p>这就是XGboost 目标函数基本解释。</p><h4 id="2-如何通过boosting算法学习到的第t棵树？"><a href="#2-如何通过boosting算法学习到的第t棵树？" class="headerlink" title="2. 如何通过boosting算法学习到的第t棵树？"></a>2. 如何通过boosting算法学习到的第t棵树？</h4><p>因为XGboost是boosting算法中一员，可采用前向分步加法来计算，第i个样本<script type="math/tex">x_i</script>​的第t步的预测值：</p><script type="math/tex; mode=display">\hat{y}_{i}^{(t)}=\hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right) \tag{4}</script><p>就是，第<script type="math/tex">t</script>步预测等于第<script type="math/tex">t-1</script>步预测值加上第t棵树的预测值，其中第<script type="math/tex">t-1</script>步预测值<script type="math/tex">\hat{y}_{i}^{(t-1)}</script>在这时可以看做已知常数。将式4代入式1有：</p><script type="math/tex; mode=display">\begin{aligned}\text{Obj}^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t)}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant }\end{aligned} \tag{5}</script><p>上式5中，只有<script type="math/tex">f_{t}\left(x_{i}\right)</script>是变量。第2步到第3步是因为对于从<script type="math/tex">i=1</script>到<script type="math/tex">i=t-1</script>时刻，对应树的复杂度由于树已经确定可以看作一个常数,数学表示如下：</p><script type="math/tex; mode=display">\sum_{i=1}^{t} \Omega\left(f_{i}\right)= \Omega\left(f_{t}\right)+\sum_{i=1}^{t-1} \Omega\left(f_{i}\right)=\Omega\left(f_{t}\right)+\text { constant } \tag{6}</script><h4 id="3-泰勒近似XGboost目标函数"><a href="#3-泰勒近似XGboost目标函数" class="headerlink" title="3. 泰勒近似XGboost目标函数"></a>3. 泰勒近似XGboost目标函数</h4><p>对于在<script type="math/tex">x_0</script>处<script type="math/tex">n</script>阶导都存在的函数<script type="math/tex">f(x)</script>,我们可以用其泰勒展开近似。</p><p>根据泰勒公式，<script type="math/tex">f(x)</script>在<script type="math/tex">(x + \Delta x)</script>展开为:</p><script type="math/tex; mode=display">f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x)( \Delta x)^{2}   \tag{7}</script><p>那么对于XGboost目标函数, 根据式5，<script type="math/tex">f(x)</script>​对应损失为<script type="math/tex">l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)</script>​。其中，</p><ul><li><script type="math/tex">t-1</script>​棵树的预测值<script type="math/tex">\hat{y}_{i}^{(t-1)}</script>​看成<script type="math/tex">x</script>​​</li><li>正在训练的<script type="math/tex">f_{t}\left(x_{i}\right)</script>看作<script type="math/tex">\Delta x</script></li></ul><p>而对于式5中， 损失函数对<script type="math/tex">x</script>​即<script type="math/tex">\hat{y}_{i}^{(t-1)}</script>​​的一阶偏导和二阶偏导分别可记为<script type="math/tex">g_i, h_i</script>​，如下式9中表示。</p><p>那么损失函数可以改写为:</p><script type="math/tex; mode=display">l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right) \approx l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right) \tag{8}</script><p>其中，</p><script type="math/tex; mode=display">g_i = \partial_{\hat{y}_{i}^{(t-1)}} \ l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)\\h_i = \partial^2_{\hat{y}_{i}^{(t-1)}} \ l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)\tag{9}</script><p>将式8代入式5得:</p><script type="math/tex; mode=display">\begin{aligned}\text{Obj}^{(t)} &\approx  \sum_{i=1}^{n} \left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }\end{aligned} \tag{10}</script><p>我们可以去掉常数项,这样就有:</p><script type="math/tex; mode=display">\begin{aligned}\text{Obj}^{(t)} &\approx  \sum_{i=1}^{n} \left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)\end{aligned} \tag{11}</script><p>这里<script type="math/tex">g_i, h_i</script>​由式9中表示。</p><p>如果使用平方损失，就有：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210817125213.png" alt="image-20210817125203044" style="zoom:25%;" /></p><h4 id="4-XGboost中的决策树及复杂度"><a href="#4-XGboost中的决策树及复杂度" class="headerlink" title="4. XGboost中的决策树及复杂度"></a>4. XGboost中的决策树及复杂度</h4><p>XGboost的基模型不仅支持决策树，还支持线性模型。本文只介绍决策树。其由两部分组成:</p><script type="math/tex; mode=display">f_{t}(x)=w_{q(x)}, \quad w \in \mathbf{R}^{T}, q: \mathbf{R}^{d} \rightarrow\{1,2, \cdots, T\} \tag{12}</script><ol><li>叶子节点的权重向量<script type="math/tex">w</script></li><li>每个实例样本到叶子节点的映射关系<script type="math/tex">q</script>(其实就是一个树型的分支结构)</li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210817131913.png" alt="image-20210817131910526" style="zoom:25%;" /></p><p>这样我们就可定义决策树的复杂度<script type="math/tex">\Omega</script>由叶子节点数目<script type="math/tex">T</script>​构成，叶子节点越少模型越简单，此外叶子节点也不能权重过大。这样复杂度就可以表示为叶子节点数目和对应权重的<script type="math/tex">L_2</script>范数。具体表示为：</p><script type="math/tex; mode=display">\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \tag{13}</script><p><strong>定义树的复杂度</strong>图示如下：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210817132048.png" alt="image-20210817131958000" style="zoom:25%;" /></p><h4 id="5-叶子节点归组"><a href="#5-叶子节点归组" class="headerlink" title="5. 叶子节点归组"></a>5. 叶子节点归组</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210817130725.png" alt="image-20210817130718200" style="zoom:25%;" /></p><p>对于上图中叶子节点3，我们可以将其所有样本<script type="math/tex">x_i</script>都划入到一个叶子节点的集合中,假设这个叶子节点为第<script type="math/tex">j</script>个，那么可以数学表达为<script type="math/tex">I_j = \{i|q(x_i) = j\}</script>。将式12和13代入式11并结合该表达式有:</p><script type="math/tex; mode=display">\begin{aligned}O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\&=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\lambda \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T\end{aligned} \tag{14}</script><p>上式中，第二步是遍历所有样本之后求每个样本的损失和，但也可从每个叶子节点出发，计算每个叶子节点中样本集合的损失和，这就是<script type="math/tex">\sum_{i \in I_{j}} g_{i}</script>  和<script type="math/tex">\sum_{i \in I_{j}} h_{i}</script>表达的意思，即划分到每个叶子节点中样本集合的损失和。<script type="math/tex">w_j</script>  表示第 <script type="math/tex">j</script>个叶子节点取值。</p><p>不妨假定以下记号：</p><script type="math/tex; mode=display">G_j= \sum_{i \in I_{j}} g_{i}\\H_j = \sum_{i \in I_{j}} h_{i} \tag{15}</script><p>其中,</p><ul><li><script type="math/tex">G_j</script>: 所有<strong>第<script type="math/tex">j</script>个叶子节点所包含的一阶偏导数</strong>之和，是一个常数。</li><li><script type="math/tex">H_j</script>: 所有<strong>第<script type="math/tex">j</script>个叶子节点所包含的二阶偏导数</strong>之和，是一个常数。</li></ul><p>将式15代入式14可得：</p><script type="math/tex; mode=display">\begin{aligned}\text{Obj}^{(t)} &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \\&=\sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T\end{aligned} \tag{16}</script><p>在第<script type="math/tex">t</script>步时，<script type="math/tex">G_j, H_j</script>是<script type="math/tex">t-1</script>步已经确定的结构，可以计算得到，是常数。因此式16只有叶子节点权重<script type="math/tex">w_j</script>是不确定的。</p><h4 id="6-树结构打分"><a href="#6-树结构打分" class="headerlink" title="6. 树结构打分"></a>6. 树结构打分</h4><p>上小节中，我们已经将目标表达式进一步简化了。现在我们来求其极小值。</p><p>不妨回忆一下一元二次函数的极小值求解，假定有如下函数：</p><script type="math/tex; mode=display">Gx+ \frac{1}{2}Hx^2, \quad H \gt 0 \tag{17}</script><p>易得：</p><script type="math/tex; mode=display">\begin{aligned}x^\star &= -\frac{b}{2a} = -\frac{G}{H}\\y^\star &= \frac{4ac-b^2}{4a} = -\frac{G^2}{2H}\end{aligned} \tag{18}</script><p>那对于式16，我们只看一个叶子节点有:</p><script type="math/tex; mode=display">G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2} \tag{19}</script><p>记得只将<script type="math/tex">w_j</script>看作变量,那么可得:</p><script type="math/tex; mode=display">w^\star = -\frac{G_{j}}{H_{j}+\lambda} \tag{20}</script><p>代入式19得：</p><script type="math/tex; mode=display">-\frac{G_{j}^{2}}{2(H_{j}+\lambda)} \tag{21}</script><p>其用来衡量每棵树结构的好坏。代入式16得到整个目标函数为：</p><script type="math/tex; mode=display">\text{Obj}^{(t)} = -\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T \tag{22}</script><p>下图是目标函数计算例子，我们可以求得目标函数对每个节点中每个样本的一阶偏导数<script type="math/tex">g_i</script>和二阶偏导数<script type="math/tex">h_i</script>,然后对于每个节点中样本集合求<script type="math/tex">G_j, H_j</script>, 最后遍历求和决策树的所有节点就得到了目标函数。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210818164113.png" alt="image-20210817173616581" style="zoom:25%;" /></p><h4 id="7-叶子节点最优划分算法"><a href="#7-叶子节点最优划分算法" class="headerlink" title="7. 叶子节点最优划分算法"></a>7. 叶子节点最优划分算法</h4><p>对于单棵树的的搜索算法，我们可以按照以下流程：</p><ol><li>列举所有可能的树结构q</li><li>计算对于树结构q的分数 <script type="math/tex">-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T</script></li><li>找到最佳的树结构，并优化叶子节点权重<script type="math/tex">w^\star = -\frac{G_{j}}{H_{j}+\lambda}</script></li></ol><p>但是这样，有可能找到无限棵树结构。在实际训练过程中，建立第t棵树时，最重要的是最优划分叶子节点，XGboost支持两种划分方法——贪心算法和近似算法。</p><p> <strong>贪心算法</strong></p><p>从树的深度为0开始</p><ol><li>对每个叶子节点尝试进行分裂</li><li>每次分裂后，原始的叶子节点会分裂为左右两个子叶子节点，原始的叶子节点中样本集将按照划分规则分散到左右两个叶子节点中</li><li>新分裂完成后，检查这次分裂是否给损失函数带来增益Gain。</li></ol><p>这里重点介绍下，分裂增益计算：</p><p>根据式22，分裂前，原始叶子节点的目标函数为：</p><script type="math/tex; mode=display">\text{Obj}_{1}=-\frac{1}{2}\left[\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]+\gamma\tag{23}</script><p>那分裂后，左右叶子节点的目标函数为:</p><script type="math/tex; mode=display">\text{Obj}_{2}=-\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]+2 \gamma \tag{24}</script><p>那么带来的增益，式23减去式24得：</p><script type="math/tex; mode=display">\operatorname{Gain}=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma \tag{25}</script><p><strong>注：该特征收益也可作为特征重要性的重要依据。</strong></p><p>那么<script type="math/tex">\operatorname{Gain} \gt 0</script>代表着分裂后，目标函数减小了，这样就可以考虑这次分裂结果了。</p><p>但是在一个叶子节点分裂时，可能有很多分裂点，每个分裂点都对应一个Gain，如何找到最优的分裂点，我们按照如下步骤来寻找：</p><ol><li>遍历每个节点的所有特征</li><li>对于每个特征，按照特征值大小排序</li><li>线性扫描，找到每个特征的最佳分类特征值</li><li>在所有特征中找到最好的分裂点，对应分类后Gain最大的特征</li></ol><p>整个过程，按照叶子节点特征中最佳分裂特征值来获得分裂后的最佳特征。这就是贪心算法，每次进行分类都遍历全部候选分割点，也叫Global scan。</p><blockquote><p>但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。</p><p>基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案：</p><ul><li><strong>特征预排序+缓存：</strong>XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。</li><li><strong>分位点近似法：</strong>对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。</li><li><strong>并行查找：</strong>由于各个特征已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。</li></ul><p>——<a href="https://mp.weixin.qq.com/s/wLE9yb7MtE208IVLFlZNkw">XGBoost超详细推导</a></p></blockquote><h4 id="8-早停和剪枝"><a href="#8-早停和剪枝" class="headerlink" title="8. 早停和剪枝"></a>8. 早停和剪枝</h4><p>在决策树和GBDT中，优化方法常见的有早停和后剪枝post-pruning。XGboost也一样：</p><ol><li>当新的一次分裂所带来的<script type="math/tex">\operatorname{Gain} \lt 0</script>时，这次分裂会使目标函数增大。计算式25过程中，就很容易明白，不能进行这次分裂。</li><li>如果树深度过深，非常容易过拟合，所以达到最大树深时，也要停止建树。就是常见超参数：<code>max_depth</code></li><li>当新的一次分裂后计算左右两个叶子节点样本权重和。如果任何一个叶子节点的样本权重(计算表达式为式20)低于某一个阈值，也要放弃此次分裂。这就是超参数<code>min_child_weight</code>，这能防止过拟合。</li></ol><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/83901304">深入理解XGBoost</a></p><p>[3] <a href="https://mp.weixin.qq.com/s/wLE9yb7MtE208IVLFlZNkw">XGBoost超详细推导</a></p><p>[4] <a href="https://www.biaodianfu.com/xgboost.html">xgboost</a></p><p>[5] <a href="http://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/xgboost/chapters/xgboost_usage.html">xgboost_usage</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XGboost </tag>
            
            <tag> booster </tag>
            
            <tag> 后剪枝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>24. ALBERT 论文笔记</title>
      <link href="2021/08/12/NLP%20Paper%2024.ALBERT%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2021/08/12/NLP%20Paper%2024.ALBERT%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="24-ALBERT-论文笔记"><a href="#24-ALBERT-论文笔记" class="headerlink" title="24. ALBERT 论文笔记"></a>24. ALBERT 论文笔记</h3><p>本文是<a href="https://arxiv.org/pdf/1909.11942.pdf">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS </a>论文笔记。</p><p>来自于：</p><ol><li>Google Research </li><li>Toyota Technological Institute at Chicago</li></ol><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>当预训练语言模型表征时提升模型size通常会提升在下游任务的性能。然而，模型越大，GPU、TPU显存消耗越大，训练时间越长。为了处理这些问题，本文使用了<strong>两项参数减少技术</strong>。还使用自监督loss，聚焦于建模句子内的连贯性，用多句子输入展示了其对下游任务有帮助。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>两项参数减少技术:</p><ol><li>Factorized embedding parameterization 分解嵌入参数：将原本的大的词汇embedding 矩阵分解为两个小的。</li><li>Cross-layer parameter sharing 跨层参数共享：防止参数随网络深度增加而增长</li></ol><p>为了提升表现:</p><p>使用SOP——sentence-order prediction，而不是NSP。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847901.png" alt="image-20211024181425288" style="zoom:50%;" /></p><h4 id="3-ALBERT-元素"><a href="#3-ALBERT-元素" class="headerlink" title="3. ALBERT 元素"></a>3. ALBERT 元素</h4><p><strong>Factorized embedding parameterization.</strong>： 在BERT类的模型中，词向量维度E=隐藏层维度。像BERT-large、xlarge中E也随H增大。从模型角度来说，WordPiece embedding意味中学习<strong>context-independent representations</strong>上下文无关表示，就是E代表着上下文无关信息。而隐藏层embedding学习的是 <strong>context-dependent representations</strong>上下文相关表示，就是H代表着上下文相关信息。预训练的目标是学习上下文相关的表示H。Factorized，就是在词表V和隐层H间插入一个低维度的E，具体变换为:</p><script type="math/tex; mode=display">O(V \times E + E \times H) \tag{1}</script><p>这样参数从以前的<script type="math/tex">O(V\times H)</script>减少到了<script type="math/tex">O(V \times E + E\times H)</script>.如果H远远大于E的话，比如H=1024, E=128，V=30,000.减小的参数量还是很大的。但最重要减少参数的在下一项技术Cross-layer parameter sharing。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847902.png" alt="image-20211024163913101" style="zoom:33%;" /></p><p>这里ALBERT-large参数为18M，参数共享。但是下表3中ALBERT-base共享与不共享参数：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847904.png" alt="image-20211024164536173" style="zoom:33%;" /></p><p>不共享参数，E从768到64参数量从108M到87M，只减少21M参数量。</p><p>共享参数，E=64的ALBERT-base参数量为10M。</p><p>具体到表1中，BERT-large这部分参数量为(30000+512)x1024=31,244,288。</p><p>ALBERT-large参数量为(30000+512)x128 + 128x1024=4,036,608,。那么Factorized embedding parameterization带来的参数减少大约是 31,244,288-4,036,608=27,207,680.大约为27M。那么只能将参数从BERT实际参数量334M-27M=307M左右。不能达到18M。因此减少的原因主要来自于参数共享。</p><p>注:这部分计算来自于 <a href="https://www.zhihu.com/question/347898375/answer/839028590">小莲子在知乎回答:如何看待瘦身成功版BERT——ALBERT？</a></p><p><strong>Cross-layer parameter sharing.</strong></p><p>在参数共享部分，有3中共享模式：只共享attention，只共享FFN，共享所有参数</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847905.png" alt="image-20211024170044858" style="zoom:50%;" /></p><p>这里要注意的是：只共享attention，性能掉的不太多，但是参数量也不会将太大，感觉都在20%左右。要达到上面那种334M到18M的减少，逐步进行了共享FFN，所有共享尝试。</p><p><strong>SENTENCE ORDER PREDICTION(SOP)</strong>:</p><p>SOP正样本是原始同一文档中采样的两个连续句子。</p><p>如: 我大四课程都完成了。接下来，我就是毕业生。这两句话作为<strong>正样本</strong>。</p><p>直接交换这两个句子变成:接下来，我就是毕业生。我大四课程都完成了。作为<strong>负样本。</strong></p><p>然后，如果输入正样本预测为1，输入负样本预测为0.跟NSP任务实现类似。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>1.<a href="https://amitness.com/2020/02/albert-visual-summary/">Visual Paper Summary: ALBERT (A Lite BERT)</a></p><p>[2]. <a href="https://www.zhihu.com/question/347898375/answer/839028590"><strong>所谓的“Lite Bert”，并不像我所期待的那样“轻量级”</strong>。</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ALBERT </tag>
            
            <tag> Factorized embedding parameterization </tag>
            
            <tag> Cross-layer parameter sharing </tag>
            
            <tag> SOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. 图片数据增强</title>
      <link href="2021/04/21/1.%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"/>
      <url>2021/04/21/1.%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="1-图片数据增强"><a href="#1-图片数据增强" class="headerlink" title="1. 图片数据增强"></a>1. 图片数据增强</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_augmentations</span>(<span class="params">path</span>):</span></span><br><span class="line">    original = cv2.imread(path) <span class="comment">#读取原图</span></span><br><span class="line">    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)<span class="comment">#转RGB</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#Transformations p=0.5一般是应用该变换的概率</span></span><br><span class="line">    <span class="comment"># RGB平移 g通道平移最多50 注意添加[]</span></span><br><span class="line">    transform_rgb = alb.Compose([RGBShift(g_shift_limit=<span class="number">50</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#色调饱和度值,3个参数：随机色调、饱和度、值变化。</span></span><br><span class="line">    transform_hsv = alb.Compose([HueSaturationValue(hue_shift_limit=<span class="number">10</span>,\</span><br><span class="line">                    sat_shift_limit=<span class="number">60</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#沿y轴水平翻转输入图片</span></span><br><span class="line">    transform_hf = alb.Compose([HorizontalFlip(always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment">#对比度受限自适应直方图均衡</span></span><br><span class="line">    transform_clahe = alb.Compose([CLAHE(clip_limit=<span class="number">10.0</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment">#随机裁剪</span></span><br><span class="line">    transform_rc = alb.Compose([RandomCrop(height=<span class="number">300</span>, width=<span class="number">300</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment"># 随机gama</span></span><br><span class="line">    transform_rg = alb.Compose([RandomGamma(gamma_limit=(<span class="number">200</span>, <span class="number">400</span>), always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment"># 随机旋转, 最多90度</span></span><br><span class="line">    transform_rot = alb.Compose([Rotate(limit=<span class="number">90</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment"># 随机中心裁剪</span></span><br><span class="line">    transform_cc = alb.Compose([CenterCrop(height=<span class="number">450</span>, width=<span class="number">450</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment"># 中值滤波</span></span><br><span class="line">    transform_mb = alb.Compose([MedianBlur(blur_limit=<span class="number">103</span>, always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment">#围绕X轴垂直翻转</span></span><br><span class="line">    transform_vf = alb.Compose([VerticalFlip(always_apply=<span class="literal">True</span>)])</span><br><span class="line">    <span class="comment">#通过从255减去像素值来反转输入图像</span></span><br><span class="line">    transform_ii = alb.Compose([InvertImg(always_apply=<span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#应用transform</span></span><br><span class="line">    transformed_rgb = transform_rgb(image=original)[<span class="string">&#x27;image&#x27;</span>]</span><br><span class="line">    transformed_hsv = transform_hsv(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_hf = transform_hf(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_clahe = transform_clahe(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_rc = transform_rc(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_rg = transform_rg(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_rot = transform_rot(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_cc = transform_cc(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_mb = transform_mb(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_vf = transform_vf(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    transformed_ii = transform_ii(image=original)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line"></span><br><span class="line">    all_transformations = [original, transformed_rgb, transformed_hsv, transformed_hf,</span><br><span class="line">                           transformed_clahe, transformed_rc, transformed_rg, transformed_rot,</span><br><span class="line">                           transformed_cc, transformed_mb, transformed_vf, transformed_ii]</span><br><span class="line">    all_names = [<span class="string">&quot;Original&quot;</span>, <span class="string">&quot;RGBShift&quot;</span>, <span class="string">&quot;HueStaurationValue&quot;</span>, <span class="string">&quot;HorizontalFlip&quot;</span>, <span class="string">&quot;CLAHE&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;RandomCrop&quot;</span>, <span class="string">&quot;RandomGamma&quot;</span>, <span class="string">&quot;Rotate&quot;</span>, <span class="string">&quot;CenterCrop&quot;</span>, <span class="string">&quot;MedianBlur&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;VerticalFlip&quot;</span>, <span class="string">&quot;InvertImg&quot;</span>]</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">28</span>, <span class="number">20</span>))</span><br><span class="line">    plt.suptitle(<span class="string">f&quot;Image Augmentations&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    <span class="keyword">for</span> k, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_transformations):</span><br><span class="line">        fig.add_subplot(<span class="number">3</span>, <span class="number">4</span>, k+<span class="number">1</span>)</span><br><span class="line">        plt.title(all_names[k])</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">example_path = <span class="string">r&quot;hellokity.jpg&quot;</span></span><br><span class="line">display_augmentations(path=example_path)</span><br></pre></td></tr></table></figure><p>效果：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261728890.png" alt="dag" style="zoom:33%;" /></p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://blog.csdn.net/zhangyuexiang123/article/details/107705311">Augmentation</a></p><p>[2] <a href="https://github.com/CrazyVertigo/awesome-data-augmentation">awesome-data-augmentation</a></p><p>[3] <a href="https://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/api/augmentations.html">augmentations</a></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> augmentation </tag>
            
            <tag> cv skills </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>17.Transformer-XL论文笔记</title>
      <link href="2021/03/13/NLP%20Paper%2017.Transformer-XL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2021/03/13/NLP%20Paper%2017.Transformer-XL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="17-Transformer-XL论文笔记"><a href="#17-Transformer-XL论文笔记" class="headerlink" title="17.Transformer-XL论文笔记"></a>17.Transformer-XL论文笔记</h3><p>本文是<a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>笔记，是后续XLM-RoBERTa的基础。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847709.png" alt="image-20211123215812006" style="zoom:33%;" /></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>Transformers拥有学习长距离依赖的潜力，但其在语言模型中被固定长度的上下文限制。本文提出了一个新颖的神经架构<strong>Transformer-XL</strong>(extra long),用其<strong>破坏时序连贯性(disrupting temporal coherence)</strong>从而能学习超出固定长度的依赖。它由segment-level循环机制和与众不同的位置编码方案组成。该方法不仅能捕获长距离依赖信息，还能解决<strong>上下文碎片化(context fragmentation problem) </strong>[就是将一段文本分割后，后一段词相对前一段词的语义关系损失掉了]。结果上， Transformer-XL学习的依赖比RNNs长80%, 比<a href="https://arxiv.org/pdf/1808.04444.pdf"><strong>vanilla Transformers</strong></a> 长450%, 在长短序列上都取得更好表现，并且在评估时比<strong>vanilla Transformers</strong>快1800+倍。开源代码地址： <a href="https://github.com/kimiyoung/transformer-xl">transformer-xl 官方</a>.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847711.png" alt="image-20211125014724948" style="zoom:33%;" /></p><h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><p>一方面，像RNNS的各种变种都在解决长距离依赖问题，RNNs本身由于梯度消失和爆炸难以优化。梯度裁剪这种技巧也不能有效解决这个问题。经验上，LSTM语言模型上下文长度最好在200左右，就没有什么进一步提升空间了。</p><p>另一方面，在注意力机制中，直接连接长距离的单词对来固定特征，能缓解优化并学习长距离信息。vanilla Transformers中设计了一系列辅助损失来训练针对character-level语言模型的深层Transformer网络。尽管其成功了，但其还是在分割的固定长度的几百个字符(characters)片段上,信息没有跨越片段。这样模型就不能捕获超过预定义上下文长度的信息。另外，<strong>固定长度片段通过选择连续的字符块来创建而不考虑句子或其他任何语义边界。因此，模型缺乏必要的上下文信息，来预测开始几个词，导致低效地优化和推断性能不佳。</strong>本文将该问题描述为<strong>上下文碎片化(context fragmentation).</strong></p><p>为了处理上述问题，提出了<strong>Transformer-XL</strong>(extra long).不再像<a href="https://arxiv.org/pdf/1808.04444.pdf"><strong>vanilla Transformers</strong></a> 一样从零开始计算新片段的隐藏状态。而是<strong>将前一个片段的隐藏状态缓存下来，跟下一个片段建立循环连接给其使用</strong>。这样，建模非常长的依赖关系也是可能的，因为信息能够通过<strong>循环连接传播</strong>。同时，<strong>前一个片段传递的信息也能解决上下文碎片化问题</strong>。</p><p>更重要的是，<strong>Transformer-XL</strong>使用相对位置而不是绝对文章，为了能让缓存的隐藏状态再利用不造成时序冲突。(下面会详细讲).因此，作为一项额外的技术贡献，引入简单但有效的相对位置编码公式，能推广使用使得注意力在评估阶段比训练阶段要更长。</p><h4 id="3-Model"><a href="#3-Model" class="headerlink" title="3. Model"></a>3. Model</h4><h5 id="3-1-Vanilla-Transformer-Language-Models"><a href="#3-1-Vanilla-Transformer-Language-Models" class="headerlink" title="3.1 Vanilla Transformer Language Models"></a>3.1 Vanilla Transformer Language Models</h5><p>为了应用Transformer 或 自注意力到语言模型，关键问题是<strong>如何训练一个Transformer来有效地编码任意长度的上下文到固定长度的表示</strong>。给定有限的内存和计算能力，一个简单的方案是将余下的上下文序列用一个非条件的Transformer decoder，类似于前馈神经网络。然而，这通常在实践中有限的资源是不可行的。</p><p>一个可行方案是直接把余下的语料分割成短的可处理片段，只在每个片段上训练模型，忽略当前文本中使用单词和之前片段单词之间的信息。这就是vanilla Transformers提出来的想法，这里称作vanilla model。如下图1所示。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847712.png" alt="image-20211124005250735" style="zoom:25%;" /></p><p>在该训练流程中，信息无论是前向还是后向都不会跨片段流过。这在使用固定长度上下文时有两个严重限制.</p><ol><li>最大长距离依赖上限在片段长度，也就几百个字符长度。<ul><li>尽管自注意力机制不像RNNS那样受梯度消失影响，vanilla model还不能完全利用该优化的优点。(训练了前面片段的隐藏状态，却无法被后面的片段利用与后面片段相关的注意力)</li></ul></li><li>尽管其可能使用padding到相关句子或其它语义边界。实际上还是简单分割长上下文到固定长度的片段。这又导致上下文破裂问题。</li></ol><p>在评估阶段,在每一步，vanilla model也输入跟片段一样长的上下文，但仅仅在最后一个位置预测一个。然后，就一步一步网右移。如图1b.这样保证，每个预测利用训练中看到的最大长度，并减轻训练阶段遇到的上下文破裂问题。但是评估阶段代价非常高。(它预测时,每预测下一个词，都要重构一遍上下文， 并从头开始计算，计算速度非常慢。)</p><h5 id="3-2-Segment-Level-Recurrence-with-State-Reuse"><a href="#3-2-Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="3.2 Segment-Level Recurrence with State Reuse"></a>3.2 Segment-Level Recurrence with State Reuse</h5><p>注: <code>Transformer-XL</code>不能将语料混洗，要按照顺序，在一个batch中一次输入。不然前一个片段和后一个片段的上下文关系就没了。</p><p>为了解决使用固定长度上下文问题，作者引入循环机制到Transformer架构中。在训练时，之前片段计算的隐藏状态序列是固定住并缓存下来，在处理新的片段是再利用，像下图2a中一样。(图中新的片段拿循环连接将前面片段的固定隐藏状态连接起来，这样就能传递信息流了，也就能利用前面片段的历史信息。)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847713.png" alt="image-20211124162604639" style="zoom:25%;" /></p><p>尽管梯度仍然停留在一个片段内，但额外的输入允许网络利用历史信息，这导致模型有长距离依赖的能力避免上下文碎片化。正式定义，假设有两个长度为L的片段:</p><script type="math/tex; mode=display">\begin{align}\boldsymbol{s}_{\tau} &= [\boldsymbol{x}_{\tau, 1}, \cdots, \boldsymbol{x}_{\tau, L}] \\\boldsymbol{s}_{\tau+1} &= [\boldsymbol{x}_{\tau+1, 1}, \cdots, \boldsymbol{x}_{\tau+1, L}] \end{align}</script><p>第<script type="math/tex">\tau</script>个片段对应生成的第<script type="math/tex">n</script>层的隐藏状态序列表示为<script type="math/tex">\boldsymbol{h}_{\tau}^n \in \mathbb{R}^{L\times d}</script>.</p><ul><li><script type="math/tex">d</script>表示隐藏状态维度，<script type="math/tex">L</script>是片段长度。</li></ul><p>那么，第<script type="math/tex">\boldsymbol{s}_{\tau+1}</script>片段的第<script type="math/tex">n</script>层隐藏状态按照以下方式生成:</p><ol><li><p>将固定并缓存下来的<script type="math/tex">\boldsymbol{h}_{\tau}^{n-1}</script>，论文叫stop-gradient，记作<script type="math/tex">SG(\cdot)</script>.</p><p>这里<script type="math/tex">[SG(\boldsymbol{h}_{\tau}^{n-1}) \circ \boldsymbol{h}_{\tau+1}^{n-1}]</script>表示：将<script type="math/tex">\boldsymbol{h}_{\tau}^{n-1}</script>和<script type="math/tex">\boldsymbol{h}_{\tau+1}^{n-1}</script>按位置拼起来。就是相邻片段对应的隐藏状态拼接。这样，<script type="math/tex">\widetilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right]</script>得到了<script type="math/tex">\widetilde{\mathbf{h}}_{\tau+1}^{n-1}</script></p></li><li><p>接下跟BERT类似得到<script type="math/tex">\mathbf{q, k, v}</script>. 其中<script type="math/tex">\mathbf{k, v}</script>依赖前一个片段的隐藏状态，是乘以<script type="math/tex">\widetilde{\mathbf{h}}_{\tau+1}^{n-1}</script>得到。</p></li><li><p>最后，过常规Transformer层得到<script type="math/tex">\mathbf{h}_{\tau+1}^n</script>.</p></li></ol><p>总的表示就是论文中，</p><script type="math/tex; mode=display">\begin{array}{l}\widetilde{\mathbf{h}}_{\tau+1}^{n-1}=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right] \\\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top}, \\\mathbf{h}_{\tau+1}^{n}=\operatorname{Transformer-Layer}\left(\mathbf{q}_{\tau+1}^{n+1}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n+1}\right)\end{array}</script><p>在评估阶段，像图2b中，如果每个片段是L，训练时划分了N个片段，它能获取的最大上下文信息长度为<script type="math/tex">N\times L</script>.(实际上GPU显存限制，设置为m).而且会比Vanilla model快1800+倍，它有缓存好的状态并完整保存下来，评估直接用，不用从头计算。</p><h5 id="3-3-Relative-Positional-Encodings"><a href="#3-3-Relative-Positional-Encodings" class="headerlink" title="3.3 Relative Positional Encodings"></a>3.3 Relative Positional Encodings</h5><p>在标准的Transformer中，序列的次序信息用positional encodings表示，记作<script type="math/tex">\mathbf{U} \in \mathbb{R}_{L_{max} \times d}</script>,其中<script type="math/tex">\mathbf{U}_i</script>表示第<script type="math/tex">i</script>列，对应片段中第<script type="math/tex">i</script>个绝对位置，<script type="math/tex">L_{max}</script>是模型能输入的最大长度。实际上，就是将word embeddings和positional encodings两者按元素相加。如果按这种方式，可以简单表示为:</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{h}_{\tau+1} &=f\left(\mathbf{h}_{\tau}, \mathbf{E}_{\mathbf{s}_{\tau+1}}+\mathbf{U}_{1: L}\right) \\\mathbf{h}_{\tau} &=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1: L}\right)\end{aligned}</script><p>其中, </p><ul><li><p><script type="math/tex">\mathbf{E}_{s_{\tau}} \in \mathbb{R}_{L \times d}</script>表示<script type="math/tex">s_{\tau}</script>序列的词嵌入。</p></li><li><p><script type="math/tex">f</script>表示Transformer 变换</p><p>可以看到，<script type="math/tex">\mathbf{E}_{s_{\tau}}, \ \mathbf{E}_{s_{\tau+1}}</script>都是和<script type="math/tex">\mathbf{U}_{1:L}</script>相加。没有区别<script type="math/tex">x_{\tau, j},\ x_{\tau+1, j}</script>，其中<script type="math/tex">j=1, \cdots, L</script>, 会造成模型损失变得陡峭。</p><p>（原因是:像BERT随机生成position embedding然后训练得到，原始Transformer中sin/cos位置编码得到，那么如果训练片段长度为4，其位置编码为[0, 1, 2, 3]。拓展上下文长度为8时位置编码应该为[0, 1, 2, 3, 0, 1, 2, 3]。这是不合理的。引入解决措施：相对位置编码。）</p></li></ul><p>举例来说,query 向量<script type="math/tex">\mathbf{q}_{\tau, i}</script>要注意到key向量<script type="math/tex">\mathbf{k}_{\tau, \le i}</script>.就是计算k去查询q时,(理论上要看到i位置之前的所有位置). 不需要知道每个k向量的绝对位置来识别其在片段中的时间次序。相反，只需要知道相对位置。实际上可以创建一系列相对位置的encodings<script type="math/tex">R \in \mathbf{R}^{L_{max} \times d}</script>。这里第<script type="math/tex">i</script>列<script type="math/tex">\mathbf{R}_i</script>表示一个<script type="math/tex">i</script>与基准位置的相对距离。为了动态地增加相对距离到注意力分数中，query向量要轻松区分不同距离上<script type="math/tex">x_{\tau, j}, \ x_{\tau+1, j}</script>的表示。并且不能丢掉时序信息，像绝对位置信息能从相对距离上递归地恢复时序信息。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847714.png" alt="image-20211125005146269" style="zoom:25%;" /></p><p>(公式跟paper一样， <a href="https://www.csie.ntu.edu.tw/~miulab/s109-adl/doc/210426_MoreBERT.pdf">图来源于陈蕴侬教授ADL slide</a>)。</p><p>这个部分推荐看<a href="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/7_Transformer.html">4</a>相对位置编码讲解。</p><ul><li>第一个改变是，将所有绝对位置编码<script type="math/tex">\mathbf{U}_j</script>表示为相对位置编码 <script type="math/tex">\mathbf{R}_{i-j}</script>。这本质上反映了先验只注意跟其相对位置的距离。<script type="math/tex">\mathbf{R}</script>是不用学习的sin编码矩阵。</li><li>引入训练参数<script type="math/tex">u \in \mathbb{R}^d</script>来替换<script type="math/tex">\mathbf{U}^T_i \mathbf{R}^T_q</script>，上面公式中c项。无论query 位置如何，对不同单词注意力bias应该保持一样。参数<script type="math/tex">v \in \mathbb{R}^d</script>来替换<script type="math/tex">\mathbf{U}^T_i \mathbf{W}^T_q</script>,如d项。</li><li>最后，论文故意分开两个权重矩阵<script type="math/tex">W_{k, E}</script>和<script type="math/tex">W_{k, R}</script>来对应地生成基于content的key向量和基于location的key向量。</li></ul><p>公式中四项理解:</p><ol><li>(a)项表示基于内容的“寻址”attention</li><li>(b)表示捕获的内容相对于每个位置bias</li><li>(c)表示内容全局的偏差</li><li>(d)表示位置全局的偏差</li></ol><blockquote><p>对应绝对位置公式:</p><ul><li><p>第一项刻画了位置i的 <code>token</code> 和位置j的 <code>token</code> 的相关性。</p></li><li><p>第二项刻画了位置i的 <code>token</code> 和位置j的 <code>position</code> 的相关性。</p></li><li><p>第三项刻画了位置i的 <code>position</code> 和位置j的 <code>token</code> 的相关性。</p></li><li><p>第四项刻画了位置i的 <code>position</code> 和位置j的 <code>position</code> 的相关性。</p><p>​                                                                    ——引用[4]</p></li></ul></blockquote><p>Transformer-XL整体计算公式如下:</p><script type="math/tex; mode=display">\begin{aligned}\widetilde{\mathbf{h}}_{\tau}^{n-1}=&\left[\mathrm{SG}\left(\mathbf{m}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau}^{n-1}\right] \\\mathbf{q}_{\tau}^{n}, \mathbf{k}_{\tau}^{n}, \mathbf{v}_{\tau}^{n}=& \mathbf{h}_{\tau}^{n-1} \mathbf{W}_{q}^{n \top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{k, E}^{n}{ }^{\top}, \widetilde{\mathbf{h}}_{\tau}^{n-1} \mathbf{W}_{v}^{n \top} \\\mathbf{A}_{\tau, i, j}^{n}=& \mathbf{q}_{\tau, i}^{n}{ }^{\top} \mathbf{k}_{\tau, j}^{n}+\mathbf{q}_{\tau, i}^{n}{\mathbf{W}}_{k, R}^{n} \mathbf{R}_{i-j} \\&+u^{\top} \mathbf{k}_{\tau, j}+v^{\top} \mathbf{W}_{k, k}^{n} \mathbf{R}_{i-j} \\\mathbf{a}_{\tau}^{n}=& \text { Masked-Softmax }\left(\mathbf{A}_{\tau}^{n}\right) \mathbf{v}_{\tau}^{n} \\\mathbf{o}_{\tau}^{n}=& \text { LayerNorm }\left(\operatorname{Linear}\left(\mathbf{a}_{\tau}^{n}\right)+\mathbf{h}_{\tau}^{n-1}\right) \\\mathbf{h}_{\tau}^{n}=& \text { Positionwise-Feed-Forward }\left(\mathbf{o}_{\tau}^{n}\right)\end{aligned}</script><p>其中,A需要对全部的(i, j)对计算<script type="math/tex">\mathbf{W}_{k, R}^{n} \mathbf{R}_{i-j}</script>。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://blog.csdn.net/magical_bubble/article/details/89060213">Transformer-XL解读</a></p><p>[2] <a href="https://www.cnblogs.com/shona/p/12041055.html">Transformer-XL</a></p><p>[3] <a href="https://carlos9310.github.io/2019/11/11/transformer-xl-and-xlnet/">transformer-XL与XLNet笔记</a></p><p>[4] <a href="http://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/7_Transformer.html">三、Transformer XL</a></p><p>[5] <a href="https://zhuanlan.zhihu.com/p/115014536">NLP算法面试必备！PTMs：NLP预训练模型的全面总结</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer-XL </tag>
            
            <tag> RPE </tag>
            
            <tag> Segment-Level Recurrence Mechanism </tag>
            
            <tag> Vanilla Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo + butterfly blog安装</title>
      <link href="2021/01/28/hexo+butterfly/"/>
      <url>2021/01/28/hexo+butterfly/</url>
      
        <content type="html"><![CDATA[<h2 id="hexo-Butterfly-blog安装"><a href="#hexo-Butterfly-blog安装" class="headerlink" title="hexo + Butterfly blog安装"></a>hexo + Butterfly blog安装</h2><h3 id="1-安装hexo-butterfly"><a href="#1-安装hexo-butterfly" class="headerlink" title="1. 安装hexo butterfly"></a>1. 安装hexo butterfly</h3><h4 id="1-安装hexo"><a href="#1-安装hexo" class="headerlink" title="1. 安装hexo"></a>1. 安装hexo</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><h4 id="2-安装butterfly"><a href="#2-安装butterfly" class="headerlink" title="2. 安装butterfly"></a>2. 安装butterfly</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/Butterfly</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将hexo根目录下<code>_config.yml</code>改为<code>theme: Butterfly</code>。</p><h5 id="平滑升级"><a href="#平滑升级" class="headerlink" title="平滑升级"></a>平滑升级</h5><blockquote><p>推荐把主题默认的配置文件<code>_config.yml</code> 复製到 Hexo 根目录下的 `_config.butterfly.yml’</p><p>Hexo會自動合併主題中的<code>_config.yml</code>和 <code>_config.butterfly.yml</code>裡的配置，如果存在同名配置，會使用<code>_config.butterfly.yml</code>的配置，其優先度較高。</p></blockquote><p>安装安装 pug 以及 stylus 的渲染器插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-pug hexo-renderer-stylus --save</span><br></pre></td></tr></table></figure><h5 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set proxy&#x3D;socks:&#x2F;&#x2F;127.0.0.1:1080</span><br><span class="line">sudo npm config set proxy&#x3D;http:&#x2F;&#x2F;127.0.0.1:1081</span><br><span class="line">sudo npm config set registry&#x3D;http:&#x2F;&#x2F;registry.npmjs.org</span><br></pre></td></tr></table></figure><h5 id="local-search"><a href="#local-search" class="headerlink" title="local_search"></a>local_search</h5><p>在hexo根目录下<code>_config.yml</code>加上</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>然后，安装插件<code>npm install hexo-generator-search --save</code>.</p><h5 id="安装mermaid插件"><a href="#安装mermaid插件" class="headerlink" title="安装mermaid插件"></a>安装mermaid插件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure><p>其它插件安装跟hexo主题一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-permalink-pinyin --save <span class="comment">#中文链接转拼音链接</span></span><br><span class="line">npm install hexo-deployer-git --save <span class="comment">#git</span></span><br><span class="line">npm install hexo-generator-seo-friendly-sitemap --save <span class="comment"># sitemap</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-cover-图片和-theme"><a href="#3-cover-图片和-theme" class="headerlink" title="3. cover 图片和 theme"></a>3. cover 图片和 theme</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">cover:</span></span><br><span class="line">  <span class="comment"># display the cover or not (是否顯示文章封面)</span></span><br><span class="line">  <span class="attr">index_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">aside_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">archives_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># the position of cover in home page (封面顯示的位置)</span></span><br><span class="line">  <span class="comment"># left/right/both</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">both</span></span><br><span class="line">  <span class="comment"># When cover is not set, the default cover is displayed (當沒有設置cover時，默認的封面顯示)</span></span><br><span class="line">  <span class="attr">default_cover:</span></span><br><span class="line">    <span class="comment"># - https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/ach.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/imgs/0.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/imgs/1.jpg</span></span><br></pre></td></tr></table></figure><p>这个图片放置在，<code>blog\themes\Butterfly\source\img\imgs</code>路径下。</p><ul><li><code>theme</code></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme_color:</span> </span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">main:</span> <span class="string">&quot;#7B90D2&quot;</span></span><br><span class="line">  <span class="attr">paginator:</span> <span class="string">&quot;#91B493&quot;</span></span><br><span class="line">  <span class="attr">button_hover:</span> <span class="string">&quot;#7B90D2&quot;</span></span><br><span class="line">  <span class="attr">text_selection:</span> <span class="string">&quot;#00c4b6&quot;</span></span><br><span class="line">  <span class="attr">link_color:</span> <span class="string">&quot;#99a9bf&quot;</span></span><br><span class="line">  <span class="attr">meta_color:</span> <span class="string">&quot;#7b90d2&quot;</span></span><br><span class="line">  <span class="attr">hr_color:</span> <span class="string">&quot;#A4D8FA&quot;</span></span><br><span class="line">  <span class="attr">code_foreground:</span> <span class="string">&quot;#F47466&quot;</span></span><br><span class="line">  <span class="attr">code_background:</span> <span class="string">&quot;rgba(27, 31, 35, .05)&quot;</span></span><br><span class="line">  <span class="attr">toc_color:</span> <span class="string">&quot;#7b90d2&quot;</span></span><br><span class="line">  <span class="attr">blockquote_padding_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line">  <span class="attr">blockquote_background_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h4><p>将<code>node_modules\hexo-generator-index\lib</code>下 <code>generator.js</code>替换如下</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">&#x27;hexo-pagination&#x27;</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> config = <span class="built_in">this</span>.config;</span><br><span class="line">    <span class="keyword">var</span> posts = locals.posts;</span><br><span class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></span><br><span class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">&#x27;page&#x27;</span>;</span><br><span class="line">    <span class="keyword">return</span> pagination(<span class="string">&#x27;&#x27;</span>, posts, &#123;</span><br><span class="line">    perPage: config.index_generator.per_page,</span><br><span class="line">    layout: [<span class="string">&#x27;index&#x27;</span>, <span class="string">&#x27;archive&#x27;</span>],</span><br><span class="line">    format: paginationDir + <span class="string">&#x27;/%d/&#x27;</span>,</span><br><span class="line">    data: &#123;</span><br><span class="line">      __index: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后在文章post头加上<code>top：10</code>0数字越大越靠前</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">Hexo</span> </span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-04-18 17:32:22</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">hexo</span></span><br><span class="line"><span class="attr">tags:</span> [<span class="string">hexo1</span>, <span class="string">hexo2</span>, <span class="string">hexo3</span>]</span><br><span class="line"><span class="attr">top:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><h4 id="新建留言板页"><a href="#新建留言板页" class="headerlink" title="新建留言板页"></a>新建留言板页</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">&quot;contact&quot;</span></span><br></pre></td></tr></table></figure><p>加上 <code>/source/contact/index.md</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">contact</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-09-30 17:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">&quot;contact&quot;</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">&quot;contact&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><h4 id="改变页面宽度"><a href="#改变页面宽度" class="headerlink" title="改变页面宽度"></a>改变页面宽度</h4><p><code>themes\Butterfly\source\css\_page\common.styl</code>下</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.layout</span></span><br><span class="line">  <span class="attribute">display</span>: flex</span><br><span class="line">  margin: <span class="number">0</span> auto</span><br><span class="line">  padding: <span class="number">2rem</span> <span class="number">20px</span></span><br><span class="line">  max-width: <span class="number">1600px</span> #正文页面宽度</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="文章统计图和标签等"><a href="#文章统计图和标签等" class="headerlink" title="文章统计图和标签等"></a>文章统计图和标签等</h3><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-charts -S</span><br></pre></td></tr></table></figure><p> 编辑 主题目录<code>/Butterfly/layout/page.pug</code> 文件，在 <code>.tag-cloud</code> 下面添加一行 <code>#tags-chart</code>，在 <code>.category-content</code> 下面添加一行 <code>#categories-chart</code>.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">when</span> <span class="string">&#x27;tags&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/tags.pug</span></span><br><span class="line">          <span class="comment">#tags-chart</span></span><br><span class="line">      <span class="string">when</span> <span class="string">&#x27;link&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/flink.pug</span></span><br><span class="line">      <span class="string">when</span> <span class="string">&#x27;categories&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/categories.pug</span></span><br><span class="line">          <span class="comment">#categories-chart</span></span><br></pre></td></tr></table></figure><h3 id="字体大小和font"><a href="#字体大小和font" class="headerlink" title="字体大小和font"></a>字体大小和font</h3><p><code>D:\study\hexo\blog_ing\themes\Butterfly\source\css\_layout\head.style</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">.site-page</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">relative</span></span><br><span class="line">  <span class="attr">padding-bottom:</span> <span class="string">.3rem</span></span><br><span class="line">  <span class="attr">text-shadow:</span> <span class="string">.05rem</span> <span class="string">.05rem</span> <span class="string">.1rem</span> <span class="string">rgba($dark-black,</span> <span class="number">.3</span><span class="string">)</span></span><br><span class="line">  <span class="attr">font-size:</span> <span class="string">.60em</span></span><br><span class="line">  <span class="attr">cursor:</span> <span class="string">pointer</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改 <code>_config.butterfly.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">font:</span></span><br><span class="line">  <span class="attr">global-font-size:</span></span><br><span class="line">  <span class="attr">code-font-size:</span></span><br><span class="line">  <span class="attr">font-family:</span> <span class="string">-apple-system,</span> <span class="string">BlinkMacSystemFont,</span> <span class="string">&quot;Segoe UI&quot;</span><span class="string">,</span> <span class="string">&quot;Helvetica Neue&quot;</span><span class="string">,</span> <span class="string">Lato,</span> <span class="string">Roboto,</span> <span class="string">&quot;PingFang SC&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft JhengHei&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft YaHei&quot;</span><span class="string">,</span> <span class="string">sans-serif</span></span><br><span class="line">  <span class="attr">code-font-family:</span> <span class="string">consolas,</span> <span class="string">Menlo,</span> <span class="string">&quot;PingFang SC&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft JhengHei&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft YaHei&quot;</span><span class="string">,</span> <span class="string">sans-serif</span></span><br></pre></td></tr></table></figure><h4 id="访问map"><a href="#访问map" class="headerlink" title="访问map"></a>访问map</h4><ol><li><p><a href="https://clustrmaps.com/">clustrmaps</a> 注册一个帐号 找到Free Tools下面的Website Widget, 点击Get Map Widget 输入你的博客网址，点击Next 根据你自己的喜好选择样式Map widget或Globe Widget 找到如下代码，copy</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">id</span>=<span class="string">&quot;clstr_globe&quot;</span> <span class="attr">src</span>=<span class="string">&quot;//clustrmaps.com/globe.js?d=xxxxxxxxxxxxx&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在<code>themes\Butterfly\layout\includes\widget</code>下创建<code>card_map.pug</code></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.card-widget</span><span class="selector-class">.card-map</span></span><br><span class="line">  <span class="selector-class">.card-content</span></span><br><span class="line">    <span class="selector-class">.item-headline</span></span><br><span class="line">      i.fas.fa-globe-asia(aria-hidden=&quot;true&quot;)</span><br><span class="line">      span= _p(&#x27;访客地图&#x27;)</span><br><span class="line">    //- 下面这行适用于<span class="number">3</span>D地图(Globe Widget)</span><br><span class="line">    &lt;script type=&quot;text/javascript&quot; id=&quot;clstr_globe&quot; src=&quot;//clustrmaps.com/globe.js?d=Cxxxxxxxxxxx&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>并且在该目录下的 <code>index.pug</code>添加:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!=partial(&#x27;includes/widget/card_webinfo&#x27;, &#123;&#125;, &#123;cache:theme.fragment_cache&#125;)</span><br><span class="line">!=partial(&#x27;includes/widget/card_map&#x27;, &#123;&#125;, &#123;cache:theme.fragment_cache&#125;)</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>博客主题配置中  <code>_config.butterfly.yml</code>中加入</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">card_maps: true //如果不想显示改为false</span><br></pre></td></tr></table></figure></li></ol><ol><li><p><del>在 <code>themes\Butterfly\languages\zh-CN.yml</code> 中找到<code>card_announcement: 公告</code>并在相应位置添加<code>card_map: 访客地图</code>访客地图可自定义写。</del></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">card_toc: 目录</span><br><span class="line">card_map: 访客地图</span><br></pre></td></tr></table></figure><p><a href="https://blog.hclonely.com/posts/57bd67ce/">参考</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 杂项 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. 用TPU分类100种花</title>
      <link href="2021/01/23/%E6%8A%80%E8%83%BD_2.%E7%94%A8TPU%E5%88%86%E7%B1%BB100%E7%A7%8D%E8%8A%B1/"/>
      <url>2021/01/23/%E6%8A%80%E8%83%BD_2.%E7%94%A8TPU%E5%88%86%E7%B1%BB100%E7%A7%8D%E8%8A%B1/</url>
      
        <content type="html"><![CDATA[<h3 id="2-用TPU分类100种花"><a href="#2-用TPU分类100种花" class="headerlink" title="2. 用TPU分类100种花"></a>2. 用TPU分类100种花</h3><p>本文是 <a href="https://www.kaggle.com/c/flower-classification-with-tpus/data">Flower Classification with TPUs</a>比赛中一个使用VGG16预训练模型在TPU上的实现 <a href="https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu/">Getting started with 100+ flowers on TPU</a> 。也可以在 <a href="https://colab.research.google.com/drive/1vubT6dgqtoWV2O28dt-BsPYqf0GV0b4a?usp=sharing">Colab 查看</a>.</p><p>首先是一些参数设置，主要是设置TPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math, re, os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, precision_score, recall_score, confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line">AUTO = tf.data.experimental.AUTOTUNE</span><br><span class="line"></span><br><span class="line"><span class="comment"># TPU 或者 GPU检测</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()</span><br><span class="line">    strategy = tf.distribute.TPUStrategy(tpu)</span><br><span class="line"><span class="keyword">except</span> ValueError: <span class="comment">#使用GPU</span></span><br><span class="line">    strategy = tf.distribute.MirroredStrategy() <span class="comment">#使用GPU或多GPU机器</span></span><br><span class="line">    strategy = tf.distribute.get_strategy() <span class="comment">#默认设置two works on CPU 或单GPU</span></span><br><span class="line">    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()<span class="comment">#GPU集群</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;加速器数目:&quot;</span>, strategy.num_replicas_in_sync) <span class="comment">#输出设备数量</span></span><br><span class="line"></span><br><span class="line">IMG_SIZE = [<span class="number">512</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">12</span></span><br><span class="line">BATCH_SIZE = <span class="number">16</span> * strategy.num_replicas_in_sync</span><br><span class="line">DATA_DIR = <span class="string">r&#x27;flowers&#x27;</span></span><br><span class="line">DATA_SIZE_SELECT = &#123;</span><br><span class="line">    <span class="number">192</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-192x192&#x27;</span>,</span><br><span class="line">    <span class="number">224</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-224x224&#x27;</span>,</span><br><span class="line">    <span class="number">331</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-331x331&#x27;</span>,</span><br><span class="line">    <span class="number">512</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-512x512&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DATA_SELECT = DATA_SIZE_SELECT[IMG_SIZE[<span class="number">0</span>]]</span><br><span class="line">TRAINING_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/train/*.tfrec&#x27;</span>)</span><br><span class="line">VALID_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/val/*.tfrec&#x27;</span>)</span><br><span class="line">TEST_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/test/*.tfrec&#x27;</span>)</span><br><span class="line"></span><br><span class="line">IMG_SIZE = [<span class="number">512</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">12</span></span><br><span class="line">BATCH_SIZE = <span class="number">16</span> * strategy.num_replicas_in_sync</span><br><span class="line">DATA_DIR = <span class="string">r&#x27;/content/work/flowers&#x27;</span></span><br><span class="line">DATA_SIZE_SELECT = &#123;</span><br><span class="line">    <span class="number">192</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-192x192&#x27;</span>,</span><br><span class="line">    <span class="number">224</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-224x224&#x27;</span>,</span><br><span class="line">    <span class="number">331</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-331x331&#x27;</span>,</span><br><span class="line">    <span class="number">512</span>: DATA_DIR + <span class="string">&#x27;/tfrecords-jpeg-512x512&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DATA_SELECT = DATA_SIZE_SELECT[IMG_SIZE[<span class="number">0</span>]]</span><br><span class="line">TRAINING_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/train/*.tfrec&#x27;</span>)</span><br><span class="line">VALID_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/val/*.tfrec&#x27;</span>)</span><br><span class="line">TEST_FILENAMES = tf.io.gfile.glob(DATA_SELECT + <span class="string">&#x27;/test/*.tfrec&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#花名列表</span></span><br><span class="line">CLASSES = [<span class="string">&#x27;pink primrose&#x27;</span>,    <span class="string">&#x27;hard-leaved pocket orchid&#x27;</span>, <span class="string">&#x27;canterbury bells&#x27;</span>, <span class="string">&#x27;sweet pea&#x27;</span>,     <span class="string">&#x27;wild geranium&#x27;</span>,     <span class="string">&#x27;tiger lily&#x27;</span>,           <span class="string">&#x27;moon orchid&#x27;</span>,              <span class="string">&#x27;bird of paradise&#x27;</span>, <span class="string">&#x27;monkshood&#x27;</span>,        <span class="string">&#x27;globe thistle&#x27;</span>,         <span class="comment"># 00 - 09</span></span><br><span class="line">           <span class="string">&#x27;snapdragon&#x27;</span>,       <span class="string">&quot;colt&#x27;s foot&quot;</span>,               <span class="string">&#x27;king protea&#x27;</span>,      <span class="string">&#x27;spear thistle&#x27;</span>, <span class="string">&#x27;yellow iris&#x27;</span>,       <span class="string">&#x27;globe-flower&#x27;</span>,         <span class="string">&#x27;purple coneflower&#x27;</span>,        <span class="string">&#x27;peruvian lily&#x27;</span>,    <span class="string">&#x27;balloon flower&#x27;</span>,   <span class="string">&#x27;giant white arum lily&#x27;</span>, <span class="comment"># 10 - 19</span></span><br><span class="line">           <span class="string">&#x27;fire lily&#x27;</span>,        <span class="string">&#x27;pincushion flower&#x27;</span>,         <span class="string">&#x27;fritillary&#x27;</span>,       <span class="string">&#x27;red ginger&#x27;</span>,    <span class="string">&#x27;grape hyacinth&#x27;</span>,    <span class="string">&#x27;corn poppy&#x27;</span>,           <span class="string">&#x27;prince of wales feathers&#x27;</span>, <span class="string">&#x27;stemless gentian&#x27;</span>, <span class="string">&#x27;artichoke&#x27;</span>,        <span class="string">&#x27;sweet william&#x27;</span>,         <span class="comment"># 20 - 29</span></span><br><span class="line">           <span class="string">&#x27;carnation&#x27;</span>,        <span class="string">&#x27;garden phlox&#x27;</span>,              <span class="string">&#x27;love in the mist&#x27;</span>, <span class="string">&#x27;cosmos&#x27;</span>,        <span class="string">&#x27;alpine sea holly&#x27;</span>,  <span class="string">&#x27;ruby-lipped cattleya&#x27;</span>, <span class="string">&#x27;cape flower&#x27;</span>,              <span class="string">&#x27;great masterwort&#x27;</span>, <span class="string">&#x27;siam tulip&#x27;</span>,       <span class="string">&#x27;lenten rose&#x27;</span>,           <span class="comment"># 30 - 39</span></span><br><span class="line">           <span class="string">&#x27;barberton daisy&#x27;</span>,  <span class="string">&#x27;daffodil&#x27;</span>,                  <span class="string">&#x27;sword lily&#x27;</span>,       <span class="string">&#x27;poinsettia&#x27;</span>,    <span class="string">&#x27;bolero deep blue&#x27;</span>,  <span class="string">&#x27;wallflower&#x27;</span>,           <span class="string">&#x27;marigold&#x27;</span>,                 <span class="string">&#x27;buttercup&#x27;</span>,        <span class="string">&#x27;daisy&#x27;</span>,            <span class="string">&#x27;common dandelion&#x27;</span>,      <span class="comment"># 40 - 49</span></span><br><span class="line">           <span class="string">&#x27;petunia&#x27;</span>,          <span class="string">&#x27;wild pansy&#x27;</span>,                <span class="string">&#x27;primula&#x27;</span>,          <span class="string">&#x27;sunflower&#x27;</span>,     <span class="string">&#x27;lilac hibiscus&#x27;</span>,    <span class="string">&#x27;bishop of llandaff&#x27;</span>,   <span class="string">&#x27;gaura&#x27;</span>,                    <span class="string">&#x27;geranium&#x27;</span>,         <span class="string">&#x27;orange dahlia&#x27;</span>,    <span class="string">&#x27;pink-yellow dahlia&#x27;</span>,    <span class="comment"># 50 - 59</span></span><br><span class="line">           <span class="string">&#x27;cautleya spicata&#x27;</span>, <span class="string">&#x27;japanese anemone&#x27;</span>,          <span class="string">&#x27;black-eyed susan&#x27;</span>, <span class="string">&#x27;silverbush&#x27;</span>,    <span class="string">&#x27;californian poppy&#x27;</span>, <span class="string">&#x27;osteospermum&#x27;</span>,         <span class="string">&#x27;spring crocus&#x27;</span>,            <span class="string">&#x27;iris&#x27;</span>,             <span class="string">&#x27;windflower&#x27;</span>,       <span class="string">&#x27;tree poppy&#x27;</span>,            <span class="comment"># 60 - 69</span></span><br><span class="line">           <span class="string">&#x27;gazania&#x27;</span>,          <span class="string">&#x27;azalea&#x27;</span>,                    <span class="string">&#x27;water lily&#x27;</span>,       <span class="string">&#x27;rose&#x27;</span>,          <span class="string">&#x27;thorn apple&#x27;</span>,       <span class="string">&#x27;morning glory&#x27;</span>,        <span class="string">&#x27;passion flower&#x27;</span>,           <span class="string">&#x27;lotus&#x27;</span>,            <span class="string">&#x27;toad lily&#x27;</span>,        <span class="string">&#x27;anthurium&#x27;</span>,             <span class="comment"># 70 - 79</span></span><br><span class="line">           <span class="string">&#x27;frangipani&#x27;</span>,       <span class="string">&#x27;clematis&#x27;</span>,                  <span class="string">&#x27;hibiscus&#x27;</span>,         <span class="string">&#x27;columbine&#x27;</span>,     <span class="string">&#x27;desert-rose&#x27;</span>,       <span class="string">&#x27;tree mallow&#x27;</span>,          <span class="string">&#x27;magnolia&#x27;</span>,                 <span class="string">&#x27;cyclamen &#x27;</span>,        <span class="string">&#x27;watercress&#x27;</span>,       <span class="string">&#x27;canna lily&#x27;</span>,            <span class="comment"># 80 - 89</span></span><br><span class="line">           <span class="string">&#x27;hippeastrum &#x27;</span>,     <span class="string">&#x27;bee balm&#x27;</span>,                  <span class="string">&#x27;pink quill&#x27;</span>,       <span class="string">&#x27;foxglove&#x27;</span>,      <span class="string">&#x27;bougainvillea&#x27;</span>,     <span class="string">&#x27;camellia&#x27;</span>,             <span class="string">&#x27;mallow&#x27;</span>,                   <span class="string">&#x27;mexican petunia&#x27;</span>,  <span class="string">&#x27;bromelia&#x27;</span>,         <span class="string">&#x27;blanket flower&#x27;</span>,        <span class="comment"># 90 - 99</span></span><br><span class="line">           <span class="string">&#x27;trumpet creeper&#x27;</span>,  <span class="string">&#x27;blackberry lily&#x27;</span>,           <span class="string">&#x27;common tulip&#x27;</span>,     <span class="string">&#x27;wild rose&#x27;</span>]  <span class="comment"># 100 - 102</span></span><br></pre></td></tr></table></figure><h4 id="1-可视化工具"><a href="#1-可视化工具" class="headerlink" title="1. 可视化工具"></a>1. 可视化工具</h4><ol><li>将<code>batch data</code>变换为<code>images</code> 和 <code>labels</code> 的<code>numpy</code>格式，并且对于后面使用的<code>test data</code>这种没有<code>label</code>的数据，将<code>label</code>置为<code>None</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">np.set_printoptions(threshold=<span class="number">15</span>, linewidth=<span class="number">80</span>)<span class="comment">#threshold 列数阈值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_to_numpy_images_and_labels</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;batch data转numpy格式的 images 和 labels&quot;&quot;&quot;</span></span><br><span class="line">    images, labels = data</span><br><span class="line">    numpy_images = images.numpy()</span><br><span class="line">    numpy_labels = labels.numpy()</span><br><span class="line">    <span class="keyword">if</span> numpy_labels.dtype == <span class="built_in">object</span>: <span class="comment">#数据是binary string 如果是object就置为None</span></span><br><span class="line">        numpy_labels = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">enumerate</span>(numpy_images)]</span><br><span class="line">        <span class="comment">#如果没有labels，只有image IDs就将labels置为None</span></span><br><span class="line">    <span class="keyword">return</span>  numpy_images, numpy_labels</span><br></pre></td></tr></table></figure><ol><li>对于预测得到的<code>label</code>和<code>correct label</code>，如果一样就标识为OK，不一样就表示NO；再加符号，以及正确的花名。实际上，就是给定格式：</li></ol><p><strong>预测的花名 [OK/NO 符号/ 正确的花名/ ]</strong>。如,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">title_from_label_and_target</span>(<span class="params">label, correct_label</span>):</span></span><br><span class="line">    <span class="keyword">if</span> correct_label <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#如果未标注label, 就返回对于label名和 True的二元组</span></span><br><span class="line">        <span class="keyword">return</span> CLASSES[label], <span class="literal">True</span></span><br><span class="line">    correct = (label == correct_label) <span class="comment">#判断label是否正确</span></span><br><span class="line">    <span class="comment">#u&quot;u2192&quot; 表明是-&gt;符号</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&#123;&#125; [&#123;&#125; &#123;&#125; &#123;&#125;]&quot;</span>.<span class="built_in">format</span>(CLASSES[label], <span class="string">&#x27;OK&#x27;</span> <span class="keyword">if</span> correct <span class="keyword">else</span> <span class="string">&#x27;NO&#x27;</span>, <span class="string">u&quot;\u2192&quot;</span> <span class="keyword">if</span> <span class="keyword">not</span> correct <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                                CLASSES[correct_label] <span class="keyword">if</span> <span class="keyword">not</span> correct <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span>), correct</span><br></pre></td></tr></table></figure><ol><li>按照标题展示每张花的图片，如果不正确就用红色，正确就用黑色。并将子图中最后一位代表绘制第几张图的变量加1.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_one_flower</span>(<span class="params">image, title, subplot, red=<span class="literal">False</span>, titlesize=<span class="number">16</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;展示每张图片&quot;&quot;&quot;</span></span><br><span class="line">    plt.subplot(*subplot)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)<span class="comment">#关闭轴</span></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(title) &gt; <span class="number">0</span>:</span><br><span class="line">        plt.title(title, fontsize=<span class="built_in">int</span>(titlesize) <span class="keyword">if</span> <span class="keyword">not</span> red <span class="keyword">else</span> <span class="built_in">int</span>(titlesize/<span class="number">1.2</span>),\</span><br><span class="line">        color=<span class="string">&#x27;red&#x27;</span> <span class="keyword">if</span> red <span class="keyword">else</span> <span class="string">&#x27;black&#x27;</span>, fontdict=&#123;<span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;center&#x27;</span>&#125;, pad=<span class="built_in">int</span>(titlesize/<span class="number">1.5</span>))</span><br><span class="line">    <span class="keyword">return</span> (subplot[<span class="number">0</span>], subplot[<span class="number">1</span>], subplot[<span class="number">2</span>]+<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ol><li><p>这个函数使用多次，会用于<code>training dataset/test dataset</code>可视化，以及预测结果可视化。</p><ol><li><p>将batch data转为numpy格式，如果没有label就将labels赋值为None</p></li><li><p>通过rows， cols来调整figure， rows是绘制图的行数, 通过一个batch中images的张数的开方来确定，本意是绘制出子图构成2x2这种方形的大图。而cols通过整个images数目//rows得到。</p><p>这时，我们要看rows和cols哪个大，两者对应着整个大图的长和宽，哪个大就将长或宽分割成<code>FIGSIZE / cols * rows</code>份</p></li><li><p>给每个子图添加标题信息</p></li><li><p>展示每张花</p></li></ol></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912194.png" alt="image-20210814001834703" style="zoom:40%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_batch_of_images</span>(<span class="params">databatch, predictions=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    展示批次图片</span></span><br><span class="line"><span class="string">    :param databatch:</span></span><br><span class="line"><span class="string">    :param predictions:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    images, labels = batch_to_numpy_images_and_labels(databatch)</span><br><span class="line">    <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        labels = [<span class="literal">None</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">enumerate</span>(images)]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将数据变为方形框适合展示的, 如果一个batch是16刚好4x4展示不然就扔掉一些数据</span></span><br><span class="line">    rows = <span class="built_in">int</span>(math.sqrt(<span class="built_in">len</span>(images)))<span class="comment">#得到images</span></span><br><span class="line">    cols = <span class="built_in">len</span>(images) // rows</span><br><span class="line"></span><br><span class="line">    FIGSIZE = <span class="number">13.0</span></span><br><span class="line">    SPACING = <span class="number">0.1</span></span><br><span class="line">    subplot = (rows, cols, <span class="number">1</span>)<span class="comment">#子图</span></span><br><span class="line">    <span class="keyword">if</span> rows &lt; cols:</span><br><span class="line">        <span class="comment">#如果行数小于列数, 就让整体figure中高作为调整对象</span></span><br><span class="line">        plt.figure(figsize=(FIGSIZE, FIGSIZE / cols * rows))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.figure(figsize=(FIGSIZE/rows*cols, FIGSIZE))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(images[:rows*cols], labels[:rows*cols])):</span><br><span class="line">        title = <span class="string">&#x27;&#x27;</span> <span class="keyword">if</span> label <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> CLASSES[label]</span><br><span class="line">        correct = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> predictions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            title, correct = title_from_label_and_target(predictions[i], label)</span><br><span class="line">        dynamic_titlesize = FIGSIZE * SPACING / <span class="built_in">max</span>(rows, cols) * <span class="number">40</span> + <span class="number">3</span></span><br><span class="line">        subplot = display_one_flower(image, title, subplot, <span class="keyword">not</span>  correct, titlesize=dynamic_titlesize)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="keyword">if</span> label <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> predictions <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        plt.subplots_adjust(wspace=<span class="number">0</span>, hspace=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ol><li>展示混淆矩阵:<ul><li>绘制混淆矩阵，并设置x,y刻度值和形式</li><li>加上有关score， precision, recall 文本</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_confusion_matrix</span>(<span class="params">cmat, score, precision, recall</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制混淆矩阵&quot;&quot;&quot;</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">15</span>), dpi=<span class="number">200</span>)</span><br><span class="line">    ax = plt.gca()<span class="comment">#get current axes</span></span><br><span class="line">    ax.matshow(cmat, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    ax.set_xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)))</span><br><span class="line">    ax.set_xticklabels(CLASSES, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">7</span>&#125;)</span><br><span class="line">    <span class="comment">#设置ax.get_xticklabels()格式45度旋转</span></span><br><span class="line">    plt.setp(ax.get_xticklabels(), rotation=<span class="number">45</span>, ha=<span class="string">&#x27;left&#x27;</span>, rotation_mode=<span class="string">&#x27;anchor&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    ax.set_yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)))</span><br><span class="line">    ax.set_yticklabels(CLASSES, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">7</span>&#125;)</span><br><span class="line">    <span class="comment"># 设置ax.get_yticklabels()格式45度旋转</span></span><br><span class="line">    plt.setp(ax.get_yticklabels(), rotation=<span class="number">45</span>, ha=<span class="string">&#x27;right&#x27;</span>, rotation_mode=<span class="string">&#x27;anchor&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#给图加上文本</span></span><br><span class="line">    titlestring = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> score <span class="keyword">is</span> <span class="keyword">not</span>  <span class="literal">None</span>:</span><br><span class="line">        titlestring += <span class="string">&#x27;f1 = &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(score)</span><br><span class="line">    <span class="keyword">if</span> precision <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        titlestring += <span class="string">&#x27;\n precision = &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(precision)</span><br><span class="line">    <span class="keyword">if</span> recall <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        titlestring += <span class="string">&#x27;\n recall = &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(recall)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(titlestring) &gt; <span class="number">0</span>:<span class="comment">#101 x位置, y=1</span></span><br><span class="line">        ax.text(<span class="number">101</span>, <span class="number">1</span>, titlestring, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">18</span>,\</span><br><span class="line">        <span class="string">&#x27;horizontalalignment&#x27;</span>: <span class="string">&#x27;right&#x27;</span>, <span class="string">&#x27;verticalalignment&#x27;</span>: <span class="string">&#x27;top&#x27;</span>, <span class="string">&#x27;color&#x27;</span>: <span class="string">&#x27;black&#x27;</span>&#125;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ol><li>绘制训练指标曲线</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_training_curves</span>(<span class="params">training, validation, title, subplot</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制各种曲线&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> subplot%<span class="number">10</span>==<span class="number">1</span>: <span class="comment">#设置第一个子图</span></span><br><span class="line">        plt.subplots(figsize=(<span class="number">10</span>, <span class="number">10</span>), dpi=<span class="number">150</span>, facecolor=<span class="string">&quot;#F0F0F0&quot;</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    ax = plt.subplot(subplot)</span><br><span class="line">    ax.set_facecolor(<span class="string">&#x27;#F8F8F8&#x27;</span>)</span><br><span class="line">    ax.plot(training)</span><br><span class="line">    ax.plot(validation)</span><br><span class="line">    ax.set_title(<span class="string">&#x27;model &#x27;</span> + title)</span><br><span class="line">    ax.set_ylabel(title)</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;epoch&quot;</span>)</span><br><span class="line">    ax.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="2-创建数据集"><a href="#2-创建数据集" class="headerlink" title="2. 创建数据集"></a>2. 创建数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_image</span>(<span class="params">image_data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;decode 图片数据并resize到指定size&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#image data uint8[0， 255]</span></span><br><span class="line">    image = tf.image.decode_jpeg(image_data, channels=<span class="number">3</span>)</span><br><span class="line">    image = tf.reshape(image, [*IMG_SIZE, <span class="number">3</span>])<span class="comment">#为TPUresize到指定SIZE</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_labeled_tfrecord</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从tfrecords读取有标签的数据并返回image和对应label&quot;&quot;&quot;</span></span><br><span class="line">    labeled_tfrec_format = &#123;</span><br><span class="line">        <span class="string">&quot;image&quot;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">        <span class="string">&quot;class&quot;</span>: tf.io.FixedLenFeature([], tf.int64),</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#按照指定格式解析单个样本</span></span><br><span class="line">    example = tf.io.parse_single_example(example, labeled_tfrec_format)</span><br><span class="line">    image = decode_image(example[<span class="string">&#x27;image&#x27;</span>])</span><br><span class="line">    label = tf.cast(example[<span class="string">&#x27;class&#x27;</span>], tf.int32) <span class="comment">#转为tf.int32</span></span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_unlabeled_tfrecord</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从tfrecords中读取无标签数据，并返回image和对应id&quot;&quot;&quot;</span></span><br><span class="line">    unlabeled_tfrec_format = &#123;</span><br><span class="line">        <span class="string">&quot;image&quot;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">        <span class="string">&quot;id&quot;</span>: tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#这里用testdataset来预测花的种类</span></span><br><span class="line">    example = tf.io.parse_single_example(example, unlabeled_tfrec_format)</span><br><span class="line">    image = decode_image(example[<span class="string">&#x27;image&#x27;</span>])</span><br><span class="line">    idx = example[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> image, idx</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">filenames, labeled=<span class="literal">True</span>, ordered=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param filenames:文件路径</span></span><br><span class="line"><span class="string">    :param labeled: 是否标注的数据</span></span><br><span class="line"><span class="string">    :param ordered: 是否有序,其实没关系，都会shuffle</span></span><br><span class="line"><span class="string">    :return: dataset</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ignore_order = tf.data.Options() <span class="comment">#设置dataset操作是否能用静态的一些方法和pipeline操作</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ordered:</span><br><span class="line">        <span class="comment"># 决定output是否是确定次序，False关闭有序增加读取速度</span></span><br><span class="line">        ignore_order.experimental_deterministic = <span class="literal">False</span></span><br><span class="line">    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)</span><br><span class="line">    dataset = dataset.with_options(ignore_order)<span class="comment">#使用该方法能加速streams，比用原本顺序读取要快</span></span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(read_labeled_tfrecord <span class="keyword">if</span> labeled <span class="keyword">else</span> read_unlabeled_tfrecord,\</span><br><span class="line">                          num_parallel_calls=AUTO)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_augment</span>(<span class="params">image, label</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;数据增强&quot;&quot;&quot;</span></span><br><span class="line">    image = tf.image.random_flip_left_right(image)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_training_dataset</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取训练数据集&quot;&quot;&quot;</span></span><br><span class="line">    dataset = load_dataset(TRAINING_FILENAMES, labeled=<span class="literal">True</span>)</span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(data_augment, num_parallel_calls=AUTO)</span><br><span class="line">    dataset = dataset.repeat() <span class="comment">#因为要训练多轮，必须repeat来保证数据量</span></span><br><span class="line">    dataset = dataset.shuffle(<span class="number">2048</span>) <span class="comment">#buffer size &gt; BATCH_SIZE</span></span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE) <span class="comment">#成batch</span></span><br><span class="line">    dataset = dataset.prefetch(AUTO)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_validation_dataset</span>(<span class="params">ordered=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取验证数据集&quot;&quot;&quot;</span></span><br><span class="line">    dataset = load_dataset(VALID_FILENAMES, labeled=<span class="literal">True</span>, ordered=ordered)</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.cache() <span class="comment">#缓存</span></span><br><span class="line">    dataset = dataset.prefetch(AUTO)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_dataset</span>(<span class="params">ordered=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取test数据集&quot;&quot;&quot;</span></span><br><span class="line">    dataset = load_dataset(TEST_FILENAMES, labeled=<span class="literal">False</span>, ordered=ordered)</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.prefetch(AUTO)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_data_items</span>(<span class="params">filenames</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回数据的数目, 输入为data/flowers00-230.tfrec, 匹配到-230.取到230然后求和&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#r&quot;-([0-9]*)\.&quot; 匹配-0到9带.的字符串，但只要group就是()内匹配字符</span></span><br><span class="line">    <span class="comment"># group(1) 0返回所有的匹配串 1返回第一组</span></span><br><span class="line">    n = [<span class="built_in">int</span>(re.<span class="built_in">compile</span>(<span class="string">r&quot;-([0-9]*)\.&quot;</span>).search(f).group(<span class="number">1</span>)) <span class="keyword">for</span> f <span class="keyword">in</span> filenames]</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(n)</span><br></pre></td></tr></table></figure><p>利用 <code>count_data_items(filenames)</code>，看看每个数据集大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES) <span class="comment">#训练集图片数目</span></span><br><span class="line">NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES) <span class="comment">#验证集图片数目</span></span><br><span class="line">NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES) <span class="comment">#测试集图片数目</span></span><br><span class="line">STEP_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE <span class="comment">#每个epoch训练步数</span></span><br><span class="line"><span class="comment"># 向上取整trick -9//2负数向下取整应该是-5</span></span><br><span class="line">VALIDATION_STEPS = -(-NUM_VALIDATION_IMAGES // BATCH_SIZE)</span><br><span class="line">test_steps = -(-NUM_TEST_IMAGES // BATCH_SIZE)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Dataset:&#123;&#125; training images, &#123;&#125; validation images, &#123;&#125; unlabeled test images&#x27;</span>.\</span><br><span class="line">      <span class="built_in">format</span>(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))</span><br><span class="line">===================================================================================</span><br><span class="line">Dataset:<span class="number">12753</span> training images, <span class="number">3712</span> validation images, <span class="number">7382</span> unlabeled test images</span><br></pre></td></tr></table></figure><h4 id="3-数据集可视化"><a href="#3-数据集可视化" class="headerlink" title="3. 数据集可视化"></a>3. 数据集可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training data shapes:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> image, label <span class="keyword">in</span> get_training_dataset().take(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(image.numpy().shape, label.numpy().shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training data label examples:&quot;</span>, label.numpy())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Validation data shapes:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> image, label <span class="keyword">in</span> get_validation_dataset().take(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(image.numpy().shape, label.numpy().shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Validation data label examples:&quot;</span>, label.numpy())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data shapes:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> image, idx <span class="keyword">in</span> get_test_dataset().take(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(image.numpy().shape, idx.numpy().shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data IDs:&quot;</span>, idx.numpy().astype(<span class="string">&#x27;U&#x27;</span>))</span><br><span class="line">================================================================</span><br><span class="line">Training data shapes:</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">Training data label examples: [<span class="number">87</span> <span class="number">48</span> <span class="number">50</span> ... <span class="number">74</span>  <span class="number">4</span> <span class="number">39</span>]</span><br><span class="line">Validation data shapes:</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">Validation data label examples: [<span class="number">73</span> <span class="number">45</span> <span class="number">76</span> ... <span class="number">69</span> <span class="number">93</span> <span class="number">53</span>]</span><br><span class="line">Test data shapes:</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">(<span class="number">16</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>) (<span class="number">16</span>,)</span><br><span class="line">Test data IDs: [<span class="string">&#x27;4fb5992b3&#x27;</span> <span class="string">&#x27;6557acff6&#x27;</span> <span class="string">&#x27;abfe5bd86&#x27;</span> ... <span class="string">&#x27;d4ae8d14a&#x27;</span> <span class="string">&#x27;b5422eec0&#x27;</span> <span class="string">&#x27;6485b01ac&#x27;</span>]</span><br></pre></td></tr></table></figure><p>训练数据展示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">training_dataset = get_training_dataset()</span><br><span class="line">training_dataset = training_dataset.unbatch().batch(<span class="number">20</span>)</span><br><span class="line">training_batch = <span class="built_in">iter</span>(training_dataset)</span><br><span class="line">display_batch_of_images(<span class="built_in">next</span>(training_batch))</span><br></pre></td></tr></table></figure><p>总共20张，只展示5张。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912195.png" alt="image-20210814003727467"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_dataset = get_test_dataset()</span><br><span class="line">test_dataset = test_dataset.unbatch().batch(<span class="number">20</span>)</span><br><span class="line">test_batch = <span class="built_in">iter</span>(test_dataset)</span><br><span class="line">display_batch_of_images(<span class="built_in">next</span>(test_batch))</span><br></pre></td></tr></table></figure><h4 id="4-建立模型和训练"><a href="#4-建立模型和训练" class="headerlink" title="4. 建立模型和训练"></a>4. 建立模型和训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    img_adjust_layer = tf.keras.layers.Lambda(<span class="keyword">lambda</span> data: tf.keras.applications.vgg16.preprocess_input(\</span><br><span class="line">        tf.cast(data, tf.float32)), input_shape=[*IMG_SIZE, <span class="number">3</span>])</span><br><span class="line">    pretrained_model = tf.keras.applications.VGG16(weights=<span class="string">&#x27;imagenet&#x27;</span>, include_top=<span class="literal">False</span>)</span><br><span class="line">    pretrained_model.trainable = <span class="literal">False</span></span><br><span class="line">    model = tf.keras.Sequential(</span><br><span class="line">        [img_adjust_layer,</span><br><span class="line">        pretrained_model,</span><br><span class="line">        tf.keras.layers.GlobalAveragePooling2D(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="built_in">len</span>(CLASSES), activation=<span class="string">&#x27;softmax&#x27;</span>)]</span><br></pre></td></tr></table></figure><p>看看模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">        loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">        metrics=[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>])</span><br><span class="line">model.summary()</span><br><span class="line">=========================================================</span><br><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line"><span class="keyword">lambda</span> (Lambda)              (<span class="literal">None</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>)       <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">vgg16 (Functional)           (<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">512</span>)   <span class="number">14714688</span>  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">global_average_pooling2d (Gl (<span class="literal">None</span>, <span class="number">512</span>)               <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (<span class="literal">None</span>, <span class="number">104</span>)               <span class="number">53352</span>     </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">14</span>,<span class="number">768</span>,040</span><br><span class="line">Trainable params: <span class="number">53</span>,<span class="number">352</span></span><br><span class="line">Non-trainable params: <span class="number">14</span>,<span class="number">714</span>,<span class="number">688</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(get_training_dataset(), steps_per_epoch=STEP_PER_EPOCH, epochs=EPOCHS,\</span><br><span class="line">        validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS)</span><br><span class="line">=================================================================</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 169s 180ms/step - loss: <span class="number">2.1719</span> - sparse_categorical_accuracy: <span class="number">0.5329</span> - val_loss: <span class="number">1.0612</span> - val_sparse_categorical_accuracy: <span class="number">0.7416</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.7915</span> - sparse_categorical_accuracy: <span class="number">0.8009</span> - val_loss: <span class="number">0.8042</span> - val_sparse_categorical_accuracy: <span class="number">0.8025</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.5611</span> - sparse_categorical_accuracy: <span class="number">0.8560</span> - val_loss: <span class="number">0.7537</span> - val_sparse_categorical_accuracy: <span class="number">0.8182</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.4409</span> - sparse_categorical_accuracy: <span class="number">0.8843</span> - val_loss: <span class="number">0.6929</span> - val_sparse_categorical_accuracy: <span class="number">0.8308</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 176ms/step - loss: <span class="number">0.3606</span> - sparse_categorical_accuracy: <span class="number">0.9047</span> - val_loss: <span class="number">0.6742</span> - val_sparse_categorical_accuracy: <span class="number">0.8367</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.3046</span> - sparse_categorical_accuracy: <span class="number">0.9168</span> - val_loss: <span class="number">0.6446</span> - val_sparse_categorical_accuracy: <span class="number">0.8483</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.2619</span> - sparse_categorical_accuracy: <span class="number">0.9290</span> - val_loss: <span class="number">0.6639</span> - val_sparse_categorical_accuracy: <span class="number">0.8443</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.2316</span> - sparse_categorical_accuracy: <span class="number">0.9366</span> - val_loss: <span class="number">0.6505</span> - val_sparse_categorical_accuracy: <span class="number">0.8505</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.1987</span> - sparse_categorical_accuracy: <span class="number">0.9460</span> - val_loss: <span class="number">0.6629</span> - val_sparse_categorical_accuracy: <span class="number">0.8473</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.1850</span> - sparse_categorical_accuracy: <span class="number">0.9495</span> - val_loss: <span class="number">0.6968</span> - val_sparse_categorical_accuracy: <span class="number">0.8448</span></span><br><span class="line">Epoch <span class="number">11</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.1624</span> - sparse_categorical_accuracy: <span class="number">0.9568</span> - val_loss: <span class="number">0.6953</span> - val_sparse_categorical_accuracy: <span class="number">0.8389</span></span><br><span class="line">Epoch <span class="number">12</span>/<span class="number">12</span></span><br><span class="line"><span class="number">797</span>/<span class="number">797</span> [==============================] - 141s 177ms/step - loss: <span class="number">0.1412</span> - sparse_categorical_accuracy: <span class="number">0.9613</span> - val_loss: <span class="number">0.6772</span> - val_sparse_categorical_accuracy: <span class="number">0.8543</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>绘制训练曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_training_curves(history.history[<span class="string">&#x27;loss&#x27;</span>], history.history[<span class="string">&#x27;val_loss&#x27;</span>], <span class="string">&#x27;loss&#x27;</span>, <span class="number">211</span>)</span><br><span class="line">display_training_curves(history.history[<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>], \</span><br><span class="line">            history.history[<span class="string">&#x27;val_sparse_categorical_accuracy&#x27;</span>], <span class="string">&#x27;accuracy&#x27;</span>, <span class="number">212</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912196.png" alt="image-20210814004305884" style="zoom:35%;" /></p><h4 id="5-混淆矩阵"><a href="#5-混淆矩阵" class="headerlink" title="5.混淆矩阵"></a>5.混淆矩阵</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#因为对数据集进行了分割，分开迭代images和labels，保证顺序才能保证两者是一对</span></span><br><span class="line">confusion_dataset = get_validation_dataset(ordered=<span class="literal">True</span>)</span><br><span class="line">image_ds = confusion_dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> image, label: image)</span><br><span class="line">label_ds = confusion_dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> image, label: label).unbatch()</span><br><span class="line"></span><br><span class="line">confusion_correct_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(label_ds.batch(NUM_VALIDATION_IMAGES))).numpy()<span class="comment">#让label也成批</span></span><br><span class="line">confusion_probabilities = model.predict(image_ds, steps=VALIDATION_STEPS)</span><br><span class="line">confusion_predictions = np.argmax(confusion_probabilities, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;正确的标签有:&quot;</span>, confusion_correct_labels.shape, confusion_correct_labels)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测标签为:&quot;</span>, confusion_predictions.shape, confusion_predictions)</span><br><span class="line">=====================================================================</span><br><span class="line">正确的标签有: (<span class="number">3712</span>,) [<span class="number">69</span> <span class="number">95</span> <span class="number">48</span> ...  <span class="number">0</span> <span class="number">57</span> <span class="number">88</span>]</span><br><span class="line">预测标签为: (<span class="number">3712</span>,) [<span class="number">49</span> <span class="number">95</span> <span class="number">48</span> ...  <span class="number">0</span> <span class="number">57</span> <span class="number">88</span>]</span><br></pre></td></tr></table></figure><p>图示混淆矩阵：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">confusion_mat = confusion_matrix(confusion_correct_labels,\</span><br><span class="line">            confusion_predictions, labels=<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)))</span><br><span class="line">score = f1_score(confusion_correct_labels, confusion_predictions,\</span><br><span class="line">                 labels=<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)), average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line">precision = precision_score(confusion_correct_labels,\</span><br><span class="line">        confusion_predictions, labels=<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)), average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line">recall = recall_score(confusion_correct_labels,\</span><br><span class="line">    confusion_predictions, labels=<span class="built_in">range</span>(<span class="built_in">len</span>(CLASSES)), average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line">cmat = (confusion_mat.T / confusion_mat.<span class="built_in">sum</span>(axis=<span class="number">1</span>)).T</span><br><span class="line">display_confusion_matrix(cmat, score, precision, recall)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;f1 score:&#123;:.3f&#125;, precision: &#123;:.3f&#125;, recall: &#123;:.3f&#125;&#x27;</span>\</span><br><span class="line">      .<span class="built_in">format</span>(score, precision, recall))</span><br><span class="line">===================================================</span><br><span class="line">f1 score:<span class="number">0.849</span>, precision: <span class="number">0.875</span>, recall: <span class="number">0.839</span></span><br></pre></td></tr></table></figure></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912198.png" alt="cmt"></p><h4 id="6-预测"><a href="#6-预测" class="headerlink" title="6. 预测"></a>6. 预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_ds = get_test_dataset(ordered=<span class="literal">True</span>)</span><br><span class="line">test_images_ds = test_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span> image, idx: image)</span><br><span class="line">prob = model.predict(test_images_ds, steps=test_steps)</span><br><span class="line">pred = np.argmax(prob, axis=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line">=======================================================</span><br><span class="line">[ <span class="number">68</span>  <span class="number">48</span>  <span class="number">45</span> ...  <span class="number">53</span> <span class="number">103</span>  <span class="number">67</span>]</span><br></pre></td></tr></table></figure><p>保存csv结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成csv</span></span><br><span class="line">test_ids_ds = test_ds.<span class="built_in">map</span>(<span class="keyword">lambda</span>  image, idx: idx).unbatch()</span><br><span class="line">test_ids = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype(<span class="string">&#x27;U&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;submission.csv&#x27;</span>, np.rec.fromarrays([test_ids, pred]),\</span><br><span class="line">    fmt=[<span class="string">&#x27;%s&#x27;</span>, <span class="string">&#x27;%d&#x27;</span>], delimiter=<span class="string">&#x27;,&#x27;</span>, header=<span class="string">&#x27;id,label&#x27;</span>, comments=<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>可视化结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 可视化结果</span></span><br><span class="line">dataset = get_validation_dataset()</span><br><span class="line">dataset = dataset.unbatch().batch(<span class="number">20</span>)</span><br><span class="line">batch = <span class="built_in">iter</span>(dataset)</span><br><span class="line"></span><br><span class="line">imgs, labels = <span class="built_in">next</span>(batch)</span><br><span class="line">prob = model.predict(tf.cast(imgs, tf.float32))</span><br><span class="line">pred = np.argmax(prob, axis=-<span class="number">1</span>)</span><br><span class="line">display_batch_of_images((imgs, labels), pred)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912199.jpg" alt="image-20210814004852977"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VGG </tag>
            
            <tag> code snippet </tag>
            
            <tag> TPU </tag>
            
            <tag> confusion matrix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.子图绘制</title>
      <link href="2021/01/06/%E6%8A%80%E8%83%BD_1.%20%E5%AD%90%E5%9B%BE%E7%BB%98%E5%88%B6/"/>
      <url>2021/01/06/%E6%8A%80%E8%83%BD_1.%20%E5%AD%90%E5%9B%BE%E7%BB%98%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-子图绘制"><a href="#1-子图绘制" class="headerlink" title="1.子图绘制"></a>1.子图绘制</h3><p>使用<code>matplotlib</code>, <code>seaborn</code>绘制子图时主要注意<code>plt.subplot(a, b, c)</code>这个方法使用，其中</p><ul><li><p><code>a</code>， 代表行数</p></li><li><p><code>b</code>， 代表列数</p></li><li><p><code>c</code>， 代表绘制在第几个</p><p>这3个参数经常写作abc这种形式。本文来自于 <a href="https://www.kaggle.com/asimislam/tutorial-python-subplots?select=heart.csv">tutorial-python-subplots</a>。</p></li></ul><p>而 <code>seaborn.countplot()</code>绘制函数可以在 <a href="https://seaborn.pydata.org/generated/seaborn.countplot.html">seaborn.countplot</a>学习,其它也类似。</p><h4 id="1-绘制1x1子图"><a href="#1-绘制1x1子图" class="headerlink" title="1. 绘制1x1子图"></a>1. 绘制1x1子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line">np.set_printoptions(threshold=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">data_dir = Path(<span class="string">r&#x27;data&#x27;</span>)</span><br><span class="line">df  = pd.read_csv(data_dir/<span class="string">&#x27;heart.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df.head(), df.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 1.绘制1x2的子图</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#subplot 1</span></span><br><span class="line">plt.subplot(<span class="number">121</span>)<span class="comment">#1行2列中第一个子图</span></span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 121&#x27;</span>)</span><br><span class="line">sns.countplot(data=df, x=<span class="string">&#x27;cp&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#子图2</span></span><br><span class="line">plt.subplot(<span class="number">122</span>)<span class="comment">#1行2列中第一个子图</span></span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 122&#x27;</span>)</span><br><span class="line">sns.scatterplot(data=df, x=<span class="string">&#x27;age&#x27;</span>, y=<span class="string">&#x27;chol&#x27;</span>, hue=<span class="string">&#x27;sex&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">=================================================</span><br><span class="line">   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target</span><br><span class="line"><span class="number">0</span>   <span class="number">63</span>    <span class="number">1</span>   <span class="number">3</span>       <span class="number">145</span>   <span class="number">233</span>    <span class="number">1</span>  ...      <span class="number">0</span>      <span class="number">2.3</span>      <span class="number">0</span>   <span class="number">0</span>     <span class="number">1</span>       <span class="number">1</span></span><br><span class="line"><span class="number">1</span>   <span class="number">37</span>    <span class="number">1</span>   <span class="number">2</span>       <span class="number">130</span>   <span class="number">250</span>    <span class="number">0</span>  ...      <span class="number">0</span>      <span class="number">3.5</span>      <span class="number">0</span>   <span class="number">0</span>     <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">2</span>   <span class="number">41</span>    <span class="number">0</span>   <span class="number">1</span>       <span class="number">130</span>   <span class="number">204</span>    <span class="number">0</span>  ...      <span class="number">0</span>      <span class="number">1.4</span>      <span class="number">2</span>   <span class="number">0</span>     <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">3</span>   <span class="number">56</span>    <span class="number">1</span>   <span class="number">1</span>       <span class="number">120</span>   <span class="number">236</span>    <span class="number">0</span>  ...      <span class="number">0</span>      <span class="number">0.8</span>      <span class="number">2</span>   <span class="number">0</span>     <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"><span class="number">4</span>   <span class="number">57</span>    <span class="number">0</span>   <span class="number">0</span>       <span class="number">120</span>   <span class="number">354</span>    <span class="number">0</span>  ...      <span class="number">1</span>      <span class="number">0.6</span>      <span class="number">2</span>   <span class="number">0</span>     <span class="number">2</span>       <span class="number">1</span></span><br><span class="line"></span><br><span class="line">[<span class="number">5</span> rows x <span class="number">14</span> columns] (<span class="number">303</span>, <span class="number">14</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912693.png" alt="image-20210810232333868" style="zoom:50%;" /></p><h4 id="2-绘制2x1的子图"><a href="#2-绘制2x1的子图" class="headerlink" title="2. 绘制2x1的子图"></a>2. 绘制2x1的子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 2.绘制2x1的子图</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">5</span>, <span class="number">10</span>), dpi=<span class="number">250</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#subplot 1</span></span><br><span class="line">plt.subplot(<span class="number">211</span>)<span class="comment">#2行1列中第一个子图</span></span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 211&#x27;</span>)</span><br><span class="line">sns.countplot(data=df, x=<span class="string">&#x27;cp&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#子图2</span></span><br><span class="line">plt.subplot(<span class="number">212</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 212&#x27;</span>)</span><br><span class="line">sns.scatterplot(data=df, x=<span class="string">&#x27;age&#x27;</span>, y=<span class="string">&#x27;chol&#x27;</span>, hue=<span class="string">&#x27;sex&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912694.png" alt="image-20210810232903674" style="zoom:20%;" /></p><h4 id="3-绘制2x3子图"><a href="#3-绘制2x3子图" class="headerlink" title="3.绘制2x3子图"></a>3.绘制2x3子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>, <span class="number">12</span>), dpi=<span class="number">250</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">231</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 231&#x27;</span>)</span><br><span class="line">sns.countplot(data=df, x=<span class="string">&#x27;cp&#x27;</span>, hue=<span class="string">&#x27;sex&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">232</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 232&#x27;</span>)</span><br><span class="line">sns.scatterplot(data=df, x=<span class="string">&#x27;age&#x27;</span>, y=<span class="string">&#x27;chol&#x27;</span>, hue=<span class="string">&#x27;sex&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">233</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 233&#x27;</span>)</span><br><span class="line"><span class="comment"># 折线图 阴影表示y在一个95%的置信范围 实线代表均值</span></span><br><span class="line">sns.lineplot(data=df, x=<span class="string">&#x27;age&#x27;</span>, y=<span class="string">&#x27;oldpeak&#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">234</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 234&#x27;</span>)</span><br><span class="line">sns.boxplot(data=df[[<span class="string">&#x27;chol&#x27;</span>, <span class="string">&#x27;trestbps&#x27;</span>, <span class="string">&#x27;thalach&#x27;</span>]]) <span class="comment">#箱图</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">235</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;heart subplot: 235&#x27;</span>)</span><br><span class="line">sns.histplot(df.age) <span class="comment">#分布直方图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912695.png" alt="image-20210810234628117" style="zoom:20%;" /></p><h4 id="4-用for循环绘制子图"><a href="#4-用for循环绘制子图" class="headerlink" title="4. 用for循环绘制子图"></a>4. 用for循环绘制子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">heart_categorical = [<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;cp&#x27;</span>, <span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;thal&#x27;</span>, <span class="string">&#x27;restecg&#x27;</span>]</span><br><span class="line">a, b, c = <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>, <span class="number">10</span>), dpi=<span class="number">250</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> heart_categorical:</span><br><span class="line">    plt.subplot(a, b, c)</span><br><span class="line">    plt.title(<span class="string">&#x27;&#123;&#125;, subplot:&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, a, b, c))</span><br><span class="line">    plt.xlabel(i)</span><br><span class="line">    sns.countplot(df[i])</span><br><span class="line">    c += <span class="number">1</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912696.png" alt="image-20210810235952454" style="zoom:20%;" /></p><h4 id="5-用for循环绘制不同类型子图"><a href="#5-用for循环绘制不同类型子图" class="headerlink" title="5.用for循环绘制不同类型子图"></a>5.用for循环绘制不同类型子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">heart_categorical = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;trestbps&#x27;</span>, <span class="string">&#x27;thalach&#x27;</span>, <span class="string">&#x27;oldpeak&#x27;</span>]</span><br><span class="line">a, b, c = <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>, <span class="number">22</span>), dpi=<span class="number">250</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> heart_categorical:</span><br><span class="line">    plt.subplot(a, b, c)</span><br><span class="line">    plt.title(<span class="string">&#x27;&#123;&#125; (dist), subplot:&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, a, b, c))</span><br><span class="line">    plt.xlabel(i)</span><br><span class="line">    sns.distplot(df[i])</span><br><span class="line">    c += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(a, b, c)</span><br><span class="line">    plt.title(<span class="string">&#x27;&#123;&#125; (box), subplot:&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, a, b, c))</span><br><span class="line">    plt.xlabel(i)</span><br><span class="line">    sns.boxplot(x = df[i])</span><br><span class="line">    c += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.subplot(a, b, c)</span><br><span class="line">    plt.title(<span class="string">&#x27;&#123;&#125; (scatter), subplot:&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, a, b, c))</span><br><span class="line">    plt.xlabel(i)</span><br><span class="line">    sns.scatterplot(data=df, x=i, y=<span class="string">&#x27;chol&#x27;</span>, hue=<span class="string">&#x27;sex&#x27;</span>)</span><br><span class="line">    c += <span class="number">1</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912697.png" alt="image-20210811000638011" style="zoom:20%;" /></p><h4 id="6-热力图子图"><a href="#6-热力图子图" class="headerlink" title="6. 热力图子图"></a>6. 热力图子图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">df2 = df[[<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;trestbps&#x27;</span>, <span class="string">&#x27;chol&#x27;</span>, <span class="string">&#x27;thalach&#x27;</span>, <span class="string">&#x27;oldpeak&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#相关性女性</span></span><br><span class="line">df_female = df2[df2[<span class="string">&#x27;sex&#x27;</span>]==<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(df_female.head())</span><br><span class="line">df_female_corr = df_female.drop([<span class="string">&#x27;sex&#x27;</span>], axis=<span class="number">1</span>).corr()</span><br><span class="line"></span><br><span class="line">df_male = df2[df2[<span class="string">&#x27;sex&#x27;</span>]==<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(df_male.head())</span><br><span class="line">df_male_corr = df_male.drop([<span class="string">&#x27;sex&#x27;</span>], axis=<span class="number">1</span>).corr()</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>), dpi=<span class="number">250</span>)</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;female.corr, subplot: 121&#x27;</span>)</span><br><span class="line">sns.heatmap(df_female_corr, annot=<span class="literal">True</span>, fmt=<span class="string">&#x27;.2f&#x27;</span>, square=<span class="literal">True</span>, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;male.corr, subplot: 122&#x27;</span>)</span><br><span class="line">sns.heatmap(df_male_corr, annot=<span class="literal">True</span>, fmt=<span class="string">&#x27;.2f&#x27;</span>, square=<span class="literal">True</span>, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">===========================================================</span><br><span class="line"> sex  age  trestbps  chol  thalach  oldpeak</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span>   <span class="number">63</span>       <span class="number">145</span>   <span class="number">233</span>      <span class="number">150</span>      <span class="number">2.3</span></span><br><span class="line"><span class="number">1</span>    <span class="number">1</span>   <span class="number">37</span>       <span class="number">130</span>   <span class="number">250</span>      <span class="number">187</span>      <span class="number">3.5</span></span><br><span class="line"><span class="number">3</span>    <span class="number">1</span>   <span class="number">56</span>       <span class="number">120</span>   <span class="number">236</span>      <span class="number">178</span>      <span class="number">0.8</span></span><br><span class="line"><span class="number">5</span>    <span class="number">1</span>   <span class="number">57</span>       <span class="number">140</span>   <span class="number">192</span>      <span class="number">148</span>      <span class="number">0.4</span></span><br><span class="line"><span class="number">7</span>    <span class="number">1</span>   <span class="number">44</span>       <span class="number">120</span>   <span class="number">263</span>      <span class="number">173</span>      <span class="number">0.0</span></span><br><span class="line">    sex  age  trestbps  chol  thalach  oldpeak</span><br><span class="line"><span class="number">2</span>     <span class="number">0</span>   <span class="number">41</span>       <span class="number">130</span>   <span class="number">204</span>      <span class="number">172</span>      <span class="number">1.4</span></span><br><span class="line"><span class="number">4</span>     <span class="number">0</span>   <span class="number">57</span>       <span class="number">120</span>   <span class="number">354</span>      <span class="number">163</span>      <span class="number">0.6</span></span><br><span class="line"><span class="number">6</span>     <span class="number">0</span>   <span class="number">56</span>       <span class="number">140</span>   <span class="number">294</span>      <span class="number">153</span>      <span class="number">1.3</span></span><br><span class="line"><span class="number">11</span>    <span class="number">0</span>   <span class="number">48</span>       <span class="number">130</span>   <span class="number">275</span>      <span class="number">139</span>      <span class="number">0.2</span></span><br><span class="line"><span class="number">14</span>    <span class="number">0</span>   <span class="number">58</span>       <span class="number">150</span>   <span class="number">283</span>      <span class="number">162</span>      <span class="number">1.0</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912698.png" alt="image-20210811001709097" style="zoom:25%;" /></p><h4 id="7-pairplot-成对绘制相关性"><a href="#7-pairplot-成对绘制相关性" class="headerlink" title="7. pairplot 成对绘制相关性"></a>7. pairplot 成对绘制相关性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.pairplot(data=df[[<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;chol&#x27;</span>, <span class="string">&#x27;trestbps&#x27;</span>]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261912699.png" alt="image-20210811002818192" style="zoom:50%;" /></p><p>可以看看这个对<code>pairplot</code>解释：<a href="https://zhuanlan.zhihu.com/p/98729226">Python可视化 | Seaborn5分钟入门(七)——pairplot</a>.</p><ul><li><code>kind</code>：用于控制非对角线上的图的类型，可选<code>&quot;scatter&quot;</code>与`”reg”</li><li><code>diag_kind</code>：控制对角线上的图的类型，可选<code>&quot;hist&quot;</code>与 <code>kde</code> ,其中: <strong>直方图（hist）+内核密度函数（kde）</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> code snippet </tag>
            
            <tag> subplot </tag>
            
            <tag> seaborn </tag>
            
            <tag> pairplot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>16. Reformer 论文翻译笔记</title>
      <link href="2020/12/23/NLP%20Paper%2016.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/12/23/NLP%20Paper%2016.%20Reformer%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="16-Reformer-论文翻译笔记"><a href="#16-Reformer-论文翻译笔记" class="headerlink" title="16. Reformer 论文翻译笔记"></a>16. Reformer 论文翻译笔记</h3><p>机构：Google Research 、U.C. Berkeley<br>        作者：Nikita Kitaev、Łukasz Kaiser、Anselm Levskaya<br>        论文地址：<a href="https://arxiv.org/pdf/2001.04451.pdf">https://arxiv.org/pdf/2001.04451.pdf</a><br>        收录会议：ICLR2020<br>        论文代码：<a href="https://github.com/google/trax/tree/master/trax/models/reformer">https://github.com/google/trax/tree/master/trax/models/reformer</a></p><p>Reformer， 主要是对Transformer的计算复杂度和内存进行优化，最关键的两个点是:</p><ul><li>局部敏感哈希</li><li>可逆的残差连接</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847632.png" alt="image-20210804203759229" style="zoom: 33%;" /></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>大型Transformer模型常常在大量任务上取得最佳成绩，但是训练这些模型是非常昂贵的。本文引入两种计算来提升Transformer的效率。</p><ul><li>第一，用局部敏感哈希代替点乘式的attention，使其空间复杂度从<script type="math/tex">O(L^2)</script>降低到<script type="math/tex">O(L\log L)</script>​，其中L是文本序列的长度。</li><li>此外，用可逆的残差连接层代理标准残差, 这使得训练过程中只需要存储移除激活值， <script type="math/tex">N</script>是层数。</li></ul><p>最终结果表明Reformer性能与Transformer相当，同时在长序列上更具有高效内存和更快。</p><h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h4><p>在大型的Transformer中每层网络参数超过了0.5B，网络层数上升到64层如<a href="http://arxiv.org/abs/1808.04444">Al-Rfou 2018 Character-level language modeling with deeper self-attention</a>论文中。另外，Transformer模型还增大序列长度，在单一样例中，上升到1.1万文本字符长度。这导致Transformer模型只能在大型的工业研究实验室中训练，并且其并行化训练让其甚至无法在但GPU上微调。</p><p>作者假设一个5亿参数的Transformer层，因为是float，每个参数占4byte,那么<script type="math/tex">5 \times 4 \times10^8 = 2 \times 10^9 \text{bytes}</script>,转化为<script type="math/tex">2 \times 10^9 / 1024 (KB)/1024(MB)/1024(GB)  \approx 2GB</script>​​​。对于64K字符的激活值，如果嵌入层尺寸为1024，batch size为8​，总共需要64K x 1K x 8 = 0.5B floats。又是2GB左右内存。如果作者的内存使用仅仅是一层上述的网络的话，作者甚至能在单一GPU上轻松训练序列长度为64K的大型Transformer模型。此外，整个用来训练BERT的语料也只需要17GB存储内存。那么，为什么作者甚至无法在单一机器上微调模型呢?</p><p>因为上面只估计了一层网络的内存和输入激活层的内存消耗，并没有考虑以下Transformer中主要的内存资源占用。</p><ul><li>N层模型的内存占用是单层的N倍大，因为实际上在反向传播过程中需要存储每层的激活值。</li><li>因为中间的前馈层大小<script type="math/tex">d_{ff}</script>​是远远大于注意力激活层<script type="math/tex">d_{model}</script>​​的​，它会占用大部分内存。</li><li>长度为<script type="math/tex">L</script>的序列注意力计算的时间和空间复杂度都是<script type="math/tex">O(L^2 )</script>​，甚至单一长度为64K的字符序列​就能耗尽GPU内存。</li></ul><p>引入Reformer模型，用以下技术能解决这些问题:</p><ul><li>可逆层，首次介绍是Gomez 2017，能只存储整个模型的一个激活值的复制，这样N倍问题消失了</li><li>在前馈层分开激活和分块处理，消除了<script type="math/tex">d_{ff}</script>因素影响，降低了内存占用。</li><li>基于局部敏感哈希的近似注意力计算，让计算复杂度从<script type="math/tex">O(L^2 )</script>降为<script type="math/tex">O(L\log L )</script>​​,这样就能处理长序列了​</li></ul><p>跟标准Transformer相比，训练流程影响微不足道。分开激活只在实现上有影响，其数值上还等于以前Transformer一样。使用可逆的残差连接代替标准的不更笨模型，但在作者实验的所有配置上有轻微影响。最重要的，注意力中的局部敏感哈希是最大的改变，这能影响训练变化，依赖于使用共存哈希的数目。</p><p>作者实验在人工任务上，文本任务(enwik8), 使用长为64K的序列；和图像生成任务(Imagenet-64 生成)，使用长为12K的训练。结果表明Reformer结果跟Transformer差不多，尤其是文本任务，有数量级级别的更高效内存效率。</p><h4 id="2-局部敏感哈希注意力"><a href="#2-局部敏感哈希注意力" class="headerlink" title="2. 局部敏感哈希注意力"></a>2. 局部敏感哈希注意力</h4><p><strong>Dot-product attention</strong>  Transformer中叫做放缩点乘注意力。输入是维度为<script type="math/tex">d_k</script>​的queries查询值和keys键值向量，以及维度<script type="math/tex">d_v</script>​的value值向量。所有的query和keys点乘，除以<script type="math/tex">\sqrt{d_k}</script>​，然后通过softmax函数获取values的权重。实际上，用如下公式进行矩阵计算:</p><script type="math/tex; mode=display">\text{Attention } (Q, K, V) = \text{softmax } (\frac{QK^T}{\sqrt d_k}) V \tag{1}</script><p><strong>多头注意力</strong>  在Transformer中，采用h次不同的线性投影queries， keys和values 来代替一个<script type="math/tex">d_{model}</script>​维度的keys， values和queries注意力函数，对应地学习得到线性投影的<script type="math/tex">d_k, d_k, d_v</script>​​注意力。注意力被并行地用于这些queries, keys和values投影子空间，得到<script type="math/tex">d_v</script>​维度的输出。它们被拼接起来，再次投影得到最终结果values。这就是多头注意力。</p><p><strong>高效内存注意力 </strong> 为了计算注意力机制的内存使用，作者把注意力集中于公式1的计算。假设Q, K, V的大小为[batch_size, length, d_model]。</p><p>主要的问题在于<script type="math/tex">QK^T</script>项，其大小为[batch_size, length, d_model。在实验部分，作者在序列长度为64K上训练模型，在该示例中，即使batch size为1，这个64K x 64K的矩阵，32位float也要16GB内存空间。这是不明智的并阻碍了在长序列上使用Transformer。值得注意的是<script type="math/tex">QK^T</script>矩阵不需要在内存中完全实现。而是可以对每个query <script type="math/tex">q_i</script>​​​​​​分开地计算注意力，这样只要在内存中计算公式1​一次，然后当需要梯度时在反向传播中再计算一次。这种计算注意的方式可能低效，但是使用内存只跟length成正比。在实验部分，作者使用高效的注意力实现来跑完整的注意力并将其作为基线。</p><p><strong>Q, K, V来自哪？</strong></p><p>前面描述的多头注意力，就是在keys, queries和values上操作，但是作者只给一个激活值A的tensor，其形状为[batch_size, length, d_model​​​​​]——来自于将句子里的字符token嵌入到向量中。为了从A中构建Q, K, 和V, Transformer用3个不同参数的线性层将A投影成Q，K和V。（就是实现多头注意力中的3个线性层如下：）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>) </span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">==================================================================    </span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  </span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  </span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>对于局部敏感哈希注意力，作者希望queries和keys相同(Q和K)。这很容易实现，就是使用同样的层从A投影到Q和K，另外使用不同的投影层得到V。作者调用该模型就像共享-QK的Transformer.在第5小节实验中，作者证明共享QK不影响表现，甚至作者加上用K的长度归一化项也没有影响。</p><p><strong>哈希注意力</strong>  对于局部敏感哈希注意力，作者开始用两个tensor，Q=K和V，其形状是[batch_size, length, d_model]。作者保持多头注意力机制完整和集中于公式1中注意力计算。正如已提到的，主要的问题在<script type="math/tex">QK^T</script>项,其大小为[batch_size, length, d_model]。但要注意的是作者实际上只对<script type="math/tex">\text{softmax }QK^T</script>。<strong>因为softmax取决于值最大的部分，那么只要集中于K中与每个<script type="math/tex">q_i</script>中最近的部分就可以了</strong>。(两个向量点乘最大值)。例如，如果K长度为64K，对于每个<script type="math/tex">q_i</script>只要考虑其中一个小的子集，就是说，只需要32或者64个最近的keys.这就更高效了，但是作者如何找到其中最近的keys呢?</p><p><strong>局部敏感哈希(LSH)</strong>  在高维空间中快速地寻找最近的点可以用局部敏感哈希LSH。如果按邻近的向量能以高概率获得相同的哈希值，远的则不能的哈希方案分配每个向量<script type="math/tex">x</script>给哈希函数<script type="math/tex">h(x)</script>​​​​，这就叫局部敏感。作者实例中，实际只要邻近向量以高概率得到相同的哈希值，并且哈希桶也以高概率具有相同大小。</p><p>作者用如图1所示的随机投影方法，得到b的哈希值，作者首先固定一个随机矩阵R，大小为[<script type="math/tex">d_k, b/2</script>​],来得到b。然后定义<script type="math/tex">h(x) = \text{argmax }\ ( [xR; -xR])</script>​,其中[u; v]表示拼接两个向量。该方法被称为LSH方案，其很容易实现并应用于批次的向量。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847633.png" alt="image-20210803153858209" style="zoom:35%;" /></p><p>如下图所示，图来自于Inference 1.就是key用不同的随机投影会得到一个值，将这些值组合起来就是对应的哈希桶，这只要保证投影时相近的点投影后在哈希桶是一样的就可以了。</p><p><img src="https://miro.medium.com/max/1392/1*fN4ck7Jd0gDilFeAZhowpA.gif" alt="img" style="zoom:60%;" /></p><p><strong>LSH注意力</strong>  了解了本文的LSH方案和通常的哈希注意力想法后，现在开始形式化本文的LSH注意力。首先，作者重写标准注意力的公式1，对于一个第i个query有:</p><script type="math/tex; mode=display">o_i = \sum_{j \in \mathcal{P}_i} \exp (q_i \cdot k_j -z(i, \mathcal{P}_i))v_j \quad \quad \text{其中} \mathcal{P}_i = \{j:i\ge j\} \tag{2}</script><p>这里，引入<script type="math/tex">\mathcal{P}_i</script>表示位置i处的query，z表示配分函数(就像softmax的归一化项)。为了表达清晰，忽略放缩项<script type="math/tex">\sqrt{d}_k</script>。</p><p>出于批次目的，作者通常在更大的<script type="math/tex">\widetilde{\mathcal{P}}_{i}=\{0,1, \ldots, l\} \supseteq \mathcal{P}_{i}</script>集合上用注意力，同时掩码不在<script type="math/tex">\mathcal{P}_i</script>​中的元素:</p><script type="math/tex; mode=display">o_i = \sum_{j \in \mathcal{P}_i} \exp (q_i \cdot k_j -m(j, \mathcal{P}_i)-z(i, \mathcal{P}_i))v_j \quad \text{其中} m\left(j, \mathcal{P}_{i}\right)=\left\{\begin{array}{ll}\infty & \text { if } j \notin \mathcal{P}_{i} \\0 & \text { otherwise }\end{array}\right. \tag{3}</script><p>上面公式的意思是，对于不能attention的位置，<script type="math/tex">m(j, \mathcal{P}_i)</script>为正无穷，那么<script type="math/tex">q_i \cdot k_j</script>​是正无穷，最后还是0。</p><p>现在作者转向LSH attention，query中位置i所能够注意到的集合<script type="math/tex">\mathcal{P}_i</script>​​​​被限制到一个哈希桶中。</p><script type="math/tex; mode=display">\mathcal{P}_i = \{ j: h(q_i) = h(k_j)\} \tag{4}</script><p>如下图2(a-b)部分是完整的attention和哈希变种attention的对比简图。（a）部分描述完整的注意力矩阵通常是稀疏的，但在计算上稀疏是没有优势的。(b)部分中,queries和keys已经按照它们的哈希桶排序了。因为相似项会以高概率掉入同样的哈希桶中，完整的attention模式可以通过只让注意力在每个桶内来近似。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847634.png" alt="image-20210803221257158" style="zoom:30%;" /></p><p>这个公式中哈希桶的大小往往不均匀，这使得跨桶批处理变得困难。并且，实际过程中桶内的queries和keys的数目也可能不一样，可能一个桶中含有大量queries但没有keys。为了缓解这个问题，作者首先通过设置<script type="math/tex">k_j = \frac{q_j}{||q_j||}</script>​​来确保<script type="math/tex">h(k_j) = h(q_j)</script>​​。接下来，作者先按照桶号对桶排序，在每个桶内按照序列的位置排序，这得到一个新的排序后的序列<script type="math/tex">i \mapsto s_j</script>​​。(如图d中序列<script type="math/tex">[q_1, q_2, q_3, q_3,q_6,q_5]</script>​到<script type="math/tex">[q_1, q_2, q_4, q_3, q_6, q_5]</script>​​)在这个排序的注意力矩阵中，一对来自同一桶中将聚类到对角线附近(如上图c部分).作者能遵循批量方法，将queries分词m块(排序后)，每块注意到自己和前一个块(如上图d)。按照之前记号，对应着下面设置:</p><script type="math/tex; mode=display">\widetilde{\mathcal{P}}_{i}=\left\{j:\left\lfloor\frac{s_{i}}{m}\right\rfloor-1 \leq\left\lfloor\frac{s_{j}}{m}\right\rfloor \leq\left\lfloor\frac{s_{i}}{m}\right\rfloor\right\} \tag{5}</script><p>如果<script type="math/tex">\max_i |\mathcal{P}_i| \lt m</script>,那么<script type="math/tex">\mathcal{P}_{i}\supseteq \widetilde{\mathcal{P}}_{i}</script>.实际上，作者设置<script type="math/tex">m = \frac{2l}{n_{\text{ buckets}}}</script>（l是queries序列长度）。平均每个桶的大小为<script type="math/tex">\frac{l}{n_{\text{ buckets}}}</script>​​,并且假定一个桶增长到两倍大小的概率足够低。LSH注意力整体流程如上图2所示。</p><blockquote><p>总结来说，整个过程就如左半边图：</p><ul><li><p>首先作者令输入序列的queries = keys</p></li><li><p>然后作者对其做LSH bucketing，得到每个query和key都在各自的bucket中（不同颜色表示）</p></li><li><p>作者跟根据bucket对query进行排序，同个bucket中，按照query原本的position进行排序。</p></li><li><p>在之后作者对于每个排序后的新序列，进行chunk 拆分</p></li><li><p>最后作者对于每个query只管制自己以及自己之前的chunk，对于这些候选集中相同bucket的key进行attend。</p><p>​                                                                                                                                                                                                    ——<a href="https://zhuanlan.zhihu.com/p/105123890">Reformer详解</a></p></li></ul></blockquote><p><strong>多轮局部敏感哈希注意力</strong> 用hash总会有很小的概率让相似的项掉落不同的桶中。这个概率可以用<script type="math/tex">n_{\text{rounds}}</script>轮不同的哈希函数<script type="math/tex">\{h^{(1)}, h^{(2)}, \cdots\}</script>​​来减小如：</p><script type="math/tex; mode=display">\mathcal{P}_{i}=\bigcup_{r=1}^{n_{\text {rounds }}} \mathcal{P}_{i}^{(r)} \quad \text { where } \mathcal{P}_{i}^{(r)}=\left\{j: h^{(r)}\left(q_{i}\right)=h^{(r)}\left(q_{j}\right)\right\} \tag{6}</script><p>多轮实例本质上是并行执行LSH注意力<script type="math/tex">n_{\text{rounds}}</script>轮；具体细节流程描述在附录A。</p><p><strong>共享QK注意力的原因掩码</strong>  在Transformer解码器中，掩码(如公式3表示的<script type="math/tex">m(j, \mathcal{P}_i )</script>​​​)被用来阻止注意到未来的位置。为了实现LSH注意力,作者将每个query/key向量和一个位置索引相连，使用用于对query/key向量进行排序的相同排列再次对位置索引排序，然后用比较操作来计算掩码。</p><p>然而注意到未来是不允许的，通常Transformer实现方法是这样做，允许注意到自身。这种做法在共享-QK公式中是不行的因为query点乘自身，总是好于点乘其它的向量。因此修改掩码来禁止token注意到自身，除非token没有其它有效的注意目标(如序列中第一个token).</p><h5 id="2-1-综合任务分析"><a href="#2-1-综合任务分析" class="headerlink" title="2.1 综合任务分析"></a>2.1 综合任务分析</h5><p>为了验证LSH注意力的表现和研究其方法，作者开始用一个综合任务:复制一个符号序列。在该任务中，每一个训练和测试样本形如<script type="math/tex">0w0w 其中w \in \{1, \cdots, N \}^*</script>​，是一个从1到N的符号序列(实验中N=127).w长度为3的示例如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847635.png" alt="image-20210804151253082" style="zoom:33%;" /></p><p>为了研究LSH注意力,作者在样本形如0w0w，其中w长度为511上训练(整个输入0w0w长为1024). 由于这是一个语言建模任务，作者总是在给定之前所有字符条件下预测下一个字符，但是作者掩码损失和准确了却只考虑了输入后半部分的位置，即那些实际可被预测到的位置。</p><p>上述任务能被一层Transformer模型完美解决(准确率100%, loss=0)。但需要注意的是，它需要非局部注意力查找(non-local attention lookups),因此依赖于有限跨度的的稀疏注意力模型都无法解决该问题。为了让其变得容易和快速训练但类似于NLP中使用的模型，作者使用一层Transformer: <script type="math/tex">d_{\text{model}}=d_{ff}=256</script>, 4个头。用4种不同设置训练模型150K步：完整的注意力， <script type="math/tex">n_{\text{rounds}}</script>​​分别为1， 2， 4的LSH注意力。</p><p>结果总结如下表2，完整注意力模型可以立即被LSH注意力模型使用，但准确率会损失一些。当从头开始训练LSH注意力，用4轮哈希的模型几乎达到完美的准确率。有趣的是，用8轮哈希来评估就是完美的。1轮或者2轮都会下降一些。<strong>模型在越少轮哈希上训练结果越差，但即使只用1轮哈希序列，用8轮哈希来评估也几乎完美。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847636.png" alt="image-20210804154335045" style="zoom:33%;" /></p><h4 id="3-可逆的Transformer"><a href="#3-可逆的Transformer" class="headerlink" title="3. 可逆的Transformer"></a>3. 可逆的Transformer</h4><p>如上面部分所示，用一种近似的方法将注意力的复杂度从长度的平方降到线性，这是可以接受。但是如下表1清楚展示，每一栏都以<script type="math/tex">b \cdot n_h \cdot l</script>项开始: <script type="math/tex">b \cdot n_h \cdot l \cdot d_k</script>或者被<script type="math/tex">b  \cdot l \cdot d_{\text{model}}</script>替换，这种内存占用代价是不可避免的。真正地,在每层之前的激活函数已经是<script type="math/tex">b  \cdot l \cdot d_{\text{model}}</script>了， 所以内存占用上<script type="math/tex">n_l</script>层模型至少要<script type="math/tex">b  \cdot l \cdot d_{\text{model}} \cdot n_l</script>。甚至更差：Transformer内部的前馈层会达到<script type="math/tex">b  \cdot l \cdot d_{\text{ff}} \cdot n_l</script>。在大型的Transformer中通常设置<script type="math/tex">d_{ff}=4K, n_l=16, l=64K</script>​，不切实际地，这会达到16GB内存占用。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847637.png" alt="image-20210804155653491" style="zoom:33%;" /></p><p>在这部分，作者将展示如何减少这种内存占用，首先用可逆层来处理<script type="math/tex">n_l</script>项部分，然后展示如何分块来处理<script type="math/tex">d_{ff}</script>​问题。每个方法在内存和时间复杂度上的效果总结和下表3。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847638.png" alt="image-20210804162632125" style="zoom:33%;" /></p><p><strong>RevNets</strong> <a href="https://arxiv.org/pdf/1707.04585.pdf">可逆残差网络</a>论文提出用于解决图片问题的ResNets中，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261847639.png" alt="image-20210804202919592" style="zoom:33%;" /></p><p>主要想法是每一层的activations可以根据下一层的activations推出来，这样就不需要存储中间的activations。普通的残差形式为<script type="math/tex">y = x + F(x)</script>，那么一对可逆层作用域一对输入有输出如:<script type="math/tex">(x_1, x_2) \mapsto (y_1, y_2)</script>，就有如下等式:</p><script type="math/tex; mode=display">y_{1}=x_{1}+F\left(x_{2}\right) \quad \quad y_{2}=x_{2}+G\left(y_{1}\right) \tag{7}</script><p>那么可以推导得到:</p><script type="math/tex; mode=display">x_2 = y_2 -G(y_1) \quad \quad x_1 = y_1 - F(x_2) \tag{8}</script><p><strong>可逆Transformer</strong>   应用RevNet想法于Transformer，原本sub-encoder block中的注意力层和前馈层是通过ResNet连接的，将其转换为RevNet。就是说把F变成注意力层，G变成前馈层。注意Layer Normalization 是移到残差块里面的:</p><script type="math/tex; mode=display">Y_1 = X_1 + \text{Attention}(X_2) \quad Y_2 = X_2 + \text{FeedForward}(Y_1) \tag{9}</script><p>可逆Transformer不需要存储每层的激活值就去掉了<script type="math/tex">n_l</script>项。在第5部分，展示了其表现和同样参数的普通Transformer一样，就是让<script type="math/tex">x_1</script>和<script type="math/tex">x_2</script>都是<script type="math/tex">d_{model}</script>​​大小来实现。</p><p><strong>Chunking</strong>  尽管可逆性消掉了<script type="math/tex">n_l</script>项，变薄的网络层仍然要大量的内存。前馈层的维度会非常大如<script type="math/tex">d_{ff}=4K</script>甚至更大。但是，跨位置序列里FFN计算是完全独立的，完整的FFN计算可分成<script type="math/tex">c</script>块:</p><script type="math/tex; mode=display">Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right] \tag{10}</script><p>这层通常通过对所有位置并行执行来批量化，但一次一块可以减少内存。可逆计算如公式8以及其反向传播也是分块的。除了前馈层，还对大词汇表的模型(超过<script type="math/tex">d_{model}</script>​单词类型)在输出时的对数概率计算分块，并一次性计算序列各部分的损失。</p><p><strong>Chunking， 大批次和参数复用</strong>  用分块和可逆层，作者在整个网络中activations占用内存与网络层数无关、尽管参数的数量随着层数的增加而增加，但参数并非如此。这个问题得到解决，因为作者可以在该层不计算时让CPU与内存交换该层的参数。在标准Transformer里，这是非常低效的因为内存转移到CPU中非常慢。然而，Reformer中batch size乘以长度非常大，因此用参数完成计算分摊了转换的成本。</p><p>后面部分就是些实验设置和结果了，就不翻译了。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0">Illustrating the Reformer</a></p><p>[2] <a href="https://blog.csdn.net/ljp1919/article/details/106061012">论文阅读笔记reformer</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/105123890">Reformer 详解</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reformer </tag>
            
            <tag> LSH </tag>
            
            <tag> RevNet </tag>
            
            <tag> Hash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15. RoBERTa 论文笔记</title>
      <link href="2020/12/19/NLP%20Paper%2015.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/12/19/NLP%20Paper%2015.%20RoBERTa%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="15-RoBERTa-论文笔记"><a href="#15-RoBERTa-论文笔记" class="headerlink" title="15. RoBERTa 论文笔记"></a>15. RoBERTa 论文笔记</h3><p>RoBERTa是<a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>的简称。本文主要是论文的阅读笔记。</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>预训练语言模型已经取得了显著的表现但是仔细比较不同方法是有挑战性的。在计算上训练非常昂贵，通常在不同大小的私有数据集上，并且超参数选择对最终结果有重大影响。作者提出一个BERT预训练的复制研究，仔细衡量许多关键超参数和训练数据大小的影响。作者发现BERT明显训练不足，并且能匹配和超越在其之后发布的每个模型表现。作者最好的模型在GLUE， RACE 和SQuAD上获得了最佳成绩。这些结果强调之前忽略的设计方案，并提出了近期报告的改进的来源的问题。作者发布了作者的模型和代码。</p><h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><p>自训练方法如ELMo， GPT，BERT， XLM， XLNet已经带来显著的性能提升，但是它们都被质疑，能否准确算出哪些方法贡献最大。在计算上训练是昂贵的，限制了可以进行微调的数目，或者说经常在不同大小的私有数据集上进行，限制了衡量建模效果的能力的发展。</p><p>作者提出BERT预训练的复制研究，包含慎重的<strong>超参数微调</strong>和<strong>训练集大小</strong>的影响评估。作者发现BERT是显著地训练不足，并提出一个改进的用来训练BERT模型的方案，作者称之为RoBERTa, 其能匹配或超越所有post-BERT方法的性能。作者改进非常简单，它们包括:</p><ol><li>训练模型更长时间，更大batch_size, 更多的数据</li><li>移除next sentence预测任务</li><li>训练更长的句子</li><li>动态改变应用于训练数据的掩码模式</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846457.png" alt="image-20211030011659476" style="zoom:33%;" /></p><p>作者也收集一个新的大型新闻数据集(CC-News) ，其大小类似于其它私人使用的数据集，以便更好地控制训练数据集大小的影响。</p><p>当控制训练数据时，作者改进的训练程序提升了已公布的BERT在GLUE和SQuAD两者上的的成绩。当在更多数据上进行更长时间的训练后，RoBERTa在公共GLUE排行榜上获得了88.5的分数，与Yang (Xlnet 2019) 88.4成绩相当。作者模型在4/9的GLUE任务上获得了最佳的成果: MNLI, QNLI, RTE 和STS-B。作者也在SQuAD和RACE上获得了相当的成绩。总的来说，作者重建了BERT的掩码语言模型训练方法和其它最近提出的有竞争性训练方法，如扰动自回归语言模型。</p><p>总的来说，本论文的贡献有:</p><ol><li>作者提出了一组的重要的BERT设计方案和训练策略，并引入更好的下游任务性能的替代方案；</li><li>作者使用一个小说数据集，CC-News，来确保使用更多数据进行预训练来进一步提升在下游任务上的表现；</li><li>作者训练提升表明掩码预训练语言模型，在正确的设计方案下，跟近期发布的所有其它方法都具有竞争力。</li></ol><p>作者发布了用Pytorch实现的模型，预训练和微调代码。</p><h4 id="2-Background"><a href="#2-Background" class="headerlink" title="2.  Background"></a>2.  Background</h4><p>本节，简洁回顾BERT预训练方法和一些作者将在接下来小节中实验检查的训练方案。</p><h5 id="2-1-Setup"><a href="#2-1-Setup" class="headerlink" title="2.1 Setup"></a>2.1 Setup</h5><p>BERT 采取两段(字符序列)连接作为输入，<script type="math/tex">x_1, \cdots,x_N 和 y_1, \cdots, y_M</script>。字符段通常由多余一个自然语句构成。两个字符段表示为一个用特殊字符分割两者的单一输入序列给BERT:<script type="math/tex">[\text{CLS}], x_1, x_2, \cdots, x_N, [\text{SEP}], y_1, \cdots, y_M, [\text{EOS}]</script>。 其中<script type="math/tex">M, N</script>限制为<script type="math/tex">M + N \lt T</script>,而<script type="math/tex">T</script>是训练中控制最大序列长度的参数。</p><p>该模型是首次预训练在大规模无标签文本语料并随后使用最终任务的有标签数据微调。</p><h5 id="2-2-Architecture"><a href="#2-2-Architecture" class="headerlink" title="2.2 Architecture"></a>2.2 Architecture</h5><p>BERT使用现在十分普遍的transformer架构，其作者将不会回顾其细节。作者使用<script type="math/tex">L</script>层的transformer架构。每个块使用A个维度为H的隐藏层的自注意力头。</p><h5 id="2-3-Training-Objectives"><a href="#2-3-Training-Objectives" class="headerlink" title="2.3 Training Objectives"></a>2.3 Training Objectives</h5><p>在预训练期间，BERT使用两个方法:掩码语言模型MLM和下一句预测NSP.</p><p><strong>Masked Language Model (MLM)</strong> </p><p>在输入序列中随机采样选择字符并替换其为特殊的字符[MASK]. MLM目标函数是预测这些mask的 tokens的交叉熵损失。BERT 统一地将输入tokens按15%可能性作为被选择字符进行替换。被选择的80%用[MASK]替换， 10%不变，10%用词汇token中随机选择替换。</p><p>在原本实现中，随机选择掩码和替换只在开始执行一次，并将其保存在训练时用；然而在实际中，数据是复制的所以掩码对于每次训练的句子来说不总是一样的。</p><p><strong>Next Sentence Prediction(NSP)</strong> </p><p>NSP 是关于预测在原始文本中的两个字符段是否是跟在一起的二分类损失。正阳样本通过从文本语料库中抽取连续的句子创建。负样本从不同文档中通过取成对片段来创建。正负样本都以同样的概率采样。</p><p>NSP目标被设计为用来提升在下游任务的表现，像自然语言推断NLI，需要推理出成对句子间的关系。</p><h5 id="2-4-Optimization"><a href="#2-4-Optimization" class="headerlink" title="2.4 Optimization"></a>2.4 Optimization</h5><p>BERT 用Adam来优化，参数如下<script type="math/tex">\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 1e-6</script>,其<script type="math/tex">L_2</script>权重衰减率为<script type="math/tex">0.01</script>. 学习率预热在前10,000步达到峰值<script type="math/tex">1e-4</script>，然后线性衰减。BERT以0.1的dropout在所有层和注意力权重，以及高斯误差线性单元GELU激活函数训练。模型以最大长度为T=512字符序列，mini-batches为<script type="math/tex">B=256</script>预训练<script type="math/tex">S = 1,000, 000</script>步迭代。</p><h5 id="2-5-Data"><a href="#2-5-Data" class="headerlink" title="2.5 Data"></a>2.5 Data</h5><p>BERT 在BOOKCorpus+英文WIKIPEDIA组合文本上训练，其包含总共16GB压缩文本。</p><h4 id="3-Experimental-Setup"><a href="#3-Experimental-Setup" class="headerlink" title="3. Experimental Setup"></a>3. Experimental Setup</h4><p>在本节中，作者描述作者的BERT复制研究实验设置。</p><h5 id="3-1-Implementation"><a href="#3-1-Implementation" class="headerlink" title="3.1 Implementation"></a>3.1 Implementation</h5><p>作者在 <a href="https://github.com/pytorch/fairseq">FAIRSEQ</a> 上再现BERT.基本上遵循原始的在小节2 中给定的BERT优化参数，除了峰值学习率lr和预热步数，它们都在每个设置中分开地第微调。 作者额外发现训练对Adam epsilon项非常敏感，并且在一些具体情况中作者在微调它后获得更好的表现或稳定性。类似地，作者发现设置<script type="math/tex">\beta_2 = 0.98</script>在大的batch size上提升了稳定性。</p><p>作者用最大为T=512字符的序列进行预训练。不同于BERT 2019，作者没有随机插入短序列，并且作者不在前90%更新步数中减少序列长度。作者仅训练完整长度的序列。</p><p>作者用混合精度的浮点运算在DGX-1机器上训练，每台机器有8x32GB NVIDIA V100 GPUs，其通过无限带宽互连。</p><h5 id="3-2-Data"><a href="#3-2-Data" class="headerlink" title="3.2 Data"></a>3.2 Data</h5><p>BERT-style 预训练严重依赖大量的文本。Baevski[2019 Cloze-driven Pretraining of Self-attention Networks] [完型填空驱动的预训练自注意力网络, 用类似英语完型填空的方法，对Transformer模型进行预训练] 论文证明增加数据集能提升在最终任务的性能。一些努力已经在比原始BERT更大更多样性的数据集上尝试了。不幸的是, 不是所有额外的数据集都会发布。对于作者研究而言，作者关注于为实验尽可能收集更多数据，允许作者为每次比较匹配整体数据质量和数量。</p><p>作者考虑5个不同大小和领域的英语语言语料库，总共超过160GB压缩文本。作者使用如下文本语料库:</p><ul><li><a href="https://github.com/soskek/bookcorpus">BOOKCORPUS</a> 加 英文维基百科。这就是原始BERT使用的数据(16GB)。</li><li>CC-News, 本论文收集于CommonCrawl News英文部分数据集。其包含63百万爬取与2016年9月到2019年2月的新闻文章 (76GB 在过滤后)。</li><li>OPENWEBTEXT,  一个开源的娱乐网页文本语料库。文本提取于最少3个赞的Reddit分享链接(38GB)。</li><li>STORIES， 一个包含CommonCrawl 的子类，其过滤后跟<strong>威诺格拉德模式</strong>故事风格类似的数据(31GB)。</li></ul><h5 id="3-3-Evalution"><a href="#3-3-Evalution" class="headerlink" title="3.3 Evalution"></a>3.3 Evalution</h5><p>遵循之前的工作，作者在下游任务中使用以下3个基准来评估RoBERTa。</p><p><strong>GLUE</strong> 通用语言理解评估基准，是9个为评估自然语言理解系统数据集集合。任务被设定为单句分类或者句子对分类。GLUE组织者提供训练集和验证集数据划分，以及一个提交服务器，允许参与者在留出测试数据上评估和比较的排行榜。</p><p>对于在小节4中的复制研究，作者报告结果是在对应单一任务训练数据微调预训练模型后的验证集结果。微调流程遵循原始BERT论文。</p><p>在小节5，作者额外地报告了源于公共排行榜测试集的结果。这些结果依赖针对几个特点任务的修改版，如在5.1小节中描述的。</p><p><strong>SQuAD</strong> 斯坦福QA数据集提供一段文本和一个问题。任务是通过从上下文中提取相关范围内容来回答这个问题。作者评估两个版本的SQuAD：V1.1 和V2.0. 在V1.1版本中上下文只是包含答案，然而V2.0一些问题就不在提供的上下文中，这使得该任务更有挑战性。</p><p>对于SQuAD V1.1 作者像BERT采用一样范围的预测方式。对于V2.0，作者添加额外二值分类器来预测该问题是否有答案，作者通过将分类和范围跨度损失项相加来联合训练。在评估阶段，只预测哪些分类是有答案的成对的范围跨度索引。</p><p><strong>RACE</strong> 阅读理解数据集，是大规模阅读理解数据集，包含超过28,000文章和将近100,000问题。数据集从中国英语考试中收集，其被设计用来作为初中和高中学生考试题。在阅读理解数据集中，每篇文章和多个问题联系在一起。对于每个问题，任务是从四个选择中选择一个正确的。RACE有大量的比其它流行的阅读理解数据集长的上下文本，并且需要推理的问题占比是非常大的。</p><h4 id="4-Training-Procedure-Analysis"><a href="#4-Training-Procedure-Analysis" class="headerlink" title="4. Training Procedure Analysis"></a>4. Training Procedure Analysis</h4><p>本节探索和量化哪些方案对于成功预训练BERT模型是重要。作者保持模型架构是固定的。特别地，作者开始用如<script type="math/tex">\text{BERT}_{BASE}, \ L=12, H=768, A=12, 110M  \ 参数</script>同样配置来训练BERT模型。</p><h5 id="4-1-Static-vs-Daynamic-Masking"><a href="#4-1-Static-vs-Daynamic-Masking" class="headerlink" title="4.1 Static vs. Daynamic Masking"></a>4.1 Static vs. Daynamic Masking</h5><p>如小节2中讨论的，BERT依赖随机掩码来预测tokens。原始BERT实现是在数据预处理时执行掩码一次，造成单一的静态掩码。为了避免在每epoch的每个训练实例上使用同样的掩码，训练数据被复制10份以便每个序列在超过40轮训练中是由10种不同方式掩码而成的。因此，每个训练序列在训练期间看起来是由4次相同的掩码构成的。</p><p>作者和动态掩码比较该策略，动态掩码是每次“喂”序列到模型是生成掩码模式。这在对多步预训练或者使用大数据时变得至关重要。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846459.png" alt="image-20210716165610591" style="zoom:30%;" /></p><p><strong>结论</strong> 如上表1，和发布的<script type="math/tex">\text{BERT}_{BASE}</script> 结果相比，作者用静态和动态掩码复现该结果。作者发现用静态掩码复现的表现类似于原始的BERT模型，但动态掩码轻微好于静态掩码。</p><p>给出动态掩码的结果和额外的效率好处，作者在剩下的实验中使用动态掩码。</p><h5 id="4-2-模型输入格式和下一句预测"><a href="#4-2-模型输入格式和下一句预测" class="headerlink" title="4.2 模型输入格式和下一句预测"></a>4.2 模型输入格式和下一句预测</h5><p>在原始的BERT预训练流程中，模型观测两个文档片段的连接，这两个片段要么是连续从同一文档中采样(p=0.5)，要么是从不同文档中采样。加上掩码语言模型目标，该模型训练通过一个辅助的下一句预测损失来预测是否被观察的文档片段来自于同一或不同文档。</p><p>NSP 损失在训练原始BERT模型被假设为一个非常重要的因子。Devlin 观察移除NSP任务会影响表现，在QNLI，MNLI，和SQuAD 1.1上有显著的表现退化。然而，近期一些工作质疑NSP 损失的必要性 [Cross-lingual Language Model Pretraining——Lample 2019]。</p><p>为了更好理解该差异，作者比较几种可替换训练格式：</p><ul><li><strong>SEGMENT-PAIR + NSP </strong>：这个遵循原始BERT使用格式，采用NSP loss. 每个都是成对的片段输入，这样每个输入都包含多个自然语句，但总长度必须小于512。</li><li><strong>SENTENCE-PAIR + NSP</strong> ： 每个输入包含一个自然语言句子对，这个句子对要么从一个文档部分中连续采样而来，要么就采样自不同文本。因为这些输入明显小于512字符长度，作者增加batch size以便字符总数保留类似于SEGMENT-PAIR + NSP。训练还是用NSP loss。</li><li><strong>FULL-SENTENCES</strong> : 每个输入由连续采样自同一文档或多个文档完整句子打包而成，另外总长度最大为512字符。这个输入可能跨文档。当作者到文档结尾时，作者开始从下一个文档中采样，并在两个文档间添加额外的分割符。训练移除NSP loss。</li><li><strong>DOC-SENTENCES</strong> : 输入构建类似于FULL-SENTENCES，处理其不跨文本采样。输入采样接近文档末尾还小于512字符，那么作者动态地在这些实例中增加batch size来达到和FULL-SENTENCES总的字符数目是相似的。训练移除NSP loss。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846460.png" alt="image-20210716203223556" style="zoom:30%;" /></p><p><strong>结论</strong>    如上表2所示4种不同设置的结果。作者开始拿原始的SEGMENT-PAIR 输入格式和SENTENCE-PAIR格式比较，都保留NSP loss，但最终都使用单一句子。发现<strong>使用单独句子让其在下游任务中表现变差</strong>，这使得作者假设是因为<strong>模型无法学到长距离依赖信息</strong>。</p><p>作者接下来比较没有NSP loss和用单一文档(DOC-SENTENCES) 形成的文本块训练。发现该设置表现优于原始发布的<script type="math/tex">\text{BERT}_{BASE}</script>结果并且<strong>移除NSP 任务不太影响下游任务表现</strong>，这是跟BERT [2019]对比。这种现象可能是原始BERT实现可能<strong>仅仅移除了损失项却仍然保留SENTENCE-PAIR输入格式</strong>。</p><p>最后作者发现限制输入序列来自于单一文档(DOC-SENTENCES)表现轻微好于从多个文档打包形成的输入(FULL-SENTENCES)。然而，因为这些DOC-SENTENCES格式结构是变化的batch sizes，为了更容易与相关工作比较，作者使用FULL-SENTENCES在剩下的实验中。</p><h5 id="4-3-Training-with-large-batches"><a href="#4-3-Training-with-large-batches" class="headerlink" title="4.3 Training with large batches"></a>4.3 Training with large batches</h5><p>在神经网络翻译的过去工作中，表明用<strong>非常大的小批次mini-batches能提升优化速度和最终任务的表现</strong>，当然学习率要恰当地增长。近期工作表明BERT也服从大的batch 训练方式。</p><p><script type="math/tex">\text{BERT}_{BASE}</script> 原始训练1M 步， 每批为256的序列。在计算成本上相当于，通过梯度累积用每批为2K的序列训练125K 步或每批为8K的序列训练31K步。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846461.png" alt="image-20210716205954879" style="zoom:30%;" /></p><p>如上表3所示， 作者跟<script type="math/tex">\text{BERT}_{BASE}</script>比较困惑度和最终任务表现， 随着每批大小增加，就是控制流入的训练数据数目。观察到<strong>用大批次训练会提升掩码语言模型目标的困惑度，也会提升最终任务的准确率</strong>。大批次数据也更容易用分布式数据系统来并行化训练，在接下来的实验中，作者用每批8K的序列来训练。</p><p>尤其是 You [Reducing bert pre-training time from 3 days to 76 minutes] 用更大的批次数据训练BETR，达到32K序列。作者留给将来工作探索大批次训练的限制。</p><h5 id="4-4-Text-Encoding"><a href="#4-4-Text-Encoding" class="headerlink" title="4.4 Text Encoding"></a>4.4 Text Encoding</h5><p>Byte-Pair Encoding(BPE) 字节对编码是介于字符级和单词级的混合表示，可以处理大规模自然语言语料库中的常见的词汇。BPE依赖subwords子词单元而不是完整单词，子词单元可以用统计分析训练语料库得到。</p><p>BPE词汇表大小通常在10k-100K子词单元。然而，对大规模和多样语料库建模时，unicode字符占了大部分，例如 Radford 2019<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2    Language Models are Unsupervised Multitask Learners</a> 考虑了其工作原因。GPT-2 引入了一个简单BPE实现，使用直接代替unicode 字符作为基本子词单元。使用自己使得其可能学到不太大(50 K units) 子词词汇表，其仍然能在不引入然后“未知”字符条件下编码任意输入文本。</p><p>原始BERT实现使用一个字符级别的30K的BPE词汇表，其是在用启发式字符化规则预处理输入后学习得到的。根据Radford 2019 GPT-2中，考虑用字节级别50K子词单元的BPE词汇表训练BERT，而不是用任何额外的预处理或字符化输入。对应着<script type="math/tex">\text{BERT}_{BASE}</script>和<script type="math/tex">\text{BERT}_{LARGE}</script>会增加大约15M和20M额外训练参数。</p><blockquote><p>基于 char-level ：原始 BERT 的方式，它通过对输入文本进行启发式的词干化之后处理得到。<br>基于 bytes-level：与 char-level 的区别在于bytes-level 使用 bytes 而不是 unicode 字符作为 sub-word 的基本单位，因此可以编码任何输入文本而不会引入 UNKOWN 标记。</p><p>​                                                                                                        ——<a href="https://blog.csdn.net/ljp1919/article/details/100666563">RoBERTa 笔记</a></p></blockquote><p>早期实验显示这些编码直接只有轻微差异，在Radford 2019 GPT-2 BPE的一些任务上得到更长的最终任务表现甚至稍差。尽管如此，作者相信统一编码方案的优势超过性能轻微下降。更多这些编码的细节比较将留给未来工作。</p><h4 id="5-RoBERTa"><a href="#5-RoBERTa" class="headerlink" title="5. RoBERTa"></a>5. RoBERTa</h4><p>在之前的小结中作者提出对BERT预训练流程进行修改来提升最终任务表现。现在合计这些改进和评估它们共同的影响。作者把这些配置叫做RoBERTa，即Robustly optimized BERT approach 强壮的BERT优化方法。特别地，RoBERTa用动态掩码(4.1节)训练， 没有NSP loss的FULL-SENTENCES 输入(4.2节)，大的mini-batches(4.3节)以及大规模的字节级编码BPE(4.4节)。</p><p>另外地，作者研究两个其它重要的因素，在之前工作中没有被强调的：</p><ol><li>用于预训练的数据</li><li>训练步数</li></ol><p>例如，近期提出的XLNet架构，预训练数据接近原始BERT的10倍。其用8倍的batch size， 优化步数减半训练，因此看起来是BERT4倍的预训练序列。</p><p>为了从其他模型方案中(如， 预训练目标)帮助理清这些因素的重要性，开始时使用<script type="math/tex">\text{BERT}_{LARGE}</script>架构的配置L=24， H=1024,A=16， 355M参数来训练RoBERTa。在一个类似的BOOKCORPUS+WIKIPEDIA数据集预训练100K步，这也用于BERT Devlin 2019 .整个预训练使用1024 块V100 GPUs 将近一天。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846462.png" alt="image-20210717001045090" style="zoom:25%;" /></p><p><strong>结论</strong> 如上表4所示， 当控制训练数据时，观察到RoBERTa相比原始报告<script type="math/tex">\text{BERT}_{LARGE}</script>的有大的提升，再次肯定了作者在小节4中提到的设计方案的重要性。</p><p>接着，联合3个小节3.2中附加的数据集和BOOK + WIKI数据。并用跟之前一样训练步数（100K）在联合数据上训练RoBERTa。总共预训练数据文本超过160GB。进一步观察其在所有下游任务的性能提升，验证数据大小和多样性在预训练中的重要性。</p><p>最后， 预训练RoBERTa 步数显著变长，预训练步数从100K到300K，最后进一步到500K。作者再次观察在下游任务上获得显著性能提升，在大部分任务上300K到500K步训练的模型性能优于<script type="math/tex">\text{XLNet}_{LARGE}</script>.注意到更长时间训练模型没有在作者的数据上出现过拟合，而从额外训练中获益。</p><p>在剩余的本文中，在3个不同基准上评估最好的RoBERTa模型，分别是GLUE， SQuAD和RACE。特别地，作者考虑在小节3.2中介绍的5个数据集上训练500K步。</p><h5 id="5-1-GLUE-Results"><a href="#5-1-GLUE-Results" class="headerlink" title="5.1 GLUE Results"></a>5.1 GLUE Results</h5><p>对于GLUE，作者考虑两个微调设置。第一个设置(单一任务， 验证)，对每个GLUE任务分别微调RoBERTa，这仅使用对应任务的训练数据。作者考虑限制超参数扫描每个任务，如batch size 为{16， 32}， lr为{1e-5, 2e-5, 3e-5}, 在开始的6%不是线性预热接着线性衰减到0.微调10轮，并基于每个任务验证集评估标准执行早停。剩下的超参数在预训练时保持不变。在该设置中，在每个任务上的5个随机初始化模型在验证集上结果的中位数作为报告，并且没有模型集成。</p><p>第二个设置(集成， test)， 通过GLUE排行榜和其它方法在测试集上比较RoBERTa。然而许多排行榜上结果依赖多任务微调，作者提交<strong>只依赖与单一任务微调</strong>。对于RTE，STS以及MRPC，发现其对微调帮助其余MNLI单一任务模型，而不是预训练RoBERTa的基线。作者探索一个稍宽的超参数空间，如附录，并在每个任务上组合5到7个模型。</p><p><strong>具体任务修改</strong>  两个GLUE任务需要特定任务的微调方法来达到有竞争性的排行榜结果。</p><p><strong>QNLI</strong> (Qusetion-answering NLI，问答自然语言推断)： 对于QNLI任务最近提交在GLUE排行榜采用成对的答案计算排名，就是候选答案挖掘于训练集并和另一个比较，一个单一的(问题， 候选答案)对被分类为正例。该计算公式明显地简化了盖伦肉，但不是直接对比于BERT。遵循最近工作，作者对测试提交采用该排名方法，但为了直接和BERT比较，报告中验证集结果基于纯粹的分类任务。</p><p><strong>WNLI</strong> (Winograd NLI，Winograd自然语言推断): 作者发现提供的NLI-format数据很难处理，反而作者使用从Super GLUE [Wang 2019 A stickier benchmark for general-purpose language understanding systems]中重新格式化后的WNLI数据，其表示查询词和所指对象的范围。作者使用边缘排名损失来微调RoBERTa,来自于[Kocijan 2019 A Surprisingly Robust Trick for the Winograd Schema Challenge]。作者使用spaCy来提取来自句子中的额外的候选名词词组并微调作者模型让其对正向相关词组比生成的任意的负向候选词组分配高分数。该计算方式不好的结果是作者只能利用正训练样本，这只包括超过一半提供的训练样本。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846463.png" alt="image-20210717035356897" style="zoom:25%;" /></p><p><strong>结论</strong>    如上表5所示，第一个设置(单一任务， 验证集)，RoBERTa在GLUE的9项任务中用验证集取得了最佳成绩。至关重要的是，RoBERTa使用跟<script type="math/tex">\text{BERT}_{LARGE}</script>一样的掩码语言建模预训练目标和架构，但始终优于<script type="math/tex">\text{BERT}_{LARGE}</script>和<script type="math/tex">\text{XLNet}_{LARGE}</script>.这引发了有关模型架构和预训练目标，和更平凡的细节如在本文中的研究的数据大小，训练时间哪个更重要的质疑。</p><p>第二个设置(集成， 测试集)， 作者将RoBERTa提交给GLUE排行榜，并取得了9个任务中4个最佳的成绩及迄今为止的最高平均分。这十分振奋人心，因为RoBERTa不依赖多任务的微调，不像大部分其它的高分提交。作者期望未来工作通过包含更多先决的多任务微调流程更进一步提升这些成绩。</p><h5 id="5-2-SQuAD-Results"><a href="#5-2-SQuAD-Results" class="headerlink" title="5.2 SQuAD Results"></a>5.2 SQuAD Results</h5><p>作者相比之前工作对SQuAD采用更简单的方法。特别是，BERT和XLNet通过额外的QA数据集来增加它们的训练数据，作者<strong>只用提供的SQuAD数据来微调RoBERTa</strong>。Yang 2019 还采用自定义的逐层计划学习率来微调XLNet，而作者在所有层使用同样的学习率。</p><p>对于SQuAD v1.1作者跟BERT使用同样的微调方案。对于V2.0， 作者添加给定一个问题是否可回答的分类任务；通过对该分类和预测范围的损失相加来联合训练分类器和范围预测。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846464.png" alt="image-20210717042714019" style="zoom:25%;" /></p><p><strong>结论</strong>    如上表6所示，在SQuAD v1.1 的验证集上，RoBERTa跟XLNet结果差不多。在V2.0验证集上，RoBERTa取得新的最佳成绩，比XLNet提升0.4个点(EM) 和 0.6个点 (F1)。</p><p>作者也提交RoBERTa到公共的SQuAD 2.0 排行榜，来评估其相对其它系统的表现。大部分顶层系统要么构建于BERT要么XLNet，两种都依赖于额外训练数据。相反，本文提交不使用任何额外的数据。</p><p>单一RoBERTa模型优于其它单一模型的提交，并且是这些模型之间不依赖数据增强的最高分。</p><h5 id="5-3-RACE-Results"><a href="#5-3-RACE-Results" class="headerlink" title="5.3 RACE Results"></a>5.3 RACE Results</h5><p>在RACE中，系统提供了一篇文章、一个相关问题和四个候选答案。系统需要区分四个候选答案哪个是正确的。</p><p>作者为该任务修改RoBERTa，将每个候选答案与其对应的问题和文章连接在一起。然后编码这四个序列和传递结果的[CLS]表示，流过全连接层，其用来预测正确答案。作者截断那些长于128个字符的问题-答案对，如果需要，截断文章使得总长度最长为512字符。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846465.png" alt="image-20210717045600233" style="zoom:25%;" /></p><p>结果如上表7，在RACE测试集上，RoBERTa在初中组和高中组都取得了最佳成绩。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>作者仔细评估了一些在预训练BERT模型时的设计方案。发现性能能大大地提升方案：</p><ol><li>训练更长时间</li><li>在更多数据上使用大的batch size</li><li>移除NSP目标</li><li>训练更长的序列</li><li>使用训练数据动态改变掩码模型</li></ol><p>本文的提升预训练方案，称作RoBERTa，在GLUE，RACE和SQuAD上取得了最佳成绩。</p><ul><li>在GLUE上没有多任务微调</li><li>在SQuAD上没有使用额外数据</li></ul><p>这些结果证明了之前整体设计方案的重要性和近期提出的可替代方案中建议BERT的预训练目标保留是有竞争力的。</p><p>添加的数据有一个<strong>小说数据集，CC-News</strong>。<a href="https://github.com/pytorch/fairseq">预训练和微调的模型和代码地址</a>.</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://github.com/brightmart/roberta_zh">中文预训练RoBERTa模型</a></p><p>[2] <a href="https://blog.csdn.net/ljp1919/article/details/100666563">RoBERTa 笔记</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/143064748">RoBERTa论文详解和代码实战</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> RoBERTa </tag>
            
            <tag> PTMs </tag>
            
            <tag> BPE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 7 Machine Translation, Attention, Subword Models</title>
      <link href="2020/12/14/CS224N%20Lecture%207%20Machine%20Translation,%20Attention,%20Subword%20Models/"/>
      <url>2020/12/14/CS224N%20Lecture%207%20Machine%20Translation,%20Attention,%20Subword%20Models/</url>
      
        <content type="html"><![CDATA[<h3 id="Section-1-Machine-Translation"><a href="#Section-1-Machine-Translation" class="headerlink" title="Section 1: Machine Translation"></a>Section 1: Machine Translation</h3><p>机翻是一个把句子x从一种语言(源语言)翻译到另一种语言(目标语言)的句子y的任务。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816031.png" alt="image-20210214095051158" style="zoom:16%;" /></p><h4 id="1990s-2010s-Statistical-Machine-Translation"><a href="#1990s-2010s-Statistical-Machine-Translation" class="headerlink" title="1990s-2010s: Statistical Machine Translation"></a>1990s-2010s: Statistical Machine Translation</h4><ul><li><p>核心思想：从数据中学习概率模型</p></li><li><p>假设我们在翻译法语→英语</p></li><li><p>我们希望给定法语句子x，找到最好的英语句子y</p><script type="math/tex; mode=display">\text{argmax }_y P(y \vert x) \tag{1}</script></li><li><p>使用贝叶斯法则将其分解为两个部分分开学习：</p><script type="math/tex; mode=display">= \text{argmax }_y P(x \vert y) P(y)\tag{2}</script></li></ul><p>上图中，x：法语， y：英语， 机器翻译的目标就是寻找y，使得$P(y \vert x)$最大，这就是式1. 接下来，我们利用贝叶斯分解为两个概率的乘积：其中$P(y )$用之前介绍过得语言模型，从英语单语的数据中学习怎么写的通顺。最简单的就用n-gram方法。$P(y \vert x)$，是指由目标语言到源语言的翻译模型，其专注于翻译模型，翻译好局部的单词和短语，这个模型学习来自于平行的数据语料。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816033.png" alt="image-20210214095424905" style="zoom:18%;" /></p><h4 id="Learning-alignment-for-SMT"><a href="#Learning-alignment-for-SMT" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h4><ul><li>Q: 怎么从平行语料中学习到翻译模型$P(x \vert y)$？</li><li>进一步分解这个任务：引入一个模型的隐变量$a$：$P(x, a \vert y)$</li><li>当a是对齐的，单词程度对应着源句子x到目标句子y。可以认为是两种语言之间单词和单词或短语和短语的一个对齐关系。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816034.png" alt="image-20210214135929263" style="zoom:18%;" /></p><h4 id="What-is-alignment"><a href="#What-is-alignment" class="headerlink" title="What is alignment?"></a>What is alignment?</h4><p>对齐是在翻译句子对中指定单词的对应关系。如下图就是英语和法语的alignment。</p><ul><li>不同语言直接的类型差异导致了复杂的对齐关系</li><li>注意：一些单词没有对应单词。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816035.png" alt="image-20210214140542816" style="zoom:16%;" /></p><p><strong>对齐是复杂的</strong></p><ul><li>多对一， 一对多， 一对一， 多对多</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816036.png" alt="image-20210214141137092" style="zoom:18%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816037.png" alt="image-20210214141300684" style="zoom:18%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816038.png" alt="image-20210214141323292" style="zoom:18%;" /></p><h4 id="Learning-alignment-for-SMT-1"><a href="#Learning-alignment-for-SMT-1" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h4><ul><li>我们学习$P(x, a \vert y)$作为许多组合因素，包括：<ul><li>指定单词对齐的概率(也取决于发送的位置)</li><li>指定单词的概率有指定丰富关系(对应单词的数目)</li><li>etc</li></ul></li><li>对齐a是一个隐变量：它们在数据中不明确具体。<ul><li>需要用特殊的算法(像最大期望算法) ，来学习有隐变量的参数分布。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816039.png" alt="image-20210214141459130" style="zoom:20%;" /></p><h4 id="Decoding-for-SMT"><a href="#Decoding-for-SMT" class="headerlink" title="Decoding for SMT"></a>Decoding for SMT</h4><p>如果计算argmax呢?</p><ul><li>穷举所有可能y然后计算这个概率? 代价太大！</li><li>答案是： 采用模型是完全独立的假设，使用动态规划获得全局最优解(像<strong>维特比算法</strong>)，启发式搜索。</li><li>这个过程称为解码</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816040.png" alt="image-20210214142536962" style="zoom:20%;" /></p><p>在搜索过程中，对概率较低的路径进行剪枝，只保留概率较大的翻译路径。如下图的搜索树，对于概率较低的路径就不往下搜索了。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816041.png" alt="image-20210214143501378" style="zoom: 20%;" /></p><p><strong>1990s-2010s： 统计机翻</strong></p><ul><li>SMT是一个非常大的研究领域</li><li>最好的系统极其复杂<ul><li>我们还有上百个关键细节没有提到</li><li>系统有许多独立设计的子组件</li><li>许多特征工程，需要设计特征来获取指定的语言现象</li><li>需要编译和维护额为的资源，像相等的短语表</li><li>维护需要大量人力，对每个语言对进行重复艰难的尝试！</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816042.png" alt="image-20210214144203912" style="zoom:20%;" /></p><h3 id="Section-2-Neural-Machine-Translation"><a href="#Section-2-Neural-Machine-Translation" class="headerlink" title="Section 2: Neural Machine Translation"></a>Section 2: Neural Machine Translation</h3><h4 id="What-is-Neural-Machine-Translation"><a href="#What-is-Neural-Machine-Translation" class="headerlink" title="What is Neural Machine Translation?"></a>What is Neural Machine Translation?</h4><ul><li>神经网络机翻是是用单一端对端神经网络做机翻的方式</li><li>神经网络架构被称为sequence-to-sequence 模型(简称seq2seq)，其包含两个RNNs。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816043.png" alt="image-20210214145948971" style="zoom:20%;" /></p><h4 id="Neural-Machine-Translation-NMT"><a href="#Neural-Machine-Translation-NMT" class="headerlink" title="Neural Machine Translation (NMT)"></a>Neural Machine Translation (NMT)</h4><p>seq2seq模型如下图， 它由两个RNN构成。左边红色部分是Encoder RNN，它负责对源语言进行编码；右边的绿色部分称为Decoder RNN，它负责对目标语言进行解码（Decode）。首先，Encoder RNN可以是任意一个RNN，比如朴素RNN、LSTM或者GRU。Encoder RNN负责对源语言进行编码，学习源语言的隐含特征。Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。Decoder RNN是一个条件语言模型，一方面它是一个语言模型，即用来生成目标语言的；另一方面，它的初始隐状态是基于Encoder RNN的输出，所以称Decoder RNN是条件语言模型。Decoder RNN在预测的时候，需要把上一个神经元的输出作为下一个神经元的输入，不断的预测下一个词，直到预测输出了结束标志符<END>，预测结束。Encoder RNN的输入是源语言的word embeding，Decoder RNN的输入是目标语言的word embeding。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816044.png" alt="image-20210214150257267" style="zoom: 20%;" /></p><p>Seq2seq是很强大的模型不仅仅用于MT，还有许多NLP任务能变大为seq2seq：</p><ul><li>文本概括</li><li>对话</li><li>解析</li><li><p>代码生成</p></li><li></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816045.png" alt="image-20210214154406061" style="zoom:20%;" /></p><ul><li>seq2seq是一个条件语言模型的例子<ul><li>LM因为解码是预测目标句子y的下一个词</li><li>条件是因为它预测也是建立在源句子x条件上</li></ul></li><li>NMT直接计算$P(y \vert x)$，如下图中所示。</li></ul><p>那么问题来了，怎么训练NMT系统呢？</p><ul><li>找一个大的平行语料</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816046.png" alt="image-20210214163621078" style="zoom:20%;" /></p><h4 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h4><p>seq2seq被优化为一个单一系统。反向传播训练是“端对端”。如下图所示，将Encoder</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816047.png" alt="image-20210214154746788" style="zoom:20%;" /></p><h4 id="Multi-layer-RNNs"><a href="#Multi-layer-RNNs" class="headerlink" title="Multi-layer RNNs"></a>Multi-layer RNNs</h4><p>多层RNNs：</p><ul><li>RNNs已经是在一个维度上是“深”的(将其展开成许多时刻)</li><li>我们也可以让其在另外的维度上变“深”，就通过应用多个RNNs——变成多层的RNN。</li><li>允许网络计算更复杂的表示，低层的RNNs应该计算低层的特征高层RNNs应该计算高层的特征</li><li>多层RNNs也叫堆叠RNNs</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816048.png" alt="image-20210214163426512" style="zoom:20%;" /></p><h4 id="Multi-layer-deep-encoder-decoder-machine-translation-net"><a href="#Multi-layer-deep-encoder-decoder-machine-translation-net" class="headerlink" title="Multi-layer deep encoder-decoder machine translation net"></a>Multi-layer deep encoder-decoder machine translation net</h4><p>多层深度编码-解码机翻网络</p><p>来自RNN第i层的隐状态是RNN第i+1层的输入；左边红色部分多层RNN堆叠在一起作为编码器，来构建源句子的意思；右边绿色部分作为解码器来生成翻译。中间就像个瓶颈一样，叫做bottleneck。实际上多层RNNs应用技巧看上篇笔记。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816049.png" alt="image-20210214191853097" style="zoom:20%;" /></p><h4 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h4><ul><li>我们看到怎样生成(或者说“解码”)目标句子，在解码的每个时间序列上采用argmax。</li><li>这就是贪婪解码(的在每个step上取最可能的单词)</li><li>这个方法的问题？</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816050.png" alt="image-20210214192822013" style="zoom:20%;" /></p><p><strong>Problems with greedy decoding</strong>  </p><ul><li>贪婪解码没有办法回溯，那怎么修复这个问题呢?</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816051.png" alt="image-20210214193344085" style="zoom:20%;" /></p><h4 id="Exhaustive-search-decoding"><a href="#Exhaustive-search-decoding" class="headerlink" title="Exhaustive search decoding"></a>Exhaustive search decoding</h4><p><strong>穷举搜索解码</strong></p><ul><li>理想情况下，我们想要找到一个(长度为T)让y最大化的翻译，如下图所示</li><li>我们尝试计算所有可能的序列y<ul><li>这意味着在解码器的每个step，我们遍历$V^t$个可能的部分翻译，其中$V$是词汇表的size，$O(V^t)$时间复杂度太高了！</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816052.png" alt="image-20210214193551720" style="zoom:20%;" /></p><h4 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h4><p>束搜索解码</p><ul><li>核心思想：在解码器每一个step上，遍历k个最可能的部分翻译(这个我们叫假设)<ul><li>k是约束大小(实际上值为5-10)</li></ul></li><li>假设$y_1, \cdots, y_t$有一个其对数概率的分数，如下图所示<ul><li>所有分数是负数，分数越高表示更好的结果</li><li>我们在每个step上遍历Top-k个来搜索高分数假设,这里Top-k不仅仅是当前$\hat y$最高的几个概率，而是截止到目前这条路径上的累计概率之和。</li></ul></li><li>束搜索不确保找到最优解</li><li>但比穷举搜索更有效率</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816053.png" alt="image-20210214194328812" style="zoom:20%;" /></p><p>如下图中，束搜索size=2时，第一个timestep保留的Top-2词为<code>he</code>和<code>I</code>，它们分别作为下一个timestep的输入。</p><p><code>he</code>输入预测输出Top-2是<code>hit</code>和<code>struck</code>，<code>hit</code>这条路径上累加概率是 <code>he</code>加上 <code>hit</code>为-1.7。同样计算其它词对应路径的概率分数。最后在这4条路径上保留$k=2$条路。所以保留 <code>hit</code>和 <code>was</code>。将其作为下一个timestep的输入；那么 <code>struck</code>和 <code>got</code>对应路径应该被剪枝。这就是，对k个假设逐个，找到top k下一个词并计算其分数。 </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816054.png" alt="image-20210214200249928" style="zoom:20%;" /></p><p>再下一个timestep同样计算，剪枝后保留<code>a</code>和<code>me</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816055.png" alt="image-20210214200232110" style="zoom:20%;" /></p><p>最终搜索树如下图所示，以看到在每个时间步都只保留了k=2个节点往下继续搜索。最后<code>pie</code>对应的路径打分最高，通过回溯法得到概率最高的翻译句子。请注意，束搜索作为一种剪枝策略，并不能保证得到全局最优解，但它能以较大的概率得到全局最优解，同时相比于穷举搜索极大的提高了搜索效率。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816056.png" alt="image-20210214220133325" style="zoom:20%;" /></p><h4 id="Beam-search-decoding-stopping-criterion"><a href="#Beam-search-decoding-stopping-criterion" class="headerlink" title="Beam search decoding: stopping criterion"></a>Beam search decoding: stopping criterion</h4><p>束搜索解码：停止标准</p><ul><li>在贪婪解码里，通常我们解码知道模型输出一个<code>&lt;END&gt;</code>结束标志符。<ul><li>例子： <code>&lt;START&gt;</code>he hit me with a pie <code>&lt;END&gt;</code></li></ul></li><li>在束约束搜索解码里，不同路径预测输出 <code>&lt;END&gt;</code> 结束标志符的timestep可能不一样，有些路径可能提取结束了，称为完全路径，暂时把这些完全路径放一边，其他路径接着束搜索。</li><li>通常我们继续束搜索直到：<ul><li>我们达到timestep T(其中T是提取设置的中断条件)</li><li>或者我们至少有n个完成的路径(其中n是提取设置的中断条件)</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816057.png" alt="image-20210214220623849" style="zoom:20%;" /></p><h4 id="Beam-search-decoding-finishing-up"><a href="#Beam-search-decoding-finishing-up" class="headerlink" title="Beam search decoding: finishing up"></a>Beam search decoding: finishing up</h4><p>束搜索解码：结束</p><ul><li>我们有了完全的假设(路径)列表。</li><li>怎么选择有最高分的top one?</li><li>每个假设$y_1, \cdots, y_t$在列表里都有一个分数，如下图。</li><li>问题在于：假设越长，分数越低。在搜索路径上，累加得越多分越低，所有根据长度对打分进行归一化。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816059.png" alt="image-20210214221530118" style="zoom:20%;" /></p><blockquote><p>NMT相比于SMT的优点：</p><ol><li>性能更好，表现在：翻译更流程，RNN擅长语言模型；能更好的利用上下文关系；能利用短语相似性</li><li>模型更简单，只需要一个神经网络，端到端训练即可，简单方便</li><li>不需要很多的人工干预和特征工程，对于不同的语言，网络结构保持一样，只需要换一下词向量</li></ol><p>NMT的不足：</p><ol><li>难以解释，表现在：难以调试，难以解释出错原因</li><li>难以控制，比如难以加入一些人工的规则，难以控制NMT不输出什么，由此可能会脑一些笑话甚至导致安全问题</li></ol></blockquote><h4 id="How-do-we-evaluate-Machine-Translation"><a href="#How-do-we-evaluate-Machine-Translation" class="headerlink" title="How do we evaluate Machine Translation?"></a>How do we evaluate Machine Translation?</h4><p>怎么评价机翻?</p><p><strong>BLEU</strong> (Bilingual Evaluation Understudy  )</p><ul><li>BLEU将机翻和人工翻译(一个或多个)进行比较重叠部分，具体公式看Assignment 4，然后基于这个计算一个相似分数<ul><li>n-gram精度</li><li>过短的机翻加上一个惩罚</li></ul></li><li>BLEU 有用但不完美<ul><li>有许多有效的方法来翻译一个句子</li><li>所以一个好的翻译可能得到一个糟糕的分数因为它和人工翻译有较低的n-gram重叠</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816060.png" alt="image-20210214232534422" style="zoom:20%;" /></p><h4 id="MT-progress-over-time"><a href="#MT-progress-over-time" class="headerlink" title="MT progress over time"></a>MT progress over time</h4><p>数据来源于(<a href="http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf">http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf</a>)</p><blockquote><p>神经机器翻译于2014年从边缘研究活动到2016年成为领先标准方法</p><ul><li>2014：第一篇 seq2seq 的文章发布</li><li>2016：谷歌翻译从 SMT 换成了 NMT</li><li>这是惊人的，由数百名工程师历经多年打造的SMT系统，在短短几个月内就被少数工程师训练过的NMT系统超越</li></ul><p><strong>So is Machine Translation solved?</strong></p><ul><li>不！</li><li>许多困难仍然存在<ul><li>词表外的单词处理</li><li>训练和测试数据之间的 <strong>领域不匹配</strong></li><li>在较长文本上维护上下文</li><li>资源较低的语言对</li></ul></li><li>使用常识仍然很难</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816061.png" alt="image-20210214233122492" style="zoom:20%;" /></p><h4 id="NMT-research-continues"><a href="#NMT-research-continues" class="headerlink" title="NMT research continues"></a>NMT research continues</h4><p>NMT是NLP深度学习的旗舰</p><ul><li>NMT研究引领了NLP深度学习的许多创新</li><li>在2021：NMT研究继续蓬勃发展<ul><li>研究这已经找到许多，许多改进在我们刚刚提到的“原生的” seq2seq NMT系统</li><li>但是我们将在接下来介绍新的一种不可或缺的改进——Attention!</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816062.png" alt="image-20210214233512856" style="zoom:20%;" /></p><h3 id="Section-3-Attention"><a href="#Section-3-Attention" class="headerlink" title="Section 3: Attention"></a>Section 3: Attention</h3><h4 id="Sequence-to-sequence-the-bottleneck-problem"><a href="#Sequence-to-sequence-the-bottleneck-problem" class="headerlink" title="Sequence-to-sequence: the bottleneck problem"></a>Sequence-to-sequence: the bottleneck problem</h4><p>seq2seq: 瓶颈问题</p><p>在下图中，Encoder RNN 的最后一个神经元的隐状态作为Decoder RNN的初始隐状态，也就是说Encoder的最后一个隐状态向量需要承载源句子的所有信息，成为整个模型的“信息”瓶颈。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816063.png" alt="image-20210214234910353" style="zoom:20%;" /></p><p>在最后一个隐藏状态，需要获取的全部关于源句子的信息。这就是瓶颈问题。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816064.png" alt="image-20210214235212101" style="zoom:20%;" /></p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><ul><li>注意机制提供了解决瓶颈问题的方法</li><li>核心思想：在每一步都解码，用直连解码器来专注于源句子中特殊的部分</li><li>开始我们用图来说明，然后用方程。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816065.png" alt="image-20210214235651799" style="zoom:20%;" /></p><h4 id="Sequence-to-sequence-with-attention"><a href="#Sequence-to-sequence-with-attention" class="headerlink" title="Sequence-to-sequence with attention"></a>Sequence-to-sequence with attention</h4><blockquote><p>具体来说，在时刻t，Decoder第t时刻的隐状态$s_t$和Encoder所有时刻的隐状态$h_1, \cdots, h_n$做点积，得到N个标量Attention score，作为Encoder每个隐状态的权重，然后使用softmax对这些权重进行归一化，得到Attention distribution。这个Attention distribution相当于Decoder在时刻t对Encoder所有隐状态的注意力分布，如下第3张图所示，”he”时刻的注意力主要分布在Encoder的第2和第4个词上，这不正是前面介绍的SMT的对齐alignment操作吗！Attention自动学习到了这种对齐操作，只不过是soft alignment。</p><p>接下来，对Encoder所有隐状态使用Attention distribution进行加权平均，得到Attention output $a_t$。把$a_t$和该时刻的隐状态$s_t$拼起来再进行非线性变换得到输出$\hat y_2$。有时，也可以把上一时刻的Attention output和当前的输入词向量拼起来作为一个新的输入，输入到Decoder RNN中。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816066.png" alt="image-20210215144443366" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816067.png" alt="image-20210215144932320" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816068.png" alt="image-20210215145000723" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816069.png" alt="image-20210215150326981" style="zoom:20%;" /></p><h4 id="Attention-in-equations"><a href="#Attention-in-equations" class="headerlink" title="Attention: in equations"></a>Attention: in equations</h4><ul><li>编码器的隐状态$h_1, \cdots, h_n \in \mathbb{R}^h$</li><li>在timestep $t$, 解码器隐状态为$s_t \in \mathbb{R}^h$</li><li>得到该时刻$t$的注意力分数</li></ul><script type="math/tex; mode=display">\mathbf{e}^t = [\mathbf{s}_t^T\mathbf{h}_1, \cdots,\mathbf{s}_t^T\mathbf{h}_N   ] \in \mathbb{R}^N</script><ul><li>对$\mathbf{e}^t $取softmax得到注意力分布$\mathbf{\alpha}^t $(这是一个概率分布，和为1)</li></ul><script type="math/tex; mode=display">\mathbf{\alpha}^t = \text{softmax}(\mathbf{e}^t ) \in \mathbb{R}^N</script><ul><li>将$\mathbf{\alpha}^t $  和编码器隐状态$\mathbf{h} $点乘得到注意力输出$\mathbf{a}_t $</li></ul><script type="math/tex; mode=display">\mathbf{a}_t = \sum_{i=1}^N \alpha_i^t\mathbf{h}_i \in \mathbb{R}^N</script><ul><li>最后，将注意力输出$\mathbf{a}_t $和解码器隐状态$s_t$拼起来，并按照非注意力seq2seq模型运行$[\mathbf{a}_t; \mathbf{s}_t  ] \in \mathbb{R}^{2h}$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816070.png" alt="image-20210215151755989" style="zoom:20%;" /></p><h4 id="Attention-is-great"><a href="#Attention-is-great" class="headerlink" title="Attention is great"></a>Attention is great</h4><blockquote><ul><li>注意力显著提高了NMT性能<ul><li>这是非常有用的，让解码器专注于某些部分的源语句</li></ul></li><li>注意力解决瓶颈问题<ul><li>注意力允许解码器直接查看源语句；绕过瓶颈</li></ul></li><li>注意力帮助消失梯度问题<ul><li>提供了跟短接远处状态的方式</li></ul></li><li>注意力提供了一些可解释性<ul><li>通过检查注意力的分布，我们可以看到解码器在关注什么</li><li>我们可以轻松得到(软)对齐</li><li>这很酷，因为我们从来没有明确训练过对齐系统</li><li>网络只是自主学习了对齐</li></ul></li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816071.png" alt="image-20210215153928196" style="zoom:20%;" /></p><h4 id="Attention-is-a-general-Deep-Learning-technique"><a href="#Attention-is-a-general-Deep-Learning-technique" class="headerlink" title="Attention is a general Deep Learning technique"></a>Attention is a general Deep Learning technique</h4><ul><li>我们已经看到attention是一种改进机翻的seq2seq模型很好的方法</li><li>然而，你可以在许多架构中用attention</li><li>注意力更普遍的定义是：<ul><li>给定一系列向量的值，或者向量查询队列，attention是一种计算关于查询队列与向量值权重和的技术</li></ul></li><li>我们有时说，查询注意到哪些值</li><li>例如，在seq2seq + attention 模型中，每个解码器的隐状态查询注意到首页解码器隐状态的某一些值。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816072.png" alt="image-20210215154326539" style="zoom:20%;" /></p><p><strong>直觉</strong></p><ul><li>加权和是值中包含的信息的选择性汇总，查询在其中确定要关注哪些值</li><li>注意是一种获取任意一组表示(值)的固定大小表示的方法，依赖于其他一些表示(查询)。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816073.png" alt="image-20210215154345002" style="zoom:20%;" /></p><h4 id="There-are-several-attention-variants"><a href="#There-are-several-attention-variants" class="headerlink" title="There are several attention variants"></a>There are several attention variants</h4><ul><li>一个值的集合$\mathbf{h}_1, \cdots, \mathbf{h}_n \in \mathbb{R}^{d_1}$，一个 查询$\mathbf{s} \in \mathbb{R}^{d_2}$</li><li>Attention总是包括<ul><li>计算注意力分数</li><li>对分数取softmax得到注意力分布</li><li>用注意力分布对隐状态值求权重和来获得注意力输出$\mathbf{a}$</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816074.png" alt="image-20210215155633285" style="zoom:20%;" /></p><p><strong>Attention 变种</strong></p><p>其实就是计算从编码器隐状态$\mathbf{h}_1, \cdots, \mathbf{h}_N \in \mathbb{R}^{d_1}$和解码器隐状态$\mathbf{s} \in \mathbb{R}^{d_2}$计算attention分数$\mathbf{e} \in \mathbb{R}^{N}$的几种方式：</p><ul><li>基本点积注意力：$\mathbf{e}_i = \mathbf{s}^T \mathbf{h}_i \in \mathbb{R}$，注假设$d_1= d2$,这就是我们之前见到的</li><li>乘法注意力：$\mathbf{e}_i = \mathbf{s}^T \mathbf{W} \mathbf{h}_i \in \mathbb{R}$, 其中$\mathbf{W} \in \mathbb{R}^{d_2 \times d_1}$是权重矩阵</li><li>加法注意力：$\mathbf{e}_i = \mathbf{v}^T \text{tanh }(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}) \in \mathbb{R}$. 其中$\mathbf{W}_1 \in \mathbb{R}^{d_3 \times d_1}, \ \mathbf{W}_2 \in \mathbb{R}^{d_3 \times d_2}$是权重矩阵，$\mathbf{v}\in \mathbb{R}^{d_3}$是权重向量，$d_3$是超参数</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816075.png" alt="image-20210215155708656" style="zoom:20%;" /></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261816076.png" alt="image-20210215164336654" style="zoom:20%;" /></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture07-nmt.pdf">cs224n-2021-lecture07-nmt</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">cs224n-2019-notes06-NMT_seq2seq_attention</a></p><p>[3] <a href="https://ruder.io/deep-learning-nlp-best-practices/index.html">Deep Learning for NLP Best Practices</a></p><p>[4] <a href="https://bitjoy.net/2019/08/02/cs224n%ef%bc%881-31%ef%bc%89translation-seq2seq-attention/">CS224N（1.31）Translation, Seq2Seq, Attention</a></p><p>[5] <a href="https://blog.csdn.net/u011613991/article/details/105616634">Machine Translation, Seq2Seq and Attention</a></p><p>[6] <a href="https://looperxx.github.io/CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/">Science is interesting.08 Machine Translation, Sequence-to-sequence and Attenti</a></p><p>[7] <a href="https://blog.csdn.net/RHJlife/article/details/107321601">MachineTranslation seq2seq Attention</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> Attention </tag>
            
            <tag> 机翻 </tag>
            
            <tag> SMT </tag>
            
            <tag> NMT </tag>
            
            <tag> Beam search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. Hexo + Matery blog 搭建</title>
      <link href="2020/12/14/hexo+matery%20blog%E6%90%AD%E5%BB%BA/"/>
      <url>2020/12/14/hexo+matery%20blog%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Hexo-Matery-blog-搭建"><a href="#1-Hexo-Matery-blog-搭建" class="headerlink" title="1. Hexo + Matery blog 搭建"></a>1. Hexo + Matery blog 搭建</h2><h3 id="1-卸载以前安装-hexo"><a href="#1-卸载以前安装-hexo" class="headerlink" title="1. 卸载以前安装 hexo"></a>1. 卸载以前安装 hexo</h3><h4 id="1-卸载-hexo-及-重新安装"><a href="#1-卸载-hexo-及-重新安装" class="headerlink" title="1. 卸载 hexo 及 重新安装"></a>1. 卸载 hexo 及 重新安装</h4><p><strong>记得备份以前文档</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo npm uninstall -g hexo-cli</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">apt-get 卸载</span></span><br><span class="line">sudo apt-get remove --purge npm</span><br><span class="line">sudo apt-get remove --purge nodejs</span><br><span class="line">sudo apt-get remove --purge nodejs-legacy</span><br><span class="line">sudo apt-get autoremove</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">手动删除 npm 相关目录</span></span><br><span class="line">sudo rm -r /usr/local/bin/npm</span><br><span class="line">sudo rm -r /usr/local/lib/node-moudels</span><br><span class="line">sudo find / -name npm</span><br><span class="line"></span><br><span class="line">sudo rm -rf /usr/local/lib/*</span><br><span class="line"></span><br><span class="line">sudo rm -r /tmp/npm* </span><br></pre></td></tr></table></figure><h4 id="2-重新安装nodejs-npm-hexo"><a href="#2-重新安装nodejs-npm-hexo" class="headerlink" title="2. 重新安装nodejs/ npm/ hexo"></a>2. 重新安装nodejs/ npm/ hexo</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">重新安装nodejs</span></span><br><span class="line">sudo curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -</span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install yarn</span><br><span class="line">sudo apt-get install nodejs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看版本</span></span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级nodejs和npm</span></span><br><span class="line">sudo npm install n -g</span><br><span class="line">sudo n stable</span><br><span class="line">sudo node -v</span><br><span class="line"></span><br><span class="line">sudo npm i -g npm to update</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 hexo</span></span><br><span class="line">sudo npm install -g hexo-cli</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><h3 id="2-安装和配置Matery"><a href="#2-安装和配置Matery" class="headerlink" title="2. 安装和配置Matery"></a>2. 安装和配置Matery</h3><h4 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h4><p>在<code>hexo</code>的<code>themes</code>目录下：</p><p><a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">Matery文档</a></p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/">blog实例</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/blinkfox/hexo-theme-matery.git</span><br></pre></td></tr></table></figure><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code> 的值为：<code>theme: hexo-theme-matery</code></p><h4 id="2-插件"><a href="#2-插件" class="headerlink" title="2. 插件"></a>2. 插件</h4><ol><li><code>sudo npm uninstall hexo-prism-plugin</code>， 如果安装过prism插件，卸载掉！然后修改 Hexo 根目录下 <code>_config.yml</code></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">highlight:</span><br><span class="line">  enable: false</span><br><span class="line">  line_number: true</span><br><span class="line">  auto_detect: false</span><br><span class="line">  tab_replace: &#x27;&#x27;</span><br><span class="line">  wrap: true</span><br><span class="line">  hljs: false</span><br><span class="line">prismjs:</span><br><span class="line">  enable: true</span><br><span class="line">  preprocess: true</span><br><span class="line">  line_number: true</span><br><span class="line">  tab_replace: &#x27;&#x27;</span><br></pre></td></tr></table></figure><ol><li><code>generator-search</code> 安装搜索插件，并在Hexo 根目录下 <code>_config.yml</code>加上3-5行</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-generator-search --save            #搜索</span><br><span class="line"></span><br><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br></pre></td></tr></table></figure><ol><li><code>hexo-permalink-pinyin</code> 安装链接中文转拼音插件，并在Hexo 根目录下 <code>_config.yml</code>加上3-5行</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo npm i hexo-permalink-pinyin --save</span><br><span class="line"></span><br><span class="line">permalink_pinyin:</span><br><span class="line">  enable: true</span><br><span class="line">  separator: &#x27;-&#x27; # default: &#x27;-&#x27;</span><br></pre></td></tr></table></figure><ol><li><code>hexo-wordcount</code> 安装字数统计插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm i --save hexo-wordcount</span><br></pre></td></tr></table></figure>并在matery主题目录下 <code>_config.yml</code>加上<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">postInfo:</span><br><span class="line">  date: true</span><br><span class="line">  update: false</span><br><span class="line">  wordCount: false # 设置文章字数统计为 true.</span><br><span class="line">  totalCount: false # 设置站点文章总字数统计为 true.</span><br><span class="line">  min2read: false # 阅读时长.</span><br><span class="line">  readCount: false # 阅读次数.</span><br></pre></td></tr></table></figure></li><li><code>hexo-filter-github-emojis</code> 安装emoji插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-filter-github-emojis --save</span><br></pre></td></tr></table></figure>在Hexo 根目录下 <code>_config.yml</code>加上<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">githubEmojis:</span><br><span class="line">  enable: true</span><br><span class="line">  className: github-emoji</span><br><span class="line">  inject: true</span><br><span class="line">  styles:</span><br><span class="line">  customEmojis:</span><br></pre></td></tr></table></figure></li><li><code>hexo-generator-feed</code> 安装订阅插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-generator-feed --save #RSS</span><br></pre></td></tr></table></figure>在 Hexo 根目录下的 _config.yml 文件中，新增以下的配置项：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">feed:</span><br><span class="line">  type: atom</span><br><span class="line">  path: atom.xml</span><br><span class="line">  limit: 20</span><br><span class="line">  hub:</span><br><span class="line">  content:</span><br><span class="line">  content_limit: 140</span><br><span class="line">  content_limit_delim: &#x27; &#x27;</span><br><span class="line">  order_by: -date</span><br></pre></td></tr></table></figure></li><li>其它插件安装<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-deployer-git --save                   #git部署</span><br><span class="line">sudo npm install hexo-generator-seo-friendly-sitemap --save #sitemap</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>公式插件插件安装</p><p>卸载<code>hexo-renderer-marked</code> 不然大量行内公式不渲染，安装<code>install hexo-renderer-kramed</code>.</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo npm uninstall hexo-renderer-marked --save</span><br><span class="line">sudo npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p><a href="https://qingstudios.com/2020/03/01/Hexo%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">参考数学公式解决方法</a></p><p>找到<code>hexo博客目录/node_modules/hexo-renderer-kramed/lib/renderer.js</code>，把下面代码修改。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.replace(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line"><span class="comment">//  return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span></span><br><span class="line">  <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装<code>mathjax</code> ,如果 已安装<code>hexo-math</code> 用<code>sudo npm uninstall hexo-math --save</code>卸载.</p><p>这样不能完全解决公式问题，还需要将不能渲染行内公式<script type="math/tex"> </script>转变为 $$$ $$$。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-renderer-mathjax --save #mathjax</span><br></pre></td></tr></table></figure><h4 id="3-proxy"><a href="#3-proxy" class="headerlink" title="3. proxy"></a>3. proxy</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set proxy=socks://127.0.0.1:1080</span><br><span class="line">sudo npm config set proxy=http://127.0.0.1:8885</span><br><span class="line">sudo npm config set registry=http://registry.npmjs.org</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-post-头和"><a href="#4-post-头和" class="headerlink" title="4. post 头和"></a>4. post 头和</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">typora</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-09-07 09:25:00</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">miller</span></span><br><span class="line"><span class="attr">img:</span> <span class="string">/source/images/xxx.jpg</span></span><br><span class="line"><span class="attr">top:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">cover:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">coverImg:</span> <span class="string">/images/1.jpg</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span></span><br><span class="line"><span class="attr">toc:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">summary:</span> <span class="string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">Markdown</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Typora</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Markdown</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#a131de</span></span><br><span class="line"></span><br><span class="line"><span class="string">.bg-color</span> &#123;</span><br><span class="line">    <span class="attr">background-image:</span> <span class="string">linear-gradient(to</span> <span class="string">right</span>, <span class="comment">#4cbf30 0%, #0f9d58 100%);</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">sudo</span> <span class="string">npm</span> <span class="string">install</span> <span class="string">hexo-heading-index</span> <span class="string">--save</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 杂项 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 6 Vanishing Gradients, Fancy RNNs, Seq2Seq</title>
      <link href="2020/12/12/CS224N%20Lecture%206%20Vanishing%20Gradients,%20Fancy%20RNNs,%20Seq2Seq/"/>
      <url>2020/12/12/CS224N%20Lecture%206%20Vanishing%20Gradients,%20Fancy%20RNNs,%20Seq2Seq/</url>
      
        <content type="html"><![CDATA[<h3 id="The-Simple-RNN-Language-Model"><a href="#The-Simple-RNN-Language-Model" class="headerlink" title="The Simple RNN Language Model"></a>The Simple RNN Language Model</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810617.png" alt="image-20210211185344655" style="zoom:20%;" /></p><p>简单的RNN语言模型。</p><h4 id="Training-an-RNN-Language-Model"><a href="#Training-an-RNN-Language-Model" class="headerlink" title="Training an RNN Language Model"></a>Training an RNN Language Model</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810619.png" alt="image-20210211190631508" style="zoom:20%;" /></p><blockquote><p>RNN在$t$时刻的输出是预测第$t+1$个词的概率分布$y^(t)$；而对于训练集中给定的文本来说，第$t+1$个词是已知的某个词，所以真实答案$y(t)$其实是一个one-hot向量，在第$x(t+1)$位置为1，其他位置为0。所以如果是交叉熵损失函数的话，表达式如上图中间的等式。RNN整体的损失就是所有时刻的损失均值。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810621.png" alt="image-20210211191026911" style="zoom:20%;" /></p><p>输入序列<code>the students opened their</code> 预测后面的词。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810622.png" alt="image-20210211191137637" style="zoom:20%;" /></p><p>训练RNN语言模型，其实loss是所有时刻t的损失的均值。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810623.png" alt="image-20210211191633630" style="zoom:20%;" /></p><blockquote><ul><li><p>然而：计算 <strong>整个语料库</strong> 的损失和梯度太昂贵了</p></li><li><p>在实践中，输入序列实际上是一个 <strong>句子</strong> 或是 <strong>文档</strong>，这就跟随机梯度下降法中，随机抽小批样本一样。</p></li><li>回忆 ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li><li>计算一个句子的损失(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li></ul></blockquote><h4 id="Training-the-parameters-of-RNNs-Backpropagation-for-RNNs"><a href="#Training-the-parameters-of-RNNs-Backpropagation-for-RNNs" class="headerlink" title="Training the parameters of RNNs: Backpropagation for RNNs"></a>Training the parameters of RNNs: Backpropagation for RNNs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810624.png" alt="image-20210211192116116" style="zoom:20%;" /></p><p>损失函数对权重矩阵W等参数的偏导是什么？</p><h3 id="Problems-with-Vanishing-and-Exploding-Gradients"><a href="#Problems-with-Vanishing-and-Exploding-Gradients" class="headerlink" title="Problems with Vanishing and Exploding Gradients"></a>Problems with Vanishing and Exploding Gradients</h3><h4 id="Vanishing-gradient-intuition"><a href="#Vanishing-gradient-intuition" class="headerlink" title="Vanishing gradient intuition"></a>Vanishing gradient intuition</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810625.png" alt="image-20210212125204219" style="zoom:20%;" /></p><p>梯度消失的直觉，根据链式法则，求每个节点对隐藏变量$\mathbf{h}_i$的梯度时，如果其接近0时，传播越深，梯度信号就会越来越小，直至消失。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810626.png" alt="image-20210212140058181" style="zoom:20%;" /></p><h4 id="Vanishing-gradient-proof-sketch-linear-case"><a href="#Vanishing-gradient-proof-sketch-linear-case" class="headerlink" title="Vanishing gradient proof sketch (linear case)"></a>Vanishing gradient proof sketch (linear case)</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810627.png" alt="image-20210212140204624" style="zoom: 20%;" /></p><ul><li><p>对于隐藏层$\mathbf{h}^{(t)}$来说，就是激活函数作用到，上一个隐藏输入和输入分布的权重和加上偏置。</p></li><li><p>如果激活函数是恒等函数，比如$\sigma(x) = x$,<br>其梯度就是上面的整体的导数的对角阵再乘以 <script type="math/tex">\mathbf{W}_{h}</script> ；即单位矩阵乘以 <script type="math/tex">\mathbf{W}_{h}</script></p></li><li><p>考虑第$i$时刻损失 <script type="math/tex">\mathbf{J}^{(i)}(\theta)</script> 对于在之前时刻 <script type="math/tex">j</script> 隐藏层 <script type="math/tex">\mathbf{h}^{(j)}</script> 的梯度，令  <script type="math/tex">l = i-j</script></p><p>如果$\mathbf{W}_{h}$本身非常小，当$l$变大时，这项会产生指数问题。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810628.png" alt="image-20210212143125560" style="zoom:20%;" /></p><ul><li>$\mathbf{W}_{h}^{l}$什么问题?</li><li>假设$\mathbf{W}_{h}$的特征值都小于1:</li><li>可以将其梯度写作$\mathbf{W}_{h}$的一组基的形式，这时，就会导致梯度消失</li><li>那非线性激活函数$\sigma$呢？也一样，除了需要证明对于在维度上一些$\gamma$独立和$\sigma$有$\lambda_i &lt; \gamma$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810629.png" alt="image-20210212144556708" style="zoom:20%;" /></p><p>为什么梯度消失是一个问题？</p><ul><li>梯度信号从远处传来会消失是因为，它比近处的小得多</li><li>因此，模型权重只会更新对应的近处影响，而不是长期的影响</li></ul><h4 id="Effect-of-vanishing-gradient-on-RNN-LM"><a href="#Effect-of-vanishing-gradient-on-RNN-LM" class="headerlink" title="Effect of vanishing gradient on RNN-LM"></a>Effect of vanishing gradient on RNN-LM</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810630.png" alt="image-20210212144854345" style="zoom:20%;" /></p><ul><li><strong>语言模型的任务</strong>：预测下一个词</li><li>为了从训练样本学习到这个，语言模型需要建立依赖于在第7步“tickets” 和最后目标词“tickets”之间的的模型。</li><li>但是如果梯度很小，模型就无法学习到这个依赖关系。因此模型无法在测试时预测类似的长距离依赖关系</li></ul><h4 id="Why-is-exploding-gradient-a-problem"><a href="#Why-is-exploding-gradient-a-problem" class="headerlink" title="Why is exploding gradient a problem?"></a>Why is exploding gradient a problem?</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810631.png" alt="image-20210212150150737" style="zoom:20%;" /></p><ul><li>如果梯度变得过大，那么SGD更新步伐将变大</li><li><p>这将导致错误的更新：我们取太大的步伐，到达一个怪异的错误的参数配置(产生大的loss)</p><ul><li>你想下你发现一个斜坡来跳跃，但突然你在低谷了</li></ul></li><li><p>在最坏的情况下，这会在你网络中导致NaN(那么你得从之前某个checkpoint重新训练)</p></li></ul><p><strong>Gradient clipping: solution for exploding gradient</strong>  </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810632.png" alt="image-20210212150859173" style="zoom:20%;" /></p><ul><li><strong>梯度裁剪</strong>：就像算法1中，如果梯度的模大于阈值，就在SGD之前缩小它</li><li><strong>直觉</strong>：取同样方向的一步，不过是一小步</li><li>实际上，记得裁剪梯度是非常重要的，但梯度爆炸是一个容易解决的问题。</li></ul><p><strong>How to fix the vanishing gradient problem?</strong>  </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810633.png" alt="image-20210212151219392" style="zoom:20%;" /></p><ul><li><p>主要问题是在，对于RNN很难学到遍历许多时刻后保留信息</p></li><li><p>在RNN变种里，隐藏状态不断被重写</p></li><li><p>RNN有独立记忆怎么样?</p><p>引入LSTM</p></li></ul><h3 id="Long-Short-Term-Memory-RNNs-LSTMs"><a href="#Long-Short-Term-Memory-RNNs-LSTMs" class="headerlink" title="Long Short-Term Memory RNNs (LSTMs)"></a>Long Short-Term Memory RNNs (LSTMs)</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810634.png" alt="image-20210212151628214" style="zoom:20%;" /></p><ul><li><p>在1997年一种类型的RNN被<a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter and Schmidhuber  </a> 提出，其作为梯度消失问题的解决措施。</p><ul><li>每个人引用这篇论文，但是现代LSTM真正关键部分源自Gers <a href="https://dl.acm.org/doi/10.1162/089976600300015015"> Learning to Forget: Continual Prediction with LSTM</a></li></ul></li><li><p>在时刻$t$, 有一个隐藏状态$\mathbf{h}^{(t)}$和cell状态$\mathbf{c}^{(t)}$</p><ul><li>都是长度为$n$的向量</li><li>cell存储长期信息</li><li>LSTM能从cell中读取、删除，和写入信息 ，cell是一个概念而不是计算机里的RAM</li></ul></li><li>通过控制三个对应的门来删除、写入、读取选择性的信息<ul><li>门也是长为$n$的向量</li><li>在每个时刻，每个门元素能打开(1), 关闭(0)，或者介于之间状态</li><li>门是动态的：基于当前上下文来计算它们的值</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810635.png" alt="image-20210212160622165" style="zoom:20%;" /></p><p>我们有一个输入序列$\mathbf{x}^{(t)}$,要计算隐藏状态$\mathbf{h}^{(t)}$序列和cell状态$\mathbf{c}^{(t)}$.在时刻$t$:</p><ul><li>遗忘门：控制上一个cell状态的保持和遗忘，$\mathbf{f}^{(t)}=\sigma(\mathbf{W}_f\mathbf{h}^{(t-1)}+\mathbf{U}_f\mathbf{x}^{(t)} + \mathbf{b}_f)$</li><li>输入门：控制新cell的哪些内容写入到cell， $\mathbf{i}^{(t)}=\sigma(\mathbf{W}_i\mathbf{h}^{(t-1)}+\mathbf{U}_i\mathbf{x}^{(t)} + \mathbf{b}_i)$</li><li>输出门：控制哪些cell的部分输出到隐藏层， $\mathbf{o}^{(t)}=\sigma(\mathbf{W}_o\mathbf{h}^{(t-1)}+\mathbf{U}_o\mathbf{x}^{(t)} + \mathbf{b}_o)$</li><li>新cell内容：写入cell的新内容： $\mathbf{\tilde c}^{(t)}=\text{tanh}(\mathbf{W}_c\mathbf{h}^{(t-1)}+\mathbf{U}_c\mathbf{x}^{(t)} + \mathbf{b}_c)$</li><li>Cell状态： 删除上一个cell“forget”中一些内容，写入“input”一些新cell内容：$\mathbf{c}^{(t)}=\mathbf{f}^{(t)}\circ \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)}\circ \mathbf{\tilde c}^{(t)}$</li><li>隐藏状态：读取从cell中“输出”一些内容：$\mathbf{h}^{(t)}=\mathbf{o}^{(t)}\circ \text{tanh} \mathbf{c}^{(t)}$</li></ul><p>注意：门都是应用的逐元素积或者说Hadamard积。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810636.png" alt="image-20210213003729927" style="zoom: 20%;" /></p><p>你可以把LSTM方程可视像上图一样， <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah LSTM</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810637.png" alt="image-20210213003942932" style="zoom: 20%;" /></p><p>具体来说，来看每个门的作用。</p><h4 id="How-does-LSTM-solve-vanishing-gradients"><a href="#How-does-LSTM-solve-vanishing-gradients" class="headerlink" title="How does LSTM solve vanishing gradients?"></a>How does LSTM solve vanishing gradients?</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810638.png" alt="image-20210213004955463" style="zoom: 20%;" /></p><p><strong>LSTM怎么解决梯度消失问题?</strong></p><ul><li>LSTM比RNN更容易保留许多时刻后的信息。<ul><li>比如，如果对于一个cell ，遗忘门设置为1，输入门设置为0，那么这个cell的信息被无限地保留</li><li>相比，普通RNN很难学到循环权重矩阵$\mathbf{W}_h$, 其保留在隐藏层state的信息</li><li>实际上，你能得到大约100个时刻而不是7个时刻</li></ul></li><li>LSTM不确保，没有梯度消失，但是提供了让模型更容易学习长矩依赖的方式</li></ul><h4 id="LSTMs-real-world-success"><a href="#LSTMs-real-world-success" class="headerlink" title="LSTMs: real-world success"></a>LSTMs: real-world success</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810639.png" alt="image-20210213151650925" style="zoom:20%;" /></p><p><strong>LSTM：现实的成功</strong></p><ul><li>在2013——2015，LSTMs开始取得最先进的成果：<ul><li>成功的任务包含，手写识别，语音识别，机翻，解析，和图片标题，以及语言模型</li><li>LSTM成为大部分NLP任务的统治方法</li></ul></li><li>在2021年，别的方法（如Transformers）已经成为许多任务的统治方法<ul><li>例如，在WMT</li><li>2016 WMT，总结报告包含 “RNN” 44次</li><li>2019 WMT ， “RNN” 7 次， “Transformer” 105次</li></ul></li></ul><h4 id="Is-vanishing-exploding-gradient-just-a-RNN-problem"><a href="#Is-vanishing-exploding-gradient-just-a-RNN-problem" class="headerlink" title="Is vanishing/exploding gradient just a RNN problem?"></a>Is vanishing/exploding gradient just a RNN problem?</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810640.png" alt="image-20210213152606764" style="zoom:20%;" /></p><p>梯度消失/爆炸仅仅是RNN中的问题吗？</p><ul><li><p>不是！所有的神经网络(包括前馈和卷积网络)都会存在梯度爆炸或是梯度消失的问题，特别是很深的网络。</p><ul><li>由于链式法则/非线性激活函数的选择，梯度当其反向传播是渐渐变小</li><li>因此，低层学习得十分缓慢(很难训练)</li></ul></li><li><p>Solution： 许多新的深层前馈/卷积结构，添加更直接的连接(这允许梯度流向更远)</p><p>例子：</p><ul><li>残差连接，“Resnet”</li><li>跳跃连接</li><li>通过默认保留信息的相同连接</li><li>这些深度网络更容易训练</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810641.png" alt="image-20210213153746863" style="zoom:20%;" /></p><p>其它方法：</p><ul><li>稠密连接的， “DenseNet”</li><li>直接连接所有将来的层</li><li>高速公路连接， “HighwayNet”</li><li>类似于残差连接，但是等同连接和变换层被一个动态门控制</li><li>受LSTMs的影响， 应用在深层前馈/卷积网络</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810642.png" alt="image-20210213154309665" style="zoom:20%;" /></p><p><strong>总结</strong>：梯度消失和梯度爆炸是普遍问题，RNNs尤其不稳定，是因为重复使用相同权重矩阵。</p><h3 id="Bidirectional-and-Multi-layer-RNNs-motivation"><a href="#Bidirectional-and-Multi-layer-RNNs-motivation" class="headerlink" title="Bidirectional and Multi-layer RNNs: motivation"></a>Bidirectional and Multi-layer RNNs: motivation</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810643.png" alt="image-20210213154604534" style="zoom:20%;" /></p><p>在情感分类任务中，</p><ul><li>我们可以把隐藏状态看作是这个句子中单词terribly的上下文表示。这个称之为上下文表示</li><li>这些上下文表示只包含左边上下文信息</li><li>那怎么表示右边上下文信息呢？</li><li>这个例子中，exciting是在右边的上下文，它修饰terribly的意思(从否定变为肯定)</li></ul><h4 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810644.png" alt="image-20210213161012591" style="zoom:20%;" /></p><p><strong>双向RNNs</strong></p><p>上图中，结合后的隐层状态，绿色就可以表示右边的上下文。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810645.png" alt="image-20210213161321755" style="zoom:20%;" /></p><p>双向RNNs，计算符号。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810646.png" alt="image-20210213161455919" style="zoom:20%;" /></p><p>简化的双向RNN，双向箭头表示双向，和描述的隐层状态被假定为结合的前向和反向状态。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810647.png" alt="image-20210213161858762" style="zoom:20%;" /></p><ul><li>注意：双向RNNs只适用于，当你输入整个序列<ul><li>它不适用于语言建模，因为LM你只有左边上下文。</li></ul></li><li>如果你有整个上下文(比如，任何编码)，双向是强大的(你要默认使用它)</li><li>例如，BERT(Bidirectional Encoder Representations from Transformers )是基于双向的强大预训练上下文表示系统。<ul><li>你将在几周后学习更多包含BERT的transformers</li></ul></li></ul><h4 id="Multi-layer-RNNs"><a href="#Multi-layer-RNNs" class="headerlink" title="Multi-layer RNNs"></a>Multi-layer RNNs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810648.png" alt="image-20210213162449032" style="zoom:20%;" /></p><ul><li>RNNs在一个维度上已经很深了(展开到许多时刻)</li><li>我们通过应用多层RNNs来让其在另一个维度deep</li><li>这允许神经网络计算更复杂的表达<ul><li>低层RNNs应该计算低层特征，而高层RNNs应该计算高层</li></ul></li><li>多层RNNs也叫作堆叠RNNs</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810650.png" alt="image-20210213162516113" style="zoom:20%;" /></p><p>RNNs的第i层隐层状态就是第i+1层的输入。</p><h4 id="Multi-layer-RNNs-in-practice"><a href="#Multi-layer-RNNs-in-practice" class="headerlink" title="Multi-layer RNNs in practice"></a>Multi-layer RNNs in practice</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810651.png" alt="image-20210213171053469" style="zoom:20%;" /></p><ul><li>高表现的RNNs经常是多层的(但没有卷积或前馈网络那么深)</li><li>例如，在2017年一篇论文，Britz et al发现神经机器翻译，2 to 4 层RNN编码器是最好的， 4层解码器是最好的。<ul><li>通常， 跳跃连接/稠密连接是在训练深度RNNs需要的</li></ul></li><li>基于Transformer的神经网络(如，BERT)通常是深层的，像12层或24层<ul><li>其有很多skipping-like连接</li></ul></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261810652.png" alt="image-20210213171117921" style="zoom:20%;" /></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">cs224n-2019-notes05-LM_RNN</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture06-fancy-rnn.pdf">cs224n-2021-lecture06-fancy-rnn</a></p><p>[3] <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">Recurrent Neural Networks Tutorial, Part 3 </a></p><p>[4] <a href="https://ilewseu.github.io/2017/12/30/RNN%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC/">循环神经网络RNN 梯度推导(BPTT)</a></p><p>[5] <a href="https://bitjoy.net/2019/07/31/cs224n%ef%bc%881-24%ef%bc%89language-models-and-rnns/">CS224N（1.24）Language Models and RNNs</a></p><p>[6] <a href="https://looperxx.github.io/CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/">Science is interesting.07 Vanishing Gradients and Fancy RNNs</a></p><p>[7] <a href="https://blog.csdn.net/RHJlife/article/details/107279537">2019年CS224N课程笔记-Lecture 7: Vanishing Gradients and Fancy RNNs</a></p><p>[8] <a href="http://klausvon.cn/2019/10/11/cs224n-2019-Lecture-7-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%9C%89%E8%B6%A3%E7%9A%84RNN/">cs224n(2019)-Lecture 7 梯度消失和有趣的RNN</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> CS224n </tag>
            
            <tag> BPTT </tag>
            
            <tag> 梯度消失和爆炸 </tag>
            
            <tag> RNNs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 5 Recurrent Neural Networks and Language Models</title>
      <link href="2020/12/10/CS224N%20Lecture%205%20Recurrent%20Neural%20Networks%20and%20Language%20Models/"/>
      <url>2020/12/10/CS224N%20Lecture%205%20Recurrent%20Neural%20Networks%20and%20Language%20Models/</url>
      
        <content type="html"><![CDATA[<h3 id="Languages-modeling-and-RNNs"><a href="#Languages-modeling-and-RNNs" class="headerlink" title="Languages modeling and RNNs"></a>Languages modeling and RNNs</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804747.png" alt="image-20210208105658497"  style="zoom:20%;" /></p><ul><li><p>语言模型是一个预测下一个词是什么的任务，</p><ul><li><code>the students opened their ________</code>这个就是预测横线上填什么单词，(完型填空……)</li></ul></li><li><p>更正式地：给定一个单词序列$\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \cdots, \boldsymbol{x}^{(t)}$</p><script type="math/tex; mode=display">P(\boldsymbol{x}^{(t+1)} \vert \boldsymbol{x}^{(t)}, \cdots, \boldsymbol{(x)}^{(1)}  )</script></li></ul><p>其中，$\boldsymbol{x}^{(t+1)} $ 是词汇表中的任何一个词。</p><ul><li>这样做的系统就是<strong>语言模型</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804749.png" alt="image-20210208134710090" style="zoom:20%;" /></p><ul><li>你也可以认为语言模型是一个将概率分配给一段文本系统</li><li>例如，如果我们有一些文本，那么该文本概率为，如上图。</li></ul><center class="half">   <img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208135233.png" style="zoom:15%;" /><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208135325.png" style="zoom:15%;" /></center><p>上面应用中就是输入就会预测提示下一个词。</p><h4 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804750.png" alt="image-20210208135459185" style="zoom:20%;" /></p><p><strong>n-gram 语言模型</strong></p><ul><li><p>定义：n-gram是一段连续词</p><ul><li>unigrams: “the”, “students”, “opened”, ”their”  只来依赖生成位置的前单个词</li><li>bigrams: “the students”, “students opened”, “opened their”  两个词</li><li>trigrams: “the students opened”, “students opened their”  3个词</li><li>4-grams: “the students opened their”  4个词</li></ul></li><li><p>想法：收集关于不同n-gram出现频率的统计数据，并用这些数据预测下一个单词。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804751.png" alt="image-20210208140335978" style="zoom:20%;" /></p><ul><li>首先，我们做一个马尔科夫假设： 后一个词只取决于前n-1个词<ul><li>问题： 我们怎么得到这些n-gram 和 n-1-gram的概率？</li><li>答案：通过其在大量文本语料库的数目。</li></ul></li></ul><h4 id="n-gram-Language-Models-Example"><a href="#n-gram-Language-Models-Example" class="headerlink" title="n-gram Language Models: Example"></a>n-gram Language Models: Example</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804752.png" alt="image-20210208140856200" style="zoom:20%;" /></p><p>假设我们在学习一个4-gram LM。只要得到：</p><ol><li><code>students opened their w</code>数目</li><li><code>students opened their</code> 数目</li></ol><p>例如，假设在语料库中：</p><ul><li><code>students opened their</code>出现1000次</li><li><code>students opened their books</code>出现400次 ，推出其概率为0.4</li><li><code>students opened their exams</code>出现100次 ，推出其概率为0.1</li></ul><p>那如果文本中出现监考呢？如果出现监考的话， 应该是exam概率比较大的。</p><h4 id="Sparsity-Problems-with-n-gram-Language-Models"><a href="#Sparsity-Problems-with-n-gram-Language-Models" class="headerlink" title="Sparsity Problems with n-gram Language Models"></a>Sparsity Problems with n-gram Language Models</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804753.png" alt="image-20210208141803236" style="zoom:20%;" /></p><p>n-gram LM的稀疏问题：</p><ul><li><strong>问题</strong> ：如果<code>students open their w</code>从未出现在数据中，那么概率值为 0</li><li><strong>(Partial)解决方案</strong> ：为每个 $w \in V$ 添加极小数 $\delta$。这叫做平滑。这使得词表中的每个单词都至少有很小的概率。</li><li><strong>问题</strong> ：如果<code>students open their</code>从未出现在数据中，那么我们将无法计算任何单词 w 的概率值</li><li><strong>(Partial)解决方案</strong> ：将条件改为<code>open their</code>。这叫做<strong>backoff</strong>。</li></ul><p><strong>注意：</strong></p><p>增大n会放稀疏问题更严重，通常，我们不能让n大于5.</p><h4 id="Storage-Problems-with-n-gram-Language-Models"><a href="#Storage-Problems-with-n-gram-Language-Models" class="headerlink" title="Storage Problems with n-gram Language Models"></a>Storage Problems with n-gram Language Models</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804754.png" alt="image-20210208142826550" style="zoom:18%;" /></p><p>增大n就是增大模型大小</p><h4 id="n-gram-Language-Models-in-practice"><a href="#n-gram-Language-Models-in-practice" class="headerlink" title="n-gram Language Models in practice"></a>n-gram Language Models in practice</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804755.png" alt="image-20210208143156261" style="zoom:18%;" /></p><p>可以按照下面网站来试试3-gram LM，<a href="https://nlpforhackers.io/language-models/">language-models</a></p><h4 id="Generating-text-with-a-n-gram-Language-Model"><a href="#Generating-text-with-a-n-gram-Language-Model" class="headerlink" title="Generating text with a n-gram Language Model"></a>Generating text with a n-gram Language Model</h4><center class="half"> <figure>    <img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208143708.png" height="220"><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208143732.png" height="220">  </figure> </center><p>上图中， 依次预测 <code>today the</code> 和 <code>today the price</code>后面的词</p><center class="half">   <img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208143813.png" height="220"><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208143809.png" height="220"> </center><p>上图中， 依次预测 <code>today the price of</code>后面的词, 生成文本</p><h4 id="How-to-build-a-neural-Language-Model"><a href="#How-to-build-a-neural-Language-Model" class="headerlink" title="How to build a neural Language Model?"></a>How to build a neural Language Model?</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804756.png" alt="image-20210208150127839" style="zoom:18%;"/></p><ul><li>回忆一下语言模型任务：<ul><li>输入：单词序列</li><li>输出：在前面词出现的情况下，下一个词出现的概率</li></ul></li><li>第三讲中的window-based neural model被应用于命名实体识别问题</li></ul><h4 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804757.png" alt="image-20210208153928654" style="zoom:20%;"/></p><p>跟命名实体识别问题中一样的网络结构</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804758.png" alt="image-20210208154057704" style="zoom:20%;" /></p><p>改进 n-gram LM:</p><ul><li>没有稀疏问题</li><li>不需要存储所有观察到的n-grams</li></ul><p>存在问题</p><ul><li>固定窗口太小</li><li>扩大窗口就需要扩大权重矩阵</li><li>窗口再大也不够用</li><li>x1和x2乘以不同权重w。处理输入<strong>不对称</strong>。</li></ul><p>需要一个能处理任何长度输入的神经网络结构</p><blockquote><p>固定窗口的神经语言模型相比于n-gram，优点是：</p><ol><li>不存在稀疏性问题。因为它不要求语料库中出现n-gram的词组，它仅仅是把每个独立的单词的词向量组合起来。只要有词向量，就有输入，至少整个模型能顺利跑通。</li><li>节省存储空间，不需要存储n-gram的组合，只需要存储每个独立的词的词向量。没有组合爆炸的问题。</li></ol><p>依然存在的问题是：</p><ol><li>固定窗口大小还是太小了，受限于窗口大小，不能感知远距离的关系。</li><li>增大窗口大小，权重矩阵W也要相应的增大，导致网络变得复杂。事实上，窗口大小不可能足够大。</li><li>输入$e^{(1)},…,e^{(4)}$对应W中的不同列，即每个e对应的权重完全是独立的，没有某种共享关系，导致训练效率比较低。</li></ol></blockquote><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804759.png" alt="image-20210208155229700" style="zoom:20%;" /></p><p>核心想法：重复使用一样的权重W。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804760.png" alt="image-20210208155300934" style="zoom:20%;" /></p><blockquote><p>RNN结构如上图所示。RNN没有所谓窗口的概念，它的输入可以是任意长度，特别适合对顺序敏感的序列问题进行建模。还是以本文的语言模型<code>the students opened their</code>为例，介绍一下RNN的内部结构。对于时刻$t$（位置）的输入词$x(t)$来说，首先把它转换为词向量$e^{(t)}$，作RNN真正的输入；然后对于隐藏层，它的输入来自两部分，一部分是$t$时刻的输入的变换$W_e e^{t}$，另一部分是上一刻的隐状态的变换$W_h h^{(t-1)}$ ，这两部分组合起来再做一个非线性变换，得到当前层的隐状态 $h^{(t)}$；最后，隐状态再接一个<code>softmax</code>层，得到该时刻的输出概率分布。</p><p>需要注意的是，RNN：</p><ul><li>每一个时刻t都可以有输出，上图仅展示了最后时刻$t=4$时的输出</li><li>$h^{(0)}$是初始隐状态，可以是根据之前的学习经验设置的，也可以是随机值，也可以是全0</li><li>整个网络中，所有时刻的$W_e, W_h, U, b_1, b_2$都是同一个参数，即不同时刻的权重是共享的，不同的是不同时刻的输入$x^{(t)}$和隐状态$ h^{(t)}$</li><li>这里的词向量$e$可以是pre-trained得到的，然后固定不动了；也可以根据实际任务进行fine-tune; 在实际任务中现场学习，最好还是用pre-trained的词向量，如果数据量很大的话，再考虑fine-tune 。</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804761.png" alt="image-20210208161834408" style="zoom:20%;" /></p><p>RNN的优点：</p><ul><li>能处理任意长度输入</li><li>计算时序t时，能使用许多之前时序的信息(理论上)</li><li>模型大小不随输入文本长度增加</li><li>同样权重应用于每个时序，无论处理多少输入这里是对称的</li></ul><p>RNN的缺点：</p><ul><li>循环计算是非常慢的</li><li>实际上，很难从前面获取信息</li></ul><blockquote><p>RNN相比于固定窗口的神经网络，其优势是：</p><ol><li>不受输入长度限制，可以处理任意长度的序列</li><li>状态t可以感知很久以前的状态</li><li>模型大小是固定的，因为不同时刻的参数$W_e, W_h, U, b_1, b_2$都是共享的，不受输入长度的影响</li><li>所有参数$W_e, W_h, U, b_1, b_2$是共享的，训练起来更高效</li></ol><p>存在的不足：</p><ol><li>训练起来很慢，因为后续状态需要用到前面的状态，是串行的，难以并行计算</li><li>虽然理论上t时刻可以感知很久以前的状态，但实际上很难，因为梯度消失的问题</li></ol></blockquote><h4 id="Training-an-RNN-Language-Model"><a href="#Training-an-RNN-Language-Model" class="headerlink" title="Training an RNN Language Model"></a>Training an RNN Language Model</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804762.png" alt="image-20210208162300905" style="zoom:18%;" /></p><ul><li>获取一个大的文本语料，单词$x^{(1)}, \cdots, x^{(T)}$序列</li><li><p>输入进RNN-LM；计算每个step t 上的输出概率分布$\hat y^{(t)}$</p><ul><li>如给定词，预测到当前的每个词的分布</li></ul></li><li>损失函数在step t 是，关于 预测概率$\hat y^{(t)}$和下一个词$\hat y ^{(t)}$之间的交叉熵</li><li>平均这个交叉熵，得到整个训练集的损失。</li></ul><blockquote><p>训练RNN依然是梯度下降。首先我们需要定义损失函数，RNN在$t$时刻的输出是预测第$t+1$个词的概率分布$y^(t)$；而对于训练集中给定的文本来说，第$t+1$个词是已知的某个词，所以真实答案$y(t)$其实是一个one-hot向量，在第$x(t+1)$位置为1，其他位置为0。所以如果是交叉熵损失函数的话，表达式如上图中间的等式。RNN整体的损失就是所有时刻的损失均值。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804764.png"  style="zoom:22%;" ></p><p>如上图中依次获得$J^{(1)}(\theta), J^{(2)}(\theta), J^{(3)}(\theta), J^{(4)}(\theta) $</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804765.png" alt="image-20210208164036434" style="zoom:20%;" /></p><blockquote><ul><li><p>然而：计算 <strong>整个语料库</strong> 的损失和梯度太昂贵了</p></li><li><p>在实践中，我们通常将x 序列 看做一个 <strong>句子</strong> 或是 <strong>文档</strong></p></li><li>回忆 ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li><li>计算一个句子的损失(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li></ul></blockquote><h4 id="Backpropagation-for-RNNs"><a href="#Backpropagation-for-RNNs" class="headerlink" title="Backpropagation for RNNs"></a>Backpropagation for RNNs</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804766.png" alt="image-20210210202011894" style="zoom:20%;" /></p><p>RNN结构，左边是未展开形式，右边是根据时间展开得网络图，注意所有时刻$U, W, V$都是一样的，为了便于里解，输出的由$o$改为$y$.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804767.png" alt="image-20210208164922233" style="zoom:20%;" /></p><p>Q: 我们怎么计算损失函数对权重w的梯度?</p><p>A: 反向传播遍历时序$t=t, …, 0$，就是之前每个时刻的梯度之和。</p><h4 id="Generating-text-with-a-RNN-Language-Model"><a href="#Generating-text-with-a-RNN-Language-Model" class="headerlink" title="Generating text with a RNN Language Model"></a>Generating text with a RNN Language Model</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804768.png" alt="image-20210208170222882" style="zoom:20%;" /></p><p>我们也可像n-gram 模型一样通过重复采样生成文本。采样输出变成下一步的输入。</p><center class="half">   <img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208170257.png" height="200"><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208170329.png" height="200"> </center><p>一些RNN-LM有趣的试验，比如生成奥巴马演讲  <a href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0">Obama-RNN</a>和 哈利波特小说。</p><center> </center><h4 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804769.png" alt="image-20210208171029512" style="zoom:20%;" /></p><ul><li>标准的LM估计指标是perplexity</li><li>等于交叉熵损失的指数，低perplexity更好！</li></ul><p><strong>RNNs have greatly improved perplexity</strong>  </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804770.png" alt="image-20210208171514290" style="zoom:18%;" /></p><h4 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804771.png" alt="image-20210208171622627" style="zoom:18%;" /></p><blockquote><ul><li>语言模型是一项基准测试任务，它帮助我们衡量我们在理解语言方面的进展</li><li>语言建模是许多NLP任务的子组件，尤其是那些涉及生成文本或估计文本概率的任务<ul><li>预测性打字</li><li>语音识别</li><li>手写识别</li><li>拼写/语法纠正</li><li>作者识别</li><li>机器翻译</li><li>摘要</li><li>对话</li><li>等等</li></ul></li></ul></blockquote><h4 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804772.png" alt="image-20210208171917577" style="zoom:18%;" /></p><blockquote><ul><li>语言模型： <strong>预测下一个单词</strong> 的系统</li><li>递归神经网络：一系列神经网络<ul><li>采用任意长度的顺序输入</li><li>在每一步上应用相同的权重</li><li>可以选择在每一步上生成输出</li></ul></li><li>递归神经网络≠≠语言模型</li><li>我们已经证明，RNNs是构建LM的一个很好的方法。</li><li>但RNNs的用处要大得多!</li></ul></blockquote><p><strong>RNN的应用</strong></p><ol><li>RNN 用来标注</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804773.png" alt="image-20210208172144061" style="zoom:18%;" /></p><ol><li>RNN用来做句子分类(情感分类等)<center class="half"><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208172246.png" height="250"><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208172432.png" height="250"> </center><center> <img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210208172331.png" style="zoom:20%;"> </center></li></ol><p>基本：用最终的隐层层来编码，使用所有隐层的最大值和均值来做逐元素</p><ol><li>RNN用来做编码器</li></ol><p>编码器和解码器是注意力机制中的一个内容，后续肯定会学习的～，编码器和解码器其实都是RNN实现的（也可能是RNN的变种）</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804774.png" alt="image-20210208173251041" style="zoom:20%;" /></p><ol><li>RNN做文本生成</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261804775.png" alt="image-20210208173342714" style="zoom:20%;" /></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture05-rnnlm.pdf">cs224n-2021-lecture05-rnnlm</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">cs224n-2019-notes05-LM_RNN</a></p><p>[3] <a href="https://bitjoy.net/2019/07/31/cs224n%ef%bc%881-24%ef%bc%89language-models-and-rnns/">RNN 梯度推导</a></p><p>[4] <a href="https://looperxx.github.io/CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">CS224n 笔记</a></p><p>[5] <a href="https://blog.csdn.net/RHJlife/article/details/107169895">BPTT</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> CS224n </tag>
            
            <tag> n-grams </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n Lecture 4 Dependency Parsing</title>
      <link href="2020/12/08/CS224N%20Lecture%204%20Dependency%20Parsing/"/>
      <url>2020/12/08/CS224N%20Lecture%204%20Dependency%20Parsing/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Two-views-of-linguistic-structure"><a href="#1-Two-views-of-linguistic-structure" class="headerlink" title="1. Two views of linguistic structure"></a>1. Two views of linguistic structure</h3><p>两个语言结构观点： Constituency = 句子结构 = 无上下文语法 context-free grammars  (CFG)</p><h4 id="context-free-grammars"><a href="#context-free-grammars" class="headerlink" title="context-free grammars"></a>context-free grammars</h4><p>句子结构组织单词成嵌套成分</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802711.png" alt="image-20210205140122821" width="300" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802713.png" alt="image-20210205140735369" width="300" /></p><p>句子及各个组织单词成嵌套成分</p><ul><li>能用CFG规则来表示语法</li><li>起步单元：单词被赋予一个类别 (part of speech = pos)</li><li>单词可以组成不同类别的短语</li><li>短语可以递归第组合成更大的短语</li></ul><p>其中，</p><blockquote><ul><li><code>Det</code> 指的是 <strong>Determiner</strong>，在语言学中的含义为 <strong>限定词</strong></li><li><code>NP</code> 指的是 <strong>Noun Phrase</strong> ，在语言学中的含义为 <strong>名词短语</strong></li><li><code>VP</code> 指的是 <strong>Verb Phrase</strong> ，在语言学中的含义为 <strong>动词短语</strong></li><li><code>P</code> 指的是 <strong>Preposition</strong> ，在语言学中的含义为 <strong>介词</strong><ul><li><code>PP</code> 指的是 <strong>Prepositional Phrase</strong> ，在语言学中的含义为 <strong>介词短语</strong></li></ul></li><li>NP→Det N</li><li>NP→Det (Adj) N</li><li>NP→Det (Adj) N PP<ul><li>PP→P NP</li></ul></li><li>VP→V P<ul><li>中文中，介词短语会出现在动词之前</li></ul></li></ul></blockquote><h4 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h4><p>依存分析是一种在计算语言学中占主导地位的观点。</p><ul><li><p>依存结构表示哪些词依赖(修饰或其参数)哪些其他词。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802715.png" alt="1560692691439" width="350" /></p></li><li><p>look是整个句子的root， 而look又依赖于crate。</p></li><li>in， the， large都是crate依赖</li><li>in the kitchen都是crate的修饰</li><li>in， the都是kitchen的依赖，同理分析by the door</li></ul><h4 id="为什么我们要句子结构"><a href="#为什么我们要句子结构" class="headerlink" title="为什么我们要句子结构?"></a>为什么我们要句子结构?</h4><ul><li>我们需要理解句子结构，为了能正确解释语言。</li><li>人类交流复杂想法通过将单词组合在一起合成更大的单元来传达复杂意思。</li><li>我们需要知道什么联系什么</li></ul><h5 id="Prepositional-phrase-attachment-ambiguity"><a href="#Prepositional-phrase-attachment-ambiguity" class="headerlink" title="Prepositional phrase attachment ambiguity"></a>Prepositional phrase attachment ambiguity</h5><p>介词短语依存歧义</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802716.png" alt="image-20210205143108203" width="350" /></p><blockquote><ul><li>警察用刀杀了那个男子<ul><li><code>cops</code> 是 <code>kill</code> 的 <code>subject</code> (subject 指 主语)</li><li><code>man</code> 是 <code>kill</code>的 <code>object</code> (object 指 宾语)</li><li><code>knife</code> 是 <code>kill</code> 的 <code>modifier</code> (modifier 指 修饰符)</li></ul></li><li>警察杀了那个有刀的男子<ul><li><code>knife</code> 是 <code>man</code> 的 <code>modifier</code> (名词修饰符，简称为 <code>nmod</code> )</li></ul></li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802717.png" alt="1560696178556" width="350" /></p><blockquote><p><code>from space</code> 这一介词短语修饰的是前面的动词 <code>count</code> 还是名词 <code>whales</code> ？</p></blockquote><h5 id="PP-attachment-ambiguities-multiply"><a href="#PP-attachment-ambiguities-multiply" class="headerlink" title="PP attachment ambiguities multiply"></a>PP attachment ambiguities multiply</h5><ul><li>关键分析决策是我们怎样”依存“不同成分<ul><li>名词短语，形容词或分词、不定式和协调等。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802718.png" alt="1560698493997" width="350" /></p><blockquote><ul><li>上述句子中有四个介词短语</li><li><code>board</code> 是 <code>approved</code> 的 主语，<code>acquisition</code> 是 <code>approved</code> 的谓语</li><li><code>by Royal Trustco Ltd.</code> 是修饰 <code>acquisition</code> 的，即董事会批准了这家公司的收购</li><li><code>of Toronto</code> 可以修饰 <code>approved, acquisition, Royal Trustco Ltd.</code> 之一，经过分析可以得知是修饰 <code>Royal Trustco Ltd.</code> 即表示这家公司的位置</li><li><code>for $27 a share</code> 修饰 <code>acquisition</code></li><li><code>at its monthly meeting</code> 修饰 <code>approved</code> ，即表示批准的时间地点</li></ul><p>面对这样复杂的句子结构，我们需要考虑 <strong>指数级</strong> 的可能结构，这个序列被称为 <strong>Catalan numbers</strong></p><p><strong>Catalan numbers</strong> : $Cn=(2n)!/[(n+1)!n!]$</p><ul><li>一个指数增长的序列，出现在许多类似树的环境中<ul><li>例如，一个 n+2 边的多边形可能的三角剖分的数量<ul><li>出现在概率图形模型的三角剖分中(CS228)</li></ul></li></ul></li></ul></blockquote><h5 id="Coordination-scope-ambiguity"><a href="#Coordination-scope-ambiguity" class="headerlink" title="Coordination scope ambiguity"></a>Coordination scope ambiguity</h5><p>协调范围歧义</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802719.png" alt="1560699777096" width="350" /></p><ul><li>一个人：[[Shuttle veteran and longtime NASA executive] Fred Gregory] appointed to board（航天飞机老兵、长期担任美国宇航局主管弗雷德-格雷戈里被任命为董事会成员）</li><li>两个人：[Shuttle veteran] and [longtime NASA executive Fred Gregory] appointed to board（航天飞机老兵和长期担任美国宇航局主管弗雷德-格雷戈里被任命为董事会成员）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802720.png" alt="1560699972471" width="350" /></p><p>Doctor: No heart, cognitive issues.</p><h5 id="Adjectival-Modifier-Ambiguity"><a href="#Adjectival-Modifier-Ambiguity" class="headerlink" title="Adjectival Modifier Ambiguity"></a>Adjectival Modifier Ambiguity</h5><p>形容词修饰歧义</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802721.png" alt="1560700236717" width="350" /></p><blockquote><p>例句：Students get first hand job experience</p><p>  first hand</p><p>   表示 第一手的，直接的，即学生获得了直接的工作经验</p><p>first<code>是</code>hand的形容词修饰语</p><ul><li>first 修饰 experience, hand 修饰 job !</li></ul></blockquote><h5 id="Verb-Phrase-VP-attachment-ambiguity"><a href="#Verb-Phrase-VP-attachment-ambiguity" class="headerlink" title="Verb Phrase (VP) attachment ambiguity"></a>Verb Phrase (VP) attachment ambiguity</h5><p>动词依存歧义</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802722.png" alt="image-20210205234615845" width="350" /></p><blockquote><p>例句：Mutilated body washes up on Rio beach to be used for Olympic beach volleyball.</p><ul><li><code>to be used for Olympic beach volleyball</code> 是 动词短语 (VP)</li><li>修饰的是 <code>body</code> 还是 <code>beach</code></li></ul></blockquote><h5 id="Dependency-paths-identify-semantic-relations-–-e-g-for-protein-interaction"><a href="#Dependency-paths-identify-semantic-relations-–-e-g-for-protein-interaction" class="headerlink" title="Dependency paths identify semantic relations – e.g., for protein interaction"></a>Dependency paths identify semantic relations – e.g., for protein interaction</h5><p>依赖路径识别语义关系</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802723.png" alt="image-20210205152626132" width="350" /></p><h3 id="2-Dependency-Grammar-and-Dependency-Structure"><a href="#2-Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="2. Dependency Grammar and Dependency Structure"></a>2. Dependency Grammar and Dependency Structure</h3><p>依存语法假定句法结构由词汇项之间的关系构成，通常是二元不对称关系(“箭头”)， 称为依赖关系。</p><p>Dependency Structure有两种表现形式：</p><ul><li>一种是直接在句子上标出依存关系箭头及语法关系</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802724.png" alt="image-20210205153758811" width="700" /></p><ul><li>另一种是将其做成树状机构（Dependency Tree Graph）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802725.png" alt="image-20210205153905754" width="500" /></p><blockquote><ul><li>箭头通常标记(<strong>type</strong>)为语法关系的名称(主题、介词对象、apposition等)。</li><li>依赖关系标签的系统，例如 <strong>universal dependency</strong> 通用依赖</li><li>箭头连接头部( head )和一个依赖(修饰词,下级,下属)<ul><li>A →依赖于 A 的事情</li></ul></li><li>通常,依赖关系形成一棵树(单头 无环 连接图)</li></ul></blockquote><h4 id="Dependency-Grammar-Parsing-History"><a href="#Dependency-Grammar-Parsing-History" class="headerlink" title="Dependency Grammar/Parsing History"></a>Dependency Grammar/Parsing History</h4><blockquote><ul><li>依赖结构的概念可以追溯到很久以前<ul><li>Pāṇini的语法(公元前5世纪)</li><li>一千年 阿拉伯语的语法的基本方法</li></ul></li><li>选区/上下文无关文法是一个新奇的发明<ul><li>20世纪发明(R.S.Wells,1947; then Chomsky)</li></ul></li><li>现代依赖工作经常源于 L. Tesnière(1959)<ul><li>是20世纪“东方”的主导方法(俄罗斯，中国，…)<ul><li>有利于更自由的语序语言</li></ul></li></ul></li><li>NLP中最早类型的解析器在美国<ul><li>David Hays 是美国计算语言学的创始人之一，他很早就(第一个)构建了依赖解析器(Hays 1962)。</li></ul></li></ul></blockquote><h4 id="Dependency-Grammar-and-Dependency-Structure-表示方法"><a href="#Dependency-Grammar-and-Dependency-Structure-表示方法" class="headerlink" title="Dependency Grammar and Dependency Structure  表示方法"></a>Dependency Grammar and Dependency Structure  表示方法</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802726.png" alt="image-20210205154337913" width="600" /></p><blockquote><ul><li>人们对箭头指向的方式不一致：有些人把箭头朝一个方向画；有人是反过来的<ul><li>Tesnière 从头开始指向依赖，本课使用此种方式</li></ul></li><li>通常添加一个伪造“ROOT”指向整个句子的头部，这样每个单词都精确地依赖于另一个节点</li></ul></blockquote><p><strong>注：从头指向依赖，加上个ROOT</strong></p><h4 id="The-rise-of-annotated-data-Universal-Dependencies-treebanks"><a href="#The-rise-of-annotated-data-Universal-Dependencies-treebanks" class="headerlink" title="The rise of annotated data: Universal Dependencies treebanks"></a>The rise of annotated data: Universal Dependencies treebanks</h4><p>标注数据的崛起：全局依存树库</p><p><a href="https://universaldependencies.org/">Universal Dependencies  </a></p><blockquote><p><strong>Universal Dependencies</strong>：我们想要拥有一个统一的、并行的依赖描述，可用于任何人类语言</p><ul><li>从前手工编写语法然后训练得到可以解析句子的解析器</li><li>用一条规则捕捉很多东西真的很有效率，但是事实证明这在实践中不是一个好主意<ul><li>语法规则符号越来越复杂，并且没有共享和重用人类所做的工作</li></ul></li><li>句子结构上的treebanks 支持结构更有效</li></ul></blockquote><h5 id="The-rise-of-annotated-data"><a href="#The-rise-of-annotated-data" class="headerlink" title="The rise of annotated data"></a>The rise of annotated data</h5><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802727.png" alt="image-20210205155453514" width="600" /></p><blockquote><p>从一开始，构建 treebank 似乎比构建语法慢得多，也没有那么有用</p><p>但是 treebank 给我们提供了许多东西</p><ul><li>劳动力的可重用性<ul><li>许多解析器、词性标记器等可以构建在它之上</li><li>语言学的宝贵资源</li></ul></li><li>广泛的覆盖面，而不仅仅是一些直觉</li><li>频率和分布信息</li><li>一种评估系统的方法</li></ul></blockquote><h5 id="Dependency-Conditioning-Preferences"><a href="#Dependency-Conditioning-Preferences" class="headerlink" title="Dependency Conditioning Preferences"></a>Dependency Conditioning Preferences</h5><p>依存条件偏好</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802728.png" alt="image-20210205160831903" width="600" /></p><p>关于依存分析的信息来源是什么？</p><blockquote><ol><li>Bilexical affinities (两个单词间的密切关系)<ul><li>[discussion →→ issues] 是看上去有道理的</li></ul></li><li>Dependency distance 依赖距离<ul><li>主要是与相邻词</li></ul></li><li>Intervening material 介于中间的物质<ul><li>依赖很少跨越介于中间的动词或标点符号</li></ul></li><li>Valency of heads 头部的配价<ul><li>对于头部，多少依赖在哪边是普遍的？</li></ul></li></ol></blockquote><h5 id="Dependency-Parsing-1"><a href="#Dependency-Parsing-1" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h5><p>依存分析</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802729.png" alt="image-20210205161740991" width="600" /></p><blockquote><ul><li>通过为每个单词选择它所依赖的其他单词(包括根)来解析一个句子</li><li>通常有一些限制<ul><li>只有一个单词是依赖于根的</li><li>不存在循环 A→B, A→B,</li></ul></li><li>这使得依赖项成为树</li><li>最后一个问题是箭头是否可以交叉(非投影的 non-projective)<ul><li>没有交叉的就是non-projectice</li></ul></li></ul></blockquote><h4 id="Projectivity"><a href="#Projectivity" class="headerlink" title="Projectivity"></a>Projectivity</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802730.png" alt="image-20210205162056100" width="600" /></p><blockquote><ul><li>定义：当单词按线性顺序排列时，没有交叉的依赖弧，所有的弧都在单词的上方</li><li>与CFG树并行的依赖关系必须是投影的<ul><li>通过将每个类别的一个子类别作为头来形成依赖关系</li></ul></li><li>但是依赖理论通常允许非投射结构来解释移位的成分<ul><li>如果没有这些非投射依赖关系，就不可能很容易获得某些结构的语义</li></ul></li></ul></blockquote><h4 id="Methods-of-Dependency-Parsing"><a href="#Methods-of-Dependency-Parsing" class="headerlink" title="Methods of Dependency Parsing"></a>Methods of Dependency Parsing</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802731.png" alt="image-20210205162858011" width="600" /></p><blockquote><ol><li>Dynamic programming<ul><li>Eisner(1996)提出了一种复杂度为 $O(n3)$的精巧算法，它生成头部位于末尾而不是中间的解析项</li></ul></li><li>Graph algorithms<ul><li>为一个句子创建一个最小生成树</li><li>McDonald et al.’s (2005) MSTParser 使用ML分类器独立地对依赖项进行评分(他使用MIRA进行在线学习，但它也可以是其他东西)</li></ul></li><li>Constraint Satisfaction<ul><li>去掉不满足硬约束的边 Karlsson(1990), etc.</li></ul></li><li>“Transition-based parsing” or “deterministic dependency parsing“<ul><li>良好的机器学习分类器 MaltParser(Nivreet al. 2008) 指导下的依存贪婪选择。已证明非常有效。</li></ul></li></ol></blockquote><h3 id="3-Greedy-transition-based-parsing-Nivre-2003"><a href="#3-Greedy-transition-based-parsing-Nivre-2003" class="headerlink" title="3. Greedy transition-based parsing [Nivre 2003]"></a>3. Greedy transition-based parsing [Nivre 2003]</h3><p>基于贪婪转换解析</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802732.png" alt="image-20210205163514787" width="600" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802733.png" alt="image-20210205172505677" width="600" /></p><p>这部分要求要有数据结构和算法基础。</p><p>Transition-based Dependency Parsing 可以看做是state machine，对于一个序列$S = w_0w_1\cdots w_n$, $w_0$是fake [ROOT], state由3部分组成$(\sigma, \beta, A)$</p><p>其中，</p><ul><li>$\sigma$是S中若干个$w_i$组成的stack</li><li>$\beta$是S中若干个$w_i$组成的buffer，将其看作一个队列，队列模拟的是人眼的阅读顺序，人眼从左到右阅读，系统也从左往右读入单词，未读入的单词就构成一个队列，队列的出口在左边。初始状态下队列就是整个句子，且顺序不变。</li><li>A是依存的arc构成的集合，如$A = {(w_i, r_1, w_j), \cdots, (w_p, r_k, w_q)}$,其中r是两个词之间的依存关系</li></ul><p>初始化状态的时候，</p><ul><li>$\sigma$只包含ROOT $w_o$, $\beta$包含所有单词$w_1, \cdots, w_n$, A是空集 $\empty$ .</li></ul><p>最终状态，</p><ul><li>$\sigma$包含ROOT和所有词，$\beta$清空，A包含所有依存arc，A就是我们需要的依存关系。</li></ul><p>状态转移有三种方式：</p><ol><li>SHFIT: 从buffer移除第一个词，并将其送入栈的顶部(前提：buffer为空)</li><li>LEFT-ARC: 将 $(w_j,r,w_i)$ 加入边的集合 A ，其中 $w_i$是stack上的次顶层的词， $w_j$是stack上的最顶层的词；然后再将$w_i$从栈中移除。（前提条件：栈至少包含两个词，且$w_i$不能是ROOT）</li><li>RIGHT_ARC：将$(w_i,r,w_j) $ 加入边的集合 A ，其中 $w_i$是stack上的次顶层的词， $w_j$是stack上的最顶层的词； 然后再将$w_j$从栈中移除。(前提条件：栈至少包含两个词)</li></ol><p>数学表示，（这样就可以理解整个过程就是上图意思）</p><script type="math/tex; mode=display">\begin{aligned}&\text{SHIFT} \qquad \sigma, w_i \vert \beta, A \quad \to \quad \sigma \vert  w_i,\beta, A \quad \text{意思:将buffer中第一个词}w_i\text{移除到栈的顶部}\\ \\&\text{LEFT-ARC}_r \qquad \sigma \vert w_i \vert w_j, \beta, A  \quad \to \quad \sigma \vert  w_j,\beta, A \cup \{r(w_j, w_i)\} \quad \\&\text{    意思:将}r(w_j, w_i)\text{加入A, 再将栈中次顶层的词}w_i\text{移除}\\\\&\text{RIGHT-ARC}_r \qquad \sigma \vert w_i \vert w_j, \beta, A  \quad \to \quad \sigma \vert  w_i,\beta, A \cup \{r(w_i, w_j)\} \quad \\&\text{    意思:将}r(w_i, w_j)\text{加入A, 再将栈中次顶层的词}w_j\text{移除}\end{aligned}</script><p>还可以看看这里面的详细例子 <a href="https://blog.csdn.net/lairongxuan/article/details/104806128">举例说明</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802734.png" alt="image-20210205182325696" width="600" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802735.png" alt="image-20210205182352998" width="600" /></p><h4 id="MaltParser-Nivreand-Hall-2005"><a href="#MaltParser-Nivreand-Hall-2005" class="headerlink" title="MaltParser [Nivreand Hall 2005]"></a><strong>MaltParser</strong> [Nivreand Hall 2005]</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802736.png" alt="image-20210205182746397" width="600"  /></p><blockquote><ul><li>我们需要解释如何选择下一步行动<ul><li>Answer：标准回答，机器学习</li></ul></li><li>每个动作都由一个有判别分类器(例如softmax classifier)对每个合法的动作进行预测<ul><li>最多三种无类型的选择，当带有类型时，最多 |R|×2+1种</li><li>Features：栈顶单词，POS；buffer中的第一个单词，POS；等等</li></ul></li><li>（在最简单的形式中是）没有搜索的<ul><li>但是，如果你愿意，你可以有效地执行一个 Beam search 束搜索(虽然速度较慢，但效果更好)：你可以在每个时间步骤中保留 k个好的解析前缀</li></ul></li><li>模型精度是略低于最新的依存解析，但是</li><li>它提供了非常快的线性时间解析，性能非常好</li></ul></blockquote><h5 id="Conventional-Feature-Representation"><a href="#Conventional-Feature-Representation" class="headerlink" title="Conventional Feature Representation"></a>Conventional Feature Representation</h5><p>传统特征表示</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802738.png" alt="image-20210205183359611" width="600"  /></p><p>栈和队列中单词、词性、依存标签的组合的特征函数，一个超长的稀疏01向量。</p><h5 id="Evaluation-of-Dependency-Parsing-labeled-dependency-accuracy"><a href="#Evaluation-of-Dependency-Parsing-labeled-dependency-accuracy" class="headerlink" title="Evaluation of Dependency Parsing: (labeled) dependency accuracy"></a>Evaluation of Dependency Parsing: (labeled) dependency accuracy</h5><p>依存解析的估计：标签的依存准确性</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802739.png" alt="image-20210205201001703" width="600" /></p><p>其中，<strong>UAS</strong> (unlabeled attachment score) 指 <strong>无标签依存正确率</strong> ，<strong>LAS</strong> (labeled attachment score) 指 <strong>有标签依存正确率</strong></p><p>一个是LAS（labeled attachment score）即只有arc的箭头方向以及语法关系均正确时才算正确，以及UAS（unlabeled attachment score）即只要arc的箭头方向正确即可。</p><p>例子中，</p><ul><li>只看箭头方向，即Gold和Parsed中数字相同有4个，4/5，即80%</li><li>看箭头和解析后的标签只有2个，即2/5=40%</li></ul><h4 id="Handling-non-projectivity"><a href="#Handling-non-projectivity" class="headerlink" title="Handling non-projectivity"></a>Handling non-projectivity</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802740.png" alt="image-20210205202329854" width="600"  /></p><blockquote><ul><li>我们提出的弧标准算法只构建投影依赖树</li><li>可能指向头的方向<ul><li>在非投影弧上宣告失败</li><li>只具有投影表示时使用依赖形式<ul><li>CFG只允许投影结构；改进扰乱头</li></ul></li><li>使用投影依赖项解析算法的后处理器来识别和解析非投影链接</li><li>添加额外的转换，至少可以对大多数非投影结构建模（添加一个额外的交换转换，冒泡排序）</li><li>转移到不使用或不需要对投射性进行任何约束的解析机制(例如，基于图的MSTParser)</li></ul></li></ul></blockquote><p>​                </p><h3 id="4-Why-train-a-neural-dependency-parser-Indicator-Features-Revisited"><a href="#4-Why-train-a-neural-dependency-parser-Indicator-Features-Revisited" class="headerlink" title="4. Why train a neural dependency parser? Indicator Features Revisited"></a>4. Why train a neural dependency parser? Indicator Features Revisited</h3><p>为什么训练神经依存解析？重提指示符特征</p><p>传统特征表示稀疏、不完全、计算代价大（SVM之类的线性分类器本身是很快的，而传统parser的95%时间都花在拼装查询特征上了）。</p><h4 id="A-neural-dependency-parser-Chen-and-Manning-2014"><a href="#A-neural-dependency-parser-Chen-and-Manning-2014" class="headerlink" title="A neural dependency parser [Chen and Manning 2014]"></a><strong>A neural dependency parser</strong> [Chen and Manning 2014]</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802741.png" alt="image-20210205203537218" width="500"  /></p><ul><li>斯坦福依存英语解析<ul><li>未便签依存分数UAS = head</li><li>标签依存分数LAS = head and label</li></ul></li></ul><p>相较于传统的graph-based方法，花了这么多功夫得到的只是0.1%的LAS提升。</p><h4 id="Distributed-Representations"><a href="#Distributed-Representations" class="headerlink" title="Distributed Representations"></a>Distributed Representations</h4><p>分布表示</p><blockquote><ul><li>我们将每个单词表示为一个d维稠密向量（如词向量）<ul><li>相似的单词应该有相近的向量</li></ul></li><li>同时，part-of-speech tags词性标签(POS)和 dependency labels 依存标签也表示为d维向量<ul><li>较小的离散集也表现出许多语义上的相似性。</li></ul></li><li><strong>NNS</strong>(复数名词)应该接近<strong>NN</strong>(单数名词)</li><li><strong>num</strong>(数值修饰语)应该接近<strong>amod</strong>(形容词修饰语)。</li></ul><p>对于Neural Dependency Parser，其输入特征通常包含三种</p><ul><li>stack和buffer中的单词及其dependent word</li><li>单词的part-of-speech tag</li><li>描述语法关系的arc label</li></ul></blockquote><h4 id="Extracting-Tokens-and-then-vector-representations-from-configuration"><a href="#Extracting-Tokens-and-then-vector-representations-from-configuration" class="headerlink" title="Extracting Tokens and then vector representations from configuration"></a>Extracting Tokens and then vector representations from configuration</h4><p>提取令牌和从配置中获得向量表示</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802742.png" alt="image-20210205214122911" width="500"  /></p><p>将上图中的单词、词性标注和依存标签转换为词向量，并将它们联结起来。</p><h4 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802743.png" alt="image-20210205214239995" width="500"  /></p><p>具体来说，将嵌入矩阵 $E_w, E_t, E_l$中提取它们对应的稠密的特征的表示，然后将这些向量 <strong>拼接起来</strong> 作为输入 $[ x_w , x_t , x_l ] $。</p><p>网络结构由一个输入层、一个隐含层Relu、一个softmax输出层组成，使用交叉熵损失函数。</p><h4 id="Dependency-parsing-for-sentence-structure"><a href="#Dependency-parsing-for-sentence-structure" class="headerlink" title="Dependency parsing for sentence structure"></a>Dependency parsing for sentence structure</h4><p>神经网络能准确决定句子结构，增加解释性。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802744.png" alt="image-20210205215417103" width="500"  /></p><blockquote><p>Chen and Manning (2014) was the first simple, successful neural dependency parser  </p></blockquote><p>然后稠密表示，让其在准确性和速度上优于其它贪婪解析器。</p><h4 id="Further-developments-in-transition-based-neural-dependency-parsing"><a href="#Further-developments-in-transition-based-neural-dependency-parsing" class="headerlink" title="Further developments in transition-based neural dependency parsing"></a>Further developments in transition-based neural dependency parsing</h4><blockquote><p>这项工作由其他人进一步开发和改进，特别是在谷歌</p><ul><li>更大、更深的网络中，具有更好调优的超参数</li><li>Beam Search 更多的探索动作序列的可能性，而不是只考虑当前的最优</li><li><p>全局、条件随机场(CRF)的推理出决策序列</p><p><a href="https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html">这就引出了SyntaxNet和Parsey McParseFace模型</a></p></li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802745.png" alt="image-20210205223047925" width="500"  /></p><h4 id="Graph-based-dependency-parsers"><a href="#Graph-based-dependency-parsers" class="headerlink" title="Graph-based dependency parsers"></a>Graph-based dependency parsers</h4><blockquote><p>为每条边的每一个可能的依赖关系计算一个分数</p><ul><li>然后将每个单词的边缘添加到其得分最高的候选头部</li><li>并对其它每个单词重复相同的操作</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802746.png" alt="image-20210205223324167" width="500" /></p><h4 id="A-Neural-graph-based-dependency-parser"><a href="#A-Neural-graph-based-dependency-parser" class="headerlink" title="A Neural graph-based dependency parser"></a>A Neural graph-based dependency parser</h4><p>[ Dozat and Manning 2017; Dozat, Qi, and Manning 2017 ]</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261802747.png" alt="image-20210205223503026" width="500"  /></p><blockquote><ul><li>在神经模型中为基于图的依赖分析注入活力<ul><li>为神经依赖分析设计一个双仿射评分模型<ul><li>也使用神经序列模型，我们将在下周讨论</li></ul></li></ul></li><li>非常棒的结果<ul><li>但是比简单的基于神经传递的解析器要慢<ul><li>在一个长度为 n 的句子中可能有 $n^2$ 个依赖项</li></ul></li></ul></li></ul></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://looperxx.github.io/CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a></p><p>[2] <a href="https://blog.csdn.net/RHJlife/article/details/107132234">2019年CS224N课程笔记-Lecture 5: Linguistic Structure: Dependency Parsing</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/66268929">CS224N笔记(五):Dependency Parsing</a></p><p>[4] <a href="https://blog.csdn.net/lairongxuan/article/details/104806128">CS224n 2019 Winter 笔记（三）：句子依存分析（Dependency Parsing）</a></p><p>[5] <a href="https://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html/2#h2-6">基于神经网络的高性能依存句法分析器</a></p><p>[6] <a href="http://fancyerii.github.io/books/depparser/">依存句法分析 部分代码及分析</a></p><p>[7] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture04-dep-parsing.pdf">cs224n-2021-lecture04-dep-parsing</a></p><p>[8] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf">cs224n-2019-notes04-dependencyparsing</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> Dependency Parsing </tag>
            
            <tag> CFG </tag>
            
            <tag> Dependency Grammar </tag>
            
            <tag> Dependency Structure </tag>
            
            <tag> Greedy transition-based parsing </tag>
            
            <tag> neural dependency parser </tag>
            
            <tag> Graph-based dependency parser </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 3 Backprop and Neural Networks</title>
      <link href="2020/12/06/CS224N%20Lecture%203%20Backprop%20and%20Neural%20Networks/"/>
      <url>2020/12/06/CS224N%20Lecture%203%20Backprop%20and%20Neural%20Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="1-梯度"><a href="#1-梯度" class="headerlink" title="1. 梯度"></a>1. 梯度</h3><p>课程计划</p><p>Lecture 4： 手推梯度和算法</p><ol><li>介绍</li><li>矩阵计算</li><li>反向传播</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759488.png" alt="image-20210205231516555" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759490.png" alt="image-20210205231832287" style="zoom:20%;" /></p><ul><li>如果给定的函数有一个输出和n个输入</li><li>梯度是相对于每个输入的偏导向量</li></ul><h4 id="Jacobian-Matrix-Generalization-of-the-Gradient"><a href="#Jacobian-Matrix-Generalization-of-the-Gradient" class="headerlink" title="Jacobian Matrix: Generalization of the Gradient"></a>Jacobian Matrix: Generalization of the Gradient</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759491.png" alt="image-20210205233107266" style="zoom:20%;" /></p><ul><li>给定一个m个输出和n个输入的函数</li><li>其Jacobian是偏导的$m \times n $矩阵</li></ul><h4 id="Chain-rule"><a href="#Chain-rule" class="headerlink" title="Chain rule"></a>Chain rule</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759492.png" alt="image-20210206194520526" width="300;" /></p><h4 id="Example-Jacobian-Elementwise-activation-Function"><a href="#Example-Jacobian-Elementwise-activation-Function" class="headerlink" title="Example Jacobian: Elementwise activation Function"></a>Example Jacobian: Elementwise activation Function</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759493.png" alt="image-20210206194651388" width="300;" /></p><p>因为n个输入n个输出，其Jacobian矩阵应该是$n \times n$</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759494.png" alt="image-20210206194820897" width="300;" /></p><p>下面是一些公式，利用定义逐个元素求导很容易证明。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759495.png" alt="image-20210206200713639" style="zoom:20%;"/></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759496.png" alt="image-20210206200750535" style="zoom:20%;" /></p><h3 id="2-Back-to-our-Neural-Net"><a href="#2-Back-to-our-Neural-Net" class="headerlink" title="2. Back to our Neural Net!"></a>2. Back to our Neural Net!</h3><p>模型如下，一个隐层的简单架构。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759497.png" alt="image-20210206200922955" style="zoom:20%;" /></p><p>让我们求分数s对b的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759498.png" alt="image-20210206201032566" style="zoom:20%;"/></p><h4 id="Break-up-equations-into-simple-pieces"><a href="#Break-up-equations-into-simple-pieces" class="headerlink" title="Break up equations into simple pieces"></a>Break up equations into simple pieces</h4><p>注意，变量和保持其维度!</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759499.png" alt="image-20210206201342047" style="zoom:20%;" /></p><h4 id="Apply-the-chain-rule"><a href="#Apply-the-chain-rule" class="headerlink" title="Apply the chain rule"></a>Apply the chain rule</h4><p>左边是关系式，右边是链式法则。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759500.png" alt="image-20210206201519446" style="zoom:20%;"/></p><p>利用方框里的公式，依次对其求导。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759501.png" alt="image-20210206201726933" style="zoom:20%;"/></p><h4 id="Write-out-the-Jacobians"><a href="#Write-out-the-Jacobians" class="headerlink" title="Write out the Jacobians"></a>Write out the Jacobians</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759502.png" alt="image-20210206201912613" style="zoom:20%;"/></p><h4 id="Re-using-Computation"><a href="#Re-using-Computation" class="headerlink" title="Re-using Computation"></a>Re-using Computation</h4><p>如果我们要求s对权重w的梯度呢？</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759503.png" alt="image-20210206202128596" style="zoom:20%;" /></p><p>$\delta$是其局部误差信号，跟上面对b的梯度一样, 推出$\mathbf{u}^T \circ f^{\prime}(z)$</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759504.png" alt="image-20210206202259920" style="zoom:20%;"/></p><h4 id="关于矩阵的导数：输出形状"><a href="#关于矩阵的导数：输出形状" class="headerlink" title="关于矩阵的导数：输出形状"></a>关于矩阵的导数：输出形状</h4><p>Derivative with respect to Matrix: Output shape , 分数s对权重的梯度形状是什么样的？</p><ul><li>1输出， nm的输入： $1 \times nm$的Jacobian。不方便接下来梯度更新</li><li>让其变为$n \times m$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759505.png" alt="image-20210206202841356" style="zoom:20%;" /></p><h4 id="矩阵的导数"><a href="#矩阵的导数" class="headerlink" title="矩阵的导数"></a>矩阵的导数</h4><p>分数s的梯度就等于局部误差  <code>x</code> 输入</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759506.png" alt="image-20210206203250225" style="zoom:20%;"/></p><h4 id="Deriving-local-input-gradient-in-backprop"><a href="#Deriving-local-input-gradient-in-backprop" class="headerlink" title="Deriving local input gradient in backprop"></a>Deriving local input gradient in backprop</h4><p>在反向传播中，推导局部输入的梯度。</p><ul><li>不妨只考虑单一权重$W_{ij}$</li><li>$W_{ij}$只影响$z_i$</li><li>例如，$W_{23}$只用于祭祀$z_2$而不是$z_1$</li></ul><p>隐层z对权重的梯度，就是每个输入。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759507.png" alt="image-20210206203526059" style="zoom:20%;"/></p><h4 id="为什么要转置"><a href="#为什么要转置" class="headerlink" title="为什么要转置?"></a>为什么要转置?</h4><p>简单说是为了计算时维度能用，本质是因为在求外积。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759508.png" alt="image-20210206204421113" style="zoom:20%;"/></p><h4 id="导数的形状应该是什么样的？"><a href="#导数的形状应该是什么样的？" class="headerlink" title="导数的形状应该是什么样的？"></a>导数的形状应该是什么样的？</h4><ul><li>相似地，分数对偏置b的梯度是一个行向量<ul><li>为了形状上的方便，称我们的梯度应该是一个列向量，因为b是一个列向量。</li></ul></li><li>分歧在Jacobian形式和形状方便<ul><li>要求作业遵循形状便利，Jacobian方便计算</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759509.png" alt="image-20210206204625437" style="zoom:20%;"/></p><p>两个建议：</p><ol><li>尽可能用Jacobian，最后reshape一下遵循形状方便<ul><li>在最后，分数s对偏置的偏导是一个列向量，导致$\delta$要转置</li></ul></li><li>总是遵循形状方便<ul><li>检查维度，来弄清楚是否要转置或重组各个项</li><li>错误信号$\delta$,到隐藏层要跟隐藏层维度一样</li></ul></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759510.png" alt="image-20210206205207422" style="zoom:20%;" /></p><h3 id="3-反向传播"><a href="#3-反向传播" class="headerlink" title="3. 反向传播"></a>3. 反向传播</h3><p>我们几乎给你展示了反向传播。本质就是链式法则的使用。</p><p>技巧：</p><p>我们在计算低层导数时利用对高层的的求导来减少计算量。<br><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759511.png" alt="image-20210206210236414" style="zoom:20%;" /></p><h4 id="Computation-Graphs-and-Backpropagation"><a href="#Computation-Graphs-and-Backpropagation" class="headerlink" title="Computation Graphs and Backpropagation"></a>Computation Graphs and Backpropagation</h4><p>软件工程中用图来表示神经网络式子：</p><ul><li>源节点：输入</li><li>内部节点：操作</li><li>边传递操作的结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759512.png" alt="image-20210206210804087" style="zoom:20%;"/></p><h4 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h4><p>在边上反向传递梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759513.png" alt="image-20210206211217584" style="zoom:20%;"/></p><h4 id="单一节点的反向传播"><a href="#单一节点的反向传播" class="headerlink" title="单一节点的反向传播"></a>单一节点的反向传播</h4><ul><li>每个节点接收一个“向上流的梯度”</li><li>目标是传递正确的“向下流的梯度”</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759515.png" alt="image-20210206211518562"  style="zoom:20%;" /></p><p>每个节点有一个局部的梯度，这个梯度是其输出相对于其输入</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759516.png" alt="image-20210206212007268" style="zoom:20%;"  /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759517.png" alt="image-20210206212751321" style="zoom:20%;"  /></p><p>往下流的梯度 = 往上流的梯度 x 局部的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759518.png" alt="image-20210206212832132" style="zoom:20%;" /></p><p>多输入代表多个局部梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759519.png" alt="image-20210206212952166" style="zoom:20%;"  /></p><p><strong>例子</strong></p><p>函数f(x)的图表示如下，每个节点输入如图。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759520.png" alt="image-20210206213631261" style="zoom:20%;"  /></p><p>对于第一个<code>加</code>节点，局部梯度: a对x， y分别为1， 1</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759521.png" alt="image-20210206213756796" style="zoom:20%;"  /></p><p>对于 <code>max</code>节点， 局部梯度： b对y, z分别为1， 0</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759522.png" alt="image-20210206214007847" style="zoom:20%;"  /></p><p>对于 <code>*</code>节点， 局部梯度：f对a， b分别为2， 3</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759523.png" alt="image-20210206214217247" style="zoom:20%;"  /></p><p>最终反向梯度如下：</p><ul><li>局部梯度，可以理解为这个节点的梯度，算不清就让其变化0.1(比较小的值)，看对应输出变化多少，局部梯度就等于输出变化量/0.1。<ul><li>加法，输入边都是1</li><li>乘法，互换输入</li><li>max，起作用的那一边是1，不起作用是0</li></ul></li><li>边下面就是反向传播的梯度，整个下一层的反向梯度就是节点局部梯度 x 反向传播的梯度。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759524.png" alt="image-20210206214935426" style="zoom:20%;"  /></p><h4 id="Gradients-sum-at-outward-branches"><a href="#Gradients-sum-at-outward-branches" class="headerlink" title="Gradients sum at outward branches"></a>Gradients sum at outward branches</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759525.png" alt="image-20210206222325837" style="zoom:20%;"  /></p><h4 id="Efficiency-compute-all-gradients-at-once"><a href="#Efficiency-compute-all-gradients-at-once" class="headerlink" title="Efficiency: compute all gradients at once"></a>Efficiency: compute all gradients at once</h4><p>计算反向传播，不正确的方式：</p><ol><li>一开始计算分数s对偏置b的梯度</li><li>然后独立计算，分数s对权重的梯度</li><li>这是重复计算</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759526.png" alt="image-20210206223516711" style="zoom:20%;"  /></p><p>正确方式：</p><ul><li>同时计算所有梯度</li><li>当手工计算梯度是，像上面类似地用$\delta$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759527.png" alt="image-20210206223651956" style="zoom:20%;"  /></p><h4 id="Back-Prop-in-General-Computation-Graph"><a href="#Back-Prop-in-General-Computation-Graph" class="headerlink" title="Back-Prop in General Computation Graph"></a>Back-Prop in General Computation Graph</h4><ol><li><p>前向：按拓扑排序访问节点</p><ul><li>计算给定预处理节点的值</li></ul></li><li><p>反向</p><ul><li>初始化 输出梯度 = 1</li><li>逆序访问所有节点</li><li>使用节点的后继的梯度来计算每个节点的梯度</li></ul><p>前向和反向复杂度都是big O()，通常，我们网络是固定层结构，所以我们使用矩阵和Jacobian。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759528.png" alt="image-20210206224157886" style="zoom:20%;" /></p><h4 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h4><p>自动微分：</p><blockquote><ul><li>梯度计算可以从 Fprop 的符号表达式中自动推断</li><li>每个节点类型需要知道如何计算其输出，以及如何在给定其输出的梯度后计算其输入的梯度</li><li>现代DL框架(Tensorflow, Pytoch)反向传播，但主要是令作者手工计算层/节点的局部导数</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759529.png" alt="image-20210206225146046" style="zoom:20%;" /></p><h4 id="Backprop-Implementations"><a href="#Backprop-Implementations" class="headerlink" title="Backprop Implementations"></a>Backprop Implementations</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759530.png" alt="image-20210206225958824" style="zoom:20%;" /></p><h4 id="Implementation-forward-backward-API"><a href="#Implementation-forward-backward-API" class="headerlink" title="Implementation: forward/backward API"></a>Implementation: forward/backward API</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759531.png" alt="image-20210206230040356" style="zoom:20%;"  /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759532.png" alt="image-20210206230101258" style="zoom:20%;"  /></p><h4 id="Manual-Gradient-checking-Numeric-Gradient"><a href="#Manual-Gradient-checking-Numeric-Gradient" class="headerlink" title="Manual Gradient checking: Numeric Gradient"></a>Manual Gradient checking: Numeric Gradient</h4><ul><li>h设置为非常小的数(1e-4)</li><li>容易正确实现</li><li>但接近非常慢</li><li>检查你的作业实现很有用</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261759533.png" alt="image-20210206230127732" style="zoom:20%;"  /></p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><blockquote><ul><li>我们已经掌握了神经网络的核心技术</li><li>反向传播：沿计算图递归应用链式法则<ul><li>[downstream gradient] = [upstream gradient] x [local gradient]</li></ul></li><li>前向传递：计算操作结果并保存中间值</li><li>反向传递：应用链式法则计算梯度</li></ul><p><strong>Why learn all these details about gradients?</strong></p><ul><li>现代深度学习框架为您计算梯度</li><li>但是，当编译器或系统为您实现时，为什么要学习它们呢？<ul><li>了解引擎下发生了什么是有用的</li></ul></li><li>反向传播并不总是完美地工作<ul><li>理解为什么对调试和改进模型至关重要</li><li>参见<a href="https://medium.com/@karpathy/yes-you-should-understandbackprop-    e2f06eab496b">Karpathy文章</a> （在教学大纲中）</li></ul></li><li>未来课程的例子:爆炸和消失的梯度</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Backpropagation </tag>
            
            <tag> gradient </tag>
            
            <tag> Automatic Differentiation </tag>
            
            <tag> CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12.2 GPT系列论文： GPT-2 笔记</title>
      <link href="2020/12/05/NLP%20Paper%2012.2%20GPT-2%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/12/05/NLP%20Paper%2012.2%20GPT-2%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="12-2-GPT系列论文：-GPT-2-笔记"><a href="#12-2-GPT系列论文：-GPT-2-笔记" class="headerlink" title="12.2 GPT系列论文： GPT-2 笔记"></a>12.2 GPT系列论文： GPT-2 笔记</h3><p>本文是 <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>无监督多任务学习语言模型，即GPT-2的论文。这里作为GPT系列论文第二篇。</p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>自然语言处理任务，如问答，机翻，阅读理解，以及概括，都是在特定任务数据集上典型的监督学习方法。作者展示了在新的百万级网页数据集WebText上训练时，这些语言模型<strong>在没有任何明确的监督数据的情况下</strong>开始学习这些任务。以文档和问题条件，用该语言模型生成的答案在CoQA数据集上达到55 F1，并且没有使用127,000+训练样本就相当或超过4个基线系统中3个的性能。<strong>语言模型至关重要的能力是零样本任务迁移的成功，并且其能在一个流行的对数线性跨任务上提升性能</strong>。作者的最大模型，GPT-2，一个15亿参数的Transformer，在零样本设置下8个测试语言模型的数据中7个取得最佳成绩，并且在WebText上仍然前拟合。模型中的样本反映这些提升并包含连贯的文本段落。这些发现表明通往构建语言处理系统的有希望路径是<strong>从语言处理系统自然发生的演示中学习执行如何执行任务</strong></p><h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h4><p>机器学习系统现在擅长(在预期上)==用大规模数据集、高容量的模型和监督学习的组合==来训练它们的任务。然而这些系统是脆弱的并且对在数据分布和特定任务有轻微改变敏感。当前系统被更好地描绘成狭隘的专家而不是称职的全才。作者想朝着能执行许多任务的通用系统——最终不需要为每个任务手工创建和标注训练数据集。</p><p>创建ML系统的占支配地位的方法是收集在目标任务上显示正确行为的训练样本，训练一个系统来模仿这些行为，然后在独立同分布的留出样本上进行试。这有助于在狭隘的专家任务上取得了成功。但在字幕模型、阅读理解系统和图像分类器上，输入的可能多样性会造成不稳定性就凸显了该方法的缺点。</p><p>作者怀疑单一领域数据集上训练单一任务导致是导致在当前系统中模型泛化能力不足。让当前架构朝着强壮系统前进似乎需要在广泛领域和任务上训练和测试性能。最近几个基准已经提出了，如GLUE和decaNLP就开始研究这个。</p><p>多任务学习对于提升通用性能是一个有希望的框架。然而，在NLP里多任务仍是一个萌芽阶段。近期工作报告了不太大的性能提升，迄今为止两项最有希望的努力分别训练了总共10和17对<code>(数据集， 目标)</code>。从元学习角度来看，每个(数据集， 目标)对是送数据集和目标分布中采样单一训练样本。近期ML系统需要成百上千的样本来催生泛化好的函数。这表明多任务很多需要许多有效的训练样本对来实现跟当前方法一样的获得成功的迹象。继续扩大数据集的创建是非常困难的，并且用当前方法蛮力实现目标设计到可能需要的程度是不行的。这就需要对于多任务学习探索额外的设置。</p><p>当前在语言任务上的最好的实现是利用一组预训练和监督学习微调。该方法历史悠久，趋向于更灵活的迁移形式。首先，是学到的词向量被用作特定任务架构的输入。然后，是RNN的上下文表示转换。近期工作表明特定任务架构不再需要，而许多自注意力块转换使有效的。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846503.png" alt="image-20210721105320348" style="zoom:30%;" /></p><p>这些方法仍然需要监督训练以便任务使用。当只有最小的或无监督数据可行时，另一项工作已经证明语言模型有希望执行特定任务，如常识推理和情感分析。</p><p>在本文中，联系两个方向的工作，并继续趋向更通用的迁移学习方法。作者证明==语言模型能在零样本设置上执行下游任务——不需要任何或架构修改==。还证明了该方法的潜力，突出语言模型在零样本设置下执行一系列广泛任务的能力。作者依赖这些任务取得了有希望的，竞争的和最佳结果。</p><h4 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h4><p>语言模型的核心方法就是Bengio 2003年[<code>neural probabilistic language model</code>]提出的：</p><script type="math/tex; mode=display">p(x) = \sum_{i=1}^n \ p(s_n|s_1, \cdots,s_{n-1}) \tag{1}</script><p>该方法允许对<script type="math/tex">p(x)</script>以及<script type="math/tex">p(s_{n-k}, \cdots, s_n|s_1, \cdots,s_{n-k-1})</script>形式的条件概率进行采样和估计。在近些年，这已经在模型表达上已经有很大的改进，可以计算这些条件概率，如自注意架构Transformer。</p><p>学习做单一任务的模型能表达为以估计条件分布<script type="math/tex">p(\text{output}|\text{input})</script>作为概率分布的框架。因为通用系统能做许多任务，甚至同样输入，它不仅仅基于输入也基于执行的任务来产生输出。这样模型就应该是<script type="math/tex">p(\text{output}|\text{input}, \text{task})</script>. 这在多任务和元学习中以各种形式设置。条件任务提出在一个架构级别实现，如具体任务的编码器和解码器或者算法级别如MAML的内外循环优化框架。但是如McCann 2018[Multitask learning as question answering]例证, 语言给特定任务提供了可行的方式，输入，和输出一起作为一个符号序列。例如，翻译训练样本能写作一个序列<code>(翻译成法文, 英文文本， 法文文本)</code>。同样地，阅读理解训练样本能写作 <code>(问题的答案，文档，问题，答案)</code>。McCann 证明这可以训练单一模型，如MQAN，来用这种形式的序列在样本上推断和做许多不同任务。</p><p>在原理上，语言模型也能够学习McCann等人 2018[Multitask learning as question answering]提到的任务，而无需明确监督哪些符号是要预测输出的。因为监督学习目标跟无监督学习目标是一致，但只在序列的一个子集上评估，无监督学习目标的全局最小也是监督学习目标的全局最小。在稍微玩具性质的设置中，Sutskerver 等人，2015[Towards principled unsupervised learning]讨论的有关密度估计作为准则的训练目标被忽略了。这个问题反而变成了作者是否能够在实践中中优化无监督学习目标来收敛。初步实现确认足够大的语言模型能够在玩具式的设置下执行多任务学习，但学习要比明确的监督学习方法慢。</p><p>尽管上面描述的给“野性语言”的混乱进行精心设计设置是一个大的进步。Weston （2016）认为，在对话背景下，需要开发一个系统能够从自然语言中直接学习并证明一个概念——学习一个QA任务，用一个“teacher”输出的前向预测不需要奖励信号。(不需要监督学习)。虽然对话是一种有吸引力的方法，但作者担心其过于严格。互联网包括不需要互动交流的大量被动可用的信息。作者推测有足够容量的语言模型将开始学习推断和执行在语言序列中证明中展示的任务以便更好预测它们，无论其获取方法是什么。如果一个语言模型能够做到这点或者将来能，它实际上将做无监督学习。作者通过分析语言模型在各种任务的零样本设置中的性能来测试是这样的。</p><h5 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h5><p>以前用单一领域的文本，进一步用网络抓取的多样的海量文本Common Crawl.</p><p>本文用作者创建手工抓取过滤来自Reddit 3 karma以上生成的数据集==WebText==，截止于12.2017，800万文档总共40GB数据。但其是其它数据集的常见数据源可能会造成测试数据和训练数据重叠。(因为这个, WebText 移除了涉及Wikipedia的文章)</p><p>自然语言中英法翻译在WebText训练数据集出现示例如下表1：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846504.png" alt="image-20210721121109730" style="zoom:30%;" /></p><h5 id="2-2-输入实现"><a href="#2-2-输入实现" class="headerlink" title="2.2 输入实现"></a>2.2 输入实现</h5><p>通用语言模型应该能计算捕获任何字符的概率。在10亿词基准上，byte-level LMs比word-level效果要差，本文发现也一样。</p><p>Byte Pair Encoding <a href="https://arxiv.org/abs/1508.07909">Sennrch等人 2915 </a>是一种介于字符级别和单词级别中间的实用语言模型，其对于高频符号序列字符级别输入和对于低频符号序列字符级别输入能有效地插值。尽管它的名字有byte的，但有关BPE的实现通常是在Unicode字符编码上操作而不是Byte。该实现需要包括整个Unicode符号以便给所有Unicode字符建模。这将导致没有添加组合符号的字符之前基础词汇表就超过130,000。这与 BPE 经常使用的32,000到64,000字符词汇表是令人却步的大。相比之下，字节级别版本的BPE只需要大小256基础词汇表。然而，由于BPE使用基于启发的贪婪的频率来构建字符词汇表，将直接导致应用字节序列的BPE会造成次优合并。作者观察BPE包含许多版本的常用词如<code>dog</code>, 因为其经常出现许多变种如 <code>dog.dog!dog?</code>.这导致就是有限的词汇表槽位和模型容量的次优化分配。为了避免这种情况，作者<strong>阻止BPE为任何字节序列跨字符类别合并。作者只例外为空格添加，这显著地提升了压缩效率，同时在跨多个词汇字符中添加最少的单词碎片</strong>。</p><p>这种输入表示<strong>允许作者将单词级别的语言模型的经验优势和字节级别方法的通用性结合起来</strong>。因此作者的方法可以给任何Unicode字符分配概率，这允许作者在不管是预处理，字符化或词汇大小的任何数据集上评估作者的模型。</p><h5 id="2-3-模型"><a href="#2-3-模型" class="headerlink" title="2.3 模型"></a>2.3 模型</h5><p>模型使用还是Transformer，在<a href="https://aigonna.com/2020/12/03/NLP%20Paper%207.1%20GPT-1%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">GPT-1</a> 上小修改。</p><ul><li>Layer Normalization 移到每个子块的输入，类似于预激活的残差网络</li><li>最后一个自注意模块后加一个Layer Normalization</li><li>用<script type="math/tex">\frac{1}{\sqrt{N}}</script>初始化残差层权重，N是残差层数目</li><li>词汇表扩大到1024</li><li>上下文从512增加到1024</li><li>batchsize 增加到512</li></ul><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h4><p>训练和基准化了4个参数大小近似对数均分分布语言模型。如下表2.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846505.png" alt="image-20210721172811015" style="zoom:30%;" /></p><ul><li>最小的近似等于原始的模型</li><li>第二小的等于最大的BERT</li><li>最大模型叫GPT-2</li><li>每个模型的学习率都在5%的留出WebText数据集上手工调到最好的困惑度</li><li>所有模型在WebText上都是欠拟合的并且留出数据集上困惑度训练更长时间还有提升。</li></ul><h5 id="3-1-语言模型"><a href="#3-1-语言模型" class="headerlink" title="3.1 语言模型"></a>3.1 语言模型</h5><p>作为零样本任务迁移的第一步，作者有兴趣了解 WebText LM 如何在零样本领域迁移中执行他们训练的最基本任务——语言建模。因为作者的模型在字节级别上操作并且不需要有损失的预处理或tokenization。作者能在任何语言模型上基准评估。在语言模型数据集上的结果通常用一个数值来报告，其被以平均负对数概率的放缩或者去幂的形式计算预测类别单元——通常是字符、直接或者单词。作者估计一样的值是通过计算一个数据集对于WebText 语言模型的对数概率并除以类别单元的数目。对于这些数据集中的许多数据集，WebText 语言模型将在分布之外进行某种意义的测试，必须预测积极标准化的文本，tokenization 组件字符如断开的标点符号，乱序的句子，甚至是<UNK>字符其在WebText中极其罕见——400亿字节只出现了26次。主要结果报告如下表3，其使用的可逆去分词器，尽可能多地移除了这些tokenization/预处理组件。由于这些去分词器都是可逆的，作者仍然可计算一个数据集的对数概率，它们可以认为是域适应的简单形式。对于用这些去分词器的GPT-2作者获得了从2.5到5的困惑度提升。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846506.png" alt="image-20210721215453597" style="zoom:30%;" /></p><p>WebText语言模型跨领域和数据集迁移良好，在零样本设置下，8个数据集中7个提升到了最佳成绩。在小数据集上，如Penn TreeBank和只有1到2百万训练字符的WikiText-2也注意到有大的提升。长依赖测试数据集LAMBADA和儿童书籍测试，都有大的提升。但在10亿单词基准测试中表现比之前工作提出的模型差。这可能使其结合了大数据集和一些破坏结构的预处理——10亿单词基准测试移除了所有长范围结构导致句子级别是打乱的。</p><h5 id="3-2-儿童书籍测试"><a href="#3-2-儿童书籍测试" class="headerlink" title="3.2 儿童书籍测试"></a>3.2 儿童书籍测试</h5><p>儿童书籍测试是创建用来检查语言模型在不同类别词上的表现，如命名实体，名词，动词以及介词。报告不采用困惑度，而使用预测对删除单词的10个选择的完型填空测试。遵循在原论文中引入的语言模型方法，根据这个语言模型计算这个选择在每个选择的概率和剩下句子条件的概率，预测最高概率作为预测结果。如表2所示，在测试集上，模型表现随着模型大小稳定提升，跟人类表现的差距在缩小。数据重叠分析如一本在CBT测试集中的书，Rudyard Kipling的《奇幻森林》,也在WebText里，但作者在验证集上报告结果没有显示显著的重叠。GPT-2在普通名词上取得了93.3%的最佳结果，在命名实体上也取得了89.1%。应用去分词器从 CBT 中删除 PTB 样式标记化组件。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846507.png" alt="image-20210721221222569" style="zoom:30%;" /></p><h5 id="3-3-LAMBADA"><a href="#3-3-LAMBADA" class="headerlink" title="3.3 LAMBADA"></a>3.3 LAMBADA</h5><p>LAMBADA数据集测试系统在文本中的对长距离依赖关系建模的能力。GPT-2提升了成绩，其造成CPT-2预测错误的原因主要是没有有效的终止符。</p><h5 id="3-4-威诺格拉德模式挑战"><a href="#3-4-威诺格拉德模式挑战" class="headerlink" title="3.4 威诺格拉德模式挑战"></a>3.4 威诺格拉德模式挑战</h5><p>威诺格拉德模式挑战被构建用来衡量一个系统做常识推理的能力，即测试解决文本歧义的能力。近期Trinh&amp;Le (2018)证明了语言模型在该挑战的巨大成功，就是语言模型用高概率来预测歧义的解决方案。作者遵循他们的问题计算公式，并用全部和部分分数技术可视化作者模型表现如下表3.GPT-2 比当前最佳准确率的提升了7%，达到70.70%。数据集非常小只有273个样本，所以作者建议阅读Trichelair 等人(2018)来帮助将该结果置于问题背景来理解。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846509.png" alt="image-20210722011634035" style="zoom:25%;" /></p><h5 id="3-5-阅读理解"><a href="#3-5-阅读理解" class="headerlink" title="3.5 阅读理解"></a>3.5 阅读理解</h5><p>CoQA是由7个不同领域的自然语言对话对构成的文档。在一个文档条件下，在验证集上，从GPT-2中贪婪解码得到历史相关的对话和最终的字符A取得55的F1。BERT监督学习取得89F1，但GPT-2的表现毕竟是无监督学习还是可以滴。</p><h5 id="3-6-摘要"><a href="#3-6-摘要" class="headerlink" title="3.6 摘要"></a>3.6 摘要</h5><p>在CNN和每日邮报数据集上测试摘要的能力。为了引导摘要行为，作者在文章后面添加文本TL;DR：并用k=2的Top-k的随机采样(Fan 等人 2018)生成100个字符，这减少了重复并鼓励更抽象的摘要而不是贪婪解码。作者使用在100个token中生成的最先的3个句子作为摘要。会聚焦于最近的文章或者不清楚具体细节，如下表4。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846510.png" alt="image-20210722020627558" style="zoom:30%;" /></p><h5 id="3-7-翻译"><a href="#3-7-翻译" class="headerlink" title="3.7 翻译"></a>3.7 翻译</h5><p>作者测试GPT-2是否开始学习如何翻译一种语言到另一种语言。为了帮助推断这是期望的任务，作者在格式如<code>english sentence=french sentence</code>的样本对的上下文中调节语言模型，然后在<code>english sentence=</code>最终提示之后，作者从模型中用贪婪解码并用第一个生成的句子作为翻译。GPT-2在WMT-14 英语-法语上取得5 BLEU，都不是太好的成绩。</p><h5 id="3-8-问答"><a href="#3-8-问答" class="headerlink" title="3.8 问答"></a>3.8 问答</h5><p>30个GPT-2生成的答案如下表5，其表现还是远远弱于30到50%的用文档提取问答的杂交的信息检索的开放领域的问答系统，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846511.png" alt="image-20210722022532552" style="zoom:30%;" /></p><h4 id="4-泛化与记忆"><a href="#4-泛化与记忆" class="headerlink" title="4. 泛化与记忆"></a>4. 泛化与记忆</h4><p>数据集越来越大可能会有数据重叠的现象，所以首先就是分析训练数据和测试数据有多少重合。</p><p>用包含WebText的8-grams的训练数据集字符创建<code>Bloom filter</code>，假阳性率上界只有<script type="math/tex">1/10^8</script>​​.用1M字符串进一步验证假阳性率的下界为0. </p><p>布隆过滤器让作者计算，给定数据集，8-grams来自该数据集也来自WebText训练数据的百分比。具体如下表6所示。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846512.png" alt="image-20210722135951172" style="zoom:25%;" /></p><p>本文又分析了在WikiText-103测试集，威诺格拉德模式挑战等数据集的重叠情况。</p><p>总的来说，WebText数据集的训练数据和特定任务的验证集有重复情况，对结果影响不大。大部分数据集没有发现大量重复的情况，具体如上表6所示。</p><p>另一种确定性的潜在方法关于WebText语言模型是否归功于其记忆，就是检查它们在留出集上的表现，如下表4，性能在WebText的训练集和测试集都类似并且都随着模型大小增加而上升。这表明GPT-2在许多方向仍然欠拟合。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846513.png" alt="image-20210722141306141" style="zoom:30%;" /></p><p>GPT-2也能写有关发现谈论独角兽的新闻文章。例子如下表13.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261846514.png" alt="image-20210722141735510" style="zoom:40%;" /></p><p>后面部分就是一些工作介绍，就不翻译了，只有GPT-2的改进就是2.3小节部分，还有其优势在摘要和各个数据集上的表现都有，这里就不写一遍了。</p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
            <tag> GPT </tag>
            
            <tag> 无监督学习 </tag>
            
            <tag> fine-tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n Lecture 2 Word Vectors,Word Senses, and Classifier Review</title>
      <link href="2020/12/04/CS224N%20Lecture%202%20Word%20Vectors%202%20and%20Word%20Senses/"/>
      <url>2020/12/04/CS224N%20Lecture%202%20Word%20Vectors%202%20and%20Word%20Senses/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Review-Main-idea-of-word2vec"><a href="#1-Review-Main-idea-of-word2vec" class="headerlink" title="1. Review: Main idea of word2vec"></a>1. Review: Main idea of word2vec</h3><p>如图，中心词预测上下文- Skip-Gram model </p><ul><li>随机一个词向量开始</li><li>在整个语料库上迭代每个词</li><li>试着用中心词预测周围词，如下图</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751546.png"  style="zoom:20%;" align=center  alt="中心词预测上下文"  /></p><ul><li><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751549.png" style="zoom:20%;" align=center  alt="图P(o|c)"  /></p></li><li><p>更新向量</p></li><li>该算法学习词向量， 在词向量空间上，获得相似性和有意义的方向。U和V是长度为N的向量，需要学习得到。</li></ul><h4 id="Word2vec-parameters-and-computations"><a href="#Word2vec-parameters-and-computations" class="headerlink" title="Word2vec parameters and computations"></a>Word2vec parameters and computations</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751550.png" style="zoom:20%;" align=center  alt="词向量计算示意" /></p><p>注意：</p><ul><li>在每个方向上都是一样的预测</li><li>期望模型对所有出现在上下文（相当频繁）的词给一个合理的高概率值估计</li></ul><h4 id="Word2vec-maximizes-objective-function-by-putting-similar-words-nearby-in-space"><a href="#Word2vec-maximizes-objective-function-by-putting-similar-words-nearby-in-space" class="headerlink" title="Word2vec maximizes objective function by putting similar words nearby in space"></a>Word2vec maximizes objective function by putting similar words nearby in space</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751551.png"  width = "300" align=center  alt="相似词在空间上的示意" /></p><h3 id="2-Optimization-Gradient-Descent"><a href="#2-Optimization-Gradient-Descent" class="headerlink" title="2. Optimization: Gradient Descent"></a>2. Optimization: Gradient Descent</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751552.png" alt="梯度下降示意图"  width = "300" align=center /></p><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><ul><li><p>更新公式（矩阵形式）</p><script type="math/tex; mode=display">\theta^{new}= \theta^{old} - \alpha \nabla_{\theta} J(\theta) \tag{1}</script><p>其中<script type="math/tex">\alpha</script>是步进或学习率</p></li><li><p>更新公式 （单一参数）</p></li></ul><script type="math/tex; mode=display">\theta_j^{new}= \theta_j^{old} - \alpha \frac{\partial }{\partial \theta_j^{old}} J(\theta) \tag{2}</script><ul><li>算法</li><li><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751553.png" alt="梯度计算code" width = "400"/></li></ul><h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><ul><li><p>问题： <script type="math/tex">J(\theta)</script>是一个在整个语料集上遍历窗口的目标函数，可能上百万规模</p><ul><li>计算其梯度非常昂贵</li><li>做单词更新就要非常久。</li><li>对于大量神经元更不好</li></ul></li><li><p>措施：</p><ul><li>SGD</li><li>重复同一窗口，然后更新每个梯度</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751554.png" alt="SGD" width = "400" /></p><h4 id="Stochastic-gradients-with-word-vectors"><a href="#Stochastic-gradients-with-word-vectors" class="headerlink" title="Stochastic gradients with word vectors!"></a>Stochastic gradients with word vectors!</h4><ul><li>重复地在每个窗口上取梯度进行SGD</li><li>在每个窗口，只有至多<script type="math/tex">2m+1</script>个词，其梯度是十分稀疏的</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751555.png" alt="稀疏梯度" width = "230" /></p><ul><li>可能仅更新实际出现的词向量</li><li>Solution: 要么需要稀疏矩阵更新操作来更新整个嵌入矩阵<code>U</code> 和 <code>V</code>的特定行，或者对每个词向量保留周围的哈希值</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751556.png" alt="词向量" width = "300" /></p><h4 id="Word2vec-More-details"><a href="#Word2vec-More-details" class="headerlink" title="Word2vec: More details"></a>Word2vec: More details</h4><p>Two model variants:</p><ol><li><p>Skip-grams (SG)： Predict context (“outside”) words (position independent) given center word</p></li><li><p>Continuous Bag of Words (CBOW)：Predict center word from (bag of) context words  </p></li></ol><h4 id="The-skip-gram-model-with-negative-sampling"><a href="#The-skip-gram-model-with-negative-sampling" class="headerlink" title="The skip-gram model with negative sampling"></a>The skip-gram model with negative sampling</h4><ul><li>归一化因子计算非常昂贵，指上面图P(o|c)分母。</li><li>因此，用负采样，用下面两个训练一个二分类logistic regression ：<ul><li>真实值：一对中心词和窗口内的上下文</li><li>随机噪声：一对中心词和随机词</li></ul></li><li>从《Distributed Representations of Words and Phrases and their Compositionality 》中，得目标函数为</li></ul><script type="math/tex; mode=display">J(\theta) = \frac{1}{T} \sum_{t=1}^{T}J_t(\theta) \tag{3}</script><p>其中，</p><script type="math/tex; mode=display">J_t(\theta) = \text{log} \ \sigma(u_o^Tv_c) + \sum_{i=1}^{k} \mathbb{E}_j \sim P(w) \left[\text{log} \ \sigma (-u_j^Tv_c) \right] \tag{4}</script><ul><li>式4的第一个对数， 最大化两个词的共现概率</li><li>更像分类的表示</li></ul><p>词向量<script type="math/tex">u_o, \ v_c</script>相邻，<script type="math/tex">(u_o, \ v_c)</script>作为正样本，再选取k个与词c不相邻的词（k一般不超过15）组成k个负样本。目标函数如下：</p><script type="math/tex; mode=display">J_{neg-sample}(o, v_c, U) = -\text{log} \ \sigma(u_o^Tv_c) + \sum_{i=1}^{k}\text{log} \ \sigma (-u_k^Tv_c)  \tag{5}</script><p>词向量<script type="math/tex">u_o, \ v_c</script>如果相近，那么<script type="math/tex">u_o^Tv_c</script>会很大，经过sigmoid后会趋于1，<script type="math/tex">-\text{log} \ \sigma(u_o^Tv_c)</script>大于0但接近0的树。</p><p>同理，设词k与词c不相邻，<script type="math/tex">u_k^Tv_c</script>是一个很小的数。</p><ul><li>词w的词频为<script type="math/tex">U(w)</script>, 选择词w作为c的不相邻概率为：</li></ul><script type="math/tex; mode=display">P(w) = U(w)^{\frac{3}{4}} / Z</script><p>其中，分母Z是归一化因子，使得<script type="math/tex">\sum P(w)=1</script>,所有词被选取概率和为1。</p><blockquote><p>We investigated a number of choices for Pn(w) and found that the unigram distribution U(w) raised to the<br>3/4rd power (i.e.）. U(w)3/4/Z) outperformed significantly the unigram and the uniform distributions, for &gt; &gt; both NCE and NEG on every task we tried including language modeling (not reported here) .——引用自 Distributed Representations of Words and Phrases and their Compositionality</p></blockquote><h3 id="3-Why-not-capture-co-occurrence-counts-directly"><a href="#3-Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="3. Why not capture co-occurrence counts directly?"></a>3. Why not capture co-occurrence counts directly?</h3><h4 id="共现矩阵X"><a href="#共现矩阵X" class="headerlink" title="共现矩阵X"></a>共现矩阵X</h4><ul><li>实现方法：窗口或者全部文档</li><li>窗口：类似word2vec, 对每个词用窗口，获取句法和语义信息</li><li>全部文档共现矩阵将给出普通topics，导致”隐语义分析“</li></ul><h4 id="Example-Window-based-co-occurrence-matrix"><a href="#Example-Window-based-co-occurrence-matrix" class="headerlink" title="Example: Window based co-occurrence matrix"></a>Example: Window based co-occurrence matrix</h4><ul><li>窗口长度1（通常5-10）</li><li>对称</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751557.png" alt="共现矩阵" width = "700" /></p><h4 id="Problems-with-simple-co-occurrence-vectors"><a href="#Problems-with-simple-co-occurrence-vectors" class="headerlink" title="Problems with simple co-occurrence vectors"></a>Problems with simple co-occurrence vectors</h4><ul><li>随词汇表的大小增大，高纬度， 需要更多存储空间</li><li>子序列分类问题模型有非常稀疏的问题</li><li>导致模型不鲁棒</li></ul><h4 id="Solution-Low-dimensional-vectors"><a href="#Solution-Low-dimensional-vectors" class="headerlink" title="Solution: Low dimensional vectors"></a>Solution: Low dimensional vectors</h4><ul><li>存储大部分重要的信息到， 一个固定的低维度的稠密向量</li><li>通常25-1000维，类似于word2vec</li><li>如何降低维度</li></ul><h4 id="Method-Dimensionality-Reduction-on-X"><a href="#Method-Dimensionality-Reduction-on-X" class="headerlink" title="Method: Dimensionality Reduction on X"></a>Method: Dimensionality Reduction on X</h4><p>奇异值分解，取前k个奇异值，来近似矩阵X，上一篇论文有代码，主要根据奇异值分解的几何意义。<a href="https://aigonna.com/2020/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/">介绍SVD</a></p><h4 id="Interesting-syntactic-patterns-emerge-in-the-vectors"><a href="#Interesting-syntactic-patterns-emerge-in-the-vectors" class="headerlink" title="Interesting syntactic patterns emerge in the vectors"></a>Interesting syntactic patterns emerge in the vectors</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751558.png" alt="COALS model from An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. ms., 2005  " width = "300"/></p><h3 id="4-Towards-GloVe-Count-based-vs-direct-prediction"><a href="#4-Towards-GloVe-Count-based-vs-direct-prediction" class="headerlink" title="4. Towards GloVe: Count based vs. direct prediction"></a>4. Towards GloVe: Count based vs. direct prediction</h3><p>基于计数的算法（LSA,HAL）和直接预测的算法（skip-gram,CBOW）各有各的优点和缺点。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751559.png" alt="比较" width = "400" /></p><p>共现概率的比值， 比值可以体现两个词之间的类比关系。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751560.png" alt="共现概率比值大小" width = "300" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751561.png" alt="共现概率比值数值" width = "300"/></p><p>举例说明就是，冰ice和固体soild与蒸汽steam和固体soild比较，因为是8.9所以说明，前者关系更大，也就是冰与蒸汽相比，冰与固体关系更大，或者说越接近。</p><h4 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h4><p>Q:How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?  </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751562.png" alt="对数一元线性概率比" width = "300"/></p><h4 id="Combining-the-best-of-both-worlds-GloVe-Pennington-et-al-EMNLP-2014"><a href="#Combining-the-best-of-both-worlds-GloVe-Pennington-et-al-EMNLP-2014" class="headerlink" title="Combining the best of both worlds GloVe [Pennington et al., EMNLP 2014]"></a>Combining the best of both worlds GloVe [Pennington et al., EMNLP 2014]</h4><p>Glove每个词只有一个词向量，我们希望词i和词j的词向量可以表示词i在词j周围的概率。</p><script type="math/tex; mode=display">w_iw_j = \text{log} P(i|j)</script><p>如果对于词x，用如下公式来推断x跟a有关还是b有关。</p><script type="math/tex; mode=display">w_x \cdot (w_a - w_b) = \text{log} \frac{P(x|a)}{P(x|b)}</script><p>$w_x \cdot (w_a - w_b) $越大， x与a有关；越小， x与b有关；适中，可能和a，b有关也有可能都无关。</p><p>人为指定一个目标函数，在最小化目标函数的过程中学习词向量。</p><script type="math/tex; mode=display">J = \sum_{i, j=1}^V f(X_{ij})(w_i^T\tilde w_j + b_i + \tilde b_j -\text{log}X_{ij}) \tag{6}</script><p>其中， w是词向量，X是共现矩阵，b是偏置。f(x)是一个人为规定的函数，近似为min(t, 100), 作用是降低常见词the””, “a”的重要性。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751563.png" alt="函数f" style="zoom:50%;" /></p><p>Golve :</p><ul><li>快速向量</li><li>适合当面语料库</li><li>小语料库和短向量也表示不错</li></ul><h3 id="5-How-to-evaluate-word-vector"><a href="#5-How-to-evaluate-word-vector" class="headerlink" title="5. How to evaluate word vector"></a>5. How to evaluate word vector</h3><p>两种方法：</p><ul><li>Intrinsic：专门设计单独的试验，由人工标注词语或句子相似度，与模型结果对比。好处是是计算速度快，但不知道对实际应用有无帮助。有人花了几年时间提高了在某个数据集上的分数，当将其词向量用于真实任务时并没有多少提高效果。</li></ul><ul><li><p>Extrinsic: 通过对外部实际应用的效果提升来体现。耗时较长，不能排除是否是新的词向量与旧系统的某种契合度产生。需要至少两个subsystems同时证明。</p><h4 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h4><p><code>a:b::c:？</code>， 词向量类比。通过词向量夹角的余弦来推断。</p></li></ul><script type="math/tex; mode=display">  d = \text{arg } \text{max }_i \frac{(x_b-x_a+x_c)^Tx_i}{\lvert \lvert x_b - x_a +x_c \rvert \rvert} \tag{7}</script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751564.png" alt="类比词向量" style="zoom:20%;" /></p><p>这样的类比在词向量中其实很简单，“woman”词向量减去“man”词向量近似于“king”词向量减去“queen”词向量即可。</p><h4 id="Glove-Visualizations"><a href="#Glove-Visualizations" class="headerlink" title="Glove Visualizations"></a>Glove Visualizations</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751565.png" alt="Glove Visualizations" width = "300" /></p><h4 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h4><p>Glove 词向量估计：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751566.png" alt="结果" width = "300" /></p><p>在不同大小的语料上，训练不同维度的词向量，在语义和语法数据集上的结果。</p><p>下图显示：</p><ul><li>更多的数据是有帮助的</li><li>wiki比新闻文本更好</li><li>维度</li><li>最合适的维度大约300</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751567.png" alt="image-20210125215527836" style="zoom:30%;" /></p><h4 id="另一个内在词向量估计"><a href="#另一个内在词向量估计" class="headerlink" title="另一个内在词向量估计"></a>另一个内在词向量估计</h4><ul><li>词向量的距离跟人类评判有关</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751568.png" width = "300" align=center  alt="词与词的距离" /></p><ul><li>词向量距离和人类评判关系</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751569.png" width = "300" align=center alt="不同模型比较" /> </p><h4 id="外在词向量估计"><a href="#外在词向量估计" class="headerlink" title="外在词向量估计"></a>外在词向量估计</h4><ul><li>所有子序列任务在一类。</li><li>一个好的词向量有直接帮助：命名实体识别：找一个人，一个组织，或地点</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751570.png" alt="命名实体识别对比" width = "300" align=center/></p><h3 id="6-词义和消歧"><a href="#6-词义和消歧" class="headerlink" title="6. 词义和消歧"></a>6. 词义和消歧</h3><h4 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy——-TACL2018"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy——-TACL2018" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy—— TACL2018"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy—— TACL2018</h4><p>一个多义词的词向量等于其各个意思的词向量的加权和。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261751571.png" alt="多义词词向量计算" width = "300" align=center /></p><p>待续….</p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p>[1] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/readings/cs224n-2019-notes02-wordvecs2.pdf">cs224n-2019-notes02-wordvecs2</a></p><p>[2] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/slides/cs224n-2020-lecture02-wordvecs2.pdf">cs224n-2020-lecture02-wordvecs2</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/60623973">2019版CS224N中文笔记(2)词向量的计算与评价</a></p><p>[4] <a href="https://zh.d2l.ai/chapter_natural-language-processing/glove.html">全局向量的词嵌入</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> negative sampling </tag>
            
            <tag> CS224n </tag>
            
            <tag> Word2vec </tag>
            
            <tag> Skip-Gram </tag>
            
            <tag> Glove </tag>
            
            <tag> 词向量评价 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12.1 GPT系列论文： GPT-1 笔记</title>
      <link href="2020/12/03/NLP%20Paper%2012.1%20GPT-1%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/12/03/NLP%20Paper%2012.1%20GPT-1%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="12-1-GPT系列论文：-GPT-1-笔记"><a href="#12-1-GPT系列论文：-GPT-1-笔记" class="headerlink" title="12.1 GPT系列论文： GPT-1 笔记"></a>12.1 GPT系列论文： GPT-1 笔记</h3><p>本文是 <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving Language Understanding by Generative Pre-Training</a>——GPT-1的论文。它是生成式预训练模型，使用的是Transformer decoder,而且还有不同。因为GPT系列有3篇论文，我会先看论文，再看相应代码。这是这个这个系列的第一篇笔记。</p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>自然语言理解包含了广泛的多样性任务，比如文本蕴涵，问答，语义相似度评估，文本分离。然而大规模的没标注的文本语料是丰富，而特定任务学习的标注数据有非常少，使得要充分做区分地训练模型非常有挑战性。作者证明通过在丰富的无标签文本语料库生成预训练<code>generative pre-training</code>语言模型，然后在每项具体任务上判别性微调<code>discriminative fine-tuning</code>，可以实现巨大的收益。对比之前的方法，作者在微调阶段使用任务感知的输入转换来实现有效的迁移，仅仅需要小小修改模型架构。作者通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。如，作者在常识推理(Stories Close Test)上提升8.9%， 在问答上提升5.7%(RACE)，文本蕴含提升1.5%(MultiNLI)。</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1  简介"></a>1  简介</h4><p>在NLP中，有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖。大部分深度学习方法需要大量人工标注的数据，这限制了它们在许多缺乏标记数据领域的适用性。在这种情况下，模型能从无标记数据中充分利用语义信息，为收集更多的标注数据提供了更多一个有价值的替代方案，标注数据昂贵又耗时。进一步来说，即便是那些有大量标注数据的场景，无监督学习得到的好的表示也能提供显著的提升。最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列NLP任务表现。</p><p>无论到什么程度，从无标注文本中充分利用词级别以外的信息是有挑战性的，有两个主要<strong>原因</strong>。</p><ol><li>不清楚在学习文本表示时，什么样的优化目标是最高效的迁移。近期研究考虑过各种各样的目标，如语言模型，机翻，语句连贯性，每种方法在不同任务上都优于其它方法。</li><li>对于将这些学习到的表征迁移到目标任务的最有效方法，目前还没有达成共识。已有的技术涉及对模型架构进行特定任务的修改、使用复杂的学习方案以及添加辅助学习目标的组合。这些不确定性使得开发有效的语言处理半监督学习方法变得困难。</li></ol><p>在本文中，作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法。目标是学习一个全局表示，迁移它来稍微适应一系列广泛的任务。作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集(目标任务)。该设置不需要这些目标任务和无标记语料库是一个领域的。并采用两段式训练流程。首先，<strong>在无标记数据上使用语言模型目标来学习神经网络初始化的参数</strong>。接着，使<strong>用对应特定任务的监督目标来调整这些参数</strong>。(预训练+微调)</p><p>对于作者的模型架构，使用的是Transformer，它被证明在许多任务上有很强的表现，如机翻，文本生成，句法解析。该模型在文本上处理长期依赖提供了更结构化的内存，相比其他替代方案如RNN，造成跨各种各样任务的迁移性能更强。在迁移阶段，作者利用源于遍历式(traversal-style)方法的特定任务的输入改写，其将结构化文本输入处理为单一的连续字符序列。如作者在实验中证明的，这些改写使得在预训练模型架构上用最小的修改就会有效。</p><p>作者在四种类型的语言理解任务评估作者的方法——自然语言推断NLI，问答，语义相识度，和文本分类。作者通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。</p><ul><li>常识推理提升8.9%(Stories Cloze Test)</li><li>问答提升5.6%(RACE)</li><li>文本蕴含提升5.5%(MultiNLI)</li><li>GLUE多任务提升5.5%.</li></ul><p>也分析了在四种不同设置下预训练模型的零次(zero-shot)表现，证明其确实为下游任务获取到了有用的语言知识。</p><h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2  相关工作"></a>2  相关工作</h4><p><strong>NLP半监督学习</strong>    预训练对于获取不同级别信息的需要，如从词级别信息到更高的(段落级别或者句子级别的)词嵌入。</p><p><strong>无监督预训练</strong>    无监督预训练+监督微调方式，Transformer比LSTM能获取长距离信息。</p><p><strong>辅助训练目标</strong>    添加一个无监督训练目标是半监督学习的一种替代形式。如POS tag，语义组块chuking, NER， 以及语言模型来提升标记的语义角色。</p><h4 id="3-框架"><a href="#3-框架" class="headerlink" title="3  框架"></a>3  框架</h4><p>作者训练流程有两个阶段。</p><ol><li>在大规模文本语料上学习高容量的语言模型</li><li>微调阶段，用标记的数据对特定任务微调模型</li></ol><h5 id="3-1-无监督预训练"><a href="#3-1-无监督预训练" class="headerlink" title="3.1 无监督预训练"></a>3.1 无监督预训练</h5><p>给定一个无监督学习的语料tokens <script type="math/tex">\mathcal{U} = \{u_1, u_2, \cdots, u_n\}</script>，使用标准的语言模型目标并最大化其似然：</p><script type="math/tex; mode=display">L_1(\mathcal{U}) = \sum_i \log P(u_i|u_{i-k}, \cdots, u_{i-1}; \Theta) \tag{1}</script><p>这里<script type="math/tex">k</script>是上下文窗口大小，条件概率P是参数<script type="math/tex">\Theta</script>的神经网络模型。这些参数会以随机梯度下降训练。</p><p>在作者的实验中，语言模型使用<strong>多层的Transformer decoder</strong>，其实transformer的变种。该模型在上下文token上使用<strong>多头自注意力操作，接一个逐位置的前馈层</strong>来生成目标字符的分布输出(比原本少了一个多头自注意力)：</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210718173745.png" alt="image-20210718172953492" style="zoom:25%;" /></p><script type="math/tex; mode=display">\begin{align}h_0 &= UW_e + W_p\\h_l &= \text{transformer_block}(h_{l-1}) \quad \forall i \in [1, n]\\P(u)&=\text{softmax }(h_nW_e^T) \tag{2}\end{align}</script><p>这里<script type="math/tex">U=(u_{-k}, \cdots, u_{-1})</script>是上下文字符向量，n是层数，<script type="math/tex">W_e</script>是字符嵌入矩阵，<script type="math/tex">W_p</script>是位置嵌入矩阵。</p><h5 id="3-2-有监督微调"><a href="#3-2-有监督微调" class="headerlink" title="3.2  有监督微调"></a>3.2  有监督微调</h5><p>在训练公式1中的目标函数模型后，作者在监督学习目标任务上调整参数。假设有标记数据集<script type="math/tex">\mathcal{C}</script>， 每个实例有输入字符的序列构成，<script type="math/tex">x^1, \cdots, x^m</script>，对应着标签<script type="math/tex">y</script>。输入通过作者的预训练模型会得到最好的transformer block的激活状态<script type="math/tex">h_l^{m}</script>,将其喂进一个参数为<script type="math/tex">W_y</script>的添加的线性输出层来预测<script type="math/tex">y</script>有：</p><script type="math/tex; mode=display">P(y|x^1,\cdots,x^m) = \text{softmax }(h_l^m W_y) \tag{3}</script><p>给出最大化的目标函数为:</p><script type="math/tex; mode=display">L(\mathcal{C}) = \sum_{(x, y)} = \log P(y|x^1, \cdots, x^m) \tag{4}</script><p>作者还发现加入语言模型作为辅助目标来微调有助于学习(a) 提升监督模型的泛化能力(b) 加速收敛。这跟之前的工作一样，观测发现用辅助目标能提上性能。尤其是，作者用以下优化(加权重<script type="math/tex">\lambda</script>)目标：</p><script type="math/tex; mode=display">L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\lambda) \tag{5}</script><p>总之，作者额外要微调的参数只有<script type="math/tex">W_y</script>， 以及分割字符嵌入矩阵(在小节 3.3中介绍)。</p><h5 id="3-3-特定任务输入转换"><a href="#3-3-特定任务输入转换" class="headerlink" title="3.3 特定任务输入转换"></a>3.3 特定任务输入转换</h5><ul><li><p>文本分类，直接微调模型</p></li><li><p>问答或文本蕴含，输入是结构化的，如有序句子对，三元组(文档，问题和答案)</p><p>因为作者的预训练模型是用连续的文本序列训练的，需要做些修改以便用在这些任务上。之前的工作提出了在迁移表征顶部学习特定任务的架构。这种方法重新引入了大量特定任务的定制化输入，并且不会对额外的架构组件使用迁移学习。相反，作者使用遍历式方法，就是将结构化输入转换为有序序列以便作者预训练能处理。这些输入转换使作者避免跨任务架构的大改。作者在下面部分和可视化插图1提供了这些输入的简洁描述。所有的转换包括添加随机初初始化的开始和结束标记<script type="math/tex">(<s>, <e>)</script>。</p></li></ul><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210718181147.png" alt="image-20210718181143640" style="zoom:30%;" /></p><p><strong>文本蕴含</strong>  拼接前提文本<script type="math/tex">p</script>和假设<script type="math/tex">h</script>为token序列，用$符来分隔两者。</p><p><strong>相似度</strong>  对于相似任务，两个比较的句子没有内在顺序。为了反映这点，作者修改输入序列来包含2种可能的顺序(用分隔符分隔)，并独立地处理2个序列表示<script type="math/tex">h^m_l</script>,逐元素相加然后送入线性输出层。</p><p><strong>问答和常识推理</strong>  对于这些任务，给定文档<script type="math/tex">z</script>,一个问题<script type="math/tex">q</script>和一个可能答案集<script type="math/tex">\{a_k \}</script>。将文档和问题跟每个可能答案拼接起来，再在其中添加一个分隔符得到<script type="math/tex">[z; q; $; a_k]</script>。每个这些序列用作者的模型独立处理后通过一个softmax层归一化来生成所有可能答案的分布。</p><h4 id="4-实验"><a href="#4-实验" class="headerlink" title="4  实验"></a>4  实验</h4><h5 id="4-1-设置"><a href="#4-1-设置" class="headerlink" title="4.1 设置"></a>4.1 设置</h5><p><strong>无监督预训练</strong>  BOOKsCOPUS数据集预训练模型。长文本能让生成模型学习到长依赖信息的条件概率。ELMO方法处理1 B Word benchmarks, 在句子级别打乱顺序以破坏长距离结构信息。达到非常低的18.4困惑度。</p><p><strong>模型的具体配置</strong>  </p><ul><li>Transformer架构：12层有自注意力头(768维隐藏层， 12个注意力头)transformer decoder结构。</li><li>逐位置前馈神经网络(position-wise feed-forward networks)：3072维内部隐藏层。</li><li>Adam 优化器方案：最大lr=2.5e-4。开始2000次从0线性上升更新，再使用cosine方案退火到0.</li><li>从512连续tokens中随机采样得到64小批次样本，训练100轮</li><li>改进版的layerNorm， 以<script type="math/tex">N(0,0.02)</script>权重初始化。</li><li>40,000合并的BPE词汇表，残差，嵌入和注意力层以0.1的Dropout来正则化。</li><li><a href="https://arxiv.org/pdf/1711.05101.pdf">改进版L2正则</a>，所有无偏差或增益权重设置为<script type="math/tex">w=0.01</script></li><li>GELU作为激活函数</li><li>使用学习的位置嵌入，而不是原始Transformer的正余弦曲线</li><li>使用<code>ftfy</code>清洗原始BooksCorpus，去掉字符和空格，再使用spaCy tokenize。</li></ul><p><strong>微调的细节</strong>  除非指定，使用无监督预训练超参数设置。分类层使用0.1的Dropout。大部分任务，lr=6.25e-5，批大小为32.在大部分任务中基本上3轮训练就足够了。lr用以训练步数的0.2%预热衰减方案。<script type="math/tex">\lambda</script>设置为0.5.</p><h5 id="4-2-监督微调"><a href="#4-2-监督微调" class="headerlink" title="4.2  监督微调"></a>4.2  监督微调</h5><p>微调任务和数据集如下：</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210718200329.png" alt="image-20210718200327246" style="zoom:25%;" /></p><p><strong>NLI</strong>  就是识别文本蕴含。涉及读取一对句子，判断它们之间的关系，是蕴含，矛盾或中立。因为存在各种变化现象，如词汇蕴含，共指，词汇和句法歧义，所以还是很有挑战性的。</p><p>使用数据集介绍(<a href="https://blog.csdn.net/qq_33583069/article/details/115734097">这儿有个部分介绍文章</a>)：</p><ol><li>图片说明 (Stanford Natural Language Inference  <a href="https://nlp.stanford.edu/projects/snli/">SNLI</a>）</li><li>transcribed speech</li><li>流行小说</li><li>政府报告的MNLI (The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库)</li><li>QNLI (Qusetion-answering NLI，问答自然语言推断)</li><li>科学考试SCITail</li><li>新闻文章的RTE(The Recognizing Textual Entailment datasets，识别文本蕴含数据集)</li></ol><p>下表2是作者模型和之前SOTA模型NLI的结果比较：</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210718231937.png" alt="image-20210718231930904" style="zoom:30%;" /></p><p>RTE数据集比较小，只有2490样本，只达到了56.0%准确率。</p><p><strong>问答和常识推理</strong>  结果如下表3，RACE数据集由初高中考试题构成。在Story Cloze和RACE提升明显。证明作者<strong>模型的具有有效处理上下文长距离的信息的能力</strong>。</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210718235705.png" alt="image-20210718235653629" style="zoom: 30%;" /></p><p><strong>语义相似度</strong>  语义相似度(或释义发现)任务涉及预测两个句子在语义上是否相等。挑战在于识别语句是否是概念改写，理解反面，处理句法歧义。使用的数据集：</p><ul><li><strong>MRPC</strong>  <a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">Microsoft Research Paraphrase corpus</a> 是一些句子对，有的是同义的，有的是不同义的。</li><li><strong>QQP</strong> Quora Question Pairs  美国知识问答网站 Quora 上的问题答案数据集</li><li>STS-B Semantic Textual similarity benchmark 语义文本相似度数据集，样本为文本对，评判两个文本语义信息的相似度，分数为1-5。</li></ul><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210719000928.png" alt="image-20210719000926549" style="zoom:30%;" /></p><p>在STS-B上有1个点的绝对提升，比<code>Single-task BiLSTM + ELMo + Attn</code>有4.2%的绝对提升。</p><p><strong>分类</strong>  两个不同分类任务的评估结果，也在上表4中。<strong>CoLA</strong>——Corpus of Linguistic Acceptability语言可接受性语料库，纽约大学发布的有关语法的数据集，该任务主要是对一个给定句子，判定其是否语法正确，因此CoLA属于单个句子的文本二分类任务。</p><p><strong>SST-2</strong>——The Stanford Sentiment Treebank, 主要针对电影评论来做情感分类，因此SST属于单个句子的文本分类任务（其中SST-2是二分类，SST-5是五分类，SST-5的情感极性区分的更细致)。</p><p>CoLA上取得45.4，SST-2取得91.3的准确率，整体得分72.8。</p><p>总体而言，在12个数据集上的9个取得SOTA结果，比许多情况下的ensemble模型要好。而且能适应不同大小数据集。</p><h4 id="5-分析"><a href="#5-分析" class="headerlink" title="5  分析"></a>5  分析</h4><p><strong>层数的迁移学习影响</strong>  从预训练到微调迁移学习过程中，如下表2，在MultiNLI和RACE上的性能随着层数的变化而变化。作者观察标准结果，在MultiNLI上转移embedding能提升结果，每一层Transformer层能带来9%额外的提升。这表明预训练模型中的每一层都包含了解决目标问题有用的功能。</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210719002555.png" alt="image-20210719002553296" style="zoom:40%;" /></p><p><strong>零样本表现</strong>  最好要弄清楚为什么预训练模型会有效？一种假设是，与LSTMs相比，潜在生成式模型（underlying generative model）在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆（attentional memory）有助于迁移。在零样本上，LSTM表现高方差，表明在迁移中，Transformer架构导入偏差是有帮助的。</p><ul><li>对于CoLA（语言可接受性），样本的得分是用生成模型分配的tokens的平均对数概率，在阈值下进行预测的。</li><li>对于SST-2(情感分析)，给每个实例样本加一个 <code>very</code> token,来限制语言模型的输出分布只有 <code>positive</code>和 <code>nagative</code>, 就是猜测被分配到高的概率值的token作为预测值。</li><li>对于RACE(问答)，在文档和问题给定条件下，将生成模型分配的平均对数概率高的token作为答案。</li><li>对于DPRD(<strong>威诺格拉德模式</strong>), 用两个可能的替换说法来代替定义的代词，在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果。</li></ul><p><strong>消融研究</strong>  不同的消融研究如下表5.</p><p>首先，作者在微调时用辅助的LM目标来检查作者模型的性能。在NLI和QQP任务上辅助LM目标有帮助。总之，就是大数据集有效，小数据集没有。</p><p>其次，分析比较2048单元的单层LSTM和Transformer，二者都加同样的辅助LM，LSTM会掉5.6平均分数。</p><p>最后，直接在监督学习任务上训练，不要预训练，这会导致在跨任务上性能降低14.8%.</p><p><img src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210719142256.png" alt="image-20210719142245290" style="zoom:33%;" /></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>作者介绍了一种框架，<strong>用单一任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果</strong>。通过在长篇连续文本的多样化语料库上预训练，作者模型获得了重要的世界知识和<strong>处理长距离依赖的能力</strong>，然后能成功迁移学习解决判别式任务，如问答，语义相似度评估，蕴含确定和文本分类，在12个的9个数据集取得了SOTA结果。使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标。作者的工作表明，实现显著的性能提升确实是可能的，并给出了提示就是Transformer类模型和长距离依赖的文本数据集最好用这种方法来训练。作者希望这会帮助无监督学习的新的研究，以及NLU和其他领域，进一步提升无监督学习工作机理。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/332690785">GPT  v1论文详解</a></p><p>[2] <a href="https://jaysaligia.site/study/NLP(8">预训练下的改良NLU</a>.html )</p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
            <tag> GPT </tag>
            
            <tag> 无监督学习 </tag>
            
            <tag> fine-tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 1 Introduction and Word Vectors</title>
      <link href="2020/12/02/CS224N%20Lecture%201%20Introduction%20and%20Word%20Vectors/"/>
      <url>2020/12/02/CS224N%20Lecture%201%20Introduction%20and%20Word%20Vectors/</url>
      
        <content type="html"><![CDATA[<h3 id="Word-meaning"><a href="#Word-meaning" class="headerlink" title="Word meaning"></a>Word meaning</h3><h4 id="Representing-words-as-discrete-symbols"><a href="#Representing-words-as-discrete-symbols" class="headerlink" title="Representing words as discrete symbols"></a>Representing words as discrete symbols</h4><p>在传统NLP中， 我们将word视为离散的符号。特别是<code>one-hot encoding</code>， 如：</p><script type="math/tex; mode=display">\text{model} = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0]\\\text{hotel} = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0] \tag{1}</script><p>其中， 向量维度= 词汇表单词数量(e.g, 500,000)</p><h4 id="Problem-with-words-as-discrete-symbols"><a href="#Problem-with-words-as-discrete-symbols" class="headerlink" title="Problem with words as discrete symbols"></a>Problem with words as discrete symbols</h4><ul><li>式2两个向量正交</li><li>对于one-hot 向量没有相似性的自然理解。</li></ul><p><strong>Solution</strong>:</p><p>替代：学习把相似性编码到向量里。</p><h4 id="Representing-words-by-their-context"><a href="#Representing-words-by-their-context" class="headerlink" title="Representing words by their context"></a>Representing words by their context</h4><ul><li>语义分布：单词的意思有其频繁出现的近邻词来给定</li><li>当一个单词w出现在文本中，其上下文是出现在附近的一系列单词（ 在一个固定的窗口内）</li><li>用许多w的文本向量来构建一个w的表示矩阵</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749155.png" alt="banking" style="zoom:20%;" /></p><h3 id="Word2vec：-Overview"><a href="#Word2vec：-Overview" class="headerlink" title="Word2vec： Overview"></a>Word2vec： Overview</h3><p>Word2vec （Mikolov et al. 2013  ） 是为了学习词向量的框架。</p><p>思路：</p><ul><li>我们有一个庞大的文本语料库</li><li>在一个固定词汇表的每个词用一个向量表示</li><li>在文本中检查每个位置<code>t</code>，其中有一个中心词<code>c</code> 和 上下文词<code>o</code></li><li>用对于<code>c</code>和<code>o</code>的词向量的相似度来计算给定中心词c的上下文的概率（或相反）</li><li>调整词向量来最大化这个概率</li></ul><p>示例：窗口和处理计算<script type="math/tex">P(w_{t+j }\vert w_t)</script></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749156.png" alt="into的窗口和概率P计算" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749157.png" alt="banking的窗口和概率P计算" style="zoom:20%;" /></p><h4 id="Word2vec-objective-function"><a href="#Word2vec-objective-function" class="headerlink" title="Word2vec: objective function"></a>Word2vec: objective function</h4><p>对于每个位置<script type="math/tex">t = 1, \cdots, T</script>,给定中心词<script type="math/tex">w_j</script> 预测在一个固定大小<code>m</code>窗口上下文</p><script type="math/tex; mode=display">Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{-m\le j \ge m ,\ j \neq0} P(w_{t+j} \vert w_{t}; \theta) \tag{2}</script><p>最大化的预测准确率式2，等于最小化目标函数：</p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{T}log L(\theta) = -\frac{1}{T}\prod_{t=1}^{T} \prod_{-m\le j \ge m ,\ j \neq0} log \ P(w_{t+j} \vert w_{t}; \theta) \tag{3}</script><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="基于SVD-的方法"><a href="#基于SVD-的方法" class="headerlink" title="基于SVD 的方法"></a>基于SVD 的方法</h3><h4 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h4><p><strong>矩阵X</strong>:</p><ul><li>我们遍历百万级的文档，每次当单词i出现在文档j中，添加一个元素<script type="math/tex">X_{ij}</script>.显然这是一个规模非常大的矩阵<script type="math/tex">\mathbb{R}^{\lvert V \rvert \times M}</script>。</li><li>M是文档数目， V是词汇表的长度</li></ul><p><strong>Notes：</strong></p><p><strong>Distributional semantics</strong> 语义分布: 这个概念表示一个<strong>基于单词通常在上下文的意义</strong>。它是一个<strong>密度</strong>估计，这能更好地获得<strong>相似度</strong>。</p><p>以下语料库包含3句话和窗口大小为1的关系：</p><ol><li>I enjoy flying.</li><li>I like NLP .</li><li>I like deep learning.</li></ol><p>结果的关系矩阵为:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749159.png" alt="Word-Document 关系矩阵" style="zoom: 20%;" /></p><p>Word-Word 共现矩阵的使用：</p><ul><li>生成 <script type="math/tex">\lvert V \rvert \times \lvert V \rvert</script>共现矩阵<script type="math/tex">X</script></li><li>对<script type="math/tex">X</script>使用SVD分解得到<script type="math/tex">X = USV^T</script></li><li>选择<script type="math/tex">U</script>的前<script type="math/tex">k</script>列来得到一个<script type="math/tex">k</script>的词向量</li><li><script type="math/tex">\frac{\sum_{i=1}^{k}\sigma_i}{\sum_{i=1}^{\lvert V \rvert}\sigma_i}</script>表明通过前k维获得的方差数量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">===============================================================================</span><br><span class="line">U: </span><br><span class="line">[[ <span class="number">0.09455612</span>  <span class="number">0.86336676</span> -<span class="number">0.12220393</span>  <span class="number">0.25575989</span> -<span class="number">0.13285143</span> -<span class="number">0.02767876</span></span><br><span class="line">   <span class="number">0.38326809</span>]</span><br><span class="line"> [ <span class="number">0.8528587</span>  -<span class="number">0.15327167</span> -<span class="number">0.21454303</span>  <span class="number">0.27874733</span>  <span class="number">0.19389029</span> -<span class="number">0.28718474</span></span><br><span class="line">  -<span class="number">0.07309257</span>]</span><br><span class="line"> [ <span class="number">0.37014698</span> -<span class="number">0.05682519</span>  <span class="number">0.17128669</span>  <span class="number">0.04188288</span> -<span class="number">0.63749818</span>  <span class="number">0.64020128</span></span><br><span class="line">  -<span class="number">0.11138756</span>]</span><br><span class="line"> [ <span class="number">0.08992443</span>  <span class="number">0.42456305</span>  <span class="number">0.30406875</span> -<span class="number">0.05063662</span>  <span class="number">0.45258658</span>  <span class="number">0.24113937</span></span><br><span class="line">  -<span class="number">0.67353925</span>]</span><br><span class="line"> [ <span class="number">0.1664946</span>   <span class="number">0.01282527</span> -<span class="number">0.45011352</span> -<span class="number">0.52355753</span>  <span class="number">0.38881548</span>  <span class="number">0.50010667</span></span><br><span class="line">   <span class="number">0.30678371</span>]</span><br><span class="line"> [ <span class="number">0.04823424</span>  <span class="number">0.20744275</span> -<span class="number">0.41149144</span> -<span class="number">0.53891401</span> -<span class="number">0.42697718</span> -<span class="number">0.36324401</span></span><br><span class="line">  -<span class="number">0.42500792</span>]</span><br><span class="line"> [ <span class="number">0.29757182</span>  <span class="number">0.06652636</span>  <span class="number">0.66731789</span> -<span class="number">0.53668218</span> -<span class="number">0.00561351</span> -<span class="number">0.25826353</span></span><br><span class="line">   <span class="number">0.32703632</span>]] </span><br><span class="line"> Sigma: </span><br><span class="line"> [<span class="number">2.72252553</span> <span class="number">2.49475</span> <span class="number">1.84142251</span> <span class="number">1.58016483</span> <span class="number">1.1834359</span>  <span class="number">0.83631332</span> <span class="number">0.61349731</span>] </span><br><span class="line"> VT: [[ <span class="number">0.76247747</span>  <span class="number">0.10249184</span>  <span class="number">0.05244776</span>  <span class="number">0.3744146</span>   <span class="number">0.1423297</span>   <span class="number">0.42256005</span></span><br><span class="line">   <span class="number">0.24525713</span>  <span class="number">0.07887119</span>]</span><br><span class="line"> [-<span class="number">0.14565329</span>  <span class="number">0.86232953</span>  <span class="number">0.42922518</span> -<span class="number">0.05629678</span>  <span class="number">0.19684915</span> -<span class="number">0.03477114</span></span><br><span class="line">   <span class="number">0.00388863</span>  <span class="number">0.08829262</span>]</span><br><span class="line"> [-<span class="number">0.14000011</span>  <span class="number">0.03239935</span> -<span class="number">0.28982777</span> -<span class="number">0.36094733</span>  <span class="number">0.5275197</span>   <span class="number">0.2458832</span></span><br><span class="line">   <span class="number">0.45541128</span> -<span class="number">0.46790183</span>]</span><br><span class="line"> [ <span class="number">0.3793133</span>   <span class="number">0.29166778</span> -<span class="number">0.17919277</span> -<span class="number">0.154927</span>   -<span class="number">0.37168199</span> -<span class="number">0.16323288</span></span><br><span class="line">  -<span class="number">0.31313145</span> -<span class="number">0.67238019</span>]</span><br><span class="line"> [-<span class="number">0.21101068</span>  <span class="number">0.15791622</span> -<span class="number">0.4730536</span>   <span class="number">0.49238473</span>  <span class="number">0.37769099</span>  <span class="number">0.15909335</span></span><br><span class="line">  -<span class="number">0.54342756</span> -<span class="number">0.03224653</span>]</span><br><span class="line"> [ <span class="number">0.07871667</span>  <span class="number">0.22214383</span> -<span class="number">0.46743578</span>  <span class="number">0.25459588</span> -<span class="number">0.02047576</span> -<span class="number">0.65220565</span></span><br><span class="line">   <span class="number">0.45669219</span>  <span class="number">0.16364998</span>]</span><br><span class="line"> [-<span class="number">0.41984324</span>  <span class="number">0.15158491</span> -<span class="number">0.06803587</span>  <span class="number">0.38091632</span> -<span class="number">0.56479943</span>  <span class="number">0.41392806</span></span><br><span class="line">   <span class="number">0.35150727</span> -<span class="number">0.19270534</span>]</span><br><span class="line"> [ <span class="number">0.08333333</span>  <span class="number">0.25</span>       -<span class="number">0.5</span>        -<span class="number">0.5</span>        -<span class="number">0.25</span>        <span class="number">0.33333333</span></span><br><span class="line">  -<span class="number">0.08333333</span>  <span class="number">0.5</span>       ]]</span><br></pre></td></tr></table></figure><h4 id="Applying-SVD-to-the-cooccurrence-matrix"><a href="#Applying-SVD-to-the-cooccurrence-matrix" class="headerlink" title="Applying SVD to the cooccurrence matrix"></a>Applying SVD to the cooccurrence matrix</h4><p>通过对X做SVD， 观察对角阵上对角元素， 基于期望百分比方差获得前k个索引：</p><script type="math/tex; mode=display">\frac{\sum_{i=1}^{k}\sigma_i}{\sum_{i=1}^{\lvert V \rvert}\sigma_i} \tag{4}</script><p><strong>对X应用SVD</strong>：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749160.png" alt="X的SVD分解维度变化" style="zoom:20%;" /></p><p>通过选择前k个奇异向量:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749161.png" alt="选取前k个维度后奇异值变化" style="zoom:20%;" /></p><p>这些方法给予我们词向量， 这不仅仅足以编码语义和句法（语音部分）信息，但其引起许多其他问题:</p><ul><li>矩阵维度经常变化（新词频繁加入和语料库大小变化）</li><li>矩阵十分稀疏，大多数词不经常出现</li><li>矩阵通常维度非常高(<script type="math/tex">\approx 10^6 \times 10^6</script>)</li><li>平方次复杂度的训练代价(e.g 执行SVD)</li><li>需要在X上创建一些技巧来解决词频的急剧不平衡</li></ul><p>一些存在来解决这些问题的措施如下：</p><ul><li>忽略一些功能词如”the”, “he”, “has”, etc.  </li><li>应用ramp window—例如，基于文档中词与词距离加上一个共现次数的权重</li><li>用皮尔逊系数将负数设为0而不是使用原始计数</li></ul><h3 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h3><ul><li><p><strong>2 algorithms</strong> : continuous bag-of-words (CBOW) and skip-gram . CBOW 目标是用周围上下文的词向量来预测中心词。Skip-gram正好相反，用中心词来预测上下文的分布或者说概率。</p></li><li><p><strong>2 training methods</strong> ：负采样或hierarchical softmax。负采样通过抽取负样本来定义目标，而hierarchical softmax用有效的树结构来对所有词汇计算概率从而定义目标。</p></li></ul><h4 id="Language-Models-Unigrams-Bigrams-etc"><a href="#Language-Models-Unigrams-Bigrams-etc" class="headerlink" title="Language Models (Unigrams, Bigrams, etc.)"></a>Language Models (Unigrams, Bigrams, etc.)</h4><p><strong>Unigram model</strong>: 假设每个词出现的完全独立</p><script type="math/tex; mode=display">p(w_1, w_2, \cdots, w_n) = \prod_{i=1}^{n} P(w_i) \tag{5}</script><p>Bigram model： 假定sequence 的概率取决于一个词在sequence中和其旁边词出现的概率：</p><script type="math/tex; mode=display">p(w_1, w_2, \cdots, w_n) = \prod_{i=1}^{n} P(w_i \vert w_{i-1}) \tag{6}</script><h4 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h4><p>{“The”, “cat”, ’over”, “the’, “puddle”}  作为上下文，可以预测或生成中心词”jumped”  。——CBOW。</p><p>CBOW细节：</p><ol><li>设置已知参数。令模型中已知参数用one-hot词向量表示。输入one-hot向量或上下文用<script type="math/tex">x^{(c)}</script>，输出为<script type="math/tex">y^{(c)}</script>。</li><li><p>未知参数：</p><ul><li><p>创建两个矩阵，<script type="math/tex">\mathcal{V} \in \mathbb{R}^{n \times \lvert V \rvert}</script>和<script type="math/tex">\mathcal{U} \in \mathbb{R}^{\lvert V \rvert \times n }</script>.其中， <script type="math/tex">n</script>是由embedding 空间定义的任意维度；</p><p> <script type="math/tex">\mathcal{V}</script>是输入词矩阵，如<script type="math/tex">\mathcal{V}</script>的第<script type="math/tex">i</script>列是当其是模型的输入时，对词 <script type="math/tex">w_i</script>的 <script type="math/tex">n</script>维嵌入空间向量。记为<script type="math/tex">n \times 1 列向量v_i</script>。</p><p> <script type="math/tex">\mathcal{U}</script>是输出词矩阵，如<script type="math/tex">\mathcal{U}</script>的第<script type="math/tex">j</script>列是当其是模型的输出时，对词 <script type="math/tex">w_j</script>的 <script type="math/tex">n</script>维嵌入空间向量。记为<script type="math/tex">1 \times n 行向量u_i</script>。</p><p>注意： 对于每个词<script type="math/tex">w_i</script>都学到两个向量</p></li></ul></li></ol><p><strong>CBOW model：</strong></p><p>从周围上下文来预测中心词：对每个词，我们希望学到两个向量</p><ul><li><script type="math/tex">v</script> 输入向量: 当这个词在上下文中</li><li><script type="math/tex">u</script> 输出向量: 当这个词是中心词</li></ul><p><strong>CBOW model 记号：</strong></p><ul><li>$ w_i $ 输入向量: 词汇表<script type="math/tex">V</script>中的单词<script type="math/tex">i</script></li><li><script type="math/tex">\mathcal{V} \in \mathbb{R}^{n \times \lvert V \rvert}</script>: 输入词矩阵</li><li><script type="math/tex">v_i</script> : <script type="math/tex">\mathcal{V}</script>的第<script type="math/tex">i</script>列，单词<script type="math/tex">w_i</script> 输入表示向量</li><li><script type="math/tex">\mathcal{U} \in \mathbb{R}^{\lvert V \rvert \times n }</script>: 输出词矩阵</li><li><script type="math/tex">u_i</script> : <script type="math/tex">\mathcal{U}</script>的第<script type="math/tex">i</script>行，单词<script type="math/tex">w_i</script> 输出表示向量 </li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749162.png" alt="图1： 证明CBOW怎么工作以及我们怎么学习转换矩阵" style="zoom:28%;" /></p><p><strong>CBOW model步骤分解：</strong></p><ol><li>为输入的上下文生成大小为<script type="math/tex">m</script>的one-hot词向量：<script type="math/tex">(x^{c-m}, \cdots, x^{c-1}, x^{c+1}, \cdots, x^{c+m} \in \mathbb{R}^{\lvert V \rvert})</script>, 注即中心词<script type="math/tex">c</script>附近<script type="math/tex">m</script>窗口内的词的词向量。</li><li>为上下文<script type="math/tex">( v_ {c-m} = \mathcal{V} x^{c-m},  \ v_ {c-m + 1} = \mathcal{V} x^{c-m +1}, \ \cdots , \  v_ {c+m} = \mathcal{V} x^{c+m} \in \mathbb{R}^n)</script>得到嵌入词向量</li><li>取这些向量的均值得到<script type="math/tex">\hat v = \frac{v_ {c-m}  + v_ {c-m+1} + \cdots + v_ {c+m}}{2m}</script></li><li>生成分数向量 <script type="math/tex">z = \mathcal{U} \hat v \in \mathbb{R}^{\lvert V \rvert}</script>. 因为相似向量的点积会高(点积的几何意义)，为了达到高分数， 将会迫使相似词彼此靠近。</li><li>将分数变为概率<script type="math/tex">\hat y = \text{softmax} (z) \in \mathbb{R}^{\lvert V \rvert}</script>。</li><li>要求生成的概率， <script type="math/tex">\hat y \in \mathbb{R}^{\lvert V \rvert}</script>, 来匹配真正的概率， <script type="math/tex">y \in \mathbb{R}^{\lvert V \rvert}</script>, 这也恰好是真实词的one-hot向量。</li></ol><p>现在，我们了解了如果我们有<script type="math/tex">\mathcal{V} \ 和 \ \mathcal{U}</script>模型怎么工作， 我们怎样学到这两个矩阵?</p><p>利用交叉熵来得到到<code>loss function</code>：</p><script type="math/tex; mode=display">H(\hat y, \ y) = -\sum_{j = 1}^{\lvert V \rvert} y_j  \text{log} (\hat y_{j}) \tag{7}</script><p>$ y $是一个one-hot 向量， 因此，可以将式7简化为：</p><script type="math/tex; mode=display">H(\hat y, \ y) = -y_i\text{log} (\hat y_{j}) \tag{8}</script><p><strong>softmax 操作 </strong>：</p><script type="math/tex; mode=display">\frac {e^{\hat y_i}}{\sum_{k = 1}^{\lvert V \rvert}e^{\hat y_k}} \tag{9}</script><p>​            <strong>Note:</strong>： <script type="math/tex">\hat y  \mapsto H(\hat y, y), \ 当 \ \hat y = y取最小值。那么当我们找到H(\hat y, y)最小值，有\hat y \approx y</script>。这意味着我们模型在预测中心词上是非常好的。</p><p>最小化 目标函数：</p><script type="math/tex; mode=display">\begin{aligned}\text{minimize} \ J&=-\log P\left(w_{c} \mid w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\&=-\log P\left(u_{c} \mid \hat{v}\right) \\\\&=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\\\&=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)\end{aligned} \tag{10}</script><p>可以用随机梯度下降来更新所有有关词向量<script type="math/tex">u_c \ 和 \  v_j</script>。</p><h4 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h4><p><strong>Skip-Gram Model :</strong></p><ul><li>给定中心词预测周围上下文</li></ul><p>大部分参数设定跟CBOW一样， 不过把<script type="math/tex">x</script>与 <script type="math/tex">y</script>交换下。 输入one-hot向量是中心词用<script type="math/tex">x</script>表示， 输出向量是周围上下文用<script type="math/tex">y</script>表示。<script type="math/tex">\mathcal{V} \ 和 \ \mathcal{U}</script>跟CBOW一样。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749163.png" alt="图2： 证明Skip-Gram Model怎么工作以及我们怎么学习转换矩阵" style="zoom:28%;"/></p><p><strong>Skip-Gram model步骤分解：</strong></p><ol><li>生成中心词的one-hot输入向量<script type="math/tex">x \in \mathbb{R}^{\lvert V \rvert}</script>。</li><li>中心词<script type="math/tex">v_{c} = \mathcal{V}x \in \mathbb{R}^n</script> 的嵌入向量。</li><li>生成分数向量 <script type="math/tex">z = \mathcal{U}  v_c \in \mathbb{R}^{\lvert V \rvert}</script>.</li><li>将分数变为概率<script type="math/tex">\hat y = \text{softmax} (z)</script>。注:<script type="math/tex">\hat y^{c-m}, \cdots, \hat y^{c-1}, \hat y^{c+1}, \cdots, \hat y^{c+m}</script>是观察的每个上下文词的概率</li><li>要求生成的概率向量来匹配真实的概率 <script type="math/tex">y^{c-m}, \cdots,  y^{c-1}, y^{c+1}, \cdots, y^{c+m}</script>,即真实输出的one-hot向量。</li></ol><p>CBOW中，是生成目标函数来估计模型。Skip-Gram 是用朴素贝叶斯分解概率表达式， 即假定输出词是完全独立的。</p><script type="math/tex; mode=display">\begin{aligned}\text{minimize} \ J&=-\log P\left(  w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}  \mid w_{c}\right) \to \text{给定中心词， 而不是给定上下文} \\&=-\log \prod_{j=0,\ j \neq m}^{2m} P\left(w_{c-m+j} \mid w_c\right) \\ \\&= -\log \prod_{j=0,\ j \neq m}^{2m} P\left(u_{c-m+j} \mid v_c\right) \\\\&=-\log \prod_{j=0,\ j \neq m}^{2m} \frac{\exp \left(u_{c-m+j}^{T} v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_c\right)} \\\\&=-\sum_{j=0,\ j \neq m}^{2m}u_{c}^{T} v_c+2m\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} v_c\right)\end{aligned} \tag{11}</script><p>利用SGD来迭代求出未知参数：</p><script type="math/tex; mode=display">\begin{align}J &=-\sum_{j=0,\ j \neq m}^{2m} \text{log} \ P(u_{c-m+j}|v_c) \\&= \sum_{j=0,\ j \neq m}^{2m} H(\hat y, \ y_{c-m+j})\end{align} \tag{12}</script><p>注： 仅仅只有一个概率向量<script type="math/tex">\hat y</script>要计算。Skip-gram  将每个上下词平等看待：模型计算独立地出现在中心词的每个上下词对中心词距离的概率。</p><h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p>注意到对<script type="math/tex">|V|</script>的计算量巨大。做目标函数的任何更新或计算时间复杂度为<code>O(|V|)</code>。采用负采样可以近似，而不是计算整个词汇表。更新以下参数：</p><ul><li>目标函数</li><li>梯度</li><li>更新准则</li></ul><p>Tomas Mikolov  的《Distributed Representations of Words and Phrases and their Compositionality》论文中提出的负采样。虽然负采样是基于Skip-Gram模型，但实际上是优化不同的目标函数。考虑一对中心词和上下文<script type="math/tex">(w, c)</script>。记<script type="math/tex">P(D=1|w, c)</script>表示<script type="math/tex">(w, c)</script>来自语料数据的概率，对应地记<script type="math/tex">P(D=0|w, c)</script>表示<script type="math/tex">(w, c)</script>不来自语料数据的概率。 接下来， 用<code>sigmoid</code>函数对<script type="math/tex">P(D=1|w, c)</script>建模：</p><script type="math/tex; mode=display">P(D=1|w, c, \theta) = \sigma(v_c^T v_w) = \frac{1}{1+e^{-v_c^T v_w}} \tag{13}</script><p>建立一个新的目标函数，如果中心词和上下文词确实在语料库中，就最大化概率<script type="math/tex">P(D=1|w, c)</script> ，如果中心词和上下文词确实不在语料库中，就最大化概率 <script type="math/tex">P(D=0|w, c)</script>。我们对这两个概率用一个简单的极大似然估计的方法（这里把<script type="math/tex">\theta</script>作为模型的参数，在该例子中是<script type="math/tex">\mathcal{V}</script>和 <script type="math/tex">\mathcal{U}</script>）。</p><script type="math/tex; mode=display">\begin{aligned}\theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\ \\&=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)\end{aligned}  \tag{14}</script><p>注意：<strong>最大化似然函函数等价于最小化负对数似然</strong></p><script type="math/tex; mode=display">J = - \left( \underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)\right) \tag{15}</script><p>注：<script type="math/tex">\tilde D</script>是负语料。无意义的句子出现概率是很低的。我们可以从语料库中随机抽取负样例<script type="math/tex">\tilde D</script>。</p><p>对于<strong>Skip-Gram</strong> ，给定中心词<script type="math/tex">c</script>的观察的上下文词<script type="math/tex">c-m+j</script>，新的目标函数为:</p><script type="math/tex; mode=display">-\text{log} \ \sigma (u_{c-m+j}^T \cdot v_c) - \sum_{k=1}^{K} \sigma (- \tilde u_k^{T} \cdot v_c) \tag{16}</script><p>对于<strong>CBOW</strong> ，给定上下文词向量<script type="math/tex">\hat v = \frac{v_{c-m} + v_{c-m+1}+\cdots+v_{c+m}}{2m}</script>观察的中心词<script type="math/tex">u_c</script>的目标函数为：</p><script type="math/tex; mode=display">-\text{log} \ \sigma (u_{c}^T \cdot \hat v) - \sum_{k=1}^{K} \sigma (- \tilde u_k^{T} \cdot \hat v) \tag{17}</script><p><strong>Skip-Gram</strong>对应的softmax损失：</p><script type="math/tex; mode=display">- u_{c-m+j}^T \cdot v_c + \text{log} \sum_{k=1}^{|V|} \text{exp} ^{ u_k^{T} \cdot v_c} \tag{18}</script><p><strong>CBOW</strong>对应的softmax损失：</p><script type="math/tex; mode=display">- u_{c}^T \cdot \hat v+ \text{log} \sum_{j=1}^{|V|} \text{exp} ^{ u_j^{T} \cdot \hat c} \tag{19}</script><p>上面公式中， <script type="math/tex">\{ \tilde u | k = 1 \cdots K \}</script> 是从<script type="math/tex">P_n(w)</script>抽样得到的。实际上效果最好的是取<script type="math/tex">\frac{3}{4}</script>的Unigram model，论文中取值。</p><h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>同样是, Tomas Mikolov  的《Distributed Representations of Words and Phrases and their Compositionality》论文中提出的Hierarchical Softmax。实际上， Hierarchical Softmax对低频词表现更好，而负采样对高频词和低维向量表现更好。</p><p>Hierarchical Softmax用二叉树表示词汇表中的所有词。树的每一个叶子节点都是一个词，并且只有一条路径从根到叶子。在该模型中，没有词的输出表示。相反， 图的每个节点， 除了根节点和叶子节点， 都跟模型要学的向量相关。</p><p>在该模型中，给定向量<script type="math/tex">w_i</script> 单词<script type="math/tex">w</script>的概率，记为<script type="math/tex">P(w|w_i)</script>,等于从根结点开始到对应<script type="math/tex">w</script> 的叶结点的随机的概率。优势是时间复杂度变为<code>O(log(|V|))</code>,对应该路径的长度。</p><p>引入一些记号：</p><ul><li><p><script type="math/tex">L(w)</script>： 从根节点到叶子节点<script type="math/tex">w</script>的路径上的节点数目。下图， <script type="math/tex">w_2</script>对应3.</p></li><li><p><script type="math/tex">v_n(w, i)</script>：该路径上第i个节点向量， 即叶子节点<code>w</code>到跟节点路径的第<code>i</code>个节点。</p><p><script type="math/tex">n(w, 1)</script>是根节点， 而 <script type="math/tex">n(w, L(w))</script>是<code>w</code>的父节点。</p></li><li><p>对于每个内部节点<code>n</code>, 任意选择其中一个孩子节点称为<script type="math/tex">ch(n)</script>，总是左节点。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261749164.png" alt="图Hierarchical Softmax的二叉树" style="zoom:20%;" /></p><p>计算概率为:</p><script type="math/tex; mode=display">P(w \vert w_i) = \prod_{j=1}^{L(w)-1} \sigma \left( \left[ n(w, j+1)=ch(n(w, j))  \right] \cdot v_{n(w, j)^{T}v_{w_i}}\right) \tag{20}</script><p>这里，</p><script type="math/tex; mode=display">[x] = {\begin{cases}1,&{\mbox{if }} \ x \text{ is true}\\-1&{\mbox{if }}\ \ x \text{ is otherwise} \end{cases}} \tag{21}</script><p>式20是风采难懂的。</p><ul><li><p>基于从根节点<script type="math/tex">(n(w, 1))</script>到叶子节点<script type="math/tex">(w)</script>的累乘项。如果我们假设<script type="math/tex">ch(n)</script>总是左节点<script type="math/tex">n</script>, <script type="math/tex">[n(w， j+1)=ch(n(w, j))]</script>就返回1， 反之则是-1.</p></li><li><p><script type="math/tex">[n(w， j+1)=ch(n(w, j))]</script>提供标准化。在节点<script type="math/tex">n</script>上， 如果从左节点到右节点的概率求和，对于任意的<script type="math/tex">v_n^Tv_{w_i}</script>有：</p></li></ul><script type="math/tex; mode=display">\sigma(v_n^Tv_{w_i}) + \sigma (-v_n^Tv_{w_i}) = 1 \tag{22}</script><p>​            同时保证了<script type="math/tex">\sum_{w=1}^{|V|} P(w|w_i) = 1</script>， 跟原来的softmax一样。</p><ul><li>使用点积对比输入向量<script type="math/tex">v_{w_i}</script>和每个内部节点向量<script type="math/tex">v_{n(w, j)}^T</script>相似度。例如，上图 <script type="math/tex">w_2</script>, 我们必须要取两条左边和一条右边来从<script type="math/tex">w_2</script>到根节点，即：</li></ul><script type="math/tex; mode=display">P(w_2|w_i) = p(n(w_2|1), \text{ left}) \cdot p(n(w_2|2), \text{ left}) \cdotp(n(w_2|3), \text{ right}) \cdot \\\\= \sigma(v_{(w_2, 1)}^Tv_{w_i} \cdot \sigma(v_{(w_2, 2)}^Tv_{w_i}\cdot \sigma(-v_{(w_2, 3)}^Tv_{w_i}) \tag{23}</script><p>为了训练模型，我们的目标仍然是最小化负对数似然<script type="math/tex">-log \ P(w|w_i)</script>。但不是更新每个输出向量， 而是更新在二叉树上从根到叶子节点的节点向量。</p><p>该方法的速度取决于二叉树构建的方式和分配到叶子节点的单词。Mikolov 使用了哈夫曼树， 将高频词分配到树的最短路径上。</p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p>[1] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/readings/cs224n-2019-notes01-wordvecs1.pdf">cs224n-2019-notes01-wordvecs1</a></p><p>[2] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/slides/cs224n-2020-lecture01-wordvecs1.pdf">cs224n-2020-lecture01-wordvecs1</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/59228268">CS224n课程笔记翻译 词向量I: 简介, SVD和Word2Vec</a></p><p>[4] <a href="http://alexminnaar.com/2015/04/12/word2vec-tutorial-skipgram.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></p><p>[5] <a href="http://alexminnaar.com/2015/05/18/word2vec-tutorial-continuousbow.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> Word2vec </tag>
            
            <tag> Skip-Gram </tag>
            
            <tag> CBOW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11. BERT 论文笔记</title>
      <link href="2020/11/29/NLP%20Paper%2011.%20BERT%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/11/29/NLP%20Paper%2011.%20BERT%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="11-BERT-论文笔记"><a href="#11-BERT-论文笔记" class="headerlink" title="11.  BERT 论文笔记"></a>11.  BERT 论文笔记</h3><p>​    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ，一种从Transformers模型得来的双向编码表征模型。</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>​    BERT: 是Bidirectional Encoder Representations from Transformers 的缩写。</p><p>特点：</p><ul><li>不像其它模型，BERT是基于所有层的左右上下文来预训练来自未标定文本的深层双向表示向量。</li><li>基于BERT得到的向量，加上一个输出层，微调下就能得到很多任务的最优模型，如：问答和语言推理，无需对特定任务架构做大量的修改。</li></ul><p>成绩：刷了11个语言任务的榜单， 包括一下：</p><blockquote><ul><li>pushing the GLUE score to 80.5% (7.7% point absolute improvement),</li><li>MultiNLI accuracy to 86.7% (4.6% absolute improvement), </li><li>SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement)</li><li>and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</li></ul></blockquote><h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><p>​    预训练语言模型已被证明对许多自然语言处理任务能有效提升。其包括语言推理和释义paraphrasing，</p><blockquote><ul><li><p><strong>token-level task</strong>: token级别的任务. 如<strong>完形填空</strong>(Cloze), 预测句子中某个位置的单词; 或者<strong>实体识别</strong>; 或是<strong>词性标注</strong>; <strong>SQuAD</strong>等.</p></li><li><p><strong>sequence-level task</strong>: 序列级别的任务, 也可以理解为句子级别的任务. 如<strong>情感分类</strong>等各种句子分类问题; 推断两个句子的是否是同义等.  </p><p>​                                                                                                                                                                                                               ——<a href="https://www.cnblogs.com/databingo/p/10182663.html">NLP任务</a></p></li></ul></blockquote><p>将<strong>预训练语言表征</strong>应用于下游任务有两种现有策略：基于特征和微调。</p><p>基于特征的方法：如ELMo， 使用特定架构包括预训练表示作为附加特征。</p><p>微调方法：如GPT , 引入最小的特定任务参数，并通过简单微调所有预训练参数在下游任务上进行训练。</p><p>在预训练中，两种方法共享相同的目标函数，它们使用<strong>单向语言模型</strong>来学习<strong>通用语言表示特征</strong>。</p><p>作者认为现在技术严重制约了预训练表示的威力，特别是微调方法。主要限制在于标准语言模型是单向的，这限制了在预训练期间可以使用的架构。例如， 在OpenAI GPT中，作者使用了从左到右的架构，其中每个token仅仅只能注意到Transformer中self-attention层的之前token。这种局限对于句子级别任务是次要的，但是对于token-level级别的任务是毁灭性的，当微调方法应用于token-level任务，如QA，结合两个方向的上下文至关重要。</p><p>在本文中，作者提出基于微调方法的<strong>BERT</strong>： <strong>Bidirectional Encoder Representions from Transformers.</strong></p><p>通过使用“掩码语言模型” MLM(masked language model) 预训练目标，BERT 缓解了之前方法单向限制。MLM随机掩码一些来自输入的tokens，目标是只看其上下文来预测掩码词在原来词汇表中的id。不像从左到右的预训练语言模型，MLM目标能够表示融合左右上下文，这允许作者预训练一个深层双向Transformer.除了MLM, 作者还使用“预测下一句”任务来联合预训练文本表示向量。</p><p>本文贡献如下：</p><ul><li>证明了双向预训练对语言表示的重要性。</li><li>展示了预训练表示能减少了对于许多繁重工程任务特定架构的需要。BERT首次在大型句子级和词汇级任务上，微调基于表示模型就取得了SOTA表现，并且优于许多特定任务的架构</li><li>BERT打破了11项NLP任务的最佳纪录</li></ul><h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h4><h5 id="2-1-基于特征的非监督方法"><a href="#2-1-基于特征的非监督方法" class="headerlink" title="2.1 基于特征的非监督方法"></a>2.1 基于特征的非监督方法</h5><p>​    学习广泛合适的词表示已经成为近十年来活跃领域，包括非神经网络和神经网络方法。预训练词嵌入是现代NLP系统的必不可少的部分，比从零开始嵌入训练提供显著的改进。为了预训练词嵌入向量，从左到右语言模型目标已经被使用，从左到右上下文中判别词的正确与否。</p><p>这些方法已经被推广到更广的粒度，像句子嵌入或段落嵌入。为了训练句子表示，先前工作把对候选下一个句子排序作为目标，从左到右给定前面句子的表示生成下一个句子词汇，或去噪自编码派生目标。</p><p>ELMo和其前身沿着不同维度生成传统词嵌入。它们从左到右和从右到左语言模型提取“文本敏感”的特征。每个字符上下文表示是从左到右和从右到左表示合并。当合并上下文词嵌入和已有的特定任务架构时，ELMo推动了几个主要NLP基准的最新进展，包括QA，情感分析，命名实体识别。</p><h5 id="2-2-非监督微调方法"><a href="#2-2-非监督微调方法" class="headerlink" title="2.2 非监督微调方法"></a>2.2 非监督微调方法</h5><p>​    与基于特征方法一样，这个方向第一个工作是只从未标记文本预训练词嵌入参数。</p><p>最近，句子和文档编码生成上下文字符表示的编码器已经被从未标记文本中预训练得到， 并且微调给监督学习的下游任务使用。这些方法的优势是很少需要从零开始学习参数。至少部分归于这个优点，OpenAI GPT 之前在许多来自GLUE基准测试的句子级别任务获得了最佳成绩。从左到右语言模型和自动编码器目标已经被用于预训练这些模型。</p><h5 id="2-3-来自监督数据的迁移学习"><a href="#2-3-来自监督数据的迁移学习" class="headerlink" title="2.3 来自监督数据的迁移学习"></a>2.3 来自监督数据的迁移学习</h5><p>​    许多工作表明从大型数据集的监督任务上做迁移学习是十分有效的，像语言推理和机器翻译。机器视觉研究也证明从预训练模型上使用迁移学习的重要性，一个有效的技巧是微调预训练在ImageNet上的模型。</p><h4 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3. BERT"></a>3. BERT</h4><p>​    本节介绍BERT和其详细实现。使用BERT框架有有两个步骤：pre-training 和fine-tuning. 在预训练阶段，模型在不同任务的未标记数据上预训练。对于微调，BERT模型首次用预训练好的参数初始化，并且所有参数是使用基于下游任务标记的数据来微调。每个下游任务有独立的微调模型，尽管它们用同样的预训练参数初始化。图1中，QA例子作为本节运行的一个示例。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843624.png" alt="image-20210427135552257" style="zoom:30%;" /></p><p>注：</p><ul><li>[CLS] 是添加在每个输入示例之前的特殊符号， </li><li>[SEP]是分隔字符，比如分隔问题/答案</li></ul><p>一个BERT的区分特征是它的跨任务统一架构。其在预训练和最终下游任务的区分特别小。</p><p><strong>模型架构</strong></p><p>BERT模型架构是多层双向基于原本实现的Transformer编码器，及其发布的tensor2tensor库(现在已遗弃)。因为使用Transformers已经变得普遍和作者的实现和原本的Transformer几乎一样，就不详述了,可以看原文和 <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>。(本博客也有Transformer笔记)。</p><p>这里注释下标记的意思：</p><ul><li>L：是层数，即Transformer 块的数目</li><li>H：隐藏单元的维数</li><li>A: 自注意头数</li></ul><p>比如：$ \text{BERT}_{\text{BASE}} $代表 L=12,  H=768, A=12总共参数为110M</p><p>​            $ \text{BERT}_{\text{LARGE}} $代表 L=24  H=1024, A=16总共参数为340M</p><p>$ \text{BERT}_{\text{BASE}} $设定和OpenAI GPT模型大小相同， 以便做比较。Transformer使用双向self-attention，而GPT使用限制的自注意，每个token仅注意它左边上下文。</p><p><strong>输入/输出表示</strong></p><p>为了是BERT处理多种下游任务，作者输入表示在一个字符序列上能任意表示单一句子和一对句子。通过这个工作，一个句子可以是任意长度的连续文本，而不是实际语义上句子。一个句子指输入到BERT的token序列，其可能是单一的句子或者是两个句子打包在一起。</p><p>作者使用30，000词汇表的WordPiece embeddings。每个序列第一个token都是一个特殊的token ([CLS]). 与此标记对应的最终隐藏状态用作分类任务聚合序列的表示。句子对被打包成一个序列，作者用两种方法区分句子。</p><ol><li>用特殊字符([SEP])分开这个句子对</li><li>增加一个可学习的嵌入到每个词中，用来区分该词属于句子A还是句子B</li></ol><p>如上图1所示，$ \boldsymbol{E} $表示输入嵌入，$ \boldsymbol{C} \in \mathbb{R}^H $表示[CLS]token的最终隐藏向量，第$i $</p><p>个输入词的最终隐藏向量是$\boldsymbol{T}_i \in \mathbb{R}^H $.</p><p>对于给定token，其输入表示由对应token，片段, 和位置嵌入构成,可视化其构成如下图2. 具体来说，就是[CLS]表示下一句是否为上一句真正的下一句(可以随机从语料中抽一个句子作为下一句)，这就是BERT第一个任务预测下一句。[SEP]表示分割token。整个BERT输入由3部分叠加：</p><ul><li>Token Embedding: 将序列打断称为词或更小单位的token</li><li>Segment Embedding: 分隔两个句子，用来Next sentence predict,如下图中句子A和B之间用[SEP]分隔</li><li>Position Embedding：不再用原始的sin/cos​函数，而是学习得到。</li></ul><p>例如 <a href="https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/bert.py">Pytorch BERT 实现</a>中，BERT Embedding就是三者相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTEmbedding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    BERT Embedding which is consisted with under features</span></span><br><span class="line"><span class="string">        1. TokenEmbedding : normal embedding matrix</span></span><br><span class="line"><span class="string">        2. PositionalEmbedding : adding positional information using sin, cos</span></span><br><span class="line"><span class="string">        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)</span></span><br><span class="line"><span class="string">        sum of all these features are output of BERTEmbedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size</span></span><br><span class="line"><span class="string">        :param embed_size: embedding size of token embedding</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)</span><br><span class="line">        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)</span><br><span class="line">        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sequence, segment_label</span>):</span></span><br><span class="line">        <span class="comment">#embedding层 就是 Token+Segment+Position</span></span><br><span class="line">        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843625.png" alt="image-20210427172624924" style="zoom:30%;" /></p><blockquote><p>如上所示，输入有 A 句「my dog is cute」和 B 句「he likes playing」这两个自然句，作者首先需要将每个单词及特殊符号都转化为词嵌入向量，因为神经网络只能进行数值计算。其中特殊符 [SEP] 是用于分割两个句子的符号，前面半句会加上分割编码 A，后半句会加上分割编码 B。</p><p>​                                                                                                                                                                                                                    ——引用[7]</p></blockquote><h5 id="3-1-预训练BERT"><a href="#3-1-预训练BERT" class="headerlink" title="3.1 预训练BERT"></a>3.1 预训练BERT</h5><p>​    不像Peter等，作者不使用传统从左到右或者从右到左语言模型来预训练BERT。相反，作者预训练BERT使用两个无监督学习任务，这节会描述。这部分如图1的左半部分。</p><p><strong>任务 # 1</strong>： Masked LM   直觉上，有理由详细双向模型完全比从左到右模型或者浅层连合从左到右和从右到左的模型更有效果。不幸的是，标准条件语言模型仅能从左到右或者从右到左训练，因此双向状态会让每个词直接看到自己(本身是将其作为标签的无监督训练，这样一来就出现了标签泄漏)，并且模型可以在多层上下文中毫无顾虑地预测目标词。</p><blockquote><p>standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly<br>“see itself”, and the model could trivially predict the target word in a multi-layered context.</p></blockquote><p>为了训练深层双向表示，作者简单地随机掩码一些百分比的输入，然后预测这些掩码字符。作者定义这个步骤为MLM，尽管其在文献里经常被定义为完形任务。在本例，遍历整个词汇表，对应掩码字符的最终隐藏状态向量被喂进一个输出softmax，像标准LM一样。在实验中，随机的在每个序列上，掩码15%的所有WordPiece 字符。对比降噪自编码器，作者仅预测掩码词而不是重构整个输入。</p><p>尽管这允许作者获得双向预训练模型，其带来的负面影响是在预训练和微调之间造成不匹配，因为[MASK]字符不经常出现在微调中。为了缓解这个问题，作者不总是替换“掩码”词为[MASK]字符，而是训练数据生成器随机选择15%的token位置来预测。假如第i个字符被选中，作者有80%情况下是用[MASK]替换，10%的情况下用一个随机字符替换，还有10%情况下不变。然后<script type="math/tex">\boldsymbol{T}_i</script>(第i个词的最终隐藏向量)将用来采用交叉熵损失来预测原本字符。如附录C.2。</p><blockquote><p>例如，在my dog is hairy 这句话中，它选择 hairy。然后执行以下步骤:</p><ul><li><p>数据生成不会总是用 [MASK] 替换被选择的单词，而是执行以下操作:</p></li><li><p>80% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</p></li><li><p>10% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</p></li><li><p>10% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy。这样做的目的是使表示偏向于实际观察到的词。</p><p>​                                                                                                                                                                                                        ——<a href="https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation/blob/master/Bidirectional_Encoder_Representations_Transformers%E7%BF%BB%E8%AF%91.md">BERT翻译</a></p></li></ul></blockquote><p><strong>任务 # 2</strong>：下一个句子预测任务(Next Sentence Prediction NSP) </p><p>许多重要的下游任务如QA和NLI都基于理解两个句子的理解，但是这不是直接被模型捕获到的。为了训练模型理解句子关系，作者预训练了一个二元的下一个句子预测任务，能从任何单语语料中简单生成。具体来说，对于每个预训练例子选择句子A和B时，50%的时候句子B是句子A的真正下一句(标记为IsNext), 而另外50%的句子B是从语料中随机抽取的句子(标记为NotNext).如图1所示， C被用来预测下一句(向量C没有微调的话不是一个有意义的句子表示)。尽管其是简单的，在5.1中作者证明，增加这个任务预训练对QA和NLI都有比较好的效果。尽管先前的工作，只有句子嵌入被迁移到下游任务，BERT迁移所有参数来初始化最终任务模型参数。</p><p><strong>预训练数据</strong> </p><p>预训练过程很大程度上遵循已有语言模型预训练文献。预训练语料使用了BooksCorpus(800M 词汇)和英文维基百科(2500M 词汇)。对于维基，作者只提取文本信息，忽略列表和表格以及标题。为了提取长连续序列，关键是使用文档级别语料库，而不是无序句子级别语料。</p><h5 id="3-2-微调BERT"><a href="#3-2-微调BERT" class="headerlink" title="3.2 微调BERT"></a>3.2 微调BERT</h5><p>​        微调是简单的因为Transformer的自注意里机制允许BERT通过交换合适的输入和输出来为许多下游任务建模——无论是单个文本还是文本对。对于涉及文本对的应用，常见模式是在应用双向交叉注意力之前，独立地编码文本对。BERT取代其使用自注意里机制来统一这两个阶段，当用自注意力有效地编码串联的文本对，这就包含了两个句子的双向交叉注意力。</p><p>对于每个任务，作者简单地连接特定任务的输入和输出喂进BERT，端到端微调所有参数。</p><ul><li>在输入中，句子A和句子B从预训练类似于(1)在段落中的句子对，(2)蕴含中的前提-假设对，(3)问题回答任务中的问题-文章对，(4)文本分类中的text-∅对。</li><li>在输出中，对于字符级别任务，字符表示被喂进到输出层，例如序列标注或QA，而为了分类[CLS]表示被喂进输出层，如需求或情感分析。</li></ul><p>对比预训练，微调相对代价小些。论文所有结果，从相同的预训练模型开始，在TPU大概1小时能被复现，或者在GPU上几小时复现。作者在小节4中描述具体任务细节，在附录A.5可以找到更多细节。</p><h4 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h4><p>​    本节展示了BERT在11项具体微调任务微调结果。</p><h5 id="4-1-GLUE"><a href="#4-1-GLUE" class="headerlink" title="4.1 GLUE"></a>4.1 GLUE</h5><p>​    通用语言理解估计(General Language Understanding Evaluation : GLUE)基准是多种语言理解任务。GLUE数据集详细描述在附录B.1.</p><p>GLUE上的微调，作者表示输入句子(句子或句子对)如第3部分描述的，用最后的隐藏向量$ C \in \mathbb{R}^H $ 对应第一个输入字符([CLS]) 作为总的表示。在微调时，只有分类层的权重$ W \in \mathbb{R}^{K\times H} $  新参数引入，这里K是标签的数目。作者用C和W来计算标准分类损失如（<script type="math/tex">\text{softmax }(CW^T）</script>)。</p><ul><li>作者使用batch_size大小为32，遍历所有数据，为所有GLUE任务微调3轮。对于每个任务，作者在验证集上选择最佳微调学习率[ 在5e-5, 4e-5, 3e-5, 和2e-5之间 ]。</li><li>另外，对于  <script type="math/tex">\text{BERT}_{\text{LARGE}}</script> ，作者发现在小数据集上微调有时不稳定，所以随机重新开始跑几次，选择在验证集上最好的模型。对于随机重启，作者使用一样的预训练节点，但执行不同的微调打乱数据并且分类层初始化。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843626.png" alt="image-20210428004212040" style="zoom:30%;" /></p><p>结果如上表1所示，在所有任务上，通过巨大的界限所有的BERT都优于现有系统，与之前最佳水平相比，获得对应的平均准确性4.5%和7.0%的提升。</p><p>注意<script type="math/tex">\text{BERT}_{\text{BASE}}</script>和OpenAI GPT的模型结构几乎相同，区分在于使用了掩码注意力。对于大型和最广泛使用的GLUE任务 MNLI， BERT比当前模型取得了4.6%的绝对准确性提升。在GLUE官方排行榜，<script type="math/tex">\text{BERT}_{\text{LARGE}}</script>取得了80.5分数，对比OpenAI GPT相比，截止本文写作时只取得了72.8.</p><p>作者发现<script type="math/tex">\text{BERT}_{\text{LARGE}}</script> 在所有任务上显著优于<script type="math/tex">\text{BERT}_{\text{LARGE}}</script>，尤其是小训练集。更多模型大小影响探索在5.2节。</p><h5 id="4-2-SQuAD-v1-1"><a href="#4-2-SQuAD-v1-1" class="headerlink" title="4.2 SQuAD v1.1"></a>4.2 SQuAD v1.1</h5><p>​    Stanford Question Answering Dataset (SQuAD v1.1) 是一个由100k个一堆问题/答案对构成的数据集。给定一个问题和一段包含答案的维基段落，任务是预测这段答案的区间。</p><p>如图1所示，QA任务，作者表示输入问题和段落打包成一个序列，对于问题用A的嵌入，段落用B的嵌入(图1中)。作者在微调时只引入一个开始向量$S \in \mathbb{R}^H$和结束向量$ E \in \mathbb{R}^H$ 。作为答案开始区间单词$ i $的概率是由$T_i$和S之间的点积并除以段落中所有词结果再过softmax：</p><script type="math/tex; mode=display">P_i=\dfrac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}</script><p>同样的式子用来计算单词作为答案区间结束的概率。作为候选区间位置i到位置j定义为：$S \cdot T_i + E \cdot T_j$，并将满足$j \ge i $最大分数作为预测结果。训练目标是正确答案开始和结束位置的对数似然的和。微调3轮，学习率为5e-5, batch size为32.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843627.png" alt="image-20210428200949428" style="zoom:30%;" /></p><blockquote><p>表2展示了问答任务中的排名前几位的模型，因此，作者使用适度的数据增强，首先对TriviaQA进行微调，然后再对SQuAD进行微调。 BERT在集成模型上比排名第一的系统高出+1.5 F1，在单个系统上比排名第一的系统高出+1.3 F1</p></blockquote><h5 id="4-3-SQuAD-v2-0"><a href="#4-3-SQuAD-v2-0" class="headerlink" title="4.3 SQuAD v2.0"></a>4.3 SQuAD v2.0</h5><p>​    SQuAD 2.0任务拓展了 v1.1的问题定义，允许在提供的段落中没有短答案存在的可能性，使得问题更合理。</p><p>使用简单方法来拓展v1.1 BERT 模型来完成这个任务。将没有答案的问题看作其答案开始和结束都在[CLS]字符。 在预测是，作者比较无答案区间分数: $ S \cdot C + E \cdot C $  和 最好的无答案区间<script type="math/tex">s_{\hat {i, j}}  = \text{max}_{j \ge i} S \cdot T_i + E \cdot T_j</script>。</p><p>当 <script type="math/tex">s_{\hat {i, j}} \gt s_{null} + \tau</script>时，作者预测没有答案，这里$ \tau $阈值在验证集上选择使得F1最大化。作者这个模型不使用TriviaQA数据。微调两轮，学习率为5e-5, batch size为48.</p><p>结果和之前排行榜比较如表3，包括将BERT作为其中组件的系统。作者发现+5.1 F1改进比之前最佳系统。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843628.png" alt="image-20210428213327589" style="zoom:30%;" /></p><h5 id="4-4-SWAG"><a href="#4-4-SWAG" class="headerlink" title="4.4 SWAG"></a>4.4 SWAG</h5><p>​    对抗生成情形The Situations With Adversarial Generations (SWAG)，数据集包含113k 句子对完整实例，用于评估常识推理。给定一个句子，任务是在四个选择中选最合理的延续。</p><p>当在SWAG上微调时，构建四个输入句子，每个包含给定句子串(句子A)和可能的延续(句子B)。引入特定任务唯一的参数是一个向量，它与[CLS]字符做点积得到表征C表示每个选择的分数，该分数再过softmax层归一化。</p><p>微调模型3轮，学习率为2e-5，batch size=16，结果在表4。<script type="math/tex">\text{BERT}_{\text{LARGE}}</script> 优于作者baseline ESIM+ELMo系统有+27.1%, OpenAI GPT 8.3% 。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843629.png" alt="image-20210428235933430" style="zoom:30%;" /></p><h4 id="5-消融研究"><a href="#5-消融研究" class="headerlink" title="5. 消融研究"></a>5. 消融研究</h4><p>​    在本节, 在BERT的许多方面执行消融实验，来理解BERT各个部分的相对重要性。消融研究在附录C可以被找到。</p><h5 id="5-1-预训练任务的影响"><a href="#5-1-预训练任务的影响" class="headerlink" title="5.1 预训练任务的影响"></a>5.1 预训练任务的影响</h5><p>​    通过评估两个预训练目标来证明BERT的深层双向重要性，它们使用同样的预训练数据，微调方案和跟<script type="math/tex">\text{BERT}_{\text{BASE}}</script> 超参数。</p><p><strong>NO NSP</strong>： 双向模型，使用“掩码 语言模型”(MLM)训练，但是没有预测下一句任务。</p><p><strong>LTR&amp;No NSP</strong> :一个只注意左边上下文的模型，使用标准的从左到右语言模型训练，而不是MLM。只注意左边的约束也应用到微调，因为没有它，引入到预训练/微调不匹配会退化下游任务表现。 另外，这个模型是没有NSP任务的预训练。这能直接跟OpenAI GPT对比，但使用作者的大型训练数据，输入表示和微调方案。</p><p>首先分析NSP任务带来的影响。在表5中，展示了移除nSP对QNLI， MNLI和SquAD的表现造成显著的不良影响。接下来，比较“No NSP”和“LTR&amp;No NSP”来评估训练双向表示方面的影响。在所有任务上，LTR模型表现比MLM差，在MRPC和SQuAD退化很大。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843630.png" alt="image-20210429005634711" style="zoom:30%;" /></p><p>对于SQuAD,直觉上LTR模型将在字符预测上表现差劲,因为token-level隐状态没有右边的上下文。为了对增强LTR系统做出最大的尝试，在BiLSTM的顶部进行随机初始化。在SQuAD上有显著的提升，但结果仍然比预训练的双向模型要差。BiLSTM在GLUE任务上是有害的。</p><p>安排分开的LTR和RTL模型预训练，然后两个方法表示每个字符并连接起来，就像ELMo一样。然而：</p><ul><li>(a) 比单向模型参数多2倍</li><li>(b) 对于QA这样的任务来说是不直观的，因为RTL模型没有以问题为条件作为打哪</li><li>(c) 这比深层双向模型要弱得多，因为其可以在每层选择使用左边右边上下文</li></ul><h5 id="5-2-Effect-of-Model-Size"><a href="#5-2-Effect-of-Model-Size" class="headerlink" title="5.2 Effect of Model Size"></a>5.2 Effect of Model Size</h5><p>​    在本节，探索模型大小在微调任务上准确性的影响。用不同层数、不同隐藏单元、不同注意力头训练BERT模型，但其有跟前面描述的同样的超参数和训练流程。</p><p>在选择的GLUE任务上结果如表6所示。表中，从5个随机重启点微调记录平均验证集准确性。可以看到大模型在四个数据集都有明显的准确性提升，即使MRPC（Microsoft Research Paraphrase Corpus）仅有3600个标记的训练样本，其显著不同于预训练任务。也许令人惊讶的是，相对于现有文献中模型，能在现有基础上取得如此显著的提升。例如，最大Transformer探究是(L=6, H=1024, A=16)，100M参数的encoder。文献中找到的最大Transformer是(L=64, H=512, A=2)使用235M 参数。相比之下，<script type="math/tex">\text{BERT}_{\text{BASE}}</script> 含有110M参数，和<script type="math/tex">\text{BERT}_{\text{LARGE}}</script>含有340M参数。</p><p>多年来人们知道增大模型大小将持续提升在大型任务上的表现，如机翻和语言建模，由表6所示的留存训练数据的LM困惑度可以证明。然而，作者相信，这是第一次证明，提供充分的预训练，将模型提升到极端规模也能在小任务上取得提升。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843631.png" alt="image-20210429152541635" style="zoom:30%;" /></p><p>Peters (2018b) 表示在下游任务混合结果，将受增加预训练双向bi-LM从两层到四层的影响，并且 Melamud (2016)提到增加隐藏层维度从200到600也是有益的，但再增加到1000不会带来提升。所有先前的工作都基于特征方法——作者假设当模型微调后直接在下游任务是要，并且其使用非常骚随机初始化附加参数，特定任务模型能从大型更昂贵的预训练表示上获益，即使下游任务数据集很小。</p><h5 id="5-3-基于特征方法使用BERT"><a href="#5-3-基于特征方法使用BERT" class="headerlink" title="5.3 基于特征方法使用BERT"></a>5.3 基于特征方法使用BERT</h5><p>​    到现在为止所有BERT结果都是使用微调方法，将一个简单分类层加到预训练模型中，并且所有参数都在下游任务上联合训练。然而，基于特征方法，从预训练模型提取固定特征，有一定的优点。</p><ul><li>首先，不是所有任务都是容易被Transformer 编码器架构表示，因此特需要加上定任务模型架构。</li><li>其次，预先计算一个昂贵的训练数据表示，然后在这种表示上运行多种更便宜模型的许多实验，还是有计算优势的。</li></ul><p>在本节，在CoNLL-2003命名实体识别任务上用BERT方法比较了两种方法。BERT输入中，使用保留大小写的WordPiec模型，并且包括通过数据提供的最大文档上下文。遵循标准实践，将其表示为标记任务，但在输出中不使用CRF层。作者使用第一个子字符的表示作为字符级别分类器遍历NER标签集的输入。</p><p>为了切除这种微调方法，使用基于特征的方法，从一层或多层中提取激活信息，而不用微调BERT任何参数。上下文嵌入被用作输入，来随机初始化在分类层前面的两层768维BiLSTM。</p><p>结果如表7所示。$ \text{BERT}_{\text{LARGE}} $ 跟最新方法表现的有竞争力。最好表现的方法连接了预训练Transformer顶部四个隐藏层字符表示，这比整个模型微调值只落后的0.3 F1。证明了对于BERT微调和基于特征的方法都是有效的。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261843632.png" alt="image-20210429173034443" style="zoom:30%;" /></p><h4 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h4><p>​    最近，归功于用语言模型迁移学习实验表现提升已经证明了，丰富的、无监督预训练是许多语言理解系统的不可分割的一部分。实际上，这些结果允许低资源任务从双向架构中获益。本文主要贡献是进一步推广这些发现到深层双向架构，允许同样预训练模型来处理广泛的NLP任务。</p><p>​    至于附录部分，就不翻译整理了，引用[3]比较详细。</p><h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p>[1] <a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co</a></p><p>[2] <a href="https://www.kaggle.com/tientd95/bert-model-for-anwsering-toeic-reading-test/comments#965336">BERT model for anwsering TOEIC reading test</a></p><p>[3] <a href="https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation/blob/master/Bidirectional_Encoder_Representations_Transformers%E7%BF%BB%E8%AF%91.md">BERT：预训练的深度双向 Transformer 语言模型</a></p><p>[4] <a href="https://www.jianshu.com/p/54347ade7c6c">Bert 论文阅读笔记</a></p><p>[5] <a href="https://www.cnblogs.com/guoyaohua/p/bert.html">BERT：语言理解的深度双向变换器预训练</a></p><p>[6] <a href="https://zhuanlan.zhihu.com/p/126200398">BERT论文解读</a></p><p>[7] <a href="https://mp.weixin.qq.com/s/vFdm-UHns7Nhbmdoiu6jWg">谷歌终于开源BERT代码：3 亿参数量，机器之心全面解读</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansformer </tag>
            
            <tag> BERT </tag>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10.2 Transformer 代码详解</title>
      <link href="2020/11/20/NLP%20Paper%2010.2%20Transformer_code/"/>
      <url>2020/11/20/NLP%20Paper%2010.2%20Transformer_code/</url>
      
        <content type="html"><![CDATA[<h2 id="10-2-Transformer-代码详解"><a href="#10-2-Transformer-代码详解" class="headerlink" title="10. 2 Transformer 代码详解"></a>10. 2 Transformer 代码详解</h2><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261834555.png" alt="Snipaste_2021-03-25_05-41-39" style="zoom:40%;" /></p><p>按照上图结构， 如果作者需要Encoder和Decoder这个主干结构，作者需要实现encoder和decoder最主要两个结构。</p><p>而encoder 和 decoder两个部分主要由：</p><ol><li>positional encoding</li><li>pad mask</li><li>ScaledDotProductAttention</li><li>MultiHeadAttention</li><li>PoswiseFeedForwardNet</li></ol><p>因此，本文逻辑为，</p><ol><li>每个子组件实现</li><li>实现encoder layer 和 decoder layer</li><li>实现Transformer</li></ol><p>所有代码来自于 <a href="https://github.com/graykode/nlp-tutorial/">nlp-tutorial</a>中的<strong>5-1.Transformer</strong>。</p><h3 id="1-每个子组件实现"><a href="#1-每个子组件实现" class="headerlink" title="1. 每个子组件实现"></a>1. 每个子组件实现</h3><h4 id="1-positional-encoding"><a href="#1-positional-encoding" class="headerlink" title="1.positional encoding"></a>1.positional encoding</h4><p>multi-head self-attention 机制抛弃RNNs架构，使得模型能够并行计算，能获取句子中长矩依赖信息。但是当每个句子同步流过Transformer encoder和decoder结构时，模型没有了每个词的任何关于位置、顺序的信息。但Transformer又需要词的顺序——位置信息。</p><p>为了添加位置信息，论文中是引入位置编码。</p><blockquote><p>we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. There are many choices of positional encodings,learned and fixed。</p></blockquote><p>然后给出了位置编码公式：</p><script type="math/tex; mode=display">\begin{align}PE(pos,2i)&=sin(\frac{pos}{10000^{2i/{d_{model}}}})\\ \\PE(pos,2i+1)&=cos(\frac{pos}{10000^{2i/{d_{model}}}})\end{align}</script><p>为了简化计算做一点小的调整：</p><script type="math/tex; mode=display">\frac{1}{10000^{2i/{d_{model}}}} = e^{-\text{log}10000^{2i/{d_{model}}}}= e^{-2i/{d_{model }}\log(10000)} = e^{2i (-\frac{\text{log}(10000)}{d_{\text{ model}}})}</script><p>这就是<code>div_term</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;实现位置编码，将公式编码得到的位置信息加到词嵌入&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout) </span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model) <span class="comment">#position encoding 位置编码 d_model = 512 </span></span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment">#这里改为0.因为版本原因</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment">#令偶数列列等于sin(位置变量)</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment">#令奇数列列等于cos(位置变量)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment">#在0列前插入以个维度，交换0， 1维度</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :] <span class="comment">#将pe加到x上得到词嵌入和pe的相加向量</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h4 id="2-mask"><a href="#2-mask" class="headerlink" title="2.  mask"></a>2.  mask</h4><ol><li><strong>padding mask</strong></li></ol><p>在模型中，一般是以batch形式输入，那么每个句子不一样时要补到最大长度。一般是0，但是这种填充位置信息是没意义的，即影响计算效率，又影响模型效果。</p><p>要避免这种影响，最后把softmax后的分数置为0。</p><script type="math/tex; mode=display">\text{softmax} = \frac{e^i}{\sum_{i=1}^{N}e^i}</script><p>如上公式中，只能将分子部分，就是将 $ i = -\infty $, 这里取 $ 10^{-9}$.</p><p>padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是作者要进行处理的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    实现pad mask</span></span><br><span class="line"><span class="string">    seq_q: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_len could be src_len or it could be tgt_len</span></span><br><span class="line"><span class="string">    seq_len in seq_q and seq_len in seq_k maybe not equal</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero) is PAD token</span></span><br><span class="line">    <span class="comment"># seq_k.data.eq(0) [batch_size, len_k]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, len_k], False is masked</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># [batch_size, len_q, len_k]</span></span><br></pre></td></tr></table></figure><ol><li><strong>Sequence mask</strong></li></ol><blockquote><p>sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，作者的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此作者需要想一个办法，把 t 之后的信息给隐藏起来。</p><p>那么具体怎么做呢？就是：<strong>产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到作者的目的</strong>。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    seq: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>) <span class="comment"># Upper triangular matrix</span></span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br></pre></td></tr></table></figure><p>实际效果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.triu(np.ones((<span class="number">2</span>, <span class="number">3</span>)), k=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">seq = torch.randn((<span class="number">2</span>, <span class="number">8</span>, <span class="number">8</span>), dtype=torch.float32)</span><br><span class="line">sub_seq_mask = get_attn_subsequence_mask(seq)</span><br><span class="line"><span class="built_in">print</span>(sub_seq_mask)</span><br><span class="line">===============================================================</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">tensor([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]], dtype=torch.uint8)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注：</p><blockquote><ul><li>对于 decoder 的 self-attention，要避免decoder后面信息，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。</li><li>其他情况，attn_mask 一律等于 padding mask。</li></ul></blockquote><h4 id="3-ScaledDotProductAttention"><a href="#3-ScaledDotProductAttention" class="headerlink" title="3.ScaledDotProductAttention"></a>3.ScaledDotProductAttention</h4><p>放缩点乘注意力机制如下图，具体公式如下：最后得到<code>context</code>向量和<code>attn</code>分数。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261834556.png" alt="image-20210325174036961" style="zoom:30%;" /></p><script type="math/tex; mode=display">\text{Attention } (Q, K, V) = \text{softmax } (\frac{QK^T}{\sqrt d_k}) V</script><p>除以$ d_k $是因为当QK乘积后会非常大，这能起到一定规模缩小的作用，不进入softmax梯度非常小区域<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment">#按照公式写出softmax里的部分， 因为K最后两个维度是 len_k, d_k交换下和Q做点乘</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) <span class="comment"># scores : [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment">#填充mask为0部分为-1e9</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>) <span class="comment"># Fills elements of self tensor with value where mask is True.</span></span><br><span class="line">        </span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)</span><br><span class="line">        context = torch.matmul(attn, V) <span class="comment"># [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure></p><h4 id="4-MultiHeadAttention"><a href="#4-MultiHeadAttention" class="headerlink" title="4. MultiHeadAttention"></a>4. MultiHeadAttention</h4><p>如果作者只计算一个attention，很难捕捉输入句中所有空间的讯息，为了优化模型，论文当中提出了一个新颖的做法：Multi-head attention。</p><p>文中8个注意力头，即把k， q， v投影到8个不同空间去，如$kW^{k_1}$等。注意，多头自注意力也引入了残差和layerNorm.</p><p>即:</p><script type="math/tex; mode=display">\text{LayerNorm } (\text{ MultiHeadAttention }(x) + x)</script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261834558.png" alt="image-20210325180723223" style="zoom:33%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>) [<span class="number">512</span>, <span class="number">64</span>*<span class="number">8</span>]</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        input_Q: [batch_size, len_q, d_model]</span></span><br><span class="line"><span class="string">        input_K: [batch_size, len_k, d_model]</span></span><br><span class="line"><span class="string">        input_V: [batch_size, len_v(=len_k), d_model]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment">#加入残差</span></span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D_new) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        <span class="comment"># input_Q * W_Q 得到Q矩阵，因为形状是[batch_size, len_q, dk*n_heads]，转换得到[batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#attention mask </span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># attn_mask : [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, n_heads * d_v) <span class="comment"># context: [batch_size, len_q, n_heads * d_v]</span></span><br><span class="line">        output = self.fc(context) <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual), attn</span><br></pre></td></tr></table></figure><p>这里还有份非常详细解释实现的pytorch实验代码，帮助理解。</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">query = torch.rand(<span class="number">12</span>, <span class="number">64</span>, <span class="number">300</span>) <span class="comment">#batch_size为64， 每个batch为12个词， query向量为300维</span></span><br><span class="line">key = torch.rand(<span class="number">10</span>, <span class="number">64</span>, <span class="number">300</span>)</span><br><span class="line">value = torch.rand(<span class="number">10</span>, <span class="number">64</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">300</span></span><br><span class="line">num_heads = <span class="number">2</span></span><br><span class="line">multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads)</span><br><span class="line">attn_output = multihead_attn(query, key, value)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(attn_output)</span><br><span class="line"><span class="built_in">print</span>(attn_output.shape)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiheadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># n_heads：多头注意力的数量</span></span><br><span class="line">    <span class="comment"># hid_dim：每个词输出的向量维度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hid_dim, n_heads, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 强制 hid_dim 必须整除 h</span></span><br><span class="line">        <span class="keyword">assert</span> hid_dim % n_heads == <span class="number">0</span></span><br><span class="line">        <span class="comment"># 定义 W_q 矩阵</span></span><br><span class="line">        self.w_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_k 矩阵</span></span><br><span class="line">        self.w_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_v 矩阵</span></span><br><span class="line">        self.w_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 缩放</span></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        bsz = query.shape[<span class="number">0</span>]</span><br><span class="line">        Q = self.w_q(query)</span><br><span class="line">        K = self.w_k(key)</span><br><span class="line">        V = self.w_v(value)</span><br><span class="line">        <span class="comment"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span></span><br><span class="line">        <span class="comment"># 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50</span></span><br><span class="line">        <span class="comment"># 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span></span><br><span class="line">        <span class="comment"># K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span></span><br><span class="line">        Q = Q.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        K = K.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        V = V.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 1 步：Q 乘以 K的转置，除以scale</span></span><br><span class="line">        <span class="comment"># [64,6,12,50] * [64,6,50,10] = [64,6,12,10]</span></span><br><span class="line">        <span class="comment"># attention：[64,6,12,10]</span></span><br><span class="line">        attention = torch.matmul(Q, K.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)) / self.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention = attention.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span></span><br><span class="line">        <span class="comment"># 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span></span><br><span class="line">        <span class="comment"># attention: [64,6,12,10]</span></span><br><span class="line">        attention = self.do(torch.softmax(attention, dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三步，attention结果与V相乘，得到多头注意力的结果</span></span><br><span class="line">        <span class="comment"># [64,6,12,10] * [64,6,10,50] = [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50]</span></span><br><span class="line">        x = torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># 这里的矩阵转换就是：把多组注意力的结果拼接起来</span></span><br><span class="line">        <span class="comment"># 最终结果就是 [64,12,300]</span></span><br><span class="line">        <span class="comment"># x: [64,12,6,50] -&gt; [64,12,300]</span></span><br><span class="line">        x = x.view(bsz, -<span class="number">1</span>, self.n_heads * (self.hid_dim // self.n_heads))</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">query = torch.rand(<span class="number">64</span>, <span class="number">12</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span></span><br><span class="line">key = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span></span><br><span class="line">value = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line">attention = MultiheadAttention(hid_dim=<span class="number">300</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">output = attention(query, key, value)</span><br><span class="line"><span class="comment">## output: torch.Size([64, 12, 300])</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p>​                                                                                                                                                                    ——<a href="https://cloud.tencent.com/developer/article/1745002">图解Transformer（完整版)</a></p></blockquote><h4 id="5-PoswiseFeedForwardNet"><a href="#5-PoswiseFeedForwardNet" class="headerlink" title="5. PoswiseFeedForwardNet"></a>5. PoswiseFeedForwardNet</h4><script type="math/tex; mode=display">\text{FFN } (x) = \text{max }(0, xW_1  + b_1)W_2 + b_2</script><p>因为加入了残差连接，就是输入变成了 $ x + f(x) $，实际上这里变成了$x+\text{FFN } (x) $.然后再过LayerNorm。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoswiseFeedForwardNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        inputs: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="comment"># [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure><h3 id="2-实现encoder-layer-和-decoder-layer"><a href="#2-实现encoder-layer-和-decoder-layer" class="headerlink" title="2. 实现encoder layer 和 decoder layer"></a>2. 实现encoder layer 和 decoder layer</h3><h4 id="1-encoder-layer"><a href="#1-encoder-layer" class="headerlink" title="1. encoder layer"></a>1. encoder layer</h4><p>每层encoder layer有一个多头自注意力，和一个position-wise全连接前馈神经网络构成。需要注意的是还有个attention 流向decoder。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261834559.png" alt="image-20210325220007707" style="zoom:40%;" /></p><p>详细如下结构如下，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261834560.png" alt="qDOrJEifusTXM8z" style="zoom: 40%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        enc_self_attn_mask: [batch_size, src_len, src_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) </span><br><span class="line">        <span class="comment"># enc_inputs to same Q,K,V</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs) <span class="comment"># enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure><h4 id="2-decoder-layer"><a href="#2-decoder-layer" class="headerlink" title="2. decoder layer"></a>2. decoder layer</h4><p>decoder layer要2个多头自注意力，就是两个多头和一个position-wise全连接前馈神经网络。在 Decoder Layer 中会调用两次 <code>MultiHeadAttention</code>，第一次是计算 Decoder Input 的 self-attention，得到输出 <code>dec_outputs</code>。然后将 <code>dec_outputs</code> 作为生成 Q 的元素，<code>enc_outputs</code> 作为生成 K 和 V 的元素，再调用一次 <code>MultiHeadAttention</code>，得到的是 Encoder 和 Decoder Layer 之间的 context vector。最后将 <code>dec_outptus</code> 做一次维度变换，然后返回 <code>dec_outputs, dec_self_attn, dec_enc_attn</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"><span class="string">        dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs) <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br></pre></td></tr></table></figure><h3 id="3-整个encoder-和-decoder构成的Transformer"><a href="#3-整个encoder-和-decoder构成的Transformer" class="headerlink" title="3. 整个encoder 和 decoder构成的Transformer"></a>3. 整个encoder 和 decoder构成的Transformer</h3><h4 id="1-实现encoder"><a href="#1-实现encoder" class="headerlink" title="1. 实现encoder"></a>1. 实现encoder</h4><p>整个encoder部分构成为：</p><p><code>pad_masked（源语言词嵌入 + 位置嵌入）+ list(encoder layer)，返回 encoder 输出和 self attention</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>) </span><br><span class="line">        <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) </span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], </span></span><br><span class="line">            <span class="comment">#enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure><h4 id="2-实现decoder"><a href="#2-实现decoder" class="headerlink" title="2. 实现decoder"></a>2. 实现decoder</h4><p>decoder 跟encoder区别在于decoder 的attention分为self attention和 encoder 输出作为K, V 上一层 decoder attention输出作为Q的attention的decoder-encoder attention。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_intpus: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batsh_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).cuda() <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="number">0</span>).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], </span></span><br><span class="line">            <span class="comment">#dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], </span></span><br><span class="line">            <span class="comment">#dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><h4 id="3-实现Transformer"><a href="#3-实现Transformer" class="headerlink" title="3. 实现Transformer"></a>3. 实现Transformer</h4><script type="math/tex; mode=display">encoder  \to decoder \to 投影层</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outpus: [batch_size, tgt_len, d_model], </span></span><br><span class="line">        <span class="comment">#dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], </span></span><br><span class="line">        <span class="comment">#dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><p>完整代码实现在 <a href="https://colab.research.google.com/drive/15yTJSjZpYuIWzL9hSbyThHLer4iaJjBD?usp=sharing">Transformer-Torch</a></p><p>如果想跑整个论文机翻的话，用这个实现 <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">attention-is-all-you-need-pytorch</a></p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">attention</a></p><p>[2] <a href="https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4">Seq2seq pay Attention to Self Attention: Part 2 中文版</a></p><p>[3] <a href="http://peterbloem.nl/blog/transformers">TRANSFORMERS FROM SCRATCH</a></p><p>[4] <a href="https://wmathor.com/index.php/archives/1438/">Transformer 详解</a></p><p>[5] <a href="http://mantchs.com/2019/09/26/NLP/Transformer/?security_verify_data=313932302c31303830">Transformer</a></p><p>[6] <a href="https://github.com/BITLsy/Transformer_tf2.0/blob/master/model/Transformer.py">Transformer_tf2.0</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansfromer </tag>
            
            <tag> Self-Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10.1 Transformer 笔记</title>
      <link href="2020/11/18/NLP%20Paper%2010.1%20Transformer%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
      <url>2020/11/18/NLP%20Paper%2010.1%20Transformer%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="10-Transformer-笔记"><a href="#10-Transformer-笔记" class="headerlink" title="10. Transformer 笔记"></a>10. Transformer 笔记</h2><p>本文是transformer 开山之作 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 的笔记。结合了 <a href="https://jalammar.github.io/illustrated-transformer">The Illustrated Transformer</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>现在主流序列转换都是基于复杂的RNN和CNN，其包含一个encoder和decoder。最好的模型是通过一个attention机制来连接encoder，decoder。作者提出了一个新的网络架构，<strong>Transformer</strong>，只基于attention，完全不用RNN和CNN。</p><p>两个翻译任务证明这个模型：又快又好。</p><ol><li>WMT 2014 英语-德语任务中BLEU为28.4，高了2 个BLEU</li><li>8xGPU 3.5天 在WMT 2014 英语-德语中，单一模型BLEU达到41.8</li></ol><p>Transformer 还可以用于其他任务， 如英语成分分析等。</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>RNN, LSTM, GRU在翻译和LM中占据中最领先的水平。无数努力都是在推进RNN模型和Encoder-Decoder的极限。</p><p>RNN 通常是对输入和输出序列的符号位置计算。在计算期间将位置和时间步对齐，它们根据前一步的隐藏状态$h_{t-1} $ 和位置$t $的输入产生隐藏状态$h_t$。这种固有序列天然妨碍训练样本的并行化， 这在长文本序列上变得至关重要， 因为内存大小限制了样本批次大小。 近期工作通过巧妙地因子分解和条件计算在计算效率方面取得重大进展。 也提升了模型表现，但顺序计算的约束依然存在。</p><p>在各种任务中，注意力机制，已经成为引人注目的序列建模和转换模型不可或缺的一部分，它允许建模依赖关系不考虑其在输入和输出序列中的距离。但少数情况下， 注意力机制和循环神经网络一起使用。</p><p>在这项工作中作者提出Transformer，这种模型架构避免循环并完全依赖于attention机制来绘制输入和输出之间的全局依赖关系。 Transformer允许进行更多的<strong>并行化</strong>，并且可以在八个P100 GPU上接受少至十二小时的训练后达到翻译质量的新的最佳结果。</p><h3 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h3><p>减少序列计算也构成Extended Neural GPU, ByteNet, ConvS2s的基础，它们都使用卷积神经网络作为基本构建模块，并行计算所有输入和输出位置的隐藏状态表示。</p><p>在这些模型中，关联两个任意输入和输出位置的信号所需要的操作次数会随着位置之间的距离而增加， ConS2S是线性的， ByteNet是对数级的。 这就使得学习远距离位置的依存关系更加困难。</p><p>在Transfomer中，这中操作减少到固定次数，尽管对attention-weighted 位置取平均降低了效果，但是我使用多头注意力来抵消这种影响。</p><p><strong>注意力，有时也叫内在注意力， 是一种注意力机制，关联单一序列不同位置的来计算序列标识</strong>。其在不同任务上取得成功，如阅读理解， 概要总结， 文本蕴涵和学习与任务无关的句子表示。</p><p>端对端的记忆网络基于注意力机制而不是序列对齐循环，并且取得了简单语言QA和语言模型任务的不错表现。</p><p>Tansformer是第一个完全依靠self-attention来计算输入和输出表示， 而不是使用序列对齐RNNs或卷积。</p><h3 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3. Model Architecture"></a>3. Model Architecture</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833353.png" alt="image-20210315145108581" style="zoom:33%;" /></p><p>大部分有竞争力的神经序列转换模型有encoder-decoder结构。这里，encoder映射一个用符号表示的输入序列$ (x_1, \cdots, x_n) $ 到 连续表示序列 $ \mathbf{z} = (z_1, \cdots, z_n)$ 。 给定 $\mathbf{z}$, 解码器然后生成一个符号输出序列 $(y_1, \cdots, y_m)$ ， 每次一个元素。在每步上模型是自回归的，当生成下一个是， 消耗之前生成符号作为输入的附加部分。</p><p>Transformer 遵循这种整体架构，每个编码器和解码器都使用堆叠的自注意力和point-wise， 然后编码器和解码器都使用全连接层连接。</p><h4 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h4><p><strong>Encoder</strong>：编码器由6个完全相同的层堆叠而成。每层都有两个子层：</p><ol><li>多头自注意力机制</li><li>position-wise全连接前馈神经网络</li><li>残差连接两个子层， 然后接Normalize。每个子层的输出为 $ \text{LayerNorm} (x  + \text{Sublayer}(x)) $， 其中 $ \text{Sublayer}(x) $是由子层本身实现的函数。</li></ol><p>为了方便残差连接， 模型示意子层和嵌入层产生的输出维度都是$d_{model} = 512 $</p><p><strong>Decoder</strong>: 编码器也有6个完全相同的层堆叠而成。 </p><p>除了编码器层中的两个子层之外， 解码器插入第3个子层，该层对编码器堆叠层的输出执行多头注意力机制。跟编码器类似，作者在每个子层也用残差连接，然后层normalization。 还修改解码器堆叠层自注意力位置， 来防止关注到后面位置。masking 结合outout的embedding都右移了一个位置的事实， 确保位置$i$ 的预测仅仅依靠比$i$小的已知位置。</p><p>借用 <a href="https://jalammar.github.io/illustrated-transformer/">jalammar</a> 教授的图来展示下整个架构。</p><p>从整体来说就是输入源语言，经过Transformer后，输出目标语言。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833355.png" alt="image-20210315195700172" style="zoom: 25%;" /></p><p>而Transformer就是Encoder and Decoder Stacks 形成的：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833356.png" alt="image-20210315195916637" style="zoom:25%;" /></p><p>将Encoder-Decoder展开就是：(6个相同的encoder+6个相同的decoder， 在decoder中，会接受输入和encoder的隐藏状态)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833357.png" alt="image-20210315200058798" style="zoom:25%;" /></p><p>而Encoder可以分为：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833358.png" alt="image-20210315200344415" style="zoom:25%;" /></p><p>简化下，Encoder和Decoder如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833359.png" alt="image-20210315200515464" style="zoom:25%;" /></p><ol><li>先将每个单词作为512维词嵌入向量输入， (这里画四格只是为了示意， 也可以选其它维度，后面再讲)</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833360.png" alt="image-20210321150620504" style="zoom:25%;" /></p><ol><li>词嵌入向量组成序列输入之后，每个单词会流经编码器的每个子层。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833361.png" alt="image-20210321151001758" style="zoom:25%;" /></p><p>接下来作者看看Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。</p><p>一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。真正编码阶段如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833362.png" alt="image-20210321152053247" style="zoom:25%;" /></p><h4 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h4><blockquote><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>Attention函数可以描述为映射query和一组key-value对到输出，其中query、keys、values和输出都是向量。 输出为value的加权和，其中分配给每个value的权重，用query和其对应的key的点积并通过softmax来计算。</p></blockquote><p>首先作者了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。</p><p>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，作者创造一个查询向量query、一个键向量key和一个值向量Value。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。</p><p>这些新向量比词嵌入向量维度更低，维度为64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833363.png" alt="image-20210321153715679" style="zoom:30%;" /></p><p>整体来说，作者那$\mathbf{x}_1$乘以 $\mathbf{W}^q$， 得到$\mathbf{q}_1$, （先不要想突然冒出来的$W^q$是怎么来的， 只要把它当做可以学到，这样计算是可以得到一个值的）。那么同样可以得到$\mathbf{k}_1$, $\mathbf{v}_1$,最后输入序列的每个词都有其对应的$\mathbf{k}, \mathbf{q}, \mathbf{v}$。</p><ul><li>下面来看看， $\mathbf{k}, \mathbf{q}, \mathbf{v}$怎么计算分数：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833364.png" alt="image-20210321154358562" style="zoom:30%;" /></p><p>如上图， $\mathbf{q}_1 \cdot \mathbf{k}_1 = 112, \ \mathbf{q}_1 \cdot \mathbf{k}_2 = 96$,  就是拿$\mathbf{q} $值去和 $ \mathbf{k} $ 的每个分量相乘，这步可以理解为拿着$\mathbf{q}$去查询相应的分数(两个向量相乘， 来比较其相似度)。</p><ul><li><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833365.png" alt="image-20210321155238135" style="zoom:25%;" /></li></ul><p>再将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。</p><ul><li>将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。</li></ul><ul><li><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833366.png" alt="image-20210321155853186" style="zoom:30%;" /></p></li></ul><p>对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在作者的例子中是对于第一个单词)。比如$z_1$ 就是  <script type="math/tex">0.88 \times  v_1</script></p><p>上面是向量注意力机制的实现， 接下来是<strong>矩阵运算实现自注意力机制</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833367.png" alt="image-20210321161048510" style="zoom:30%;" /></p><p>第一步是计算查询矩阵、键矩阵和值矩阵。为此，作者将将输入句子的词嵌入表示为矩阵$X$，将其乘以作者训练的权重矩阵($W^Q，W^K，W^V$)。</p><ul><li><p>这里，矩阵$X$的每一行代表着输入句子中的一个单词的词向量。那么其大小为$ m \times 512$, 而 $\mathbf{v}$ 为 $m \times 64$, 因此$W^Q$的size为$ 512 \times 64$</p><p>这就是图中3个格子和4个格子的差异原因。</p></li></ul><p>最后，因为是矩阵，可以用公式表示如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833368.png" alt="image-20210321161913245" style="zoom:30%;" /></p><h5 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><p>这部分跟上面的有重复，只为了对比原文。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833369.png" alt="image-20210321160347624" style="zoom:30%;" /></p><p>“Scaled Dot-Product Attention” 如图2， 输入由一系列$ d_k $ 维的查询值和键， 以及$d_v $ 的值values构成 。计算 所有查询值 $ q$ 和 键$k$ 的点积， 除以 $ \sqrt d_k$, 再应用softmax来获得value的权重。</p><p>实际上，应用矩阵计算，公式如下：</p><script type="math/tex; mode=display">\text{Attention } (Q, K, V) = \text{softmax } (\frac{QK^T}{\sqrt d_k}) V \tag{1}</script><p>两种常用的注意力函数是加法注意力和点乘注意力。除了放缩因子$ 1/\sqrt d_k $之外， 点乘注意力论文中一样。加法attention使用单个隐藏层的前馈网络来计算兼容性函数。 虽然两者在理论上的复杂性相似，但在实践中点积attention的速度更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>在$d_k$值比较小的时， 两种机制执行性能相近；</p><p>当$d_k $比较大时，加法attention比不带缩放的点积attention性能好.</p><p>作者怀疑，$d_k$ 比较大的时候， 点乘增长迅速，把softmax的值推向梯度非常小的区域(想一下softmax函数图像)。为了抵消这个， 采用缩放因子。</p><h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>相比于 $ d_{model} $ 维 queries， keys， values的单头注意力函数， 作者发现将queries， keys， values 经过$h$次不同的线性投影后， 相应地，学习到的线性投影到$d_q, d_k, 和d_v$维效果更好。在每一次投影后的 query、key 和 value 上，作者并行地执行注意力函数，产生输出值，其维度为 $d_v$。这些输出值进行连接然后再次投影，产生最终的值， 如上图右。</p><p>多头注意力允许模型的不同表示子空间联合关注不同位置的信息。如果只有一个注意力头，平均值将抑制这些信息。</p><script type="math/tex; mode=display">\begin{aligned}\\\text{MultiHead }(Q, K, V) = \text{Concat } (\text{head}_1 , \cdots, \text{head}_h) W^O\\\\\text{where   } \text{head} _i = \text{Attention  } ( QW^Q_i, KW_i^K, VW_i^V) \end{aligned} \tag{2}</script><p>其中, 投影的参数矩阵为 <script type="math/tex">W_i^Q \in \mathbb{R}^{d_{ \text{model} } \times d_k},  \quad W_i^K \in \mathbb{R}^{d_{ \text{model} } \times d_k}, \quad  W_i^V \in \mathbb{R}^{d_{ \text{model}} \times d_v}</script></p><p>这里采用h = 8 个并行的attention layer或者heads。对于每个head， 使用<script type="math/tex">d_k = d_v=d_{model}/h=64</script>. 由于每个head的维度减小， 总的计算开销跟全维度的单头相似。</p><p>多头注意力机制在两方面提高了注意力层的性能：</p><p>1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在$z_1$中有或多或少的体现，但是它可能被实际的单词本身所支配。如果作者翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，作者会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p><p>2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来作者将看到，对于“多头”注意机制，作者有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此作者对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833370.png" alt="image-20210321192403805" style="zoom:30%;" /></p><p>在“多头”注意力机制下，作者为每个他保持独立的查询、键、值的权重矩阵，从而得到不同的查询、键、值矩阵。按照论文中， 作者做不同的自注意力计算八次，将得到8个不同的$Z$矩阵</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833371.png" alt="image-20210321192807930" style="zoom:30%;" /></p><p>而前馈层不需要8个不同的矩阵， 其只需要一个，所以将8个矩阵压缩成一个矩阵。实际上是把8个矩阵拼接在一起，然后用一个附加矩阵$W^O$ 与其相乘，得到融合的矩阵$Z$。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833372.png" alt="image-20210321193057238" style="zoom:30%;" /></p><p>这就是多头注意力的全部。将其集中在一个图片中如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833373.png" alt="image-20210321193318126" style="zoom:30%;" /></p><h5 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h5><p>Transformer使用多头注意力在3个不同方面：</p><ul><li>在 <code>encoder-decoder attention</code> 层中, queries来自于前一层的decoder层， 内存中的keys和values来自于 decoder 的输出。 这允许decoder中每个位置注意输入序列中的每个位置。这模仿seq2seq 典型的encoder-decoder 注意力机制。</li><li>编码器包含self-attention层。 在self-attention层中，所有的keys、values和queries来自同一个地方，在这里, 是编码器中前一层的输出。 编码器中的每个位置都可以关注编码器前一层的所有位置。</li><li>相似地， 解码器中的self-attention层允许解码器中的每个位置都关注解码器中直到并包括该位置的所有位置。 作者需要阻止解码器中的向左信息流来保持自回归性质。 通过屏蔽softmax的输入中所有不合法连接的值（设置为 $ -\infty$ ），作者在缩放的点积attention中实现了这一点。</li></ul><h4 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h4><p>除了附加的attention子层外，encoder和decoder的每层都包含一个前馈全连接层， 该层独立且同等第应用于每个位置。其由两个线性变换，中间加一个ReLU激活函数。</p><script type="math/tex; mode=display">\text{FFN } (x) = \text{max }(0, xW_1  + b_1)W_2 + b_2 \tag{3}</script><p>另一种描述方式是两个内核为1的卷积。输入和输出的维度都是<script type="math/tex">d_{model} = 512</script>, 内部层的维度为 <script type="math/tex">d_{ff}=2048</script></p><h4 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a>3.4 Embeddings and Softmax</h4><p>跟其他序列转换模型一样，们使用学习到的嵌入层将输入token和输出token转换为维度为 <script type="math/tex">d_{model}</script> 维的向量。 </p><p>作者还使用普通的线性变换和softmax函数将解码器输出转换为预测的下一个token的概率。 在作者的模型中，两个嵌入层之间和pre-softmax线性变换共享相同的权重矩阵。 在嵌入层中，作者将这些权重乘以 <script type="math/tex">\sqrt d_{model}</script>.</p><h4 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h4><p>由于作者的模型不包含循环和卷积，为了让模型利用序列的顺序，作者必须注入序列中关于token相对或者绝对位置的一些信息。</p><p>为此，作者在编码器堆和解码器堆的底部给输入 Embedding 添加“位置编码”。位置编码具有与 Embedding 相同的维数$d_{model} $，因此这两者可以相加。此外，有若干种位置编码的方式可供作者选择、学习和修正。</p><p>在这项工作中，作者使用不同频率的正弦和余弦函数作为位置编码：</p><script type="math/tex; mode=display">\begin{aligned}PE(pos,2i)&=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ \\PE(pos,2i+1)&=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\end{aligned}</script><p>其中 $\text{pos} $是位置，$i $是维度。也就是说，位置编码的每个维度对应于正弦曲线。波长形成从 <script type="math/tex">2 \pi</script> 到 <script type="math/tex">1000 \cdot \ 2\pi</script> 的几何级数。作者之所以选择这个函数是因为作者假定它允许模型容易学习对相对位置的关注，因为对任意确定的偏移k ,  <script type="math/tex">\text{PE} _{pos + k}</script>能被表示成 <script type="math/tex">\text{PE} _{pos }</script> 的线性关系。</p><p>在google 开源实现<a href="https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py">get_timing_signal_1d()</a> 可以找到它，作者这么设计的原因是考虑到在NLP任务中，除了单词的绝对位置，单词的相对位置也非常重要。根据公式 :</p><script type="math/tex; mode=display">\begin{aligned}\text{sin } (\alpha + \beta) = \text{sin } \alpha   \text{cos }  \beta + \text{cos }\alpha\text{sin }\beta \\\\\text{cos} (\alpha + \beta) = \text{cos } \alpha   \text{cos }  \beta - \text{sin }\alpha\text{sin }\beta\end{aligned}</script><p>这表明位置 $k + p$ 的位置向量可以表示为位置 $k$ 的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p><p>也用了embedding学习位置关系， 但结果两者差不多结果。 作者选择正弦曲线， 因为它可以允许模型推断比训练的输入的更长的序列。</p><p>Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833374.png" alt="image-20210321205135836" style="zoom:30%;" /></p><p>为了让模型理解单词的顺序，作者添加了位置编码向量，就是上图中的<code>EMBEDDING WITH TIME SIGNAL</code> 中的$\mathbf{x}$。</p><p>如果作者假设词嵌入的维数为4，则实际的位置编码如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833375.png" alt="image-20210321205449652" style="zoom:30%;" /></p><p>这个模式会是什么样子？</p><p>在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。作者已经对它们进行了颜色编码。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833376.png" alt="image-20210321210941801" style="zoom:30%;" /></p><p>20个字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。</p><p> <strong>LayerNorm</strong></p><blockquote><p>与之相关的是Batch Normalization，这个技巧能够让模型收敛的更快。但是Batch Normalization有一个问题——它需要一个minibatch的数据，而且这个minibatch不能太小(比如1)。另外一个问题就是它不能用于RNN，因为同样一个节点在不同时刻的分布是明显不同的。当然有一些改进的方法使得可以对RNN进行Batch Normalization，比如论文<a href="https://arxiv.org/abs/1603.09025">Recurrent Batch Normalization</a>，有兴趣的读者可以自行阅读 。</p><p>Transformer里使用了另外一种Normalization技巧，叫做Layer Normalization。作者可以通过对比Layer Normalization和Batch Normalization来学习。</p><p>假设作者的输入是一个minibatch的数据，作者再假设每一个数据都是一个向量，则输入是一个矩阵，每一行是一个训练数据，每一列都是一个特征。BatchNorm是对每个特征进行Normalization，而LayerNorm是对每个样本的不同特征进行Normalization，因此LayerNorm的输入可以是一行(一个样本)。</p><p>如下图所示，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833377.png" alt="image-20210323110020405" style="zoom:25%;" /></p><p>输入是(3,6)的矩阵，minibatch的大小是3，每个样本有6个特征。BatchNorm会对6个特征维度分别计算出6个mean和std，然后用这两个mean和std来分别对6个特征进行Normalization，(Batch Normalization是对每批中样本的特征normalize，那么特征维数是每个样本向量的长度，因此normalize后是6个mean和std).</p><p>计算公式如下：</p><script type="math/tex; mode=display">\begin{split}\mu_j &=\frac{1}{m}\sum_{i=1}^{m}x_{ij} \\\sigma_j^2 & = \frac{1}{m}\sum_{i=1}^{m}(x_{ij}-\mu_j)^2 \\\hat{x}_{ij} & =\frac{x_{ij}-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}\end{split}</script><p>而LayerNorm是分别对3个样本的6个特征求mean和std，因此可以得到3个mean和std，然后用这3个mean和std对3个样本来做Normalization，(LayerNorm是对每个样本进行normalize，样本数是3，因此normalize后是3个mean和std)</p><p>计算公式如下：</p><script type="math/tex; mode=display">\begin{split}\mu_i &=\frac{1}{n}\sum_{j=1}^{n}x_{ij} \\\sigma_i^2 & = \frac{1}{m}\sum_{j=1}^{m}(x_{ij}-\mu_i)^2 \\\hat{x}_{ij} & =\frac{x_{ij}-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}\end{split}</script><p>因为LayerNorm的每个样本都是独立计算的，因此minibatch可以很小甚至可以是1。实验证明LayerNorm不仅在普通的神经网络中有效，而且对于RNN也非常有效。</p><p>BatchNorm看起来比较直观，作者在数据预处理也经常会把输入Normalize成均值为0，方差为1的数据，只不过它引入了可以学习的参数使得模型可以更加需要重新缓慢(不能剧烈)的调整均值和方差。而LayerNorm似乎有效奇怪，比如第一个特征是年龄，第二个特征是身高，把一个人的这两个特征求均值和方差似乎没有什么意义。论文里有一些讨论，都比较抽象。当然把身高和年龄平均并没有什么意义，但是对于其它层的特征，作者通过平均”期望”它们的取值范围大体一致，也可能使得神经网络调整参数更加容易，如果这两个特征实在有很大的差异，模型也可以学习出合适的参数让它来把取值范围缩放到更合适的区间。</p><p>—— 引用自<a href="http://fancyerii.github.io/2019/03/09/transformer-illustrated/">LayerNorm</a></p></blockquote><p> <strong>残差连接</strong></p><p>每个self-Attention层都会增加一个残差连接，然后是一个LayerNorm层。</p><p>如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833378.png" alt="image-20210323112019296" style="zoom:30%;" /></p><p>具体来说，就是输入$x_1, x_2$经过self-attention层后，输出$z_1, z_2$。</p><p>然后和残差连接的输入$x_1, x_2$加起来一起输入到LayerNorm，再输出到全连接层。</p><p>全连接层也是一个残差连接和一个LayerNorm，最后输出到最后一层。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833379.png" alt="image-20210323112523316" style="zoom:30%;" /></p><p>而Decoder类似于Encoder，如下图：区别在于Dncoder多了一个Encoder-Decoder Attention层。</p><p>Encoder-Decoder Attention层输入除了来自Self-Attention之外还有Encoder最后一层所有时刻的输出。其Query来自下一层，而Key和Value来自Encoder的输出。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261833380.png" alt="image-20210323151539332" style="zoom:30%;" /></p><h3 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4. Why Self-Attention"></a>4. Why Self-Attention</h3><p>在本节，作者比较self_attention 和CNN、RNN的各个方面，它们通常用于映射不同长度符号序列表示$(x_1, \cdots, x_n)$到另一个等长的序列$(z_1, \cdots, z_n)$, 其中$x_i, z_i \in \mathbb{R}^d$ ，例如经典的序列转换中的编码器或编码器隐藏层。使用Self-attention的动机是一下3个方面：</p><ol><li><p>每层的总体计算复杂度。</p></li><li><p>大量计算能够并行，用最小序列操作数来衡量。</p></li><li><p>网络中长距离依赖的路径长度。学习长距离依赖是许多序列转换任务的关键。另外一个关键因素影响学习这种依赖是前向和方向信号必须穿过网络的路径长度。</p><p>任何输入位置和输出序列的组合之间的路径越短，这种长距离依赖也就越容易学习到。</p></li></ol><h3 id="5-Training"><a href="#5-Training" class="headerlink" title="5. Training"></a>5. Training</h3><h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p>使用Adam 优化器， $\beta_1=0.9, \beta_2=0.98 和 \epsilon = 10^{-9}$,学习率变化：</p><script type="math/tex; mode=display">l_\text{rate} = d_{model}^{-0.5} \cdot \ min(\text{step_num}^{-0.5}, \text{step_num} \cdot \text{warm_steps}^{-1.5})</script><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p><strong>优点</strong>：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p><p><strong>缺点</strong>：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</p><p>​                                                                                                                                                                                                    ——<a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer</a></p></blockquote><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture14-transformers.pdf">Transformers and Self-Attention For Generative Models guest lecture by Ashish Vaswani</a></p><p>[2] <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></p><p>[3*] <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p><p>[4] <a href="https://medium.com/@CecileLiu/good-article-translation-and-sharing-attention-model-292f04b07b8b">Good Article Translation and Sharing — Attention Model</a></p><p>[5*] <a href="http://fancyerii.github.io/2019/03/09/transformer-illustrated/">Transformer图解</a></p><p>[6] <a href="https://www.jianshu.com/p/25fc600de9fb">BERT泛读系列（一）——《Attention is All You Need》论文笔记</a></p><p>[7] <a href="https://www.yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html">翻译</a></p><p>[8] <a href="https://zhuanlan.zhihu.com/p/53138481">Transformer-Attention Is All You Need笔记</a></p><p>[9] <a href="https://charon.me/posts/model/transformer/">transformer</a></p><p>[10] <a href="https://codle.net/attention-is-all-you-need/">attention-is-all-you-need</a></p><p>[11] <a href="https://www.cnblogs.com/dogecheng/p/11911909.html">手把手教你用Pytorch-Transformers——实战（二）</a></p><p>[12] <a href="https://zhuanlan.zhihu.com/p/106867810">Transformer理论源码细节详解</a></p><p>[13] <a href="https://turingforchinese.home.blog/tag/bert/">Shreya Gherani：BERT庖丁解牛</a></p><p>[14] <a href="https://cloud.tencent.com/developer/article/1745002">图解Transformer（完整版</a></p><p>[15] <a href="https://mp.weixin.qq.com/s/jx-2Ai2YKbwODW6uJaF3hQ">保姆级教程：硬核图解Transformer</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansfromer </tag>
            
            <tag> Self-Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. 类的继承</title>
      <link href="2020/11/01/4.%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/"/>
      <url>2020/11/01/4.%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/</url>
      
        <content type="html"><![CDATA[<h2 id="4-类的继承"><a href="#4-类的继承" class="headerlink" title="4. 类的继承"></a>4. 类的继承</h2><h3 id="1-继承-Inheritance"><a href="#1-继承-Inheritance" class="headerlink" title="1. 继承 Inheritance"></a>1. 继承 Inheritance</h3><ul><li>继承是面向对象程序设计的精髓之一</li><li>实现了以类为单位的高抽象级别代码复用</li><li>继承是新定义类能够几乎完全使用原有类属性与方法的过程</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261744306.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">继承</div> </center><h3 id="2-子类、父类、超类"><a href="#2-子类、父类、超类" class="headerlink" title="2. 子类、父类、超类"></a>2. 子类、父类、超类</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261744944.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">子类、父类、超类</div> </center><h3 id="3-类继承的构建"><a href="#3-类继承的构建" class="headerlink" title="3. 类继承的构建"></a>3. 类继承的构建</h3><p>在定义类是声明继承关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;(<span class="params">&lt;基类名<span class="number">1</span>&gt;, &lt;基类名<span class="number">2</span>&gt;</span>)：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>基类名可以带路径：<code>ModuleName.BaseClassName</code></p><h3 id="4-类继承的使用"><a href="#4-类继承的使用" class="headerlink" title="4. 类继承的使用"></a>4. 类继承的使用</h3><ul><li>基类的属性基本等同于定义在派生类中</li><li>派生类可以直接使用基类的类属性、实例属性</li><li>派生类可以直接使用基类的各种方法</li><li>使用基类的类方法和类属性时，要用基类的类名调用</li></ul><p><code>isinstance(obj, cls)</code> <code>issubclass(子类名，父类名)</code>判断继承关系的内置函数。</p><p><strong>Python采用深度优先、从左至右的方法实施多继承。</strong></p><p><strong>派生类的约束</strong>：</p><ul><li>派生类只能继承基类的公开属性和方法</li><li>派生类不能继承基类的私有属性和私有方法</li></ul><p><code>object</code>类是<code>Python</code>所有类的基类</p><ul><li>object是python最基础类的名字，不要用中文去理解</li><li>所有类定义时默认继承object类</li><li>保留属性和保留方法本质上是object类的属性和方法</li></ul><h3 id="5-Python对象的三个要素"><a href="#5-Python对象的三个要素" class="headerlink" title="5. Python对象的三个要素"></a>5. Python对象的三个要素</h3><ul><li>标识identity：对象一旦构建不会改变，用<code>id()</code>获取</li><li>type：对象的类型，用<code>type()</code>获取</li><li>值value:分为可变mutab与不可变immmutable</li></ul><p><code>id(x)</code>， <code>x is y</code>的使用。</p><h3 id="6-类的属性的重载"><a href="#6-类的属性的重载" class="headerlink" title="6. 类的属性的重载"></a>6. 类的属性的重载</h3><ul><li>属性重载：派生类定义并使用了与基类相同名称的属性</li><li>方法重载：派生类定义并使用了与基类相同名称的方法</li></ul><p><strong>原则</strong>：</p><ul><li>步骤1：优先使用派生类重定义的属性和方法</li><li>步骤2：然后寻找基类的属性和方法</li><li>步骤3：在寻找超类的属性和方法</li></ul><h3 id="7-类的方法重载"><a href="#7-类的方法重载" class="headerlink" title="7. 类的方法重载"></a>7. 类的方法重载</h3><ul><li><p>完全重载：派生类完全重定义与基类名称的方法，直接在派生类中定义同名方法即可</p></li><li><p>增量重载：派生类拓展定义与基类相同名称的方法</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;(<span class="params">&lt;基类名<span class="number">1</span>&gt;, &lt;基类名<span class="number">2</span>&gt;</span>)：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line"><span class="built_in">super</span>().&lt;基类方法名&gt;([参数列表])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. 面向对象</title>
      <link href="2020/10/29/1.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"/>
      <url>2020/10/29/1.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-面向对象"><a href="#1-面向对象" class="headerlink" title="1. 面向对象"></a>1. 面向对象</h2><h3 id="1-面向对象编程万物皆对象"><a href="#1-面向对象编程万物皆对象" class="headerlink" title="1. 面向对象编程万物皆对象"></a>1. 面向对象编程万物皆对象</h3><h4 id="1-对象？"><a href="#1-对象？" class="headerlink" title="1. 对象？"></a>1. 对象？</h4><p><strong>对象</strong>：独立的存在 或 作为目标的事物</p><ol><li>独立性：对象都存在清晰的边界，重点在于划分边界</li><li>功能性：对象都能表现出一些功能、操作或行为</li><li>交互性：对象之间存在交换，如：运算和继承</li></ol><h4 id="2-Python语言的“万物皆对象”"><a href="#2-Python语言的“万物皆对象”" class="headerlink" title="2. Python语言的“万物皆对象”"></a>2. Python语言的“万物皆对象”</h4><ul><li>Python语言中所有的数据类型都行对象、函数是对象、模块是对象</li><li>Python所有类都继承与最基础类object</li><li>Python语言中数据类型的操作功能都是类方法的体现</li></ul><h4 id="3-OOP：-Object-Oriented-Programming"><a href="#3-OOP：-Object-Oriented-Programming" class="headerlink" title="3. OOP： Object-Oriented Programming"></a>3. OOP： Object-Oriented Programming</h4><ul><li>OOP: 面向对象编程，一种编程思想，重点在于高抽象的复用代码</li><li>OOP把对象当做程序的基本单元，对象包含数据和操作数据的函数</li><li>OOP本质是把问题解决， 抽象为以对象为中心的计算机程序</li><li>OOP在较大规模或复杂项目中十分有用，OOP可以提高协作产量</li><li>OOP最主要价值在于代码复用</li><li>OOP只是一种编程方式，并非解决问题的高级方法</li></ul><h4 id="4-面向对象-VS-面向过程"><a href="#4-面向对象-VS-面向过程" class="headerlink" title="4. 面向对象 VS 面向过程"></a>4. 面向对象 VS 面向过程</h4><ul><li>面向过程： 以<strong>解决问题的过程步骤</strong>为核心编写程序的方式</li><li>面向对象： 以<strong>问题对象构建和应用</strong>为核心的编程程序的方式</li><li>所有OOP能解决的问题，面向过程都能解决</li></ul><h4 id="5-OOP三个特征"><a href="#5-OOP三个特征" class="headerlink" title="5. OOP三个特征"></a>5. OOP三个特征</h4><ul><li>封装： 属性和方法的抽象，用数据和操作数据的方法来形成对象逻辑</li><li>继承：代码复用的高级抽象，用对象之间的继承关系来形成代码复用</li><li>多态： 方法灵活性的抽象，让对象的操作更加灵活，更多复用代码</li></ul><h4 id="6-封装Encapsulation"><a href="#6-封装Encapsulation" class="headerlink" title="6. 封装Encapsulation"></a>6. 封装Encapsulation</h4><ul><li>属性的抽象： 对象的属性(变量)进行定义、隔离及保护</li><li>方法的抽象： 对类的方法(函数)进行定义、隔离及保护</li><li>目标是形成一个类对外可操作属性和方法的接口</li></ul><h4 id="7-继承-Inheritance"><a href="#7-继承-Inheritance" class="headerlink" title="7. 继承 Inheritance"></a>7. 继承 Inheritance</h4><ul><li>继承是面向对象程序设计的精髓之一</li><li>定义了以类为单位的高级抽象级别代码复用</li><li>继承是新定义类能够几乎完全使用原有类属性与方法的过程</li></ul><h4 id="8-多态-Polymorphism"><a href="#8-多态-Polymorphism" class="headerlink" title="8. 多态 Polymorphism"></a>8. 多态 Polymorphism</h4><ul><li><p>参数类型的多态：一个方法能够处理多个类型的能力</p></li><li><p>参数形式的多态： 一个方法能够接受多个参数的能力</p></li><li><p>多态是OOP的一个传统概念，Python天然支持多态，不需要特殊语法</p></li></ul><h3 id="2-Python-面向对象术语概述"><a href="#2-Python-面向对象术语概述" class="headerlink" title="2. Python 面向对象术语概述"></a>2. Python 面向对象术语概述</h3><h4 id="1-类-class-和-对象-Object"><a href="#1-类-class-和-对象-Object" class="headerlink" title="1. 类 class 和 对象 Object"></a>1. 类 class 和 对象 Object</h4><ul><li>类： 逻辑抽象和产生对象的模板，一组变量和函数的特定编排</li><li>对象： 具体表达数据及操作的实体，相当于程序中的“变量“</li><li>实例化：从类到对象的过程，所有“对象”都源于某个“类”</li></ul><h4 id="2-常用术语"><a href="#2-常用术语" class="headerlink" title="2. 常用术语"></a>2. 常用术语</h4><ul><li><p>对象： 类对象、实例对象</p></li><li><p>属性： 存储数据的“变量”， 包括：类属性、实例属性</p></li><li><p>方法：操作数据的“函数”</p><p>包括：</p><ul><li>类方法 <code>__classmethod__</code></li><li>实例方法</li><li>自由方法</li><li>静态方法 <code>__staticmethod__</code></li><li>保留方法</li></ul></li><li><p>三个特性： 封装、继承、多态</p></li><li><p>继承： 基类、派生类、子类、父类、超类</p></li><li><p>命名空间： 程序元素作用域的表达</p></li><li><p>构造和析构：生成对象和删除对象的过程</p></li></ul><p><strong>类对象 vs 实例对象</strong></p><ul><li><p>类对象：<code>Class Object</code>,维护每个Python类基本信息的数据结构</p></li><li><p>实例对象： Instance Object, Python类实例后产生的对象，简称：对象。</p></li><li><p>这是一组概念，类对象全局只有一个，实例对象可以生产多个。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3. 类的封装</title>
      <link href="2020/10/29/3.%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/"/>
      <url>2020/10/29/3.%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="3-类的封装"><a href="#3-类的封装" class="headerlink" title="3. 类的封装"></a>3. 类的封装</h2><h3 id="1-封装Encapsulation"><a href="#1-封装Encapsulation" class="headerlink" title="1. 封装Encapsulation"></a>1. 封装Encapsulation</h3><p>封装是属性和方法的抽象。</p><ul><li>属性的抽象：对类的属性(变量)进行定义、隔离和包护</li><li>方法的抽象：对类的方法(函数)进行定义、隔离和包护</li><li>目标是形成一个类对外可操作属性和方法的接口</li></ul><pre class="mermaid">graph LRA((类CLass))-->  B(属性 Attributes)A --> C(方法 Methods)</pre><p><strong>属性：</strong></p><ul><li>私有属性：只能在类内部访问</li><li>公有属性：可以通过类、对象名访问</li></ul><p><strong>方法：</strong></p><ul><li>公有方法：只能在类内部使用</li><li>公开方法：可以通过类、对象名访问</li></ul><h3 id="2-私有类属性和公开类属性"><a href="#2-私有类属性和公开类属性" class="headerlink" title="2. 私有类属性和公开类属性"></a>2. 私有类属性和公开类属性</h3><p>私有属性：仅供当前类访问的类属性，子类也不能访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;__私有类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>私有类属性名开始需要有两个下划线<code>__</code>,如 <code>__count</code>。</p><ul><li>只能在类的内部被方法所访问</li><li>不能通过&lt;类名&gt;.&lt;属性名&gt; 或 &lt;对象名&gt;.&lt;属性名&gt;方式访问</li><li>有效保证了属性维护的可控性</li></ul><p>公开类属性：即类属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><h3 id="3-公开实例属性和私有实例属性"><a href="#3-公开实例属性和私有实例属性" class="headerlink" title="3. 公开实例属性和私有实例属性"></a>3. 公开实例属性和私有实例属性</h3><p><strong>公开实例属性：</strong>即实例属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p><strong>私有实例属性：</strong>仅供当前类内部访问的实例属性，子类也不能访问</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;__实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p>私有实例属性名开始需要有两个下划线<code>__</code>,如<code>__name</code>.</p><ul><li>只能在类的内部被方法所访问</li><li>不能通过&lt;类名&gt;.&lt;属性名&gt; 或 &lt;对象名&gt;.&lt;属性名&gt;方式访问</li><li>有效保证了属性维护的可控性</li></ul><h3 id="4-私有方法和公开方法"><a href="#4-私有方法和公开方法" class="headerlink" title="4.私有方法和公开方法"></a>4.私有方法和公开方法</h3><p><strong>私有方法</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> &lt;<span class="title">__</span>方法名&gt;(<span class="params">self, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>私有方法名开始要有2个下划线<code>__</code>,如<code>__getCount()</code>.</p><ul><li>各类方法都可以通过增加双下划线变为私有方法</li><li>私有方法从形式上保护了Python类内部使用的函数逻辑</li><li>私有与公有是程序员逻辑，不是安全逻辑，重视约定</li></ul><h3 id="5-类的保留属性和保留方法"><a href="#5-类的保留属性和保留方法" class="headerlink" title="5. 类的保留属性和保留方法"></a>5. 类的保留属性和保留方法</h3><p><strong>保留属性</strong>：也叫特殊属性，Special Attributes。</p><ul><li>特点：以双下划线开头和结尾。</li><li>作用：为理解Python类提供统一的属性接口</li><li>属性值：具有特定含义，类定义后直接使用</li></ul><div class="table-container"><table><thead><tr><th>保留属性</th><th>描述</th></tr></thead><tbody><tr><td><code>__name__</code></td><td>类的名称</td></tr><tr><td><code>__qualname__</code></td><td>以.分隔从模块全局命名空间开始的类名称</td></tr><tr><td><code>__bases__</code></td><td>类所继承的基类名称</td></tr><tr><td><code>&lt;类&gt;.__dict__</code></td><td>包含类成员信息的字典，key是属性和方法名称，value是地址</td></tr><tr><td><code>&lt;对象&gt;.__dict__</code></td><td>包含对象属性信息的字典，key是属性名称，value是值</td></tr><tr><td><code>__class__</code></td><td>对象所对应的类信息，即type信息</td></tr><tr><td><code>__doc__</code></td><td>类描述，写在类定义下的首行字符串，不能继承</td></tr><tr><td><code>__module__</code></td><td>类所在模块的名称</td></tr></tbody></table></div><p><strong>保留方法：</strong></p><p>也叫特殊方法，Special Methods</p><ul><li>特点：以双下划线开头和结尾。</li><li>作用：为操作Python类提供统一的属性接口</li><li>方法逻辑：具有特定含义，一般与操作符关联，类定义需要重载。</li></ul><div class="table-container"><table><thead><tr><th>保留方法</th><th><code>对应操作</code></th><th><code>描述</code></th></tr></thead><tbody><tr><td><code>obj.__init__()</code></td><td><code>obj=ClasssName()</code></td><td>初始化实例对象的函数逻辑</td></tr><tr><td><code>obj.__del__()</code></td><td><code>del obj</code></td><td>删除实例对象</td></tr><tr><td><code>obj.__repr__()</code></td><td><code>repr(obj)</code></td><td>定义对象可打印字符串的函数逻辑</td></tr><tr><td><code>obj.__str__()</code></td><td><code>str(obj)</code></td><td>定义对象字符串转换操作的逻辑函数</td></tr><tr><td><code>obj.__bytes__()</code></td><td><code>bytes(obj)</code></td><td>定义对象字节串转换操作的函数逻辑</td></tr><tr><td><code>obj.__format__()</code></td><td><code>obj.format()</code></td><td>定义对象格式化输出的函数逻辑</td></tr><tr><td><code>obj.__hash_()</code></td><td><code>hash(obj)</code></td><td>定义对象哈希操作的函数逻辑</td></tr><tr><td><code>obj.__bool__()</code></td><td><code>bool(obj)</code></td><td>定义对象布尔运算的函数逻辑</td></tr><tr><td><code>obj.__len__()</code></td><td><code>len(obj)</code></td><td>定义对象长度操作的函数逻辑</td></tr><tr><td><code>obj.__reversed__()</code></td><td><code>obj.reversed()</code></td><td>定义对象逆序的函数逻辑</td></tr><tr><td><code>obj.__abs__()</code></td><td><code>abs(obj)</code></td><td>定义对象绝对值操作的函数逻辑</td></tr><tr><td><code>obj.__int__()</code></td><td><code>int(obj)</code></td><td>定义对象整数转换的函数逻辑</td></tr></tbody></table></div><p><code>dir(obj)</code>查看对象所有的属性和方法。</p><p><a href="https://diveintopython3.net/special-method-names.html#acts-like-function">特殊方法</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Python 类的构建</title>
      <link href="2020/10/29/2.%E7%B1%BB%E7%9A%84%E6%9E%84%E5%BB%BA/"/>
      <url>2020/10/29/2.%E7%B1%BB%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="2-Python-类的构建"><a href="#2-Python-类的构建" class="headerlink" title="2.Python 类的构建"></a>2.Python 类的构建</h2><h3 id="1-Class"><a href="#1-Class" class="headerlink" title="1. Class"></a>1. Class</h3><h4 id="1-class的定义"><a href="#1-class的定义" class="headerlink" title="1. class的定义"></a>1. class的定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名每个单词首字母大写&gt;:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;类描述块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#解释</span></span><br><span class="line">    语句</span><br></pre></td></tr></table></figure><ul><li>​    类描述：在类定义后的首行，以独立字符串形式定义，<code>&lt;类名&gt;.__doc__</code>属性来访问。</li></ul><h4 id="2-初始化方法和self"><a href="#2-初始化方法和self" class="headerlink" title="2. 初始化方法和self"></a>2. 初始化方法和self</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>)：</span></span><br><span class="line"><span class="class">语句块</span></span><br></pre></td></tr></table></figure><p>类实例化时所用的方法，可以接收参数并完成初识操作。</p><p><code>__init__</code>:</p><ul><li>参数：第一个参数是<code>self</code>，表示类<strong>实例自身</strong>，<strong>其它参数是实例参数</strong>。</li><li>返回值：构造方法没有返回值，或者返回<code>None</code>，不然<code>TypeError</code>.</li></ul><p><code>self</code>: 在类定义内部代表类的实例</p><ul><li><code>self</code>是Python面向对象约定的一个类参数</li><li><code>self</code>代表类的实例，在类内部，<code>self</code>用于组合访问实例相关的属性和方法</li><li>相比，类名代表类对象本身</li></ul><h4 id="3-类的属性和方法"><a href="#3-类的属性和方法" class="headerlink" title="3. 类的属性和方法"></a>3. 类的属性和方法</h4><ul><li>类的属性：类中定义的变量，用来描述类的一些特性参数</li><li>类的方法：类中定义且与类相关的函数，用来给出类的操作功能</li><li>属性和方法是类对外交互所提供的两种接口方式</li></ul><h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><ul><li><p>类属性：类对象的属性，由所有实例对象所共享</p></li><li><p>实例属性：实例对象的属性，由各实例对象所共享</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p><strong>类属性</strong>实例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeopleClass</span>:</span></span><br><span class="line">    init_num = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        PeopleClass.init_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">p1 = PeopleClass(<span class="string">&quot;Tom&quot;</span>, <span class="number">23</span>)</span><br><span class="line">p2 = PeopleClass(<span class="string">&quot;Jack&quot;</span>, <span class="number">24</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;类总数:&quot;</span>, PeopleClass.init_num)</span><br><span class="line"><span class="built_in">print</span>(p1.name, p2.name)</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">类总数: <span class="number">2</span></span><br><span class="line">Tom Jack</span><br></pre></td></tr></table></figure><p>每次实例化，都会调用<code>__init__</code>方法，那么累变量改变为2，因此类属性也为2。</p><p><strong>属性是类内部定义的变量</strong></p><ul><li>类属性：类对象的属性，有所有实例对象所共享<ul><li>访问：&lt;类名&gt;.&lt;类属性&gt; 或 &lt;对象名&gt;.&lt;类属性&gt; </li></ul></li><li>实例属性：实例对象的属性，由各实例对象所独享<ul><li>访问：&lt;对象名&gt;.&lt;实例属性&gt; </li></ul></li></ul></li></ul><h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><p><strong>方法</strong>是类内部定义的函数</p><ul><li>实例方法：实例对象的方法，由各实例对象独享，最常用的形式</li><li>类方法： <strong>类对象</strong>的方法，由<strong>所有实例对象</strong>共享</li><li>自由方法：类中的一个普通函数，由类所在命名空间管理，<strong>类对象独享</strong></li><li>静态方法：类中的一个普通函数，由<strong>类对象和实例对象共享</strong></li><li>保留方法： 由双下划线开始和结束的方法，保留使用,如<code>__len__()</code></li></ul><p><strong>实例方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class">    <span class="title">def</span> &lt;方法名&gt;(<span class="params">self, [参数列表]</span>)：</span></span><br><span class="line"><span class="class">        语句</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure><p>​    实例方法采用&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用</p><p><strong>类方法</strong></p><p>l    类方法是与类对象相关的函数，由所有实例对象共享</p><p>​        </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params">cls, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>类方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 或&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用。</p><ul><li>类方法至少包含一个参数，表示类对象，一般用<code>cls</code></li><li><code>@classmethod</code> 是装饰器，类方法定义所必须</li><li>类方法只能操作类属性和其他类方法，不能操作实例属性和实例方法</li></ul><p><a href="https://www.programiz.com/python-programming/methods/built-in/classmethod">类方法的使用</a></p><p><strong>自由方法</strong></p><p>自由方法是定义在类命名空间中的普通函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params">cls, [参数列表]</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>自由方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 方式使用。&lt;类名&gt;表示命名空间。</p><ul><li>自由方法不需要<code>self</code>、<code>cls</code>这类参数，可以没有参数</li><li>自由方法只能操作类属性和类方法，不能操作实例属性和实例方法</li><li>自由方法的使用只能使用&lt;类名&gt;</li></ul><p><strong>静态方法</strong></p><p>静态方法是定义在类中的普通函数，能够被所有实例对象共享</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params"> [参数列表]</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>静态方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 或&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用。</p><ul><li>静态方法可以没有参数，可以理解为定义在类中的普通函数</li><li>@staticmethod是装饰器，静态方法定义所必须</li><li>静态方法只能操作类属性和其他类方法， 不能操作实例属性和实例方法</li><li>相比自由方法，静态方法能够使用<strong>类名</strong>和<strong>对象名</strong>两种操作方式使用</li></ul><h4 id="4-类的析构函数"><a href="#4-类的析构函数" class="headerlink" title="4. 类的析构函数"></a>4. 类的析构函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span>(<span class="params">self</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>析构函数在<strong>真实</strong>删除实例对象是被调用。</p><ul><li>函数名和参数：Python解释器内部约定，保留方法</li><li>调用条件：当实例对象被真实删除是，才调用该函数内语句</li><li>真实删除：当前对象的引用次数为0或当前程序退出(垃圾回收机制)</li></ul><p><strong>Python类的内存管理</strong></p><ul><li>实例对象：真实创建的对象，分配对象内存</li><li>实例引用：实例的指针，仅分配指针内存</li></ul><p>内存管理机制：</p><ul><li><p>在删除对象前，Python解释器会检查引用次数</p></li><li><p>引用次数不为0，则仅删除当前引用；0，则删除对象</p></li><li><p>如果程序退出，则由垃圾回收机制删除对象</p><p>利用<code>sys.getrefcount(&lt;对象名&gt;)</code>获取对象引用次数，其返回值为被引用值+1.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. 用python处理NLP</title>
      <link href="2020/10/21/1.%20NLP%20%E7%94%A8python%E5%A4%84%E7%90%86/"/>
      <url>2020/10/21/1.%20NLP%20%E7%94%A8python%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h3 id="1-用python处理NLP"><a href="#1-用python处理NLP" class="headerlink" title="1. 用python处理NLP"></a>1. 用python处理NLP</h3><p>本文是[Sanjaya’s Blog] 中 <a href="https://sanjayasubedi.com.np/nlp/nlp-intro/">Natural Language Processing with Python</a>的笔记，但不会翻译所有内容。</p><p> <a href="https://github.com/aigonna/nlp-tutorial/blob/main/nlp_with_python.ipynb">jupyter notebook 链接</a></p><p>NLP是让计算机理解人类语言。广泛应用于信息检索(搜索引擎)、文本分类，自然语言生成等等。</p><p>典型的NLP应用流程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726440.png" alt="image-20210422171600732" style="zoom:33%;" /></p><h4 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1. 预处理"></a>1. 预处理</h4><h5 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h5><p>预处理一般要进过提取token，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;This warning shouldn&#x27;t be taken lightly.&quot;</span></span><br><span class="line"><span class="built_in">print</span>(text.split(sep=<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">====================================================</span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;warning&#x27;</span>, <span class="string">&quot;shouldn&#x27;t&quot;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;taken&#x27;</span>, <span class="string">&#x27;lightly.&#x27;</span>]</span><br></pre></td></tr></table></figure><p>用re去掉标点符号punctuation character.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">clean_text = re.sub(<span class="string">r&quot;\p&#123;P&#125;+&quot;</span>, <span class="string">&quot;&quot;</span>, text)</span><br><span class="line"><span class="built_in">print</span>(clean_text.split())</span><br><span class="line">=================================</span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;warning&#x27;</span>, <span class="string">&#x27;shouldnt&#x27;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;taken&#x27;</span>, <span class="string">&#x27;lightly&#x27;</span>]<span class="comment">#去掉了shouldn&#x27;t中的标点符号</span></span><br></pre></td></tr></table></figure><p>其中：比 <code>str.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation))</code>要简洁吧！</p><p><a href="https://stackoverflow.com/questions/50302046/what-does-pp-mean">\p{P}</a></p><ul><li><code>\p&#123;P&#125;</code> is “Any punctuation character” 标点符号</li><li><code>\p&#123;Z&#125;</code> is “Any whitespace character” 空格</li></ul><p>也可以用spacy. </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">安装语言词典</span></span><br><span class="line">pip install spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line"><span class="built_in">print</span>([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line">===============================================</span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;warning&#x27;</span>, <span class="string">&#x27;should&#x27;</span>, <span class="string">&quot;n&#x27;t&quot;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;taken&#x27;</span>, <span class="string">&#x27;lightly&#x27;</span>, <span class="string">&#x27;:)&#x27;</span>, <span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure><h5 id="停用词移除"><a href="#停用词移除" class="headerlink" title="停用词移除"></a>停用词移除</h5><p>因为这些字符中像a, an, the等出现非常频繁，但又没什么意义，还加大计算量，影响后续结果，所以去掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>([(token.text, token.is_stop) <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure><p>更常用的是：</p><p><code>text_rev_stops = [word for word in tokenize if not word in stopwords]</code></p><h5 id="Stemming-词干提取"><a href="#Stemming-词干提取" class="headerlink" title="Stemming 词干提取"></a>Stemming 词干提取</h5><p>比如cats 的词干是cat， meeting词干是meet</p><h5 id="Lemmatisation-词元提取"><a href="#Lemmatisation-词元提取" class="headerlink" title="Lemmatisation 词元提取"></a>Lemmatisation 词元提取</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> ([(token.text, token.lemma_) <span class="keyword">for</span> token <span class="keyword">in</span> nlp(<span class="string">&quot;we are meeting tomorrow&quot;</span>)])</span><br><span class="line"><span class="built_in">print</span> ([(token.text, token.lemma_) <span class="keyword">for</span> token <span class="keyword">in</span> nlp(<span class="string">&quot;i am going to a meeting&quot;</span>)])</span><br><span class="line">================================================================================</span><br><span class="line">[(<span class="string">&#x27;we&#x27;</span>, <span class="string">&#x27;-PRON-&#x27;</span>), (<span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;be&#x27;</span>), (<span class="string">&#x27;meeting&#x27;</span>, <span class="string">&#x27;meet&#x27;</span>), (<span class="string">&#x27;tomorrow&#x27;</span>, <span class="string">&#x27;tomorrow&#x27;</span>)]</span><br><span class="line">[(<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), (<span class="string">&#x27;am&#x27;</span>, <span class="string">&#x27;be&#x27;</span>), (<span class="string">&#x27;going&#x27;</span>, <span class="string">&#x27;go&#x27;</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;meeting&#x27;</span>, <span class="string">&#x27;meeting&#x27;</span>)]</span><br></pre></td></tr></table></figure><h5 id="POS-词性标注"><a href="#POS-词性标注" class="headerlink" title="POS 词性标注"></a>POS 词性标注</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> ([(token.text, token.pos_) <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line">========================================================</span><br><span class="line">[(<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;DET&#x27;</span>), (<span class="string">&#x27;warning&#x27;</span>, <span class="string">&#x27;NOUN&#x27;</span>), (<span class="string">&#x27;should&#x27;</span>, <span class="string">&#x27;VERB&#x27;</span>), (<span class="string">&quot;n&#x27;t&quot;</span>, <span class="string">&#x27;ADV&#x27;</span>), (<span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;VERB&#x27;</span>), (<span class="string">&#x27;taken&#x27;</span>, <span class="string">&#x27;VERB&#x27;</span>), (<span class="string">&#x27;lightly&#x27;</span>, <span class="string">&#x27;ADV&#x27;</span>), (<span class="string">&#x27;:)&#x27;</span>, <span class="string">&#x27;NOUN&#x27;</span>), (<span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;NOUN&#x27;</span>), (<span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;NOUN&#x27;</span>), (<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;PUNCT&#x27;</span>)]</span><br></pre></td></tr></table></figure><h5 id="第一部分总结"><a href="#第一部分总结" class="headerlink" title="第一部分总结"></a>第一部分总结</h5><p>使用包括spacy, NLTK， gensim, textblob来预处理，为后续训练和推理做准备。</p><h4 id="2-特征提取"><a href="#2-特征提取" class="headerlink" title="2. 特征提取"></a>2. 特征提取</h4><p>这部分介绍了sklearn和spacy。</p><h5 id="二值化编码-Binary-Encoding"><a href="#二值化编码-Binary-Encoding" class="headerlink" title="二值化编码 Binary Encoding"></a>二值化编码 Binary Encoding</h5><p>不是one-hot而是一个跟词汇表一样长的，文本出现的位置为1的向量。</p><p>这部分使用文本为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">texts = [</span><br><span class="line">    <span class="string">&quot;blue car and blue window&quot;</span>,</span><br><span class="line">    <span class="string">&quot;black crow in the window&quot;</span>,</span><br><span class="line">    <span class="string">&quot;i see my reflection in the window&quot;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>先建立词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab = <span class="built_in">sorted</span>(<span class="built_in">set</span>(word <span class="keyword">for</span> sentence <span class="keyword">in</span> texts <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(vocab), vocab)</span><br><span class="line">==================================================================================</span><br><span class="line"><span class="number">12</span> [<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;crow&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;reflection&#x27;</span>, <span class="string">&#x27;see&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;window&#x27;</span>]</span><br></pre></td></tr></table></figure><p>按词汇出现位置来向量化输入文本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_transform</span>(<span class="params">text</span>):</span></span><br><span class="line">    output = np.zeros(<span class="built_in">len</span>(vocab))</span><br><span class="line">    words = <span class="built_in">set</span>(text.split())</span><br><span class="line">    <span class="comment">#如果每个词在词汇表中就把该位置置为1</span></span><br><span class="line">    <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab):</span><br><span class="line">        output[i] = v <span class="keyword">in</span> words</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(binary_transform(<span class="string">&quot;i saw crow&quot;</span>))</span><br><span class="line">=====================================================</span><br><span class="line">[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br></pre></td></tr></table></figure><p>也可以直接用sklearn中的<code>CountVectorizer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">vec = CountVectorizer(binary=<span class="literal">True</span>)</span><br><span class="line">vec.fit(texts)</span><br><span class="line"><span class="built_in">print</span>([w <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">sorted</span>(vec.vocabulary_.keys())])</span><br><span class="line">===========================================================</span><br><span class="line">[<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;crow&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;reflection&#x27;</span>, <span class="string">&#x27;see&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;window&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">pd.DataFrame(vec.transform(texts).toarray(), columns=<span class="built_in">sorted</span>(vec.vocabulary_.keys()))</span><br></pre></td></tr></table></figure><p>输出如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726442.png" alt="image-20210422233214499" style="zoom: 40%;" /></p><h5 id="Counting"><a href="#Counting" class="headerlink" title="Counting"></a>Counting</h5><p>Counting很像Binary，但这不仅会统计会不会出现，还会计算单词出现多少次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vec = CountVectorizer(binary=<span class="literal">False</span>) <span class="comment">#默认为False,可以不写</span></span><br><span class="line">vec.fit(texts)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(vec.transform(texts).toarray(), columns=<span class="built_in">sorted</span>(vec.vocabulary_.keys()))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726443.png" alt="image-20210422233536372" style="zoom:40%;" /></p><p>输出比二值化多了出现次数的信息，本质上我们会用出现次数来作为权重。但是在实际应用中，因为 <code>a, an, have</code>等等词将会有更大的权重。接下来会证明这方法在搜索引擎中的局限性。</p><h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>TF-IDF是<strong>term frequency-inverse document frequency</strong>，中文: 词频—逆文档频率。</p><script type="math/tex; mode=display">TF= \frac{\text{某个词在文章中出现的次数}}{\text{文章总词数}}\\\\IDF =\text{log} (\frac{\text{语料中总文档数}}{\text{包含该词的文档数} + 1})</script><p>如果一个词越常见，IDF越小越接近于0.加1是为了避免分母为0.</p><script type="math/tex; mode=display">\text{TF-IDF} = \text{TF} \times  \text{IDF}</script><p>更习惯写法是：</p><script type="math/tex; mode=display">\text{tf-idf }(t, d, D) = \text{tf }(t, d)  \times \text{idf }(t, D)</script><p>其中：</p><ul><li>$t$ 是词(或统计项)</li><li>$d$ 是出现该词的文档总词数</li><li>$D$ 是语料库总文档数</li></ul><p>使用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vec = TfidfVectorizer()</span><br><span class="line">vec.fit(texts)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(vec.transform(texts).toarray(), columns=<span class="built_in">sorted</span>(vec.vocabulary_.keys()))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726444.png" alt="image-20210423000135355" style="zoom:40%;" /></p><p>这部分主要介绍了不同方法转换文本为数值特征，然后“喂”给机器学习模型。</p><h4 id="3-文本聚类"><a href="#3-文本聚类" class="headerlink" title="3. 文本聚类"></a>3. 文本聚类</h4><p>聚类算法有Kmeans， DBSCAN， Spectral clustering(谱聚类)， 层次聚类等等。</p><ul><li>KMeans 可以用在没见过的数据集上</li><li>DBSCAN不可以用在新的没见过的数据上</li></ul><p><a href="https://jange.readthedocs.io/en/latest/tutorial.html#classification">jange 使用</a>， 这好像作者自己写的一个包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> jange <span class="keyword">import</span> ops, stream, vis</span><br><span class="line"></span><br><span class="line">ds = stream.from_csv(</span><br><span class="line">    <span class="string">&quot;https://raw.githubusercontent.com/jangedoo/jange/master/dataset/bbc.csv&quot;</span>,</span><br><span class="line">    columns=<span class="string">&#x27;news&#x27;</span>,</span><br><span class="line">    context_column=<span class="string">&quot;type&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract clusters</span></span><br><span class="line">result_collector = &#123;&#125;</span><br><span class="line">clusters_ds = ds.apply(</span><br><span class="line">    ops.text.clean.pos_filter(<span class="string">&quot;NOUN&quot;</span>, keep_matching_tokens=<span class="literal">True</span>),</span><br><span class="line">    ops.text.encode.tfidf(max_features=<span class="number">5000</span>, name=<span class="string">&quot;tfidf&quot;</span>),</span><br><span class="line">    ops.cluster.minibatch_kmeans(n_clusters=<span class="number">5</span>),</span><br><span class="line">    result_collector=result_collector,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Get features extracted by tfidf and reduce the dimensions</span></span><br><span class="line">features_ds = result_collector[clusters_ds.applied_ops.find_by_name(<span class="string">&quot;tfidf&quot;</span>)] </span><br><span class="line">reduced_features = features_ds.apply(ops.dim.pca(n_dim=<span class="number">2</span>)) <span class="comment">#用pca降到2维</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualization</span></span><br><span class="line">vis.cluster.visualize(reduced_features, clusters_ds)</span><br></pre></td></tr></table></figure><p>这个会报错，<code>ValueError: not enough values to unpack (expected 2, got 0)</code></p><p>放下原文效果：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726445.png" alt="cluster_jange" style="zoom:50%;" /></p><p>用sklearn分析bbc 5类文档的数据集，总数为2225文档，包含商业，娱乐， 政策， 运动和科技。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_files</span><br><span class="line"></span><br><span class="line">random_state = <span class="number">0</span></span><br><span class="line">data = load_files(data_dir, encoding=<span class="string">&#x27;utf-8&#x27;</span>, decode_error=<span class="string">&#x27;replace&#x27;</span>, random_state=random_state)</span><br><span class="line">df = pd.DataFrame(<span class="built_in">list</span>(<span class="built_in">zip</span>(data[<span class="string">&#x27;data&#x27;</span>], data[<span class="string">&#x27;target&#x27;</span>])), columns=[<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br><span class="line">df.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><strong>特征提取</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#特征提取</span></span><br><span class="line">vec = TfidfVectorizer(stop_words=<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">vec.fit(df.text.values)</span><br><span class="line">features = vec.transform(df.text.values)</span><br></pre></td></tr></table></figure><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cls = MiniBatchKMeans(n_clusters=<span class="number">5</span>, random_state=random_state)</span><br><span class="line">cls.fit(features)</span><br><span class="line">cls.predict(features)</span><br><span class="line">cls.labels_</span><br></pre></td></tr></table></figure><p><strong>可视化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">2</span>, random_state=random_state)</span><br><span class="line">reduced_features = pca.fit_transform(features.toarray()) <span class="comment">#将特征降到2为</span></span><br><span class="line">reduced_cluster_centers = pca.transform(cls.cluster_centers_) <span class="comment">#将聚类中心降到2D</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">16</span>))</span><br><span class="line">plt.scatter(reduced_features[:,<span class="number">0</span>], reduced_features[:, <span class="number">1</span>], c=cls.predict(features))</span><br><span class="line">plt.scatter(reduced_cluster_centers[:,<span class="number">0</span>], reduced_cluster_centers[:,<span class="number">1</span>], marker=<span class="string">&#x27;x&#x27;</span>, s=<span class="number">150</span>, c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726446.png" alt="vispca" style="zoom:40%;" /></p><p><strong>评估</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> homogeneity_score <span class="comment">#适合标签在0-1之间</span></span><br><span class="line"></span><br><span class="line">homogeneity_score(df.label, cls.predict(features))</span><br><span class="line">=========================================</span><br><span class="line"><span class="number">0.5433462110559382</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score <span class="comment">#标签在-1,1</span></span><br><span class="line">silhouette_score(features, labels=cls.predict(features))</span><br><span class="line">=====================================================================</span><br><span class="line"><span class="number">0.009927737289334684</span></span><br></pre></td></tr></table></figure><h4 id="4-主题建模"><a href="#4-主题建模" class="headerlink" title="4. 主题建模"></a>4. 主题建模</h4><blockquote><p>0: game match player win<br>        1: government minister election</p></blockquote><p>上面两句话，0句是运动类，1句是政策类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>, disable=[<span class="string">&#x27;parser&#x27;</span>, <span class="string">&#x27;ner&#x27;</span>])<span class="comment">#语言模型 禁止参数</span></span><br><span class="line">random_state = <span class="number">0</span></span><br><span class="line">data_dir = <span class="string">r&quot;/content/bbc&quot;</span></span><br><span class="line">data = load_files(data_dir, encoding=<span class="string">&#x27;utf-8&#x27;</span>, decode_error=<span class="string">&#x27;replace&#x27;</span>, random_state=random_state)</span><br><span class="line">df = pd.DataFrame(<span class="built_in">list</span>(<span class="built_in">zip</span>(data[<span class="string">&#x27;data&#x27;</span>], data[<span class="string">&#x27;target&#x27;</span>])), columns=[<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">only_nouns</span>(<span class="params">texts</span>):</span></span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> nlp.pipe(texts):</span><br><span class="line">        <span class="comment">#因为名词对于模型影响最大就只用NOUN</span></span><br><span class="line">        noun_text = <span class="string">&#x27; &#x27;</span>.join(token.lemma_ <span class="keyword">for</span> token <span class="keyword">in</span> doc <span class="keyword">if</span> token.pos_ == <span class="string">&#x27;NOUN&#x27;</span>) </span><br><span class="line">        output.append(noun_text)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line">df[<span class="string">&#x27;text&#x27;</span>] = only_nouns(df[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">df.head()</span><br><span class="line">=======================================================</span><br><span class="line">textlabel</span><br><span class="line"><span class="number">0</span>boss bag award executive business magazine tit...<span class="number">0</span></span><br><span class="line"><span class="number">1</span>copy bumper sale fi shooter game copy sale com...<span class="number">4</span></span><br><span class="line"><span class="number">2</span>msp climate warning climate change control dec...<span class="number">2</span></span><br><span class="line"><span class="number">3</span>pavey success view week race track bronze inju...<span class="number">3</span></span><br><span class="line"><span class="number">4</span>tory rethink association candidate election ag...<span class="number">2</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261726447.png" alt="image-20210423142201549" style="zoom:40%;" /></p><p><strong>训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">n_topics = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line">vec = TfidfVectorizer(max_features=<span class="number">5000</span>, stop_words=<span class="string">&#x27;english&#x27;</span>, max_df=<span class="number">0.85</span>, min_df=<span class="number">2</span>)</span><br><span class="line">features = vec.fit_transform(df.text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> NMF</span><br><span class="line">cls = NMF(n_components=n_topics, random_state=random_state)</span><br><span class="line">cls.fit(features)</span><br><span class="line">===========================================================</span><br><span class="line">NMF(alpha=<span class="number">0.0</span>, beta_loss=<span class="string">&#x27;frobenius&#x27;</span>, init=<span class="literal">None</span>, l1_ratio=<span class="number">0.0</span>, max_iter=<span class="number">200</span>,</span><br><span class="line">    n_components=<span class="number">5</span>, random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>, solver=<span class="string">&#x27;cd&#x27;</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">    verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><code>cls.components_</code>将会是一个[n<em>topics, n_features].这儿回事[5, 5000]. `cls.components</em>.shape`可以看到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#向量里找到唯一次的列表</span></span><br><span class="line">features = vec.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="comment">#最影响每个主题的单词数</span></span><br><span class="line">n_top_words = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, topic_vec <span class="keyword">in</span> <span class="built_in">enumerate</span>(cls.components_):</span><br><span class="line">    <span class="built_in">print</span>(i, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="comment">#topic_vec.argsort() 词索引按最小分数和最大分数生成的arry</span></span><br><span class="line">    <span class="comment">#[-1:-n_top_words:-1]切片到最大15个词</span></span><br><span class="line">    <span class="keyword">for</span> fid <span class="keyword">in</span> topic_vec.argsort()[-<span class="number">1</span>:-n_top_words:-<span class="number">1</span>]:</span><br><span class="line">        <span class="built_in">print</span>(features[fid], end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">=============================================================</span><br><span class="line"><span class="number">0</span> growth sale economy year company market share rate price firm profit oil analyst month </span><br><span class="line"><span class="number">1</span> film award actor star actress director nomination movie year comedy role festival prize category </span><br><span class="line"><span class="number">2</span> game player match team injury club time win season coach goal victory title champion </span><br><span class="line"><span class="number">3</span> election party government tax minister leader people campaign chancellor plan issue voter country taxis </span><br><span class="line"><span class="number">4</span> phone people music technology service user broadband software computer tv network device video site </span><br></pre></td></tr></table></figure><p><strong>预测</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">new_articles = [</span><br><span class="line">     <span class="string">&quot;Playstation network was down so many people were angry&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Germany scored 7 goals against Brazil in worldcup semi-finals&quot;</span>           </span><br><span class="line">]</span><br><span class="line">cls.transform(vec.transform(new_articles)).argsort(axis=<span class="number">1</span>)[:,-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="5-最近邻搜索"><a href="#5-最近邻搜索" class="headerlink" title="5. 最近邻搜索"></a>5. 最近邻搜索</h4><p><strong>数据预处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"></span><br><span class="line">bunch = fetch_20newsgroups(remove=<span class="string">&#x27;headers&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(bunch), bunch.keys())</span><br><span class="line">===========================================================</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">sklearn</span>.<span class="title">utils</span>.<span class="title">Bunch</span>&#x27;&gt; <span class="title">dict_keys</span>(<span class="params">[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;filenames&#x27;</span>, <span class="string">&#x27;target_names&#x27;</span>, <span class="string">&#x27;target&#x27;</span>, <span class="string">&#x27;DESCR&#x27;</span>]</span>)</span></span><br></pre></td></tr></table></figure><p>看看每个bunch数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bunch.data[<span class="number">0</span>]</span><br><span class="line">====================================================================</span><br><span class="line"> <span class="string">&#x27;I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n&#x27;</span></span><br></pre></td></tr></table></figure><p>提取特征:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vec = TfidfVectorizer(max_features=<span class="number">10000</span>)</span><br><span class="line">features = vec.fit_transform(bunch.data)</span><br><span class="line"><span class="built_in">print</span>(features.shape)</span><br><span class="line">================================================================</span><br><span class="line">(<span class="number">11314</span>, <span class="number">10000</span>)</span><br></pre></td></tr></table></figure><p><strong>训练</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import NearestNeighbors</span><br><span class="line">knn &#x3D; NearestNeighbors(n_neighbors&#x3D;10, metric&#x3D;&#39;cosine&#39;)</span><br><span class="line">knn.fit(features)</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">NearestNeighbors(algorithm&#x3D;&#39;auto&#39;, leaf_size&#x3D;30, metric&#x3D;&#39;cosine&#39;,</span><br><span class="line">                 metric_params&#x3D;None, n_jobs&#x3D;None, n_neighbors&#x3D;10, p&#x3D;2,</span><br><span class="line">                 radius&#x3D;1.0)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">knn.kneighbors(features[<span class="number">0</span>:<span class="number">1</span>], return_distance=<span class="literal">False</span>)</span><br><span class="line">===================================================</span><br><span class="line">array([[   <span class="number">0</span>,  <span class="number">958</span>, <span class="number">8013</span>, <span class="number">8266</span>,  <span class="number">659</span>, <span class="number">5553</span>, <span class="number">3819</span>, <span class="number">2554</span>, <span class="number">6055</span>, <span class="number">7993</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">knn.kneighbors(features[<span class="number">0</span>:<span class="number">1</span>], return_distance=<span class="literal">True</span>)</span><br><span class="line">===========================================</span><br><span class="line">(array([[<span class="number">0.</span>        , <span class="number">0.35119023</span>, <span class="number">0.62822688</span>, <span class="number">0.64738668</span>, <span class="number">0.66613124</span>,</span><br><span class="line">         <span class="number">0.67267273</span>, <span class="number">0.68149664</span>, <span class="number">0.68833514</span>, <span class="number">0.70024449</span>, <span class="number">0.70169709</span>]]),</span><br><span class="line"> array([[   <span class="number">0</span>,  <span class="number">958</span>, <span class="number">8013</span>, <span class="number">8266</span>,  <span class="number">659</span>, <span class="number">5553</span>, <span class="number">3819</span>, <span class="number">2554</span>, <span class="number">6055</span>, <span class="number">7993</span>]]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">input_texts = [<span class="string">&quot;any recommendations for good ftp sites?&quot;</span>, <span class="string">&quot;i need to clean my car&quot;</span>]</span><br><span class="line">input_features = vec.transform(input_texts)</span><br><span class="line"></span><br><span class="line">D, N = knn.kneighbors(input_features, n_neighbors=<span class="number">2</span>, return_distance=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> input_text, distances, neighbors <span class="keyword">in</span> <span class="built_in">zip</span>(input_texts, D, N):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input text = &quot;</span>, input_text[:<span class="number">200</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> dist, neighbor_idx <span class="keyword">in</span> <span class="built_in">zip</span>(distances, neighbors):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Distance = &quot;</span>, dist, <span class="string">&quot;Neighbor idx = &quot;</span>, neighbor_idx)</span><br><span class="line">        <span class="built_in">print</span>(bunch.data[neighbor_idx][:<span class="number">200</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">200</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">200</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">==========================================================================</span><br><span class="line">Input text =  any recommendations for good ftp sites? </span><br><span class="line"></span><br><span class="line">Distance =  <span class="number">0.5870334253639387</span> Neighbor idx =  <span class="number">89</span></span><br><span class="line">I would like to experiment <span class="keyword">with</span> the INTEL <span class="number">8051</span> family.  Does anyone out  </span><br><span class="line">there know of <span class="built_in">any</span> good FTP sites that might have compiliers, assemblers,  </span><br><span class="line">etc.?</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">Distance =  <span class="number">0.6566334116701875</span> Neighbor idx =  <span class="number">7665</span></span><br><span class="line">Hi!</span><br><span class="line"></span><br><span class="line">I am looking <span class="keyword">for</span> ftp sites (where there are freewares <span class="keyword">or</span> sharewares)</span><br><span class="line"><span class="keyword">for</span> Mac. It will <span class="built_in">help</span> a lot <span class="keyword">if</span> there are driver source codes <span class="keyword">in</span> those </span><br><span class="line">ftp sites. <span class="type">Any</span> information <span class="keyword">is</span> appreciated. </span><br><span class="line"></span><br><span class="line">Thanks <span class="keyword">in</span> </span><br><span class="line">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">========================================================================================================================================================================================================</span><br><span class="line"></span><br><span class="line">Input text =  i need to clean my car </span><br><span class="line"></span><br><span class="line">Distance =  <span class="number">0.6592186982514803</span> Neighbor idx =  <span class="number">8013</span></span><br><span class="line">In article &lt;<span class="number">49422</span>@fibercom.COM&gt; rrg@rtp.fibercom.com (Rhonda Gaines) writes:</span><br><span class="line">&gt;</span><br><span class="line">&gt;I<span class="string">&#x27;m planning on purchasing a new car and will be trading in my &#x27;</span><span class="number">90</span></span><br><span class="line">&gt;Mazda MX-<span class="number">6</span> DX.  I<span class="string">&#x27;ve still got 2 more years to pay o</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">Distance =  0.692693967282819 Neighbor idx =  7993</span></span><br><span class="line"><span class="string">I bought a car with a defunct engine, to use for parts</span></span><br><span class="line"><span class="string">for my old but still running version of the same car.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The car I bought has good tires.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Is there anything in particular that I should do to</span></span><br><span class="line"><span class="string">stor</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">========================================================================================================================================================================================================</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sklearn </tag>
            
            <tag> nlp-tutorial </tag>
            
            <tag> tf-idf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9.ELMO 论文笔记</title>
      <link href="2020/10/19/NLP%20Paper%209.%20ELMO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/19/NLP%20Paper%209.%20ELMO%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="9-ELMO-论文笔记"><a href="#9-ELMO-论文笔记" class="headerlink" title="9.ELMO 论文笔记"></a>9.ELMO 论文笔记</h3><p>本文是 <a href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a>笔记.ELMO是Embeddings from Language Models的简称。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261832730.png" alt="image-20211201220735638" style="zoom:20%;" /></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文作者引入了一种新的深层的上下文词表示，该模型能:</p><ol><li>使用词的复杂特征</li><li>在不同语境下词的多义性</li></ol><p>本文的词向量是学习深层双向语言模型的内部状态的功能得到的。作者发现这些表示能轻松加到已存在的模型上，并且在6大NLP问题上有显著的提升，像QA，文本蕴含，情感分析。还分析了暴露预训练网络的深层内部是至关重要的，这允许下游模型来混合不同类型的半监督信号。</p><h4 id="3-1-Bidirectional-language-models"><a href="#3-1-Bidirectional-language-models" class="headerlink" title="3.1 Bidirectional language models"></a>3.1 Bidirectional language models</h4><p>给定N个token的序列，<script type="math/tex">(t_1, t_2, \cdots, t_n)</script>，前馈语言模型在给定前<script type="math/tex">(t_1, t_2, \cdots, t_{n-1})</script>的情况下来计算<script type="math/tex">t_k</script>的概率:(LM就是一个预测下一个词的任务)</p><script type="math/tex; mode=display">p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^Np(t_k|t_1, t_2, \cdots, t_{k-1}) \tag{1}</script><p>如果通过token embeddings 或 字符卷积后再输入到L层的前向LSTMs.在位置k,每层LSTM输出一个上下独立的表示为<script type="math/tex">\overrightarrow{\mathbf{h}}_{k,j}^{LM}</script>，其中<script type="math/tex">j=1, \cdots L.</script>(表示第几层)。最上面一层LSTM的输出<script type="math/tex">\overrightarrow{\mathbf{h}}_{k,j}^{LM}</script>被用来给softmax层预测下一个token<script type="math/tex">t_{k+1}</script>。</p><p>反向的LM类似于前向LM,除了将输入序列反向，目标也变成预测前一个词:</p><script type="math/tex; mode=display">p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^Np(t_k|t_1, t_2, \cdots, t_{N}) \tag{2}</script><p>这可以用类似的前向LM实现，对于反向LSTM层，给定<script type="math/tex">(t_{k+1}, \cdots,t_N)</script>生成的<script type="math/tex">t_k</script></p><p>表征为 <script type="math/tex">\overleftarrow{\mathbf{h}}_{k,j}^{LM}</script>.</p><p>这样双向LM结合了前向和反向的LM特点。目标就是最大化两者联合的对数似然函数:</p><script type="math/tex; mode=display">\begin{array}{l}\sum_{k=1}^{N}\left(\log p\left(t_{k} \mid t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)\right. \\\left.\quad+\log p\left(t_{k} \mid t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)\end{array} \tag{3}</script><p>作者将前、后向的token表征<script type="math/tex">\Theta_x</script>和softmax层<script type="math/tex">\Theta_s</script>参数联系起来，同时保持每个方向的LSTM参数分开。但在方向间共享些权重而不是使用完全独立的参数。</p><h4 id="3-2-ELMo"><a href="#3-2-ELMo" class="headerlink" title="3.2 ELMo"></a>3.2 ELMo</h4><p>ELMo是biLM中中间层表征的任务特定的组合。对于每个token<script type="math/tex">t_k</script>,L层biLM计算<script type="math/tex">2L+1</script>表征的集合：</p><script type="math/tex; mode=display">\begin{aligned}R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\&=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}\end{aligned} \tag{4}</script><p>其中<script type="math/tex">\mathbf{h}_{k, j}^{L M} = \left[\overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{LM} \right]</script>.当<script type="math/tex">j=0</script>时，就是token层，因此可以简写为式4中第二行。</p><p>ELMo将所有层的<script type="math/tex">\mathbf{R}</script>(式4)压缩成单一向量,<script type="math/tex">\mathbf{ELMo}_k = E(\mathbf{R}_k; \Theta)</script>.最简单的就只选择最上面一层作为token的表示,<script type="math/tex">\mathbf{h}_{k, L}^{LM}</script>.更通用做法是，对所有层求权重和:</p><script type="math/tex; mode=display">\mathbf{E L M o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M} \tag{1}</script><p>在(1)中，(按论文标号)<script type="math/tex">s^{task}</script>表示经过softmax-normalized权重和放缩参数<script type="math/tex">\gamma^{task}</script>允许任务模型放缩ELMo向量。这是个经验参数，也能帮助优化过程。</p><h4 id="3-3-Using-biLMs-for-supervised-NLP-tasks"><a href="#3-3-Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="3.3 Using biLMs for supervised NLP tasks"></a>3.3 Using biLMs for supervised NLP tasks</h4><p>给定一个预训练双向语言模型和对目标NLP任务的监督架构，这是使用双向biLM来提升任务模型是非常简单的。就是运行biLM并记录每层的每个词的表征。然后，让终端任务模型学习这些表征的线性组合。如下所述。</p><p>首先,考虑没有biLM的监督模型底层是差不多的架构，这就允许ELMo以一种固定的规则加到上面。给定一个token<script type="math/tex">(t_1, \cdots, t_N)</script>，使用预训练的词嵌入和选择基于character的表征形成一个上下文无关的token表征是标准的。然后，模型形成一个上下文敏感的表征<script type="math/tex">\mathbf{h}_k</script>，通常使用双向RNNs，CNNs或者前馈神经网络得到。(就是拿这些网络得到token表征输入到biLMs中)。</p><p>为了添加ELMo到监督模型，首先固定biLM的权重，token表征<script type="math/tex">\mathbf{x}_k</script>然后跟ELMo向量<script type="math/tex">\mathbf{ELMo}_k^{task}</script>合并，将这个增强的表征<script type="math/tex">[\mathbf{x}_k; \mathbf{ELMo}_k^{task}]</script>输入到具体任务的RNN中。像SNLI,SquaD，NLP推理和问答任务。作者观察到通过引入针对具体任务的线性权重和用这个增强表征<script type="math/tex">[\mathbf{x}_k; \mathbf{ELMo}_k^{task}]</script>能进一步提升表现。</p><p>最后，加入dropout和<script type="math/tex">\lambda||\mathbf{W}||^2_2</script>到ELMo中，.</p><h4 id="3-4-Pre-trained-bidirectional-language-model-architecture"><a href="#3-4-Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="3.4 Pre-trained bidirectional language model architecture"></a>3.4 Pre-trained bidirectional language model architecture</h4><p>为了在保持纯粹的基于character的输入表征的同时，平衡语言模型整体的复杂度与模型大小和下游任务的计算需求，将单个最佳模型CNN-BIG-LSTM中的embedding和隐藏层维度都减半。最后模型使用L=2的biLSTM，含有4096个units和512维度映射和一个残差连接第一层和第二层。上下文不敏感类型表征用2048character n-gram卷积后接两个高速层和一个线性映射到512的表征。结果上，biLM给每个输入token提供了3个表征，包括那些外部训练集得到的纯粹的character input。对比而言，传统词嵌入方法在固定词表上只提供一层token的表征。</p><h4 id="4-Evalution"><a href="#4-Evalution" class="headerlink" title="4. Evalution"></a>4. Evalution</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261832731.png" alt="image-20211202235427126" style="zoom:20%;" /></p><p>ELMo在大部分任务上都有大的提升。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261832732.png" alt="image-20211203000814840" style="zoom:20%;" /></p><p>ELMo的低层侧重于词性等语法特征，高层侧重于语义特征。上表5中是语义消歧任务，第二层要好与第一层。上表6中是POS任务，第一层要好于第二层。如下图，来自于[4].</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261832733.png" alt="image-20211203003146790" style="zoom: 25%;" /></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>ELMo的优点:</p><ol><li>利用双向的LSTM抽取word embedding，能有效解决一定程度上的一词多义。</li><li>基于字符：ELMo表示纯粹基于字符，然后经过CharCNN之后再作为词的表示，解决了OOV问题，而且输入的词表也很小。</li><li>通用性强，如文中在6大任务中都有比较好的表现。</li></ol><p>缺点:</p><ol><li>使用LSTM抽取特征，效果不如后来的Transformer。</li><li>使用拼接的方式来融合上下文特征，效果有限。</li><li>训练耗时长，RNN要等上一步隐藏状态计算完后才能开始下一步计算。</li></ol><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://blog.csdn.net/Magical_Bubble/article/details/89160032">ELMo解读（论文 + PyTorch源码</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/63115885">NAACL2018:高级词向量(ELMo)详解(超详细) 经典</a></p><p>[3] <a href="https://www.cnblogs.com/wwj99/p/12295999.html">论文翻译——Deep contextualized word representations</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/49271699">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></p><p>[5] <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/ELMo.html">EMLo</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ELMo </tag>
            
            <tag> biLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6. Capsule net 论文笔记</title>
      <link href="2020/10/17/NLP%20Paper%206.Capsule%20Net%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/17/NLP%20Paper%206.Capsule%20Net%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="6-Capsule-net-论文笔记"><a href="#6-Capsule-net-论文笔记" class="headerlink" title="6. Capsule net 论文笔记"></a>6. Capsule net 论文笔记</h3><p>本文是2017年深度学习奠基人Geoffrey Hintons的 <a href="https://arxiv.org/pdf/1710.09829.pdf">Dynamic Routing Between Capsules</a>笔记。</p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>一个capsule胶囊是一组神经元，其激活向量(就是低层的输入向量)表示的是特定类型的实体(如对象或对象的部分)的实例化参数。本文使用<strong>激活向量的长度</strong>来表示实体存在的概率和<strong>其方向来表示实例化后的参数</strong>。活的胶囊通过变换矩阵在低一层做预测，然后作为高层胶囊的实例化参数。当多个预测一致时，高层胶囊激活。本文在求低层到高层胶囊的权重时没有使用反向传播，而使用动态路由来更新：低层胶囊偏好将其输出传送到高层胶囊，使其激活向量和预测值的点积值较大。</p><h4 id="胶囊网络计算过程"><a href="#胶囊网络计算过程" class="headerlink" title="胶囊网络计算过程"></a>胶囊网络计算过程</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831994.png" alt="image-20211018024501596" style="zoom: 33%;" /></p><p>上图来源于 <a href="https://pechyonkin.me/capsules-2/">Understanding Hinton’s Capsule Networks. Part 2. How Capsules Work.</a>.</p><p><strong>Step1:</strong> <strong>预测向量</strong><script type="math/tex">\hat{\mathbf{u}}_{j|i}</script>。</p><p>如上图左侧<script type="math/tex">\mathbf{u}_{i} \in \mathbb{R}^{k \times 1}, i=1,2, \ldots, n</script>表示低层胶囊输入，<script type="math/tex">n</script>表示胶囊数目。图示就是3个，<script type="math/tex">k</script>表示每个胶囊激活向量的长度。比如<script type="math/tex">\mathbf{u}_1</script>表示眼睛这种低层胶囊特征，(其它向量分别表示鼻子、嘴巴等特征用来检测人脸)，作者会将其转换成所谓的<strong>预测向量</strong><script type="math/tex">\hat{\mathbf{u}}_{j|i}</script>，就是乘以一个转换矩阵:</p><script type="math/tex; mode=display">\hat{\mathbf{u}}_{j \mid i}=\mathbf{W}_{i j} \mathbf{u}_{i} \tag{1}</script><p>这个矩阵<script type="math/tex">\mathbf{W}_{i j}</script>是通过<strong>反向传播</strong>学习的。</p><p><strong>Step2:</strong> 高层胶囊<script type="math/tex">j</script>输入<script type="math/tex">\mathbf{s}_{j}</script>。</p><p>然后将得到的预测向量<script type="math/tex">\hat{\mathbf{u}}_{j \mid i}</script>进行加权求和:</p><script type="math/tex; mode=display">\mathbf{s}_{j}=\sum_{i} c_{i j} \hat{\mathbf{u}}_{j \mid i} \tag{2}</script><p>这就是图示“求和“符号部分的数学表示，其中<script type="math/tex">c_{i j}</script>是权重系数，论文中叫耦合系数(coupling coefficients),由<strong>动态路由算法</strong>决定。并且要满足<script type="math/tex">\sum_j c_{i j}=1</script>。其表示低层胶囊<script type="math/tex">i</script>对高层胶囊<script type="math/tex">j</script>的权重大小。这样，作者就得到了<strong>高层胶囊<script type="math/tex">j</script>的输入</strong><script type="math/tex">\mathbf{s}_{j}</script>。</p><blockquote><p>coupling coefficients that are determined by the iterative dynamic routing process</p></blockquote><p><strong>Step 3:</strong> 高层胶囊<script type="math/tex">j</script>输出<script type="math/tex">\mathbf{v}_{j}</script>。</p><p>如图示Squash函数部分，即将得到的输入经过squash函数获得输出<script type="math/tex">\mathbf{v}_{j}</script>。因为胶囊网络是拿胶囊的向量的模长来表示高层胶囊表示的实体存在的概率。因此，论文提出了非线性函数<strong>squashing</strong>。其可以保证<strong>短向量压缩趋于0，长向量趋近于1</strong>，并且保持<strong>向量方向不变</strong>。</p><blockquote><p>use a non-linear “squashing” function to ensure that short vectors get shrunk to almost zero length and long vectors get shrunk to a<br>length slightly below 1.</p></blockquote><script type="math/tex; mode=display">\mathbf{v}_{j}=\frac{\left\|\mathbf{s}_{j}\right\|^{2}}{1+\left\|\mathbf{s}_{j}\right\|^{2}} \frac{\mathbf{s}_{j}}{\left\|\mathbf{s}_{j}\right\|} \tag{3}</script><ul><li>当<script type="math/tex">\mathbf{s}_{j}</script>是一个长向量，那么<script type="math/tex">\mathbf{v}_{j}\rightarrow \frac{\mathbf{s}_{j}}{\left\|\mathbf{s}_{j}\right\|}</script></li><li>当<script type="math/tex">\mathbf{s}_{j}</script>是一个短向量，那么<script type="math/tex">\mathbf{v}_{j}\rightarrow \left\|\mathbf{s}_{j}\right\|\mathbf{s}_{j}</script></li></ul><p>squash函数1D可视化图如下:(可以看看其趋近值)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831995.png" alt="image-20211018203636281" style="zoom:25%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">T_EPSILON = <span class="number">1e-6</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squash</span>(<span class="params">x, axis=-<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    不是公式3原始的样子, 简化成趋近长向量计算</span></span><br><span class="line"><span class="string">    text version of squash, slight different from original one</span></span><br><span class="line"><span class="string">    :param x: vector</span></span><br><span class="line"><span class="string">    :param axis: int</span></span><br><span class="line"><span class="string">    :return: vector</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    s_squared_norm = (x ** <span class="number">2</span>).<span class="built_in">sum</span>(axis, keepdim=<span class="literal">True</span>)</span><br><span class="line">    scale = nn.sqrt(s_squared_norm + T_EPSILON)</span><br><span class="line">    <span class="keyword">return</span> x / scale</span><br></pre></td></tr></table></figure><p>贴个胶囊网络和普通神经网络的区别表:(也来自于[1])。</p><p>注：胶囊网络胶囊输入是<strong>向量</strong>，神经网络神经元输入是<strong>标量</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831996.png" alt="image-20211018204402653" style="zoom:33%;" /></p><p>现在经过3个步骤梳理作者大概知道高层胶囊<script type="math/tex">j</script>输出<script type="math/tex">\mathbf{v}_{j}</script>是怎么计算的。转换矩阵<script type="math/tex">\mathbf{W}_{i j}</script>可以用反向传播算法学习得到，但第二步中</p><p>耦合系数<script type="math/tex">c_{i j}</script>是通过所谓的<strong>动态路由算法</strong>来更新的，那么到底是怎么计算的呢？</p><h4 id="动态路由算法"><a href="#动态路由算法" class="headerlink" title="动态路由算法"></a>动态路由算法</h4><p>动态路由算法伪代码:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831997.png" alt="image-20211019142949164" style="zoom:30%;" /></p><p>按照行号逐一解释:</p><ol><li>定义算法输入:低层<script type="math/tex">l</script>,(因为胶囊网络也是多层的，它的高层输入来自于低层的输出)；该低层<script type="math/tex">l</script>所有胶囊的输出<script type="math/tex">\hat{\mathbf{u}}_{j \mid i}</script>,（就是低层胶囊<script type="math/tex">i</script>经过转换矩阵变换成的向量输入到高层胶囊<script type="math/tex">j</script>; 以及路由迭代次数(超参数，一般设置为3,或4).</li><li>定义一个临时变量<script type="math/tex">b_{ij}</script>,初始化为0.用于迭代计算低层胶囊<script type="math/tex">l</script>中胶囊<script type="math/tex">i</script>到高层<script type="math/tex">l+1</script>中胶囊<script type="math/tex">j</script>.</li><li>迭代<script type="math/tex">r</script>次.</li><li>将<script type="math/tex">b_{ij}</script>经过softmax转换为总和为1的类概率权重，其实，第一轮<script type="math/tex">c_{ij}=1/\max(j)</script>都相等.高层胶囊数目<script type="math/tex">\max(j)</script>, (按照最大熵也应该是这样).</li><li>按照上面式2，作者就可以求出高层胶囊输入<script type="math/tex">\mathbf{s}_{j}</script>.</li><li>类似地按照式3求出高层胶囊<script type="math/tex">j</script>输出<script type="math/tex">\mathbf{v}_{j}</script>.</li><li>将<script type="math/tex">b_{ij}</script>更新为<script type="math/tex">b_{ij}+\hat{\mathbf{u}}_{j \mid i} \cdot \mathbf{v}_{j}</script>.因为<script type="math/tex">\hat{\mathbf{u}}_{j \mid i} \cdot \mathbf{v}_{j}</script>如果是正值且越来越大的话表示<script type="math/tex">\hat{\mathbf{u}}_{j \mid i}</script>与<script type="math/tex">\mathbf{v}_{j}</script>越来越相似。(这个过程也有点类似于聚类,如李宏毅教授<a href="https://www.bilibili.com/video/BV1Qx411V75M/">Capsule</a>)</li></ol><p>最后返回高层胶囊<script type="math/tex">j</script>输出<script type="math/tex">\mathbf{v}_{j}</script>.给个简化示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_routing</span>(<span class="params">x, num_caps, dim_caps, routings=<span class="number">4</span></span>):</span></span><br><span class="line">    batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">    input_num_capsule = x.size(<span class="number">1</span>)</span><br><span class="line">    W = torch.randn(batch_size, input_num_capsule, num_caps*dim_caps)</span><br><span class="line">    u_hat_vecs = torch.matmul(x, W)</span><br><span class="line">    u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule, num_caps, dim_caps))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#交换维度变成:batch_size, num_caps, input_num_capsule, dim_caps</span></span><br><span class="line">    u_hat_vecs = u_hat_vecs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment">#初始化一个[batch_size, num_caps, input_num_capsule]全0的矩阵b, 4, 2, 3</span></span><br><span class="line">    b = torch.zeros_like(u_hat_vecs[:, :, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(routings):</span><br><span class="line">        b = b.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        c = F.softmax(b, dim=<span class="number">2</span>)</span><br><span class="line">        c = c.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)<span class="comment">#换回原来维度</span></span><br><span class="line">        b = b.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#c * u hat vector 过squash得到输出v eisnum 在k这个维度求和</span></span><br><span class="line">        <span class="comment">#其代表dim_caps即每个胶囊维度上求和</span></span><br><span class="line">        v = squash(torch.einsum(<span class="string">&#x27;bij, bijk-&gt;bij&#x27;</span>, (c, u_hat_vecs)))</span><br><span class="line">        <span class="keyword">if</span> i &lt; routings -<span class="number">1</span>: <span class="comment">#如果不到迭代次数 继续求和</span></span><br><span class="line">            b = torch.einsum(<span class="string">&#x27;bij, bijk-&gt;bij&#x27;</span>, (v, u_hat_vecs))</span><br><span class="line">    <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">x = torch.randn((<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">num_caps = <span class="number">2</span></span><br><span class="line">caps_dim = <span class="number">3</span></span><br><span class="line">v_out = dynamic_routing(x, num_caps, caps_dim, routings=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(v_out, v_out.shape)</span><br></pre></td></tr></table></figure><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418">Understanding Hinton’s Capsule Networks</a></p><p>[2] <a href="https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/">Dynamic-Routing-Between-Capsules</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/184928582">浅谈胶囊网络与动态路由算法</a></p><p>[4] <a href="https://leovan.me/cn/2021/03/capsule-network/">capsule-network</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Capsule </tag>
            
            <tag> 动态路由算法 </tag>
            
            <tag> squash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.HAN 论文笔记</title>
      <link href="2020/10/14/NLP%20Paper%205.%20HAN%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/14/NLP%20Paper%205.%20HAN%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="5-HAN-论文笔记"><a href="#5-HAN-论文笔记" class="headerlink" title="5. HAN 论文笔记"></a>5. HAN 论文笔记</h2><p>来自于<a href="https://aclanthology.org/N16-1174.pdf">Hierarchical Attention Networks for Document Classification</a> 论文。</p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>本文提出一种层次attention网络用于文档分类。该模型有两个特别的特点：(i) 其采用层次结构来翻译文档的层次结构。(ii) 在词-句子层次应用两种层次的的注意力机制，当构建文档表示时使其能以不同的注意力来关注越重要或越不重要的的内容。在六个大型文本分类任务进行的实验证明了，提出的架构有大幅度提升，优于之前的方法。可视化注意力层表明模型选择了信息丰富的词和句子。</p><h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h4><p>文本分类是NLP中的一个基本任务。目标是给文本分配标签。广泛应用包括主题标记，情感分类，和垃圾邮件检查。文本分类的传统方法用稀疏的词汇特征来表示文档，如n-grams，然后在该表示上用线性模型或核方法来分类。最近的方法多使用深度学习，如CNN和基于LSTM的循环神经网络来学习文本表示。</p><p>作者只要贡献是一个新的架构，层次注意力网络Hierarchical Attention Network (HAN)，设计用来获取关于文档结构的两个基本的直觉。首先，因为文档有层次结构(词组成句子，句子组成文档)，作者同样构建文档表示，即第一步构建句子的表示然后聚合这些句子表示来表征文档。第二步，观察到在文档中不同词和句子是有不同信息的。并且，词和句子的重要性高度依赖上下文，如，在不同上下文中，同样的词或句子可能重要性不同 （3.5节Context dependent attention weights 中会介绍）。为了包括对该事实的敏感性，作者模型包含两个层次的注意力机制——一个是在word level 另一个在sentence level——这让该模型在构建文档表征是自动给予更多或更少注意力给独立的词或句子。为了说明这个机制，如下图Fig.1实例，其是一个简短的Yelp 评论，任务是根据评论预测1-5星的评级。直觉地，第一、三句在帮助预测星级上有更强的信息；在这些句子中，包含在该评论中的词 <code>delicious,</code><br><code>a-m-a-z-i-n-g</code>贡献了更多暗示积极态度的信息。注意力提供两个好处: 它不仅通常带来更好的表现，而且还可以提供哪些词和句子对于分类决策有帮助的直觉，这在应用和分析中是非常有价值的。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831391.png" alt="image-20211014000432011" style="zoom:33%;" /></p><p>关键区别于之前工作是HAN用上下文来发现什么时候一个序列的token是相关的而不是简单地从上下文中过滤这些token。为了评估该模型表现，对比其他常见文本分类架构，在第3部分中的6个数据集上测试。该模型大幅度优于之前方法。</p><h4 id="2-Hierarchical-Attention-Networks"><a href="#2-Hierarchical-Attention-Networks" class="headerlink" title="2. Hierarchical Attention Networks"></a>2. Hierarchical Attention Networks</h4><p>HAN整体结构如下图Fig.2所示。包括几个部分: 一个词序列编码器，一个词级别注意力层，一个句子编码器和一个句子基本注意力层。接下来详细介绍。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261831392.png" alt="image-20211014120019262" style="zoom:50%;" /></p><h5 id="2-1-基于GRU的序列编码器"><a href="#2-1-基于GRU的序列编码器" class="headerlink" title="2.1 基于GRU的序列编码器"></a>2.1 基于GRU的序列编码器</h5><p>GRU使用门控机制追踪序列状态而不是使用独立的记忆cells。其有两种类型的门控:重置门<script type="math/tex">r_t</script>和更新门<script type="math/tex">z_t</script>。（<a href="https://aigonna.com/2020/09/20/4.RNN%EF%BC%8C%20LSTM%20%EF%BC%8C%20GRU%20%E7%BB%93%E6%9E%84%E8%A7%A3%E9%87%8A%E5%92%8C%E5%85%B6%E5%9C%A8Pytorch%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/">具体看以前这篇</a>）这两个门一起控制多少信息来更新状态。在<script type="math/tex">t</script>时刻，GRU计算新的隐藏状态为：</p><script type="math/tex; mode=display">h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde h_t , \tag{1}</script><p>前一时刻隐藏状态<script type="math/tex">h_{t-1}</script>当前时刻新的隐藏状态<script type="math/tex">h_t</script>直接是线性插值关系。更新门<script type="math/tex">z_t</script></p><p>决定多少以前信息保留还是多少新信息添加。<script type="math/tex">z_t</script>的更新公式如:</p><script type="math/tex; mode=display">z_t = \sigma(W_zx_t + U_zh_{t-1} + b_z), \tag{2}</script><p>(就是输入<script type="math/tex">x_t</script>与<script type="math/tex">W_z</script>点乘再加上更新的隐藏信息，即<script type="math/tex">U_z</script>与上一时刻隐藏状态<script type="math/tex">h_t</script>点乘。再加个偏置整体过sigmoid函数。)</p><p>其中<script type="math/tex">x_t</script>是时刻<script type="math/tex">t</script>的序列向量。候选状态<script type="math/tex">\tilde h_t</script>用类似于传统RNN计算:</p><script type="math/tex; mode=display">\tilde h = \text{tanh }(W_hx_t + r_t \odot (U_hh_{t-1}) + b_h), \tag{3}</script><p>（输入<script type="math/tex">x_t</script>与<script type="math/tex">W_h</script>点乘后再加上重置门控制的上一时刻隐藏状态信息流入，加个bias；再过tanh函数）。</p><p>这里<script type="math/tex">r_t</script>是重置门，控制过去隐藏状态多少流入候选隐藏状态。如果<script type="math/tex">r_t</script>为0，就丢掉过去状态。重置门更新公式如:</p><script type="math/tex; mode=display">r_{t}=\sigma\left(W_{r} x_{t}+U_{r} h_{t-1}+b_{r}\right), \tag{4}</script><h5 id="2-2-层次注意力"><a href="#2-2-层次注意力" class="headerlink" title="2.2 层次注意力"></a>2.2 层次注意力</h5><p>本节将集中讲述文档-级别分类任务。假定一个文档有<script type="math/tex">L</script>个句子<script type="math/tex">s_i</script>,并且每个句子有<script type="math/tex">T_i</script>个词。词<script type="math/tex">w_{it}</script>，其中<script type="math/tex">t \in [1, T]</script>代表词在第<script type="math/tex">i</script>个句子中。提出的模型(HAN)将原始文档映射成一个向量表示，在该向量表示上构建一个分类器来做文档分类。接下来，作者将展示怎样使用层级结构从词向量来逐步构建文档级别的向量。</p><p><strong>Word Encoder</strong> 给定一个由词<script type="math/tex">w_{it}</script>组成的句子，<script type="math/tex">t \in [0, T]</script>。首先将词embed成向量，就是通过一个embedding 矩阵<script type="math/tex">W_e, x_{ij} = W_ew_{ij}</script>。这里使用双向GRU通过前后两个方向来汇总单词信息来获取单词的注释，因此能将上下文信息包含在注释中。双向GRU包含前向GRU 记为<script type="math/tex">\overrightarrow{f}</script> （因为现代文本一般是从左向右读一行, 在批次处理embedding 词向量时将这个顺序的输入看作前向）， 其读取句子<script type="math/tex">s_i</script>从词<script type="math/tex">w_{i1}</script>到<script type="math/tex">w_{iT}</script>,后向就是反方向读取。那么有:</p><script type="math/tex; mode=display">\begin{align}&x_{it} = W_ew_{it}, \quad \ t \in [1, T],\\&\overrightarrow{h}_{it} = \overrightarrow{\text{GRU}} \left( x_{it}\right),\quad \ t \in [1, T],\\&\overleftarrow{h}_{it} = \overleftarrow{\text{GRU}} \left( x_{it}\right),\quad \ t \in [T, 1], \tag{4}\end{align}</script><p>作者通过合并给定词<script type="math/tex">w_{it}</script>前向隐藏状态<script type="math/tex">\overrightarrow{h}_{it}</script>和后向<script type="math/tex">\overleftarrow{h}_{it}</script>, 如<script type="math/tex">h_{it} = [\overrightarrow{h}_{it}, \overleftarrow{h}_{it} ]</script>，这汇总了中心词<script type="math/tex">w_{it}</script>的周围的上下文句子信息。</p><p>注: 就是Fig.2中的word encoder部分。</p><p>式4中，</p><ul><li>第一个式子代表前向从1到T个词的embedding 后的<script type="math/tex">x_{it}</script>;</li><li>第二个式子是前向从1到T个词的GRU的隐藏状态</li><li>第二个式子是后向从T到1个词的GRU的隐藏状态</li></ul><p><strong>Word Attention</strong></p><p>并非所有的词都对句子意义的表示有同等贡献。因此，引入注意力机制来提取，对句子意义表征重要的词，并合并这些重要信息的词表征来构成句向量。具体地，</p><script type="math/tex; mode=display">\begin{align}u_{i t} &=\tanh \left(W_{w} h_{i t}+b_{w}\right) \\\\\alpha_{i t} &=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)} \\ \\s_{i} &=\sum \alpha_{i t} h_{i t} \tag{5}\end{align}</script><p>亦即，</p><ol><li>首先“喂”入词注释<script type="math/tex">h_{it}</script>到一层MLP得到<script type="math/tex">u_{it}</script>来作为<script type="math/tex">h_{it}</script>的隐藏状态表示，</li><li>然后将词的重要程度衡量为<script type="math/tex">u_{it}</script>与上下文向量<script type="math/tex">u_w</script>的相似度，</li><li>并将其通过softmax函数归一化得到重要性权重<script type="math/tex">\alpha_{it}</script></li></ol><p><strong>注</strong>: 上面这三句话代表式5中前2个公式。</p><p>最后，基于上面的权重<script type="math/tex">\alpha</script>对词注释<script type="math/tex">h_{it}</script>求权重和得到的值作为句向量<script type="math/tex">s_i</script>（这儿有点滥用记号了）.（就是式5最后一个公式）上下文向量<script type="math/tex">u_w</script>可以看做是一个固定的query代表着“什么是信息性词”而不是平常被用在记忆网络中的词。这个向量是随机初始化后然后在训练整个网络中得到的。</p><p>式4中3个公式，<code>pytorch</code>代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SelfAttention, self).__init__()</span><br><span class="line">        self.W = nn.Linear(input_size, hidden_size, <span class="literal">True</span>)</span><br><span class="line">        self.u = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        u = torch.tanh(self.W(x))</span><br><span class="line">        a = F.softmax(self.u(u), dim=<span class="number">1</span>)</span><br><span class="line">        x = a.mul(x).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>Sentence Encoder</strong></p><p>这部分跟Word encoder类似，就不会详细介绍了。</p><p>上面得到了句向量<script type="math/tex">s_i</script>， 过双向的GPU来编码该句子：</p><script type="math/tex; mode=display">\begin{array}{l}&\vec{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), \quad \ i \in[1, L] \\\\&\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right),\quad \ t \in[L, 1]\end{array} \tag{6}</script><p>然后作者合并得到两个句注释得到该句子<script type="math/tex">i</script>的注释<script type="math/tex">h_i = [\overrightarrow{h}_{i}, \overleftarrow{h}_{i}]</script>。这个句子注释汇总了句子<script type="math/tex">i</script>邻近的句子信息但仍然聚焦在句子<script type="math/tex">i</script>上。</p><p><strong>Sentence Attention</strong></p><p>还是利用注意力机制，将得到的句子注释<script type="math/tex">h_i</script>过MLP线性映射后过<script type="math/tex">\tanh</script>函数得到其表示向量<script type="math/tex">u_i</script>。然后计算其余上下的邻近句子向量<script type="math/tex">u_s</script>的注意力分数<script type="math/tex">\alpha_i</script>，最后求<script type="math/tex">\alpha_i</script>与句子注释<script type="math/tex">h_i</script>的权重和。</p><script type="math/tex; mode=display">\begin{align}u_{i} &=\tanh \left(W_{s} h_{i}+b_{s}\right) \\\\\alpha_{i} &=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)} \\\\v &=\sum_{i} \alpha_{i} h_{i} \tag{7}\end{align}</script><p>最后得到的文档向量<script type="math/tex">v</script>汇总了一个文档中所有的句子信息。其中<script type="math/tex">u_s</script>也是随机初始化后联合训练得到的。</p><h5 id="2-3-文档分类"><a href="#2-3-文档分类" class="headerlink" title="2.3 文档分类"></a>2.3 文档分类</h5><p>文档向量<script type="math/tex">v</script>是文档的高层次表征，用来做分类特征：</p><script type="math/tex; mode=display">p = \operatorname{softmax}(W_cv+b_c) \tag{8}</script><p>分类损失：</p><script type="math/tex; mode=display">L = -\sum_d \log p_{d_j} \tag{8}</script><p>其中<script type="math/tex">j</script>是文档d的标签。</p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> Hierarchical Attention </tag>
            
            <tag> NLP classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. Fasttext 分类器论文Bag of Tricks for Efficient Text Classification</title>
      <link href="2020/10/02/NLP%20Paper%204.%20fasttext%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/10/02/NLP%20Paper%204.%20fasttext%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="4-Fasttext-分类器论文Bag-of-Tricks-for-Efficient-Text-Classification"><a href="#4-Fasttext-分类器论文Bag-of-Tricks-for-Efficient-Text-Classification" class="headerlink" title="4. Fasttext 分类器论文Bag of Tricks for Efficient Text Classification"></a>4. Fasttext 分类器论文Bag of Tricks for Efficient Text Classification</h3><ul><li><a href="https://arxiv.org/pdf/1607.01759.pdf">原文链接</a></li></ul><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文意在探索一种简单又有效的文本分类基准。实验证明了fast text 分类器 <code>fastText</code>在准确率上表现几乎与一些深度学习模型不相上下，并且在训练和评估上快很多。fastText可以使用标准多核cpu上训练大于10亿词汇，训练时间小于10分钟，并且区分312K类别的50w条句子用时少于1min。</p><h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><p>文本分类在NLP许多应用上是非常重要的任务，如搜索，信息检索，排序和文档分类。近来，基于神经网络建模变得越来越受欢迎。虽然，在实际中这些模型取得不错的表现，它们在训练和测试时，变得相对较慢，限制在于其使用非常大的数据集。</p><p>与此同时，线性分类器经常被看作文本分类任务的baseline。尽管它们很简单，如果特征使用合适的话也能取得非常好的表现。其也有潜力拓展到非常大的语料上。</p><p>在本文中，探索如何将这些baseline，在文本分类的上下文中，拓展到有巨大输出空间的非常大的语料库上。受到近期成果，有效词表示学习启发，作者证明了有等级约束和快速损失近似的线性模型在10亿词汇上能10分钟内训练完，并且跟最小成果取得相当的表现。在两个不同任务上评估fastText的质量，分别是命名标注预测和情感分析。</p><h4 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h4><p>简单和句子分类的有效基准是表示句子为BoW并训练为一个线性分类器，如logistic 回归或SVM。然而， 线性分类器不共享类别间的特征参数。当一些类别只有少量样本时，这可能限制其生成大的输出空间的上下文。该问题的通用解决方案分解线性分类器为第等级矩阵和使用多层神经网络。</p><p>图1展示一个简单的有等级约束线性模型。第一个权重矩阵A是所有词汇的查询表。词表示经过平均后得到文本表示，接着喂进线性分类器。文本表示是一个隐藏能潜在被利用的变量。该架构类似于MIkolov的cbow, 不过这里中间词被标签代替。(就是预测中心变成了预测文本标签)。 使用softmax <script type="math/tex">f</script> 来计算所有预定义类别的概率分布。对应<script type="math/tex">N</script> 个文档集，最小化所有类别负的对数似然：</p><script type="math/tex; mode=display">-\frac{1}{N} \sum_{n=1}^N y_n \text{log}(f(BAx_n))</script><p>其中， <script type="math/tex">x_n</script>是标准化后 n个文档的特征 (词向量)， <script type="math/tex">y_n</script>是标签， <script type="math/tex">A, B</script>是权重矩阵， <script type="math/tex">f</script>是softmax函数。 这个模型异步地训练在多个CPU上，使用线性衰减学习率的SGD。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830923.png" alt="image-20210509210903912" style="zoom:33%;" /></p><h5 id="2-1-层次softmax"><a href="#2-1-层次softmax" class="headerlink" title="2.1 层次softmax"></a>2.1 层次softmax</h5><p>当类别数目很大时，计算线性分类器非常昂贵。更具体地，计算复杂度是<script type="math/tex">O(kh)</script>，其中<script type="math/tex">k</script>是类别数目， <script type="math/tex">h</script>是文本表示的维度。为了减少计算时间，使用基于哈夫曼编码树的层次softmax。</p><p>在训练时， 计算复杂度将掉到<script type="math/tex">O(h\text{log}_2(k))</script>.</p><p>在测试时搜索大部分可能的类别，层次softmax也具有优势。每个节点分配一个概率，表示从根到该节点的路径的概率。如果节点在深度<script type="math/tex">l+1</script>, 父节点为<script type="math/tex">n_1, \cdots, n_l</script>，其可能的概率是:</p><script type="math/tex; mode=display">P(n_{n+1}) = \prod_{i=1}^lP(n_i)</script><p>这意味着节点概率总是小于其父节点。用深度优先搜索树，并追踪叶子间最大的概率，使得作者能抛弃任何与相关小概率的分支。实际上，作者在测试时观察到复杂度降为<script type="math/tex">O(h\text{log}_2(k))</script>。 该方法使用二叉堆进一步拓展，计算前T个目标计算代价为<script type="math/tex">O(\text{log(T)})</script>。</p><h5 id="2-2-N-gram-features"><a href="#2-2-N-gram-features" class="headerlink" title="2.2 N-gram features"></a>2.2 N-gram features</h5><p>词袋忽略词的顺序，但是考虑词的顺序往往计算代价高昂。而本文使用n-grams集合作为附加的特征来获取一些局部的词的顺序信息。实际中这非常有效也取得了跟直接使用词序差不多的结果。</p><p>通过使用hashing trick方法，实现了一种快速且空间利用率高的n-grams空间映射。</p><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h4><p>在两个不同任务上评估fastText.</p><ol><li>在情感分析问题上，拿它和已存在的文本分类器比较</li><li>评估其容量以便拓展到标注预测数据集的大的输出空间上。这里，模型可以使用<code>Vowpal Wabbit</code> 库实现，但在实际观察中，专门实现可以快2-5倍。</li></ol><h5 id="3-1-情感分析"><a href="#3-1-情感分析" class="headerlink" title="3.1 情感分析"></a>3.1 情感分析</h5><p><strong>数据集和baselines</strong>  使用了8个数据集并按照Zhang的评估指标。选取了来自Zhang的n-grams和TFIDF baselines ，也有<code>character level convolutional model (char-CNN)</code>字符级卷积模型，字符级卷积循环网络 char-CRNN，<code>very deep convolutional network (VDCNN)</code>非常深的卷积网络作为对比模型。</p><p>表1是：在8个数据集上测试准确率的比较。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830925.png" alt="image-20210510005117097" style="zoom:30%;" /></p><p>表2：训练一轮时间比较，fastText和 char-CNN、VDCNN</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830926.png" alt="image-20210510005501465" style="zoom:30%;" /></p><p>表3：测试准确性比较</p><blockquote><p>our method is competitive with the methods presented in Tang et al. (2015). We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance. Unlike Tang et al. (2015), fastText does not use pre-trained word embeddings, which can be explained the 1% difference in accuracy.</p><p>fastText跟Tang的模型方法比较。在验证集上，调试超参数n-gram为5时表现最好。不像Tang，fastText不使用预训练词嵌入，准确性只有1%的差。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830927.png" alt="image-20210510005644575" style="zoom:30%;" /></p><h5 id="3-2-标注预测"><a href="#3-2-标注预测" class="headerlink" title="3.2 标注预测"></a>3.2 标注预测</h5><p>在YFCC100M数据集上，结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830928.png" alt="image-20210510010657030" style="zoom:30%;" /></p><p>实验结果如下：比较小的隐藏层效果就差不多，加上bigram效果会好点</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261830929.png" alt="image-20210510010744181" style="zoom:25%;" /></p><h4 id="4-讨论和结果"><a href="#4-讨论和结果" class="headerlink" title="4. 讨论和结果"></a>4. 讨论和结果</h4><p>提出了一种简单的文本分类方法。不像非监督的word2vec训练词向量，作者的词特征能平均后合在一起构成良好的句子表示。在几个任务中， fastText获得了跟受深度学习启发提出的方法差不多的效果，而且更快。尽管深度神经网络在理论上有比浅层模型更强的表示能力，不清楚如果简单的文本分类问题像情感分析是合适的任务来评估它们。作者公开了代码以便研究机构能在作者工作上面轻松搭建。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://www.jiqizhixin.com/articles/2020-07-03-14">Word Embedding Papers | 经典再读之fastText</a></p><p>[2] <a href="https://blog.csdn.net/tingkr/article/details/84023813">fasttext论文</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> n-grams </tag>
            
            <tag> fastText </tag>
            
            <tag> char CNN </tag>
            
            <tag> char CRNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. shap 解释模型</title>
      <link href="2020/09/25/ML_shap/"/>
      <url>2020/09/25/ML_shap/</url>
      
        <content type="html"><![CDATA[<h3 id="2-shap-解释模型"><a href="#2-shap-解释模型" class="headerlink" title="2. shap 解释模型"></a>2. shap 解释模型</h3><h4 id="1-shap解释回归模型"><a href="#1-shap解释回归模型" class="headerlink" title="1. shap解释回归模型"></a>1. shap解释回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> stop_words</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> string, re</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><p><strong>导入数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;https://query.data.world/s/yd24ckbjzyp7h6zp7bacafpv2lgfkh&quot;</span>, encoding=<span class="string">&quot;ISO-8859-1&quot;</span>)</span><br><span class="line">display(df.shape)</span><br><span class="line">display(df[<span class="string">&quot;relevance&quot;</span>].value_counts()/df.shape[<span class="number">0</span>])</span><br><span class="line">=================================================================================</span><br><span class="line">(<span class="number">8000</span>, <span class="number">15</span>)</span><br><span class="line">no          <span class="number">0.821375</span></span><br><span class="line">yes         <span class="number">0.177500</span></span><br><span class="line"><span class="keyword">not</span> sure    <span class="number">0.001125</span></span><br><span class="line">Name: relevance, dtype: float64</span><br></pre></td></tr></table></figure><p>去除not sure：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = df[df.relevance != <span class="string">&#x27;not sure&#x27;</span>]</span><br><span class="line">df.shape</span><br><span class="line">======================================================================</span><br><span class="line">(<span class="number">7991</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;relevance&#x27;</span>] = df.relevance.<span class="built_in">map</span>(&#123;<span class="string">&#x27;yes&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>:<span class="number">0</span>&#125;)</span><br><span class="line">df = df[[<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;relevance&#x27;</span>]]</span><br><span class="line">df.shape</span><br><span class="line">======================================================================</span><br><span class="line">(<span class="number">7991</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">stopwords = stop_words.ENGLISH_STOP_WORDS</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean</span>(<span class="params">doc</span>):</span> <span class="comment">#doc is a string of text</span></span><br><span class="line">    doc = doc.replace(<span class="string">&quot;&lt;/br&gt;&quot;</span>, <span class="string">&quot; &quot;</span>) <span class="comment">#This text contains a lot of &lt;br/&gt; tags.</span></span><br><span class="line">    doc = <span class="string">&quot;&quot;</span>.join([char <span class="keyword">for</span> char <span class="keyword">in</span> doc <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation <span class="keyword">and</span> <span class="keyword">not</span> char.isdigit()])</span><br><span class="line">    doc = <span class="string">&quot; &quot;</span>.join([token <span class="keyword">for</span> token <span class="keyword">in</span> doc.split() <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> stopwords])</span><br><span class="line">    <span class="comment">#remove punctuation and numbers</span></span><br><span class="line">    <span class="keyword">return</span> doc</span><br><span class="line"></span><br><span class="line">x = df.text</span><br><span class="line">y = df.relevance</span><br><span class="line"><span class="built_in">print</span>(x.shape, y.shape)</span><br><span class="line">=========================================================================</span><br><span class="line">(<span class="number">7991</span>,) (<span class="number">7991</span>,)</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape, y_train.shape)</span><br><span class="line">=========================================================================</span><br><span class="line">(<span class="number">5993</span>,) (<span class="number">5993</span>,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vect = TfidfVectorizer(min_df=<span class="number">5</span>)</span><br><span class="line">x_train_dtm = vect.fit_transform(x_train)</span><br><span class="line">x_test_dtm = vect.transform(x_test)</span><br><span class="line">model = LogisticRegression(class_weight=<span class="string">&#x27;balanced&#x27;</span>)</span><br><span class="line">model.fit(x_train_dtm, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_class = model.predict(x_test_dtm)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: &quot;</span>, accuracy_score(y_test, y_pred_class))</span><br><span class="line">==============================================================================</span><br><span class="line">Accuracy:  <span class="number">0.7382382382382382</span></span><br></pre></td></tr></table></figure><p><strong>shap 解释模型</strong></p><p>这里有个比较全面的实例 <a href="https://mathpretty.com/10699.html">模型解释–SHAP Value的简单介绍</a> 。使用步骤：</p><ol><li>实例化线性解释器 <code>shap.LinearExplainer()</code> <ul><li>不同模型对应不同解释器：<ul><li><strong>TreeExplainer</strong> : Support XGBoost, LightGBM, CatBoost and scikit-learn models by Tree SHAP.</li><li><strong>DeepExplainer (DEEP SHAP)</strong> : Support TensorFlow and Keras models by using DeepLIFT and Shapley values.</li><li><strong>GradientExplainer</strong> : Support TensorFlow and Keras models.</li><li><strong>KernelExplainer (Kernel SHAP)</strong> : Applying to any models by using <strong>LIME and Shapley values</strong>.</li></ul></li></ul></li><li>对数据进行解释</li><li>对结果进行可视化</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line">explainer = shap.LinearExplainer(model, x_train_dtm,  feature_perturbation=<span class="string">&quot;intervebtional&quot;</span>)</span><br><span class="line">shap_values = explainer.shap_values(x_test_dtm)</span><br><span class="line">x_test_array = x_test_dtm.toarray()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">pprint(df[<span class="string">&#x27;text&#x27;</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(dpi=<span class="number">120</span>)</span><br><span class="line">shap.initjs()</span><br><span class="line">shap.summary_plot(shap_values, x_test_array, feature_names=vect.get_feature_names())</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261827488.png" alt="Snipaste_2021-05-18_15-58-56" style="zoom:30%;" /></p><p>上图说明：</p><ol><li>特征重要性：变量重要程度由上往下递减，这里看到economy, 对于整个预测来说是比较重要的</li><li>水平方向是每个特征对于对应预测的影响</li><li>原始值：红色代表观察的数值大，蓝色代表观察的数值小</li><li>相关程度：dollar对于决策这篇文章是不是跟美国经济相关程度</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shap.initjs()</span><br><span class="line">shap.force_plot(</span><br><span class="line">    explainer.expected_value, shap_values[<span class="number">0</span>, :], x_test_array[<span class="number">0</span>, :],</span><br><span class="line">    feature_names=vect.get_feature_names()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261827053.png" alt="Snipaste_2021-05-18_16-00-34" style="zoom:30%;" /></p><p>上图说明：</p><ol><li><a href="https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">原始论文</a> 说base_value是 <script type="math/tex">E(\hat y)</script>: the value that would be predicted if we did not know any features for the current output.可以理解为预测值或者预测期望。</li><li>红色和蓝色: 将预测值推高的特征值显示为红色，推低的显示为蓝色。</li><li>经济： 对该文章是否跟美国经济有关有积极影响，将预测值推向右边。</li></ol><h4 id="2-shap-解释lstm"><a href="#2-shap-解释lstm" class="headerlink" title="2. shap 解释lstm"></a>2. shap 解释lstm</h4><p>注： tf使用为1.15.2，1.14也行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> re, os, sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, GlobalMaxPooling1D,\</span><br><span class="line">                        Conv1D, MaxPooling1D, Embedding, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> Constant</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">max_seq_len = <span class="number">1000</span></span><br><span class="line">max_num_words = <span class="number">2000</span></span><br><span class="line">emb_dim = <span class="number">100</span></span><br><span class="line">valid_split = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">20000</span></span><br><span class="line">maxlen = <span class="number">1000</span></span><br></pre></td></tr></table></figure><p><strong>导入数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_directory_data</span>(<span class="params">directory</span>):</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    data[<span class="string">&quot;sentence&quot;</span>] = []</span><br><span class="line">    data[<span class="string">&quot;sentiment&quot;</span>] = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> os.listdir(directory):</span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(os.path.join(directory, file_path), <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data[<span class="string">&quot;sentence&quot;</span>].append(f.read())</span><br><span class="line">            data[<span class="string">&quot;sentiment&quot;</span>].append(re.match(<span class="string">&quot;\d+_(\d+)\.txt&quot;</span>, file_path).group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame.from_dict(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge positive and negative examples, add a polarity column and shuffle.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">directory</span>):</span></span><br><span class="line">    pos_df = load_directory_data(os.path.join(directory, <span class="string">&quot;pos&quot;</span>))</span><br><span class="line">    neg_df = load_directory_data(os.path.join(directory, <span class="string">&quot;neg&quot;</span>))</span><br><span class="line">    pos_df[<span class="string">&quot;polarity&quot;</span>] = <span class="number">1</span></span><br><span class="line">    neg_df[<span class="string">&quot;polarity&quot;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> pd.concat([pos_df, neg_df]).sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_and_load_datasets</span>(<span class="params">force_download=<span class="literal">False</span></span>):</span></span><br><span class="line">    dataset = tf.keras.utils.get_file(</span><br><span class="line">        fname=<span class="string">&quot;aclImdb.tar.gz&quot;</span>,</span><br><span class="line">        origin=<span class="string">&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot;</span>,</span><br><span class="line">        extract=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_df = load_dataset(os.path.join(os.path.dirname(dataset),</span><br><span class="line">                                         <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;train&quot;</span>))</span><br><span class="line">    test_df = load_dataset(os.path.join(os.path.dirname(dataset),</span><br><span class="line">                                        <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;test&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train, test = download_and_load_datasets()</span><br></pre></td></tr></table></figure><p><strong>数据预处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">train_texts = train[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line">train_labels = train[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line">test_texts = test[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line">test_labels = test[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">labels_index = &#123;<span class="string">&#x27;pos&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;neg&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(num_words=max_num_words)</span><br><span class="line">tokenizer.fit_on_texts(train_texts)</span><br><span class="line">train_sequences = tokenizer.texts_to_sequences(train_texts)<span class="comment">#将文本转为词索引</span></span><br><span class="line">test_sequences = tokenizer.texts_to_sequences(test_texts)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Found %s unique tokens.&quot;</span>%<span class="built_in">len</span>(word_index))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将文本转为等长的向量</span></span><br><span class="line">train_valid_data = pad_sequences(train_sequences, maxlen=max_seq_len)</span><br><span class="line">test_data = pad_sequences(test_sequences, maxlen=max_seq_len)</span><br><span class="line">train_valid_labels = to_categorical(np.asarray(train_labels))</span><br><span class="line">test_labels = to_categorical(np.asarray(test_labels))</span><br><span class="line"></span><br><span class="line"><span class="comment">#划分数据集</span></span><br><span class="line">indices = np.arange(train_valid_data.shape[<span class="number">0</span>])</span><br><span class="line">np.random.shuffle(indices)</span><br><span class="line"></span><br><span class="line">train_valid_data = train_valid_data[indices]</span><br><span class="line">train_valid_labels = train_valid_labels[indices]</span><br><span class="line">num_valid_samples = <span class="built_in">int</span>(valid_split * train_valid_data.shape[<span class="number">0</span>])</span><br><span class="line">x_train = train_valid_data[:-num_valid_samples]</span><br><span class="line">y_train = train_valid_labels[:-num_valid_samples]</span><br><span class="line">x_val = train_valid_data[-num_valid_samples:]</span><br><span class="line">y_val = train_valid_labels[-num_valid_samples:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;划分数据集为训练测试集完毕！&quot;</span>)</span><br><span class="line">============================================================================</span><br><span class="line">Found <span class="number">88582</span> unique tokens.</span><br><span class="line">划分数据集为训练测试集完毕！</span><br></pre></td></tr></table></figure><p><strong>模型训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_features = vocab_size + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;定义训练LSTM模型: &quot;</span>)</span><br><span class="line">lstm = Sequential() <span class="comment">#shap只能用Sequential搭建deepnet模型</span></span><br><span class="line">lstm.add(Embedding(max_num_words, <span class="number">128</span>))</span><br><span class="line">lstm.add(LSTM(<span class="number">128</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">lstm.add(Dense(<span class="number">2</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">lstm.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">             optimizer=<span class="string">&#x27;Adam&#x27;</span>,</span><br><span class="line">             metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LSTM开始训练！&quot;</span>)</span><br><span class="line">lstm.fit(x_train, y_train,</span><br><span class="line">         batch_size=<span class="number">32</span>, epochs=<span class="number">2</span>,</span><br><span class="line">         validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure><p><strong>shap 解释lstm</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"></span><br><span class="line">shap.initjs()</span><br><span class="line">explainer = shap.DeepExplainer(lstm, x_train[:<span class="number">20</span>])</span><br><span class="line"><span class="comment">#解释每个预测值要2*背景数据，下面解释10个</span></span><br><span class="line">shap_values = explainer.shap_values(x_val[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>获取验证集上前10个词对应的索引矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">words = imdb.get_word_index()</span><br><span class="line">num2word = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words.keys():</span><br><span class="line">    num2word[words[w]] = w</span><br><span class="line">x_val_words = np.stack([np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: num2word.get(x, <span class="string">&quot;NONE&quot;</span>), x_val[i]))) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shap.initjs()</span><br><span class="line"></span><br><span class="line">shap.force_plot(explainer.expected_value[<span class="number">0</span>], shap_values[<span class="number">0</span>][<span class="number">0</span>], x_val_words[<span class="number">0</span>],</span><br><span class="line">                text_rotation=<span class="number">30</span>,</span><br><span class="line">                matplotlib=<span class="literal">True</span>,</span><br><span class="line">                show=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261827484.png" alt="image-20210518202004105" style="zoom:30%;" /></p><p>补充材料：<a href="https://cloud.tencent.com/developer/article/1485897">如何解决机器学习树集成模型的解释性问题</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> shap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. lime 解释LSTM模型</title>
      <link href="2020/09/24/ML_lime/"/>
      <url>2020/09/24/ML_lime/</url>
      
        <content type="html"><![CDATA[<h3 id="1-lime-解释LSTM模型"><a href="#1-lime-解释LSTM模型" class="headerlink" title="1. lime 解释LSTM模型"></a>1. lime 解释LSTM模型</h3><p>本文是 <a href="https://github.com/practical-nlp/practical-nlp/blob/master/Ch4/09_Lime_RNN.ipynb">Practical NLP</a>部分的笔记。</p><h4 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h4><p>下载数据并解压得到数据集<script type="math/tex">\Longrightarrow</script> 读取对应训练和测试数据路径<script type="math/tex">\Longrightarrow</script> 获取pos和neg文件夹<script type="math/tex">\Longrightarrow</script> 读取这两个文件夹中的文本内容和txt文件名代表的评分并合并成一个df。开始解压后的文件目录如下: (tensorflow版本使用为:1.15.2)</p><blockquote><p>aclImdb<br> ├── imdbEr.txt<br> ├── imdb.vocab<br> ├── README<br> ├── test<br> │   ├── labeledBow.feat<br> │   ├── neg<br> │   ├── pos<br> │   ├── urls_neg.txt<br> │   └── urls_pos.txt<br> └── train<br>     ├── labeledBow.feat<br>     ├── neg<br>     ├── pos<br>     ├── unsup<br>     ├── unsupBow.feat<br>     ├── urls_neg.txt<br>     ├── urls_pos.txt<br>     └── urls_unsup.txt</p></blockquote><p>要得到train和test两个数据集：</p><ol><li><p>先读取pos这个级别的文件夹里面的类似 <code>2021_3.txt</code>文本和对应评分级别</p><pre><code>`tf.gfile.GFile(目录, &#39;r&#39;)`读取文件会快很多</code></pre><ul><li>创建一个<code>data=&#123;&#125;</code> 字典</li><li>定义字典构成k-v</li><li>获取k-v对的值</li><li>从字典生成pandas中的df</li></ul></li><li><p>利用上面函数生成train和test文件夹里的pos和neg级别的数据</p><ul><li>分别读取pos和neg</li><li>添加一个正负评价的标签</li><li>合并生成对应数据</li></ul></li><li><p>按路径下载数据并解压，再利用上面函数生成训练和测试数据</p></li></ol><ul><li>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.utils.get_file(</span><br><span class="line">         fname=<span class="string">&quot;aclImdb.tar.gz&quot;</span>, </span><br><span class="line">         origin=<span class="string">&quot;url/aclImdb_v1.tar.gz&quot;</span>, </span><br><span class="line">         extract=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><code>train_df, test_df</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os, re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="comment">#pip install tensorflow==1.14.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dir_data</span>(<span class="params"><span class="built_in">dir</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取数据集内容，返回一个df，其中df由tf快速读取的</span></span><br><span class="line"><span class="string">    文本内容列sentence和文件名代表的评分级别构成。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    data[<span class="string">&#x27;sentence&#x27;</span>] = []</span><br><span class="line">    data[<span class="string">&#x27;sentiment&#x27;</span>] = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> os.listdir(<span class="built_in">dir</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.gfile.GFile(os.path.join(<span class="built_in">dir</span>, file_path), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data[<span class="string">&#x27;sentence&#x27;</span>].append(f.read())</span><br><span class="line">            <span class="comment">#文件名格式为2021_3.txt 这里match后group(1)就是3这个情感评分</span></span><br><span class="line">            data[<span class="string">&#x27;sentiment&#x27;</span>].append(re.match(<span class="string">&#x27;\d+_(\d+)\.txt&#x27;</span>, file_path).group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame.from_dict(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params"><span class="built_in">dir</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用load_dir_data()获取对应数据df后，并按照相应的文件夹代表的正反评价</span></span><br><span class="line"><span class="string">    来标注对应标签，最后返回合并后的打乱的重置索引后的df</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pos_df = load_dir_data(os.path.join(<span class="built_in">dir</span>, <span class="string">&#x27;pos&#x27;</span>))</span><br><span class="line">    neg_df = load_dir_data(os.path.join(<span class="built_in">dir</span>, <span class="string">&#x27;neg&#x27;</span>))</span><br><span class="line">    pos_df[<span class="string">&#x27;polarity&#x27;</span>] = <span class="number">1</span></span><br><span class="line">    neg_df[<span class="string">&#x27;polarity&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> pd.concat([pos_df, neg_df]).sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_load_datasets</span>(<span class="params">force_download=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    按路径下载数据集，并解压得到数据集；</span></span><br><span class="line"><span class="string">    再将其用load_dataset处理得到相应df</span></span><br><span class="line"><span class="string">    返回训练和测试df</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dataset = tf.keras.utils.get_file(</span><br><span class="line">    fname=<span class="string">&quot;aclImdb.tar.gz&quot;</span>, </span><br><span class="line">    origin=<span class="string">&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot;</span>, </span><br><span class="line">    extract=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    train_df = load_dataset(os.path.join(os.path.dirname(dataset), <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;train&quot;</span>))</span><br><span class="line">    test_df = load_dataset(os.path.join(os.path.dirname(dataset), <span class="string">&quot;aclImdb&quot;</span>, <span class="string">&quot;test&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br><span class="line"></span><br><span class="line">train, test = download_load_datasets()</span><br><span class="line">train.sample(<span class="number">5</span>)</span><br><span class="line">===========================================================================</span><br><span class="line">sentencesentimentpolarity</span><br><span class="line"><span class="number">16710</span>Ironically <span class="keyword">for</span> a play unavailable on film <span class="keyword">or</span> v...<span class="number">7</span><span class="number">1</span></span><br><span class="line"><span class="number">20589</span>On the back burner <span class="keyword">for</span> years (so it was report...<span class="number">1</span><span class="number">0</span></span><br><span class="line"><span class="number">10690</span>I will stat of <span class="keyword">with</span> the plot Alice, having sur...<span class="number">4</span><span class="number">0</span></span><br><span class="line"><span class="number">5756</span>Envy stars some of the best. Jack Black, Ben S...<span class="number">1</span><span class="number">0</span></span><br><span class="line"><span class="number">4702</span>Another rape of History&lt;br /&gt;&lt;br /&gt;This movie ...<span class="number">3</span><span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="2-模型建立和训练"><a href="#2-模型建立和训练" class="headerlink" title="2. 模型建立和训练"></a>2. 模型建立和训练</h4><p><strong>利用sklearn自定义pipeline</strong></p><ul><li>实现TextsToSequences()和Padder()两个类</li><li>利用 <code>make_pipeline()</code>形成<code>fit_on_texts()</code>、<code>texts_to_sequences()</code>、<code>pad_sequences()</code>处理流</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, GlobalMaxPool1D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv1D, MaxPooling1D, Embedding, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> Constant</span><br><span class="line"></span><br><span class="line"><span class="comment">#序列最大长度</span></span><br><span class="line">max_seq_len = <span class="number">1000</span></span><br><span class="line">max_num_words = <span class="number">20000</span></span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">valid_split = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">20000</span></span><br><span class="line">maxlen = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">train_texts = train[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line">train_labels = train[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line">test_texts = test[<span class="string">&#x27;sentence&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">label_index = &#123;<span class="string">&#x27;pos&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;neg&#x27;</span>:<span class="number">0</span>&#125;</span><br><span class="line">test_labels = test[<span class="string">&#x27;polarity&#x27;</span>].values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextsToSequences</span>(<span class="params">Tokenizer, BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    我们继承BaseEstimator, TransformerMixin就能自定义fit和transform实现</span></span><br><span class="line"><span class="string">    自定义的pipeline，并且 BaseEstimator还能传入*wargs **kwargs</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, texts, y=<span class="literal">None</span></span>):</span></span><br><span class="line">       <span class="comment"># 使用Tokenizer.fit_on_texts()将文本tokenize,</span></span><br><span class="line">       <span class="comment">#  返回self实现fit().transform()串式调用    </span></span><br><span class="line">        self.fit_on_texts(texts)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, texts, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment">#Tokenizer.texts_to_sequences()将文本转换为sequence</span></span><br><span class="line">        <span class="comment"># sklearn不能处理DataFrame转换为np</span></span><br><span class="line">        <span class="keyword">return</span> np.array(self.texts_to_sequences(texts))</span><br><span class="line"></span><br><span class="line">seq = TextsToSequences(num_words=vocab_size) </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Padder</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    填充裁剪不等长文本到一样长</span></span><br><span class="line"><span class="string">    只有长于maxlen列表的结尾保留</span></span><br><span class="line"><span class="string">    而短于maxlen的列表则用零填充。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, maxlen=<span class="number">500</span></span>):</span></span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line">        self.max_index = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.max_index = pad_sequences(x, maxlen=self.maxlen).<span class="built_in">max</span>()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, x, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = pad_sequences(x, maxlen=self.maxlen)</span><br><span class="line">        x[x &gt; self.max_index] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">padder = Padder(maxlen)</span><br></pre></td></tr></table></figure><p><strong>训练LSTM模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Bidirectional, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">max_features = vocab_size + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_model</span>(<span class="params">max_features</span>):</span></span><br><span class="line">    lstm = Sequential()</span><br><span class="line">    lstm.add(Embedding(max_num_words, <span class="number">128</span>))</span><br><span class="line">    lstm.add(LSTM(<span class="number">128</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">    lstm.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    lstm.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                 optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                 metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> lstm</span><br><span class="line"></span><br><span class="line">sklearn_lstm = KerasClassifier(build_fn=lstm_model, epochs=<span class="number">2</span>,</span><br><span class="line">                               batch_size=<span class="number">32</span>,</span><br><span class="line">                               max_features=max_features,</span><br><span class="line">                               verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">pipeline = make_pipeline(seq, padder, sklearn_lstm)</span><br><span class="line">pipeline.fit(train_texts, train_labels)</span><br></pre></td></tr></table></figure><p><strong>预测正确率</strong>: 看起来结果还不错，测试准确率: 83.74%。但是什么特征对模型其作用呢?模型是不是合理预测呢?这就要用到lime。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_preds = pipeline.predict(test_texts)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试准确率: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(<span class="number">100</span>*metrics.accuracy_score(y_preds, test_labels)))</span><br><span class="line">====================================================================================</span><br><span class="line">测试准确率: <span class="number">83.74</span>%</span><br></pre></td></tr></table></figure><h4 id="3-lime-解释模型"><a href="#3-lime-解释模型" class="headerlink" title="3. lime 解释模型"></a>3. lime 解释模型</h4><p><strong>lime</strong>使用流程是:</p><ol><li>选取实例样本</li><li>创建对应解释器</li><li>用解释器解释实例样本特征跟训练模型近似权重</li><li>绘制权重图形来看对应特征对模型的作用，比如正向或者负向，判断是否合理</li></ol><p>训练的LSTM模型，预测样本1：<code>This was an excellent movie...</code>是正向评价，跟标签也是一致的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选取一个测试实例</span></span><br><span class="line">idx = <span class="number">11</span></span><br><span class="line">text_sample = test_texts[idx]</span><br><span class="line">class_names = [<span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;样本&#123;&#125;: 最后1000个词(模型使用的部分)&#x27;</span>.<span class="built_in">format</span>(idx))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(text_sample.split()[-<span class="number">1000</span>:]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;概率(正向的) = &quot;</span>, pipeline.predict_proba([text_sample])[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真正的类别: %s&quot;</span> %class_names[test_labels[idx]])</span><br><span class="line">=======================================================================</span><br><span class="line">样本<span class="number">11</span>: 最后<span class="number">1000</span>个词(模型使用的部分)</span><br><span class="line">This was an excellent movie - fast-paced, well-written <span class="keyword">and</span> had an intriguing plot. The special effects were innovative, especially <span class="keyword">in</span> the opening scene. The training segment got a bit silly but overall it was a tense movie.</span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - 0s 117ms/step</span><br><span class="line">概率(正向的) =  <span class="number">0.96135324</span></span><br><span class="line">真正的类别: positive</span><br></pre></td></tr></table></figure><p>那么在模型上预测起作用的是什么特征呢? 单词，从其权重绘制图来看，最重要的是 <code>excellent</code> <code>tense</code>等这些词，权重在0.1以上，而负向词都小于0.05，因此判断是正向的。这看其来是比较合理的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">from</span> lime.lime_text <span class="keyword">import</span> LimeTextExplainer</span><br><span class="line"></span><br><span class="line">explianer = LimeTextExplainer(class_names=class_names)</span><br><span class="line">explanation = explianer.explain_instance(text_sample,</span><br><span class="line">                pipeline.predict_proba, num_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将explanation转换为有序字典</span></span><br><span class="line">weights = OrderedDict(explanation.as_list())</span><br><span class="line"><span class="comment">#转换为对应列的df</span></span><br><span class="line">lime_weights = pd.DataFrame(&#123;<span class="string">&#x27;words&#x27;</span>: <span class="built_in">list</span>(weights.keys()),</span><br><span class="line">                             <span class="string">&#x27;weights&#x27;</span>: <span class="built_in">list</span>(weights.values())&#125;)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>), dpi=<span class="number">120</span>)</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;words&#x27;</span>, y=<span class="string">&#x27;weights&#x27;</span>, data=lime_weights)</span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Sample &#123;&#125; features weights given by LIME&#x27;</span>.<span class="built_in">format</span>(idx))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261826528.png" alt="image-20210517191454032" style="zoom:30%;" /></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lime </tag>
            
            <tag> sklearn </tag>
            
            <tag> pipeline </tag>
            
            <tag> lstm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>skip-gram 负采样代码笔记</title>
      <link href="2020/09/21/2.%20skip-gram-ns%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
      <url>2020/09/21/2.%20skip-gram-ns%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="2-skip-gram-负采样代码笔记"><a href="#2-skip-gram-负采样代码笔记" class="headerlink" title="2. skip-gram 负采样代码笔记"></a>2. skip-gram 负采样代码笔记</h3><p><a href="https://github.com/neubig/nn4nlp-code/blob/master/04-efficiency-pytorch/wordemb-skip-ns.py">原代码地址</a>，是CMU NLP课程代码。</p><h4 id="1-导入包"><a href="#1-导入包" class="headerlink" title="1. 导入包"></a>1. 导入包</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h4 id="2-定义skip-gram词嵌入层"><a href="#2-定义skip-gram词嵌入层" class="headerlink" title="2. 定义skip-gram词嵌入层"></a>2. 定义skip-gram词嵌入层</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbSkip</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nwords, emb_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(WordEmbSkip, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot; word embeddings &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#sparse=True权重矩阵的梯度变成稀疏向量</span></span><br><span class="line">        self.word_embedding = torch.nn.Embedding(nwords, emb_size, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># initialize the weights with xavier uniform (Glorot, X. &amp; Bengio, Y. (2010))</span></span><br><span class="line">        torch.nn.init.xavier_uniform_(self.word_embedding.weight)</span><br><span class="line">        <span class="string">&quot;&quot;&quot; context embeddings&quot;&quot;&quot;</span></span><br><span class="line">        self.context_embedding = torch.nn.Embedding(nwords, emb_size, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># initialize the weights with xavier uniform (Glorot, X. &amp; Bengio, Y. (2010))</span></span><br><span class="line">        torch.nn.init.xavier_uniform_(self.context_embedding.weight)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># useful ref: https://arxiv.org/abs/1402.3722</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word_pos, context_positions, negative_sample=<span class="literal">False</span></span>):</span></span><br><span class="line">        embed_word = self.word_embedding(word_pos)    <span class="comment"># 1 * emb_size</span></span><br><span class="line">        embed_context = self.context_embedding(context_positions)  <span class="comment"># n * emb_size</span></span><br><span class="line">        score = torch.matmul(embed_context, embed_word.transpose(dim0=<span class="number">1</span>, dim1=<span class="number">0</span>)) <span class="comment">#score = n * 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># following is an example of something you can only do in a framework that allows</span></span><br><span class="line">        <span class="comment"># dynamic graph creation </span></span><br><span class="line">        <span class="comment">#使用负采样</span></span><br><span class="line">        <span class="keyword">if</span> negative_sample:</span><br><span class="line">              score = -<span class="number">1</span>*score</span><br><span class="line">        obj = -<span class="number">1</span> * torch.<span class="built_in">sum</span>(F.logsigmoid(score))</span><br><span class="line">        <span class="keyword">return</span> obj</span><br></pre></td></tr></table></figure><h4 id="3-定义一些超参数"><a href="#3-定义一些超参数" class="headerlink" title="3. 定义一些超参数"></a>3. 定义一些超参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">K=<span class="number">3</span> <span class="comment">#number of negative samples</span></span><br><span class="line">N=<span class="number">2</span> <span class="comment">#length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)</span></span><br><span class="line">EMB_SIZE = <span class="number">128</span> <span class="comment"># The size of the embedding</span></span><br><span class="line"></span><br><span class="line">embeddings_location = <span class="string">&quot;embeddings.txt&quot;</span> <span class="comment">#the file to write the word embeddings to</span></span><br><span class="line">labels_location = <span class="string">&quot;labels.txt&quot;</span> <span class="comment">#the file to write the labels to</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We reuse the data reading from the language modeling class</span></span><br><span class="line">w2i = defaultdict(<span class="keyword">lambda</span>: <span class="built_in">len</span>(w2i))</span><br><span class="line"></span><br><span class="line"><span class="comment">#word counts for negative sampling</span></span><br><span class="line">word_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">S = w2i[<span class="string">&quot;&lt;s&gt;&quot;</span>]</span><br><span class="line">UNK = w2i[<span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br></pre></td></tr></table></figure><p><strong>读取数据函数</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_dataset</span>(<span class="params">filename</span>):</span></span><br><span class="line">  <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">      line = line.strip().split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> word <span class="keyword">in</span> line:</span><br><span class="line">        word_counts[w2i[word]] += <span class="number">1</span></span><br><span class="line">      <span class="keyword">yield</span> [w2i[x] <span class="keyword">for</span> x <span class="keyword">in</span> line]</span><br></pre></td></tr></table></figure><p><strong>读取数据生成w2i, i2w</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Read in the data</span></span><br><span class="line">train = <span class="built_in">list</span>(read_dataset(<span class="string">&quot;data/ptb/train.txt&quot;</span>))</span><br><span class="line">w2i = defaultdict(<span class="keyword">lambda</span>: UNK, w2i)</span><br><span class="line">dev = <span class="built_in">list</span>(read_dataset(<span class="string">&quot;data/ptb/valid.txt&quot;</span>))</span><br><span class="line">i2w = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> w2i.items()&#125;</span><br><span class="line">nwords = <span class="built_in">len</span>(w2i)</span><br><span class="line"></span><br><span class="line">=========================================================</span><br><span class="line">一些变量信息:</span><br><span class="line">w2i:</span><br><span class="line">    defaultdict(&lt;function __main__.&lt;<span class="keyword">lambda</span>&gt;()&gt;,</span><br><span class="line">            &#123;<span class="string">&#x27;&lt;s&gt;&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">             <span class="string">&#x27;&lt;unk&gt;&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;aer&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">             <span class="string">&#x27;banknote&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">             <span class="string">&#x27;berlitz&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">             <span class="string">&#x27;calloway&#x27;</span>: <span class="number">5</span>,...&#125;</span><br><span class="line"></span><br><span class="line">i2w:    </span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;&lt;s&gt;&#x27;</span>,</span><br><span class="line">     <span class="number">1</span>: <span class="string">&#x27;&lt;unk&gt;&#x27;</span>,</span><br><span class="line">     <span class="number">2</span>: <span class="string">&#x27;aer&#x27;</span>,</span><br><span class="line">     <span class="number">3</span>: <span class="string">&#x27;banknote&#x27;</span>,</span><br><span class="line">     <span class="number">4</span>: <span class="string">&#x27;berlitz&#x27;</span>,</span><br><span class="line">     <span class="number">5</span>: <span class="string">&#x27;calloway&#x27;</span>,</span><br><span class="line">     <span class="number">6</span>: <span class="string">&#x27;centrust&#x27;</span>, ...&#125;            </span><br></pre></td></tr></table></figure><p><strong>按照论文取词频的3/4次方,计算每个词的概率</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># take the word counts to the 3/4, normalize</span></span><br><span class="line">counts =  np.array([<span class="built_in">list</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> word_counts.items()])[:,<span class="number">1</span>]**<span class="number">.75</span></span><br><span class="line">normalizing_constant = <span class="built_in">sum</span>(counts)</span><br><span class="line">word_probabilities = np.zeros(nwords)</span><br><span class="line"><span class="keyword">for</span> word_id <span class="keyword">in</span> word_counts:</span><br><span class="line">  word_probabilities[word_id] = word_counts[word_id]**<span class="number">.75</span>/normalizing_constant</span><br></pre></td></tr></table></figure><p> <strong>将i2w写入label_location文件</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(labels_location, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> labels_file:</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nwords):</span><br><span class="line">    labels_file.write(i2w[i] + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="3-建模"><a href="#3-建模" class="headerlink" title="3. 建模"></a>3. 建模</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize the model</span></span><br><span class="line">model = WordEmbSkip(nwords, EMB_SIZE)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">type</span> = torch.LongTensor</span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> use_cuda:</span><br><span class="line">    <span class="built_in">type</span> = torch.cuda.LongTensor</span><br><span class="line">    model.cuda()</span><br></pre></td></tr></table></figure><p><strong>损失计算</strong> : 负采样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate the loss value for the entire sentence</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_sent_loss</span>(<span class="params">sent</span>):</span></span><br><span class="line">    <span class="comment"># add padding to the sentence equal to the size of the window</span></span><br><span class="line">    <span class="comment"># as we need to predict the eos as well, the future window at that point is N past it</span></span><br><span class="line">    all_neg_words = np.random.choice(nwords, size=<span class="number">2</span>*N*K*<span class="built_in">len</span>(sent), replace=<span class="literal">True</span>, p=word_probabilities)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step through the sentence</span></span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sent):</span><br><span class="line">        <span class="comment"># 采样生成词的位置列表 如果x&gt;=0，就是0或者中心词左边的位置</span></span><br><span class="line">        <span class="comment"># x &lt; len 就是0或者中心词右边</span></span><br><span class="line">        pos_words = [sent[x] <span class="keyword">if</span> x &gt;= <span class="number">0</span> <span class="keyword">else</span> S <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(i-N,i)] + \</span><br><span class="line">                     [sent[x] <span class="keyword">if</span> x &lt; <span class="built_in">len</span>(sent) <span class="keyword">else</span> S <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,i+N+<span class="number">1</span>)]</span><br><span class="line">        pos_words_tensor = torch.tensor(pos_words).<span class="built_in">type</span>(<span class="built_in">type</span>)</span><br><span class="line">        neg_words = all_neg_words[i*K*<span class="number">2</span>*N:(i+<span class="number">1</span>)*K*<span class="number">2</span>*N]</span><br><span class="line">        neg_words_tensor = torch.tensor(neg_words).<span class="built_in">type</span>(<span class="built_in">type</span>)</span><br><span class="line">        target_word_tensor = torch.tensor([word]).<span class="built_in">type</span>(<span class="built_in">type</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#<span class="doctag">NOTE:</span> technically, one should ensure that the neg words don&#x27;t contain the </span></span><br><span class="line">        <span class="comment">#      the context (i.e. positive) words, but it is very unlikely, so we can ignore that</span></span><br><span class="line"></span><br><span class="line">        pos_loss = model(target_word_tensor, pos_words_tensor)</span><br><span class="line">        neg_loss = model(target_word_tensor, neg_words_tensor, negative_sample=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        losses.append(pos_loss + neg_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.stack(losses).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><h4 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ITER <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;started iter %r&quot;</span> % ITER)</span><br><span class="line">    <span class="comment"># Perform training</span></span><br><span class="line">    random.shuffle(train)</span><br><span class="line">    train_words, train_loss = <span class="number">0</span>, <span class="number">0.0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sent_id, sent <span class="keyword">in</span> <span class="built_in">enumerate</span>(train):</span><br><span class="line">        my_loss = calc_sent_loss(sent)</span><br><span class="line">        <span class="comment"># 统计loss和训练词数</span></span><br><span class="line">        train_loss += my_loss.item()</span><br><span class="line">        train_words += <span class="built_in">len</span>(sent)</span><br><span class="line">        <span class="comment"># Back prop while training</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        my_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> (sent_id + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;--finished %r sentences&quot;</span> % (sent_id + <span class="number">1</span>))</span><br><span class="line">            train_ppl = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">if</span> train_loss / train_words &gt; <span class="number">709</span> <span class="keyword">else</span> math.exp(train_loss / train_words)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;after sentences %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs&quot;</span> % (</span><br><span class="line">sent_id + <span class="number">1</span>, train_loss / train_words, train_ppl, time.time() - start))</span><br><span class="line">    train_ppl = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">if</span> train_loss / train_words &gt; <span class="number">709</span> <span class="keyword">else</span> math.exp(train_loss / train_words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs&quot;</span> % (</span><br><span class="line">    ITER, train_loss / train_words, train_ppl, time.time() - start))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Evaluate on dev set</span></span><br><span class="line">    dev_words, dev_loss = <span class="number">0</span>, <span class="number">0.0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> sent_id, sent <span class="keyword">in</span> <span class="built_in">enumerate</span>(dev):</span><br><span class="line">        my_loss = calc_sent_loss(sent)</span><br><span class="line">        dev_loss += my_loss.item()</span><br><span class="line">        dev_words += <span class="built_in">len</span>(sent)</span><br><span class="line">    dev_ppl = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">if</span> dev_loss / dev_words &gt; <span class="number">709</span> <span class="keyword">else</span> math.exp(dev_loss / dev_words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs&quot;</span> % (</span><br><span class="line">    ITER, dev_loss / dev_words, dev_ppl, time.time() - start))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;saving embedding files&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(embeddings_location, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> embeddings_file:</span><br><span class="line">        W_w_np = model.word_embedding.weight.data.cpu().numpy()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nwords):</span><br><span class="line">            ith_embedding = <span class="string">&#x27;\t&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, W_w_np[i]))</span><br><span class="line">            embeddings_file.write(ith_embedding + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>TODO</strong> 有时间比较下 negative sampling和 NCE </p><p>[1] <a href="https://arxiv.org/pdf/1410.8251.pdf">NCE 论文</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/250220371">NCE loss</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> skip-gram </tag>
            
            <tag> negative sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN， LSTM ， GRU 结构解释和其在Pytorch中的使用</title>
      <link href="2020/09/20/4.RNN%EF%BC%8C%20LSTM%20%EF%BC%8C%20GRU%20%E7%BB%93%E6%9E%84%E8%A7%A3%E9%87%8A%E5%92%8C%E5%85%B6%E5%9C%A8Pytorch%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>2020/09/20/4.RNN%EF%BC%8C%20LSTM%20%EF%BC%8C%20GRU%20%E7%BB%93%E6%9E%84%E8%A7%A3%E9%87%8A%E5%92%8C%E5%85%B6%E5%9C%A8Pytorch%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="RNN，-LSTM-，-GRU-结构解释和其在Pytorch中的使用"><a href="#RNN，-LSTM-，-GRU-结构解释和其在Pytorch中的使用" class="headerlink" title="RNN， LSTM ， GRU 结构解释和其在Pytorch中的使用"></a>RNN， LSTM ， GRU 结构解释和其在Pytorch中的使用</h3><h4 id="1-RNN-结构和内部计算"><a href="#1-RNN-结构和内部计算" class="headerlink" title="1. RNN 结构和内部计算"></a>1. RNN 结构和内部计算</h4><p>RNN结构示意图（这里没画bias， 但公式里写了，借的李宏毅教授PPT图）</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261739248.png" alt="image-20210406154230330" style="zoom: 33%;" /></p><p>$x$为当前状态下数据的输入， $h$表示接收到的上一个节点的输入。</p><p> $y$为当前节点状态下的输出，而 $h^{\prime} $为传递到下一个节点的输出。</p><p>真实计算公式为：</p><script type="math/tex; mode=display">\begin{align}\boldsymbol{h}_t &= \text{tanh} (\boldsymbol{W}_h [\boldsymbol{x}_t, \boldsymbol{h}_{t-1}] + b_t)\\ \\\boldsymbol{y}_t &= \text{tanh} (\boldsymbol{W}_0 \boldsymbol{h}_t  + b_t)\end{align}</script><p>通过上图的公式可以看到，输出$\boldsymbol{h}^{\prime}$与 $\boldsymbol{x}$ 和 $\boldsymbol{h}$的值都相关。</p><p>而 $\boldsymbol{y}$ 则常常使用 $\boldsymbol{h}^{\prime}$投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。</p><p>对这里的$\boldsymbol{y}$如何通过  $\boldsymbol{h}^{\prime}$ 计算得到往往看具体模型的使用方式。</p><p>简单使用rnn实例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入尺寸, 隐藏层神经元个数， 层数</span></span><br><span class="line">rnn = nn.RNN(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line">input_data = Variable(torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>))  <span class="comment">#[seq_len, batch_size, word_dim]</span></span><br><span class="line"><span class="comment">#如果传入网络时，不特别注明隐状态，那么输出的隐状态默认参数全是0</span></span><br><span class="line">h_0 = Variable(torch.randn(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>))  <span class="comment">#[lnum_layers*direction, batch_size, hidden_size]</span></span><br><span class="line">output,h_t = rnn(input_data,h_0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.size())  <span class="comment">#seq,batch,hidden_size</span></span><br><span class="line"><span class="built_in">print</span>(h_t.size())   <span class="comment">#layer*direction,batch,hidden_size</span></span><br><span class="line"><span class="built_in">print</span>(rnn.weight_ih_l0.size())</span><br><span class="line">++++++++++++++++++++++++++++++++++++++++++++++</span><br><span class="line">torch.Size([<span class="number">100</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">torch.Size([<span class="number">50</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure><p>通过序列形式的输入，我们能够得到如下形式的RNN。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740390.png" alt="image-20210406170914456" style="zoom:33%;" /></p><p>序列模型中RNN的搭建：</p><p>使用 <code>nn.RNN(input_size, hidden_size,  num_layers)</code>中参数解释</p><ul><li><strong>input_size</strong> – The number of expected features in the input x</li><li><strong>hidden_size</strong> – The number of features in the hidden state h</li><li><strong>num_layers</strong> – Number of recurrent layers.</li><li><strong>bidirectional</strong> – <code>If  True, becomes a bidirectional RNN. Default: False</code></li></ul><p>输出是$y_t,  h_t$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        分别代表输入最后一维尺寸</span></span><br><span class="line"><span class="string">        隐藏层最后一维尺寸</span></span><br><span class="line"><span class="string">        输出层最后一维尺寸</span></span><br><span class="line"><span class="string">        层数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        <span class="comment">#实例化RNN，参数分别是输入，隐藏层神经元数目，层数</span></span><br><span class="line">        self.rnn = nn.RNN(input_size, hidden_size, num_layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#实例化线性层，将RNN输出转化为指定维度</span></span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        <span class="comment">#实例化softmax将输出变为类别概率</span></span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input1, hidden</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param input: 输入， 1xn_letters</span></span><br><span class="line"><span class="string">        :param hidden: 隐藏层张量， self.layers x 1 x self.hidden_size</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#RNN输入预定义为3为将其拓展一个维度</span></span><br><span class="line">        input1 = input1.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#将input, hidden输入rnn，如果layer=1，那么rr恒等于hn</span></span><br><span class="line">        rr, hn = self.rnn(input1, hidden)</span><br><span class="line">        <span class="comment">#将rnn输出通过线性变换和softmax，同时返回hn作为后续RNN输入</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(rr)), hn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#初始化一个self.num_layers, 1, self.hidden_size形状的0张量</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure><h4 id="2-LSTM"><a href="#2-LSTM" class="headerlink" title="2. LSTM"></a>2. LSTM</h4><p>LSTM是long short-term memory,与RNN区别是</p><ul><li>LSTM两个传递状态：$c^t $ cell state,  $h^t$ hidden state </li><li>RNN一个传递状态: $h^t$ </li></ul><p>注： RNN中$h^t$等价于 LSTM中 $c^t$ </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740391.png" alt="image-20210406172803901" style="zoom: 33%;" /></p><p>而对于input gate, forget gate, output gate中是$[x^t, h^{t-1}]$向量拼接后乘以相应权重矩阵再过sigmoid函数转换为0到1的数值，作为门控状态。</p><p>需要注意的是z是tanh函数，转换为-1到1，因为是输入数据不是门控状态。</p><p>因此有下面4个公式：（忽略bias）</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{z} &= \text{tanh}(\boldsymbol{W} [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}])\\\boldsymbol{z}^i &= \sigma(\boldsymbol{W}^i [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}])\\\boldsymbol{z}^f &= \sigma(\boldsymbol{W}^f [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}])\\\boldsymbol{z}^o &= \sigma(\boldsymbol{W}^o [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}])\end{aligned}</script><p>注：[]表示两个向量拼接，而 $\boldsymbol{W}$四个权重矩阵都是训练学习得到的。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740392.png" alt="image-20210406173622441" style="zoom:33%;" /></p><p>然后根据上面四个信号$ \boldsymbol{z}, \boldsymbol{z}^f, \boldsymbol{z}^i, \boldsymbol{z}^o $, 由下列公式得到输出， cell state和隐藏状态。</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{c}^t &= \boldsymbol{z}^f \odot \boldsymbol{c}^{t-1} + \boldsymbol{z}^i \odot \boldsymbol{z}\\ \boldsymbol{h}^t &= \boldsymbol{z}^o \odot \text{tanh }\boldsymbol{c}^{t}\\\boldsymbol{y}^t &= \sigma (\boldsymbol{W}^\prime\boldsymbol{h}^{t})\end{aligned}</script><ul><li>cell state ：等于 上一个cell state 与遗忘门信号 $\boldsymbol{z}^f  $ Hadamard product，和 输入门信号 $ \boldsymbol{z}^i  $与输入信号$\boldsymbol{z}$ 的Hadamard product之和</li><li>hidden state: 等于输出门信号 $\boldsymbol{z}^o $ 与 过tanh后的 cell state 的Hadamard product</li><li>输出： 等于上一个时间步后权重矩阵$\boldsymbol{W}$ 与 隐藏状态过sigmoid</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740393.png" alt="image-20210406213738525" style="zoom:20%;" /></p><p>Pytorch中LSTM实例使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">lstm = nn.LSTM(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)<span class="comment">#[input_size hidden_size num_layers]</span></span><br><span class="line"><span class="comment">#输入变量</span></span><br><span class="line">input_data = Variable(torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)) <span class="comment">#[seq_len, batch_size, word_dim]</span></span><br><span class="line"><span class="comment">#初始隐状态</span></span><br><span class="line">h_0 = Variable(torch.randn(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>)) <span class="comment">#[num_layers*direction, batch_size, hidden_size]</span></span><br><span class="line"><span class="comment">#输出记忆细胞</span></span><br><span class="line">c_0 = Variable(torch.randn(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>)) <span class="comment">#[num_layers*direction, batch_size, hidden_size]</span></span><br><span class="line"><span class="comment">#输出变量</span></span><br><span class="line">output,(h_t,c_t) = lstm(input_data,(h_0,c_0))</span><br><span class="line"><span class="built_in">print</span>(output.size()) <span class="comment">#[seq_len, batch_size, hidden_size]</span></span><br><span class="line"><span class="built_in">print</span>(h_t.size())</span><br><span class="line"><span class="built_in">print</span>(c_t.size())</span><br><span class="line"><span class="comment">#参数大小为(50x4,20),是RNN的四倍</span></span><br><span class="line"><span class="built_in">print</span>(lstm.weight_ih_l0)</span><br><span class="line"><span class="built_in">print</span>(lstm.weight_ih_l0.size())</span><br><span class="line"></span><br><span class="line">++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br><span class="line">torch.Size([<span class="number">100</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1123</span>, -<span class="number">0.0734</span>, -<span class="number">0.0408</span>,  ..., -<span class="number">0.0715</span>,  <span class="number">0.0920</span>,  <span class="number">0.0409</span>],</span><br><span class="line">        [-<span class="number">0.1310</span>, -<span class="number">0.0566</span>, -<span class="number">0.0761</span>,  ..., -<span class="number">0.0909</span>,  <span class="number">0.0081</span>, -<span class="number">0.0258</span>],</span><br><span class="line">        [ <span class="number">0.0264</span>,  <span class="number">0.0297</span>,  <span class="number">0.0800</span>,  ...,  <span class="number">0.0869</span>, -<span class="number">0.0008</span>, -<span class="number">0.0621</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">0.0116</span>,  <span class="number">0.0778</span>, -<span class="number">0.1181</span>,  ...,  <span class="number">0.1066</span>,  <span class="number">0.0610</span>,  <span class="number">0.0846</span>],</span><br><span class="line">        [ <span class="number">0.0216</span>, -<span class="number">0.0159</span>,  <span class="number">0.1354</span>,  ...,  <span class="number">0.0726</span>, -<span class="number">0.0238</span>, -<span class="number">0.1158</span>],</span><br><span class="line">        [-<span class="number">0.1081</span>, -<span class="number">0.0760</span>, -<span class="number">0.0722</span>,  ...,  <span class="number">0.0506</span>,  <span class="number">0.0550</span>,  <span class="number">0.1033</span>]],</span><br><span class="line">       requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.Size([<span class="number">200</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure><p>在seq2seq任务中，使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        分别代表输入最后一维尺寸</span></span><br><span class="line"><span class="string">        隐藏层最后一维尺寸</span></span><br><span class="line"><span class="string">        输出层最后一维尺寸</span></span><br><span class="line"><span class="string">        层数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        <span class="comment">#实例化RNN，参数分别是输入，隐藏层神经元数目，层数</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#实例化线性层，将RNN输出转化为指定维度</span></span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        <span class="comment">#实例化softmax将输出变为类别概率</span></span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input1, hidden, c</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        LSTM输入有3个</span></span><br><span class="line"><span class="string">        :param input1: 输入， 1xn_letters</span></span><br><span class="line"><span class="string">        :param hidden: 隐藏层张量， self.layers x 1 x self.hidden_size</span></span><br><span class="line"><span class="string">        :param c 张量， self.layers x 1 x self.hidden_size</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#RNN输入预定义为3为将其拓展一个维度</span></span><br><span class="line">        input1 = input1.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#将input, hidden输入rnn，如果layer=1，那么rr恒等于hn</span></span><br><span class="line">        rr, (hn, cn) = self.lstm(input1, (hidden, c))</span><br><span class="line">        <span class="comment">#将rnn输出通过线性变换和softmax，同时返回hn作为后续RNN输入</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(rr)), hn, cn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#初始化一个self.num_layers, 1, self.hidden_size形状的0张量</span></span><br><span class="line">        c = hidden = torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br><span class="line">        <span class="keyword">return</span> hidden, c</span><br></pre></td></tr></table></figure><h4 id="3-GRU"><a href="#3-GRU" class="headerlink" title="3. GRU"></a>3. GRU</h4><p>GRU(Gate Recurrent Unit)结构示意图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740394.png" alt="image-20210407123554544" style="zoom:33%;" /></p><p>最重要的是，两个门控reset gate 和 update gate:</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740395.jpg" alt="v2-7fff5d817530dada1b279c7279d73b8a_r" style="zoom:33%;" /></p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{r}^t &= \sigma(\boldsymbol{W}^r [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}]) \\\boldsymbol{z}^t &= \sigma(\boldsymbol{W}^z [\boldsymbol{x}^t, \boldsymbol{h}^{t-1}])\end{aligned}</script><p>然后得到重置的隐藏层状态${\boldsymbol{h}^{t-1}}^\prime= \boldsymbol{h}^{t-1} \odot\boldsymbol{r}^{t} $. 将这个隐藏状态与$\boldsymbol{x}^t$ 拼接再过tanh将重置的隐藏状态缩放为-1~1.得到${\boldsymbol{h}^{t}}^\prime$</p><p> 即</p><script type="math/tex; mode=display">{\boldsymbol{h}^{t}}^\prime= \text{tanh }(\boldsymbol{W}[\boldsymbol{x}^t, {\boldsymbol{h}^{t-1}}^\prime])=\text{tanh }(\boldsymbol{W}[\boldsymbol{x}^t,  \boldsymbol{h}^{t-1} \odot\boldsymbol{r}^{t})</script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740396.png" alt="v2-390781506bbebbef799f1a12acd7865b_r" style="zoom:33%;" /></p><p>这里${\boldsymbol{h}^{t}}^\prime$ 主要包含了当前输入$ {\boldsymbol{x}^{t}} $数据, 还有有选择地添加上一个时间隐藏状态，这里主要是重置门信号作用(你可以理解为乘以一个0~1的数来控制添加隐藏状态的多少)。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261740397.png" alt="image-20210407151241968" style="zoom:20%;" /></p><p>最后一步是$\boldsymbol{h}^{t}$，也是输出。计算公式为：</p><script type="math/tex; mode=display">\boldsymbol{y}^{t}  =\boldsymbol{h}^{t} = (1-\boldsymbol{z}^t ) \odot \boldsymbol{h}^{t-1} + \boldsymbol{z}^t \odot {\boldsymbol{h}^{t}}^\prime</script><ul><li><p>${\boldsymbol{h}^{t}}^\prime$  相当于LSTM中的hidden state : $  \boldsymbol{h}^t = \boldsymbol{z}^o \odot \text{tanh }\boldsymbol{c}^{t} $</p></li><li><p>而$\boldsymbol{h}^{t} $ 相当于cell state</p></li></ul><p>实际上，GRU是用一个update gate来替换LSTM的input gate 和 forget gate. </p><p>在 LSTM 中这两个门分别是</p><ul><li>控制输入多少(输入是上一个时间状态和当前输入总的信息量)</li><li>控制遗忘信息量的多少</li></ul><p>而update gate就是上面的$\boldsymbol{z}^t$ ,是一个0~1的控制信号。那么：</p><ul><li>$\boldsymbol{z}^t  $理解等价于input gate 控制信号，  $\boldsymbol{z}^t \odot {\boldsymbol{h}^{t}}^\prime$ 就等效于经过input gate后的信息</li><li>$(1-\boldsymbol{z}^t )$ 等价于 forget gate 控制信号， $(1-\boldsymbol{z}^t ) \odot \boldsymbol{h}^{t-1}$ 等效于 经过 forget gate后的信息</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">gru = nn.GRU(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#输入变量</span></span><br><span class="line">input_data = Variable(torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)) <span class="comment">#[seq_len, batch_size, word_dim]</span></span><br><span class="line"><span class="comment">#初始隐状态</span></span><br><span class="line">h_0 = Variable(torch.randn(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>)) <span class="comment">#[num_layers*direction, batch_size, hidden_size]</span></span><br><span class="line"><span class="comment">#输出变量</span></span><br><span class="line">output,(h_n,c_n) = gru(input_data)  </span><br><span class="line"><span class="built_in">print</span>(output.size()) <span class="comment">#[seq_len, batch_size, word_dim]</span></span><br><span class="line"><span class="built_in">print</span>(h_n.size()) <span class="comment">#[batch_size, word_dim]</span></span><br><span class="line"><span class="built_in">print</span>(gru.weight_ih_l0)</span><br><span class="line"><span class="built_in">print</span>(gru.weight_ih_l0.size()) <span class="comment">#[hidden_size+seq_len, word_dim]</span></span><br><span class="line"></span><br><span class="line">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><br><span class="line">torch.Size([<span class="number">100</span>, <span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">torch.Size([<span class="number">32</span>, <span class="number">50</span>])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.0465</span>,  <span class="number">0.0917</span>,  <span class="number">0.1357</span>,  ..., -<span class="number">0.1283</span>,  <span class="number">0.1264</span>, -<span class="number">0.0320</span>],</span><br><span class="line">        [ <span class="number">0.1384</span>,  <span class="number">0.1072</span>,  <span class="number">0.0073</span>,  ..., -<span class="number">0.0772</span>, -<span class="number">0.1295</span>,  <span class="number">0.0233</span>],</span><br><span class="line">        [-<span class="number">0.0627</span>, -<span class="number">0.1012</span>, -<span class="number">0.0415</span>,  ...,  <span class="number">0.1311</span>,  <span class="number">0.1374</span>, -<span class="number">0.0566</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">0.0131</span>,  <span class="number">0.0995</span>,  <span class="number">0.0054</span>,  ..., -<span class="number">0.0350</span>, -<span class="number">0.1220</span>,  <span class="number">0.1143</span>],</span><br><span class="line">        [-<span class="number">0.0836</span>,  <span class="number">0.1239</span>,  <span class="number">0.0860</span>,  ...,  <span class="number">0.0837</span>,  <span class="number">0.0172</span>, -<span class="number">0.0170</span>],</span><br><span class="line">        [ <span class="number">0.0637</span>,  <span class="number">0.0501</span>,  <span class="number">0.0091</span>,  ..., -<span class="number">0.1201</span>,  <span class="number">0.0788</span>,  <span class="number">0.0468</span>]],</span><br><span class="line">       requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.Size([<span class="number">150</span>, <span class="number">20</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>seq2seq中使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRU</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, num_layers=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GRU, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Instantiate the predefined nn.GRU, its three parameters are input_size, hidden_size, num_layers</span></span><br><span class="line">        self.gru = nn.GRU(input_size, hidden_size, num_layers)</span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        rr, hn = self.gru(<span class="built_in">input</span>, hidden)</span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(rr)), hn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.num_layers, <span class="number">1</span>, self.hidden_size)</span><br></pre></td></tr></table></figure><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/Seq%20(v2">Seq-to-seq Learning</a>.pdf )</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的LSTM</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GRU </tag>
            
            <tag> LSTM </tag>
            
            <tag> RNN </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.Efficient Estimation ofWord Representations in Vector Space——Mikolov论文笔记</title>
      <link href="2020/09/14/NLP%20Paper%201.Efficient%20Estimation%20ofWord%20Representations%20in%20Vector%20Space%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>2020/09/14/NLP%20Paper%201.Efficient%20Estimation%20ofWord%20Representations%20in%20Vector%20Space%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="词向量空间中词表示的有效估计"><a href="#词向量空间中词表示的有效估计" class="headerlink" title="词向量空间中词表示的有效估计"></a>词向量空间中词表示的有效估计</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>提出了两种计算连续词向量表示模型。<br>        特点：计算低，在语法和语义上的词相似度任务上精度有所提升</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h4><p>以前模型的优点缺点：</p><ul><li>统计翻译模型的缺陷:把单词当做原子单元，作为语料字典的索引失去了相似性。</li><li>N-gram优点能在任何数据集上训练。</li></ul><p>简单模型在任务上的限制。如，语音识别：要求语音数据质量高，规模大，但不超过10亿级别的词汇量。因此简单技术改进不会有任何进步。</p><p>随着机翻技术发展，现在能大规模数据上训练复杂模型。非常成功的概念是分布式词表示。例如，基于神经语言模型明显地优于N-gram模型</p><h5 id="论文目标"><a href="#论文目标" class="headerlink" title="论文目标"></a>论文目标</h5><p>主要目标：从百万级词汇的大数据集中学习高质量的词向量。之前方法不能在百万级别词上学到词向量在50-100维之间。</p><p>词从原空间映射到词向量子空间后，词与词之间不仅更近，而且有更多相似度。</p><p>惊人的是，词表示的相似度超越了简单的语法规则。如:</p><script type="math/tex; mode=display">\vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} = \vec{\text{Queen}}</script><p>本文，试图通过开发新模型来最大化词向量操作的准确性，以保留词之间的线性规则。作者设计新的全面的测试集用于测量语法和语义规则，并且其展示了许多规则可以高准确率学到。此外，作者讨论训练时间和经典如何依赖词向量的维度和训练数据量。</p><h5 id="之前的工作"><a href="#之前的工作" class="headerlink" title="之前的工作"></a>之前的工作</h5><p>词表示作为联系向量有很长的历史。流行的<strong>NNLM</strong>模型由Bengio提出，是前馈神经网络，有一个线性投影层和非线性隐藏层，用来学习词向量表示的联合分布和统计语言模型。</p><p>另一个有趣的<strong>NNLM</strong>架构是词向量首先用单隐层的神经网络学习到。然后这个词向量被用来训练<strong>NNLM</strong>。这个词向量不是在构建整个<strong>NNLM</strong>中学到的。作者直接扩展了该架构，集中于第一步，用简单模型学到词向量。</p><h3 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2.模型架构"></a>2.模型架构</h3><p>以前估计连续词表示，用LSA(Latent Semantic Analysis)和LDA (Latent Dirichlet Allocation).本文用神经网络学习，在保留词之间的线性规则上，表现好于LSA；而LDA在大数据集上计算代价昂贵。</p><p>为了比较不同模型架构，作者首先定义计算复杂度为充分训练模型所需要的参数的数量。接下来，最小化计算复杂度时最大化准确率。</p><p>模型复杂度正比于：</p><script type="math/tex; mode=display">O = E \times T \times Q \tag{1}</script><p>其中，E是训练轮数，T是训练集里词的数目，Q接下来会具体定义。通常，E是3-50轮，T可达到十亿数量级，所有模型都使用随机梯度下降和反向传播。</p><h4 id="前馈神经网络-（NNLM）"><a href="#前馈神经网络-（NNLM）" class="headerlink" title="前馈神经网络 （NNLM）"></a>前馈神经网络 （NNLM）</h4><p>下图是<strong>NNLM</strong>，来源于 <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829788.png" alt="image-20210305150250100" style="zoom:28%;" /></p><p>包含输入层， 投影层， 隐藏层和输出层。在输入层， N个预测词之前的单词用1xV的one-hot编码，V是词汇表大小。输入层用共享的投影矩阵，然后被投影到NxD投影层 P,。因为在给定时间只有N个输入有效，构成投影层是相对廉价的操作。     </p><p>NNLM架构在投影层和隐藏层的计算变得复杂， 因为投影层上的值是稠密的。通常选择N=10, 投影层P可能是500-2000，与此同时，隐藏层通常为500-1000.而且，隐藏通常用来计算字典中所有词概率分布， 这导致输出层是维度V。因此，每一个训练样本的计算复杂度为：(H是隐藏层节点个数)</p><script type="math/tex; mode=display">Q = N \times D + N \times D \times H + H\times V \tag{2}</script><p>其中主要计算量来自于$H \times V$。然而，几个实际解决方案提出来避免计算量。比如石油分层版本的softmax，或者避免在训练中通过使用没有归一化的模型来避免完全归一模型。 用二叉树表示词汇表， 需要被估计的输出单元数目能降到$\text{log}_2(V)$。因此，主要计算复杂度源自于$N \times D \times H $项。</p><p>在本文模型中，作者使用层次softmax， 词汇表表示成哈夫曼树。这延续之前的研究， 词频对于在神经网络语言模型中获取类别是有效的。哈夫曼树，分配给高频词端的二进制编码，这进一步减少了要估计的输出单元的数目：虽然平衡二叉树有$\text{log}_2(V) $的输出要估计， 但基于层次的softmax哈夫曼树只需要<script type="math/tex">\text{log}_2 (\text{Unigram_perplexity}(V))</script>(V的一元困惑度)。 举例来说， 当词汇表是100万， 使用该结构估计时大约快两倍。但对于神经网络语言模型来说，计算瓶颈在$N \times D \times H $ 项， 作者后面将替代不含隐藏层， 严重取决于softmax归一化效率。</p><h4 id="循环神经网络语言模型-（RNNLM）"><a href="#循环神经网络语言模型-（RNNLM）" class="headerlink" title="循环神经网络语言模型 （RNNLM）"></a>循环神经网络语言模型 （RNNLM）</h4><p>基于语言模型的循环神经网络， 提出来克服前馈神经语言模型的限制，比如需要具体的上下文长度(模型的顺序为N)， 因为理论上比起浅层神经网络， RNNs能更有效地表示更复杂的模式。RNN模型没有投影层，只有输入，隐藏层和输出层。 这类模型的特殊之处是连接自身隐藏层间的循环矩阵， 用来形成短期记忆，因为来自过去的信息可以有隐藏层状态表示， 隐藏层状态根据当前输入和上时间中的隐层状态来更新。</p><p>循环神经网络训练每个样本是的复杂度为：</p><script type="math/tex; mode=display">Q = H \times H + H \times V \tag{3}</script><p>其中，词表示D和隐藏层H有同样维度。这样通过使用层次softmax  $H \times V$ 有效地降为$H \times \text{log}_2(V)$。 复杂度主要来自于 $ H \times H$</p><h4 id="平行训练神经网络"><a href="#平行训练神经网络" class="headerlink" title="平行训练神经网络"></a>平行训练神经网络</h4><p>为了在大数据集上训练模型，作者在大规模分布式框架之上实现了几个模型叫做DistBelief, 包括前馈神经语言模型和本文中提出的新模型。该框架允许作者并行地运行同一模型的多个副本，并且每个副本通过保留所有参数的中央服务器来同步更新其梯度。对应这种并行训练，作者使用称为Adagrad的自适应学习速率程序小批次异步梯度下降。在该框架下， 通常使用100或更多模型副本，每个腹部在数据中的不同机器上使用不同<strong>CPU</strong>内核。</p><h3 id="3-新的对数线性模型"><a href="#3-新的对数线性模型" class="headerlink" title="3. 新的对数线性模型"></a>3. 新的对数线性模型</h3><p>在本节中， 作者提出两种新模型架构，用于学习分布式词表示， 并试图最小化计算复杂度。从前一节的主要研究中， 大部分复杂度源自于模型中非线性的隐藏层。虽然这是是的神经网络如此吸引人的地方， 作者觉得探索简单模型， 其可能无法像神经网络一样精确地表示数据，但可以在更多数据上有效地训练。</p><p>新的架构直接遵循作者早期工作中提出的， 其发现神经语言模型可以分成两步成功训练 (1.2节): 首先， 连续词向量用简单模型学习，然后 <strong>N-gram NNLM</strong> 在这些分布式词表示上训练。算人后面大量的工作重点学习词向量，作者认为之前提出的是最简单的。注意，相关模型<strong>Hinton</strong>等人早就提出了。</p><h4 id="连续词袋模型-Bag-of-Words"><a href="#连续词袋模型-Bag-of-Words" class="headerlink" title="连续词袋模型 (Bag-of-Words)"></a>连续词袋模型 (Bag-of-Words)</h4><p>首先提出价格类似于前馈<strong>NNLM</strong>， 其中非线性隐藏层被移除并且投影层为所有词所共享(不仅仅是投影矩阵)：隐藏， 所有词都被投影到相同位置(它们词向量被平均)。作者叫这种架构为词袋模型，因为在历史记录里词的顺序不影响投影。 此外，作者还使用前面的词；作者在下一节要介绍的，用四个前面的词和四个后面的词作为输入建立的一个对数线性分类器任务上得到最佳表现，其中，训练标准是正确分类当前(中间)词。训练复杂度为：</p><script type="math/tex; mode=display">Q = N \times D + D \times \text{log}_2(V) \tag{4}</script><p>其中， N是输入数，D是输入one-hot维度， V是词汇表大小    </p><p>作者把这个模型称为CBOW， 不同于标准词袋模型，它使用上下文的连续分布表示。如下图，注意输入层和投影层的权重矩阵，由所有词共现，跟NNLM一样。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829789.png" alt="image-20210306103847361" style="zoom:25%;" /></p><h4 id="连续跳字模型-Continuous-Skip-gram-Model"><a href="#连续跳字模型-Continuous-Skip-gram-Model" class="headerlink" title="连续跳字模型 (Continuous Skip-gram Model)"></a>连续跳字模型 (Continuous Skip-gram Model)</h4><p>第二个类似于CBOW的架构，但不再基于上下文预测中心词，它试着优化一个基于同一个句子中的其他词的词分类器。更精确地说，作者用每一个当前词作为带连续投影层的输入， 然后预测当前词之前和之后一定范围内的词。作者发现增加范围能够梯度词向量的质量，但也增加了计算复杂度。因为距离当前词远的词不如距离它仅的词更相关，所以在作者的训练集中通过采用更少来给更远的词小的权重。</p><p>该架构的训练复杂度正比于：</p><script type="math/tex; mode=display">Q = C \times (D + D \times \text{log}_2(V) )\tag{5}</script><p>这里, C是词与词的最大距离。如果作者选择C=5, 对每个训练的词作者将随机选取一个在<1, C>的数字，然后用当前词的前R个词和后R个词作为当前词的正确标签。这需要作者对$R \times 2$个单词做分类， 当前词作为分类器的输入，当前词的前R个词和后R个词作为输出。在下面实验中，作者取C=10.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829790.png" alt="image-20210306121224008" style="zoom:25%;" /></p><h3 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h3><p>通过加大在词向量维度并在大数据集上训练后，相似词之间的词向量在空间上是相近的(文中 提出small相似词的问题，用$ \vec{X} = \vec{\text{biggest}} - \vec{\text{big}} + \vec{\text{small}} $ 找到smallest,然后用余弦相似度找到跟$\vec{X}$ 相近的词向量，就可以回答这个问题)</p><p>词向量在5类语义和9类句法关系测试如下表：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829791.png" alt="image-20210306122123397" style="zoom:25%;" /></p><h4 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h4><p>为了衡量词向量的质量，作者定义了一个综合测试集，包含五种语义问题和九种句法问题。上图显示了每类中的两个示例。总的来说，有8869个语义问题和10675个句法问题。每个类别中的问题都是通过两个步骤创建的：首先，手动创建类似单词对的列表。然后，通过连接两个词对形成一个大的问题列表。例如，列出了68个美国大城市及其所属的州，并通过随机选择两个词对，形成了大约 2.5k 的问题。测试集中只包含单个标记词，因此不存在多词实体（such as New York）。</p><p>评价模型结果好坏的标准就是上述的词向量线性运算，如果通过线性运算得到的单词与正确的单词是完全一致的，那么就代表该词向量是正确的。所以同义词很难被计算出来，因为没有对同义词的输入，所以模型不可能达到 100% 的准确率。但是该模型的准确率与某些任务是正相关的，所以还是有一些用处的。</p><h4 id="最大化准确度"><a href="#最大化准确度" class="headerlink" title="最大化准确度"></a>最大化准确度</h4><p>使用谷歌新闻语料库训练词向量(6B tokens)， 限制词汇表大小为1百万。下表是CBOW在不同维度的词向量和不同数据量上测试结果。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829792.png" alt="image-20210306125620794" style="zoom:25%;" /></p><p>可以看到到达某个点之后，增加词向量维度或增加数据量带来的提升很小。但流行的是在相对大的数据上训练词向量。增大数据量由公式4可知，增大2倍数据，差不多也增大2倍复杂度。</p><p>训练每3轮以初始学习率为0.025线性下降的SGD反向传播，最后一轮为0.</p><h4 id="模型架构比较"><a href="#模型架构比较" class="headerlink" title="模型架构比较"></a>模型架构比较</h4><p>传统模型的比较工作，使用相同的数据集，相同 640 维度的词向量，也不仅限使用 30k 的单词，并且使用了全部的测试集数据。以下是训练结果的比较：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829793.png" alt="image-20210306132629622" style="zoom:30%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829794.png" alt="image-20210306132959540" style="zoom:25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829795.png" alt="image-20210306133118236" style="zoom:25%;" /></p><h3 id="5-学习到的关系示例"><a href="#5-学习到的关系示例" class="headerlink" title="5. 学习到的关系示例"></a>5. 学习到的关系示例</h3><p>表8中是遵循各种关系的词。通过前面定义的：两个词向量相减再将其加上另一个词向量，例如， Paris - France + Italy = Rome. </p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261829281.png" alt="image-20210306133317569" style="zoom:25%;" /></p><p>另外提高准确性的方法是提供多个关系的例子，然后将得到的关系向量平均，在本文测试中准确性提升了10%左右。</p><h3 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h3><p>在本文中，在一系列的语法和语义的语言任务上不同模型的表现，研究了词表示向量的质量。作者观察到用简单模型架构而不是流行的神经网络模型（包括前馈和循环）是可能训练得到高质量的词向量。因为更低的计算复杂度，从大规模数据集上训练得到非常精确的高维词向量是可能得到的。使用<strong>DistBelief</strong> 分布式框架，它能在万亿级别的语料上训练<strong>CBOW</strong>和<strong>Skip-gram</strong>模型。这比比类似模型的先前最佳结果大几个数量级。</p><p>词向量可以成功的应用于知识库中事实的自动扩展，还可以用于验证已有事实的准确性。从机翻实验结果来看也非常有前景。在未来，将作者的技术与潜在关系分析或其它进行比较也很有趣。作者想想作者的全面测试集将帮助研究社区提升估计词向量的现有技术。作者也期望高质量词向量将成为未来NLP应用的关键一环。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://hiyoungai.com/posts/fcba888f.html">论文阅读《Efficient Estimation of Word Representations in Vector Space》</a></p><p>[2] <a href="http://littlehaes.com/2018/06/06/word2vec%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-%E9%83%A8%E5%88%86/">word2vec论文翻译(部分)</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/332572542">2.Embedding系列一(word2vec)</a></p><p>[4] <a href="https://spaces.ac.cn/archives/4299">不可思议的Word2Vec 1.数学原理</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CBOW </tag>
            
            <tag> Skip-gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Backpropagation 梯度推导 台湾国立大学应用深度学习</title>
      <link href="2020/09/04/2.%20%E5%8F%B0%E6%B9%BE%E5%9B%BD%E7%AB%8B%E5%A4%A7%E5%AD%A6%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
      <url>2020/09/04/2.%20%E5%8F%B0%E6%B9%BE%E5%9B%BD%E7%AB%8B%E5%A4%A7%E5%AD%A6%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="Backpropagation-算法推导"><a href="#Backpropagation-算法推导" class="headerlink" title="Backpropagation 算法推导"></a>Backpropagation 算法推导</h2><p>本文是NTU陈蕴侬教授，应用深度学习课程反向传播算法推导的笔记。原文在：<a href="https://www.csie.ntu.edu.tw/~miulab/s108-adl/doc/200310_Backprop.pdf">200310_Backprop.pdf</a></p><h3 id="1-全连接神经网络的公式"><a href="#1-全连接神经网络的公式" class="headerlink" title="1. 全连接神经网络的公式"></a>1. 全连接神经网络的公式</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735433.png" alt="台湾国立大学-Applied Deep Learning" style="zoom:20%;" /></p><p>如图所示，全连接前馈神经网络，实现了<script type="math/tex">f: \mathbb{R}^N \to \mathbb{R}^M</script></p><script type="math/tex; mode=display">y = f(x) = \delta(W^L\cdots\delta(W^2\delta(W^1x+b^1)+b^2)\cdots+b^L)</script><p>先定义一些参数：</p><script type="math/tex; mode=display">\begin{aligned}&a_i^l: 第l层第i个神经元的输出\\ &a^l: 第l层的输出向量\\\\ &z_i^l:第l层第i个神经元激活函数的输入\\ &z^l:第l层神经元激活函数的输入向量\\\\ &w_{ij}^l:从i到j的第l层的神经元权重\\ &w^l:第l层的神经元权重向量\\\\ &b_i^l:第l层第i个神经元的偏置\\ &b^l:第l层的神经元偏置向量\end{aligned}</script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735434.png" alt="台湾国立大学-Applied Deep Learning" style="zoom: 20%;" /></p><p>具体来说， 先看最后一层关系——从a到z,如上图。而从z到a：如下图</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735435.png" alt="台湾国立大学-Applied Deep Learning" style="zoom: 20%;" /></p><p>注意z到a的中间加了激活函数。这里也会产生一个梯度，$ \frac{\partial a}{ \partial z} $</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735436.png" alt="image-20210131221452682" style="zoom:20%;" /></p><p>全连接神经网络的前向过程。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735437.png" alt="image-20210131221642843" style="zoom:28%;" /></p><p>对神经网络的梯度下降算法及记号。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735438.png" alt="image-20210131221809835" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735439.png" alt="image-20210131221905621" style="zoom:20%;" /></p><p>前向与后向传播比较：</p><ul><li>在前馈神经网络中<ul><li>前向<ul><li>信息流向从输入x到输出y</li><li>在训练中，前向传播能持续知道产生标量$C(\theta)$</li></ul></li><li>反向<ul><li>允许信息从损失反向流过整个网络，放在计算完整的梯度</li><li>适合任何函数<br><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735440.png" alt="image-20210131222038385" style="zoom:25%;" /></li></ul></li></ul></li></ul><p>看这里，红色所示到底代表损失函数对单一权重$w^{l}_{ij}$的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735441.png" alt="image-20210131224836877" style="zoom:20%;" /></p><p>具体推导：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735443.png" alt="image-20210131224913140" style="zoom:20%;" /></p><p>这里就等于 </p><script type="math/tex; mode=display">\frac{\partial C(\theta)}{ \partial w_{ij}^l} = \frac{\partial C(\theta)}{ \partial z_i^l} \frac{\partial \partial z_i^l}{ \partial w_{ij}^l}</script><p>其实，就是关于$\theta$损失函数关于参数$w_{ij}^l$的梯度 可以 表达为： <strong>损失函数</strong>对于<strong>激活函数输入z的梯度</strong>，乘以 <strong>激活函数输入z</strong>对<strong>对应权重的梯度</strong>。那后面这一项等于什么呢? 下面两张张slide解释了：</p><ul><li>当层数大于1，等于对应的前一层激活函数的输出a</li><li>当层数为1时， 等于对应的训练的输入input x</li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735444.png" alt="image-20210131225045235" style="zoom:20%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735445.png" alt="image-20210131225900693" style="zoom:20%;" /></p><p>总结一下，激活函数输出z对权重的梯度,如下。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735446.png" alt="image-20210131230224382" style="zoom:20%;" /></p><p><strong>现在只要求</strong>$\frac{\partial C(\theta)}{ \partial z}$， 如下所示</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735447.png" alt="image-20210131231518204" style="zoom:25%;" /></p><p>其中，引入个残差符号$\delta_i^j$, 代表第l层第i个节点的梯度。而且，利用从L层到1层的传播特性。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735448.png" alt="image-20210131231627348" style="zoom:20%;" /></p><p>先计算l层的梯度，因为激活函数输入z，发生$\Delta z$的变化， 激活函数的输出就会发生$\Delta a$的变化， 就等于输出y的变化$\Delta y$,引起$\Delta C$的变化。</p><p>而$    \frac{\partial C}{ \partial y_i}$, 跟损失函数有关，比如均方差损失，交叉熵损失….</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735449.png" alt="image-20210131232503772" style="zoom:20%;" /></p><p>而$\frac{\partial y_i}{ \partial z_i^L}$的梯度就是激活函数在$z^L$的梯度，因此总体可以看作是激活函数在$z^L$的梯度与损失函数关于输出的梯度逐元素积。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735450.png" alt="image-20210131232706263" style="zoom:20%;" /></p><p>放在整个网络中看，就是损失函数对第$l+1$层每一个z的梯度乘以每个层对$l$层a的梯度乘以第$l$层a对z的梯度。红色箭头处是$\frac{\partial C}{ \partial z_k^{l+1}}$ 等于 $\delta_k^{l+1}$, 课程上有讲k写成i了。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735451.png" alt="image-20210131233721367" style="zoom:20%;" /></p><p>那么轻松简化成下图中.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735452.png" alt="image-20210201001111396" style="zoom:20%;" /></p><p>回想下梯度传播原理，就是每层梯度等于激活函数在$z_i$处梯度乘以(对应权重和下一层梯度)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735453.png" alt="image-20210201002822378" style="zoom:20%;" /></p><p>写成矩阵形式就是如下</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735454.png" alt="image-20210201003120881" style="zoom:20%;" /></p><p>总体来看整个网络中，损失函数对权重的梯度等于损失函数对$l$层激活函数输入z的梯度，以及损失函数对权重的梯度</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735455.png" alt="image-20210201003242871" style="zoom:20%;" /></p><p>反向传播中，激活函数输入z对权重的梯度，分为1层网络和大于1层两种情况，分别如下</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735456.png" alt="image-20210201003517878" style="zoom:20%;" /></p><p>反向传播中每层激活函数处梯度，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735457.png" alt="image-20210201003725179" style="zoom:20%;" /></p><p>优化中的梯度下降</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735458.png" alt="image-20210201003840083" style="zoom:20%;" /></p><p>总结:</p><p>计算损失函数关于权重的梯度基于从前向和反向传播中的两个提前计算量：</p><ul><li>前向传播中$\delta_i^l$等于，第l层激活函数在$z^l$处梯度与 权重w和下一层$\delta$的 Hadamard积(如果是最后一层就是损失函数对z的梯度)</li><li>前向传播中， 激活函数输出或输入x</li></ul><p>而这个梯度就等于上述两者相乘。(本质上是，这个神经元的输入和对应梯度的乘积，而梯度等于方向导数下降的最快方向)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261735459.png" alt="image-20210201003921459" style="zoom:25%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Backpropagation </tag>
            
            <tag> DL </tag>
            
            <tag> NLP </tag>
            
            <tag> 2020 Spring NTU ADL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Precision Recall F1-score和AUC 评价指标解释</title>
      <link href="2020/08/19/ML_F1%20AUC%20ROC%E8%A7%A3%E9%87%8A/"/>
      <url>2020/08/19/ML_F1%20AUC%20ROC%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h3 id="Precision-Recall-F1-score和AUC-评价指标解释"><a href="#Precision-Recall-F1-score和AUC-评价指标解释" class="headerlink" title="Precision Recall F1-score和AUC 评价指标解释"></a>Precision Recall F1-score和AUC 评价指标解释</h3><h4 id="1-Precision，-Recall，-F1-score解释"><a href="#1-Precision，-Recall，-F1-score解释" class="headerlink" title="1. Precision， Recall， F1-score解释"></a>1. Precision， Recall， F1-score解释</h4><p>这些名词都建立在混淆矩阵之上confusion matirx，如下所示：</p><div class="table-container"><table><thead><tr><th>真实情况</th><th>预测Positive</th><th>Negative</th></tr></thead><tbody><tr><td>正例</td><td>TP 真正例</td><td>FN 假反例</td></tr><tr><td>反例</td><td>FP 假正例</td><td>TN 真反例</td></tr></tbody></table></div><p>查准率和查全率分别为：</p><script type="math/tex; mode=display">\begin{aligned}&P \text{ :precision} =  \frac{TP}{TP + FP} \\&R \text{ :recall}=  \frac{TP}{TP + FN} \\ \end{aligned}</script><ul><li><p>查准率：就是你找到的正例中有多少是真正的正例(实际上是比率)，就等于真正的正例/ 所有预测的正例</p></li><li><p>查全率：就是你在原始样本中找到了多少真正的正例，所有原始样本数就是正例和反例和，在混淆矩阵中就是$TP + FN$，实际上就等于真正的正例/所有原始样本数。更通俗地说，把目标样本找全了没，而<strong>查准率</strong>是你找准了多少正样本。</p></li></ul><p>而F1-score是：它是P和R的调和平均：</p><script type="math/tex; mode=display">F_1 = \frac{2}{1/P+ 1/R} = \frac{2 \times P \times R} {P + R}</script><p><strong>Macro-F1和Micro-F1</strong></p><ul><li>Macro-F1和Micro-F1是相对于多标签分类而言的。</li><li>Micro-F1，计算出所有类别总的Precision和Recall，然后计算F1。</li><li>Macro-F1，计算出每一个类的Precison和Recall后计算F1，最后将F1平均。</li></ul><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span>(<span class="params">actual, predicted, label</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot; A helper function to calculate f1-score for the given `label` &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># F1 = 2 * (precision * recall) / (precision + recall)</span></span><br><span class="line">    tp = np.<span class="built_in">sum</span>((actual==label) &amp; (predicted==label))</span><br><span class="line">    fp = np.<span class="built_in">sum</span>((actual!=label) &amp; (predicted==label))</span><br><span class="line">    fn = np.<span class="built_in">sum</span>((predicted!=label) &amp; (actual==label))</span><br><span class="line">    </span><br><span class="line">    precision = tp/(tp+fp)</span><br><span class="line">    recall = tp/(tp+fn)</span><br><span class="line">    f1 = <span class="number">2</span> * (precision * recall) / (precision + recall)</span><br><span class="line">    <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1_macro</span>(<span class="params">actual, predicted</span>):</span></span><br><span class="line">    <span class="comment"># `macro` f1- unweighted mean of f1 per label</span></span><br><span class="line">    <span class="keyword">return</span> np.mean([f1(actual, predicted, label) </span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> np.unique(actual)])</span><br><span class="line"></span><br><span class="line">actual = np.array([<span class="string">&quot;A&quot;</span>,<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>,<span class="string">&quot;C&quot;</span>])</span><br><span class="line">predicted = np.array([<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>,<span class="string">&quot;C&quot;</span>])</span><br><span class="line">label=np.array([<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>,<span class="string">&quot;A&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(f1(actual, predicted, label))</span><br><span class="line"><span class="built_in">print</span>(f1_macro(actual, predicted))</span><br><span class="line">======================================================</span><br><span class="line"><span class="number">0.8571428571428571</span></span><br><span class="line"><span class="number">0.7777777777777777</span></span><br></pre></td></tr></table></figure><p>Keras实现 <a href="https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras">Keras Macro F1-Score Implementation</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    y_pred = K.<span class="built_in">round</span>(y_pred)</span><br><span class="line">    tp = K.<span class="built_in">sum</span>(K.cast(y_true*y_pred, <span class="string">&#x27;float&#x27;</span>), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># tn = K.sum(K.cast((1-y_true)*(1-y_pred), &#x27;float&#x27;), axis=0)</span></span><br><span class="line">    fp = K.<span class="built_in">sum</span>(K.cast((<span class="number">1</span>-y_true)*y_pred, <span class="string">&#x27;float&#x27;</span>), axis=<span class="number">0</span>)</span><br><span class="line">    fn = K.<span class="built_in">sum</span>(K.cast(y_true*(<span class="number">1</span>-y_pred), <span class="string">&#x27;float&#x27;</span>), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    p = tp / (tp + fp + K.epsilon())</span><br><span class="line">    r = tp / (tp + fn + K.epsilon())</span><br><span class="line"></span><br><span class="line">    f1 = <span class="number">2</span>*p*r / (p+r+K.epsilon())</span><br><span class="line">    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)</span><br><span class="line">    <span class="keyword">return</span> K.mean(f1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Samples</span></span><br><span class="line">y_true = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">y_pred = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Shape y_true:&#x27;</span>, y_true.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Shape y_pred:&#x27;</span>, y_pred.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;sklearn Macro-F1-Score:&#x27;</span>, f1_score(y_true, y_pred, average=<span class="string">&#x27;macro&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Custom Macro-F1-Score:&#x27;</span>, K.<span class="built_in">eval</span>(f1(y_true, y_pred)))</span><br><span class="line">==========================================================</span><br><span class="line">Shape y_true: (<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">Shape y_pred: (<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">sklearn Macro-F1-Score: <span class="number">0.5999999999999999</span></span><br><span class="line">Custom Macro-F1-Score: <span class="number">0.5999999</span></span><br></pre></td></tr></table></figure><h4 id="2-ROC-和-AUC"><a href="#2-ROC-和-AUC" class="headerlink" title="2.ROC 和 AUC"></a>2.ROC 和 AUC</h4><p><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Receiver operating characteristic</a> wiki有全面的介绍。 先说说ROC曲线, “受试者工作特征——ReceiverOperating Characteristic”。与P_R曲线使用P, R为纵横坐标不同，ROC使用的是：(以FPR为x轴，TPR为y轴画图，就得到了ROC曲线)</p><ul><li><p>纵轴：<code>TPR  True Positive Rate</code>简称TPR</p></li><li><p>横轴:  <code>FPR False Positive Rate</code>简称FPR</p></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210825225424.png" alt="image-20210825224621965" style="zoom:60%;" /></p><p>  计算公式为：</p><script type="math/tex; mode=display">\begin{aligned}&\text{ TPR}=  \frac{TP}{TP + FN} \\&\text{ FPR}=  \frac{FP}{TN + FP} \\ \end{aligned}</script><p>对照上图，</p><ul><li><strong>TPR就是针对1这列计算，TP除以1这一列的和</strong></li><li><strong>FPR就是针对0这列计算，FP除以0这一列的和</strong></li></ul><p>将<strong>同一模型每个阈值</strong> 的 (FPR, TPR) 座标都画在ROC空间里，就成为<strong>特定模型的ROC曲线</strong></p><blockquote><p>ROC曲线下方的面积（英语：Area under the Curve of ROC (AUC ROC)），其意义是：</p><ul><li>因为是在1x1的方格里求面积，AUC必在0~1之间。</li><li>假设阈值以上是阳性，以下是阴性；</li><li>若随机抽取一个阳性样本和一个阴性样本，分类器<strong>正确判断</strong>阳性样本的值高于阴性样本之<strong>几率</strong> =AUC=AUC<a href="https://zh.wikipedia.org/wiki/ROC曲线#cite_note-1">[1]</a>。</li><li>简单说：<strong>AUC值越大的分类器，正确率越高。</strong></li></ul></blockquote><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://www.guoyaohua.com/classification-metrics.html#f1-score">一文读懂分类算法常用评价指标</a></p><p>[2] <a href="https://www.zhihu.com/question/30643044">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></p><p>[3] <a href="http://www.mashangxue123.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1969668933.html">分类模型的评估方法-F分数(F-Score)</a></p><p>[4] <a href="https://wiki.shileizcc.com/confluence/pages/viewpage.action?pageId=42533247">评估分类模型</a></p><p>[5] <a href="https://blog.csdn.net/zjn295771349/article/details/84961596?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;dist_request_id=1332042.11310.16192443080075203&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control">F1分数(F1 Score)详解及tensorflow、numpy实现</a></p><p>[6] <a href="https://stackoverflow.com/questions/64860091/computing-macro-average-f1-score-using-numpy-pythonwithout-using-scikit-learn">computing-macro-average-f1-scor</a></p><p>[7] <a href="https://zhuanlan.zhihu.com/p/360765777">AUC的面试</a></p><p>[8] <a href="https://www.zhihu.com/question/39840928">如何理解机器学习和统计中的AUC？</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Precision </tag>
            
            <tag> Recall </tag>
            
            <tag> F1 </tag>
            
            <tag> ROC </tag>
            
            <tag> AUC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 11. 高斯混合模型</title>
      <link href="2020/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%2011.%20%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%2011.%20%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-11-高斯混合模型"><a href="#机器学习-白板推导-11-高斯混合模型" class="headerlink" title="机器学习-白板推导 11. 高斯混合模型"></a>机器学习-白板推导 11. 高斯混合模型</h3><h4 id="1-GMM-介绍"><a href="#1-GMM-介绍" class="headerlink" title="1. GMM 介绍"></a>1. GMM 介绍</h4><blockquote><p>通过将更<strong>基本的概率分布</strong>(如高斯分布)进行<strong>线性组合的叠加</strong>方法，可以被形式化为<strong>概率模型</strong>，被称为混合模型(mixture distributions)。</p><p>​                                                                                                            ——PRML p82</p></blockquote><p>而<strong>GMM(Gaussian Mxture Model)</strong>是将多个高斯分布线性叠加在一起的概率模型。</p><p>如下图所示一维高斯混合分布例子。(PRML p82)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857769.png" alt="image-20210616144725503" style="zoom:25%;" /></p><p>从两个角度来看GMM：</p><ol><li><strong>从几何角度</strong>：加权平均。</li></ol><p>每个高斯分布可表示为<script type="math/tex">N(\mu_k, \Sigma_k)</script>，根据定义加权平均即：</p><script type="math/tex; mode=display">P(x) = \sum_{k=1}^K \alpha_k N(\mu_k, \Sigma_k) , \quad 且 \sum_{k=1}^K \alpha_k=1 \tag{1}</script><ol><li><strong>从混合模型(或者生成模型)角度</strong></li></ol><p>我们假定数据为<script type="math/tex">x: \text{observerd variable}</script>, 隐变量为<script type="math/tex">z: \text{latent variable}</script>，其中<script type="math/tex">z</script>是属于哪个高斯分布的隐变量(离散随机变量)。</p><div class="table-container"><table><thead><tr><th>z</th><th><script type="math/tex">c_1</script></th><th><script type="math/tex">c_2</script></th><th>…</th><th><script type="math/tex">c_K</script></th></tr></thead><tbody><tr><td>p(z)</td><td><script type="math/tex">p_1</script></td><td><script type="math/tex">p_2</script></td><td>…</td><td><script type="math/tex">p_k</script></td></tr></tbody></table></div><p>这里<script type="math/tex">p_k</script>相当于几何角度的加权权重。而根据上述定义画出高斯混合模型的概率图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857770.png" alt="image-20210616154604263" style="zoom:25%;" /></p><p>其中<script type="math/tex">N</script>代表样本个数。实际上，如下图所示(<a href="https://github.com/roboticcam/machine-learning-notes/blob/master/files/em.pdf">徐亦达em讲义</a> )，对于每个样本点，有可能来自于<script type="math/tex">c_1, c_2, c_3</script>这3个类别的高斯分布，只是概率大小不同而已，如上表所示对应不同的概率。这就是引入隐变量z的意义，它代表着我们无法确定的量，但是决定了样本的来自与来哪些高斯分布，我们计算时按照概率线性叠加来确定模型。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857771.png" alt="image-20210616154719144" style="zoom:50%;" /></p><h4 id="2-MLE来求解GMM"><a href="#2-MLE来求解GMM" class="headerlink" title="2. MLE来求解GMM"></a>2. MLE来求解GMM</h4><p>由上小节中，我们知道GMM的pdf由观测数据和隐变量来决定：</p><script type="math/tex; mode=display">\begin{align}P(x) &= \sum_zP(x, z) \\&= \sum_{k=1}^K P(x, z=c_k)\\&= \sum_{k=1}^K P(z=c_k)P(x| z=c_k)\\&= \sum_{k=1}^K p_k \cdot N(x|\mu_k, \Sigma_k) \\\end{align}\tag{2}</script><p>其中，<script type="math/tex">\sum_{k=1}^Kp_k=1</script>现在，整个模型中数据和参数可表示为：</p><script type="math/tex; mode=display">\begin{align}&X：  \text{observerd variable}, \quad X= (x_1, x_2, \cdots, x_N)\\&(X, Z)：  \text{complete variable}\\&\Theta: \text{parameter}, \quad \Theta=(p_1, p_2, \cdots,p_k, u_1, \cdots, u_k, \Sigma_1, \cdots, \Sigma_k)\\\end{align}\tag{3}</script><p>那么根据式2用MLE有：</p><script type="math/tex; mode=display">\begin{align}\hat \Theta_{MLE} &=  \text{argmax}_\theta \log P(X)\\& = \text{argmax}_\theta \log \prod_{i=1}^NP(x_i) \\& = \text{argmax}_\theta\sum_{i=1}^N \log \sum_{i=1}^K p_k \cdot N(x_i|\mu_k, \Sigma_k) \\\end{align}\tag{4}</script><p>对于式4，<script type="math/tex">\log</script>里面是连加，非常难以计算其解析解。一般使用EM算法来计算。</p><h4 id="3-EM求解GMM-E步"><a href="#3-EM求解GMM-E步" class="headerlink" title="3. EM求解GMM(E步)"></a>3. EM求解GMM(E步)</h4><p>直接用<a href="https://aigonna.com/2020/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%2010.%20EM%20%E7%AE%97%E6%B3%95/">EM 算法</a>公式7结论，即Q函数：</p><script type="math/tex; mode=display">\begin{align}\theta^{(t+1)} &=\text{argmax}_\theta \int_Z \log P(X, Z|\theta) P(Z|X, \theta^{(t)}  dZ\\&= \text{argmax}_\theta E_{P(Z|X, \theta^{(t)})}[\log P(X, Z|\theta)]\end{align} \tag{5}</script><p>我们发现要用到后验概率<script type="math/tex">p(z|x)</script>和联合概率。</p><script type="math/tex; mode=display">\begin{align}p(x, z) &= p(z)p(x|z) =p_z \cdot \mathcal{N}(x|\mu_z, \Sigma_z)\\p(z|x)&= \frac{p(x,z)}{p(x)} = \frac{p_z \cdot \mathcal{N}(x|\mu_z, \Sigma_z)}{\sum_{k=1}^Kp_k \cdot \mathcal{N}(x|\mu_k, \Sigma_k)}\end{align} \tag{6}</script><p>根据式5有：</p><script type="math/tex; mode=display">\begin{align}Q(\theta, \theta^{(t)}) &= \sum_Z \log p(X, Z|\theta) \cdot p(Z|X, \theta^{(t)})\\&=\sum_{z_1, z_2, \cdots, z_N} \log \prod_{i=1}^N p(x_i, z_i|\theta) \cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\&=\sum_{z_1, z_2, \cdots, z_N} \sum_{i=1}^N\log  p(x_i, z_i|\theta) \cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\&=\sum_{z_1, z_2, \cdots, z_N} \left ( \log  p(x_1, z_1|\theta) + \log  p(x_2, z_2|\theta) + \cdots + \log  p(x_1, z_1|\theta) \log  p(x_i, z_i|\theta)  \right )\cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\\end{align} \tag{7}</script><p>对于式7中，我们提出<script type="math/tex">\log  p(x_1, z_1|\theta)</script>项来化简：</p><script type="math/tex; mode=display">\begin{align}&\sum_{z_1, z_2, \cdots, z_N} \log  p(x_1, z_1|\theta) \cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\&= \sum_{z_1, z_2, \cdots, z_N} \log  p(x_1, z_1|\theta) \cdot  p(z_1|x_1,\theta^{(t)}) \prod_{i=2}^N p(z_i|x_i,\theta^{(t)}) \\&= \sum_{z_1} \log  p(x_1, z_1|\theta) \cdot  p(z_1|x_1,\theta^{(t)}) \sum_{z_2, \cdots, z_N}\prod_{i=2}^N p(z_i|x_i,\theta^{(t)}) \\&= \sum_{z_1} \log  p(x_1, z_1|\theta) \cdot  p(z_1|x_1,\theta^{(t)}) \sum_{z_2} p(z_2|x_2,\theta^{(t)})\sum_{z_3} p(z_3|x_3,\theta^{(t)})\cdots\sum_{z_N} p(z_N|x_N,\theta^{(t)})  \\&= \sum_{z_1} \log  p(x_1, z_1|\theta) \cdot  p(z_1|x_1,\theta^{(t)}) \cdot 1\cdot 1\cdots1 \\&= \sum_{z_1} \log  p(x_1, z_1|\theta) \cdot  p(z_1|x_1,\theta^{(t)}) \end{align} \tag{8}</script><p>同理，我们可以得到<script type="math/tex">\log  p(x_N, z_N|\theta)</script>简化表示，</p><script type="math/tex; mode=display">\begin{align}&\sum_{z_1, z_2, \cdots, z_N} \log  p(x_i, z_i|\theta) \cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\&= \sum_{z_i} \log  p(x_i, z_i|\theta) \cdot  p(z_i|x_i,\theta^{(t)}) \end{align} \tag{9}</script><p>综上，式7化简并用式6代入：</p><script type="math/tex; mode=display">\begin{align}Q(\theta, \theta^{(t)}) &=\sum_{z_1, z_2, \cdots, z_N} \left ( \log  p(x_1, z_1|\theta) + \log  p(x_2, z_2|\theta) + \cdots + \log  p(x_1, z_1|\theta) \log  p(x_i, z_i|\theta)  \right )\cdot  \prod_{i=1}^N p(z_i|x_i,\theta^{(t)}) \\&=\sum_{i=1}^{N} \sum_{z_i} \log  p(x_i, z_i|\theta) \cdot  p(z_i|x_i,\theta^{(t)}) \\&=\sum_{i=1}^{N} \sum_{z_i} \log \left[p_{z_i} \cdot \mathcal{N}(x_i|\mu_{z_i}, \Sigma_{z_i})\right]\cdot \frac{p_{z_i}^{(t)}  \cdot   \mathcal{N}(x_{z_i}|\mu_{z_i}^{(t)} , \Sigma_{z_i}^{(t)} )}{\sum_{k=1}^Kp_k^{(t)} \cdot \mathcal{N}(x_i|\mu_k^{(t)} , \Sigma_k^{(t)} )}\\\end{align} \tag{10}</script><p>这里，由于<script type="math/tex">\theta^{(t)}</script>是计算得到，跟<script type="math/tex">\theta</script>无关，我们目的是求<script type="math/tex">\theta</script>,不妨简写下。</p><script type="math/tex; mode=display">\frac{p_{z_i}^{(t)}  \cdot   \mathcal{N}(x_{z_i}|\mu_{z_i}^{(t)} , \Sigma_{z_i}^{(t)} )}{\sum_{k=1}^Kp_k^{(t)} \cdot \mathcal{N}(x_i|\mu_k^{(t)} , \Sigma_k^{(t)} )} = p(z_i|x_i, \theta^{(t)}) \tag{11}</script><p>根据式10， 11继续简化：</p><script type="math/tex; mode=display">\begin{align}Q(\theta, \theta^{(t)}) &=\sum_{i=1}^{N} \sum_{z_i} \log \left[p_{z_i} \cdot \mathcal{N}(x_i|\mu_{z_i}, \Sigma_{z_i})\right]\cdotp(z_i|x_i, \theta^{(t)})  \\&= \sum_{z_i} \sum_{i=1}^{N}  \log \left[p_{z_i} \cdot \mathcal{N}(x_i|\mu_{z_i}, \Sigma_{z_i})\right]\cdotp(z_i|x_i, \theta^{(t)})  \\&= \sum_{k=1}^K \sum_{i=1}^{N}  \log \left[p_{k} \cdot \mathcal{N}(x_i|\mu_{k}, \Sigma_{k})\right]\cdot p(z_i=c_K|x_i, \theta^{(t)})  \\&= \sum_{k=1}^K \sum_{i=1}^{N}  \log \left[p_{k} + \log\mathcal{N}(x_i|\mu_{k}, \Sigma_{k})\right]\cdot p(z_i=c_K|x_i, \theta^{(t)})  \\\end{align} \tag{12}</script><h4 id="4-EM求解GMM-M步"><a href="#4-EM求解GMM-M步" class="headerlink" title="4. EM求解GMM(M步)"></a>4. EM求解GMM(M步)</h4><p>EM算法中M步的迭代公式为：</p><script type="math/tex; mode=display">Q^{(t+1)} = \text{argmax}_\theta \ Q(\theta, \theta^{(t)}) \tag{13}</script><p>我们只介绍怎么求参数中的<script type="math/tex">p_k</script>,其余参数类似求解，根据式12有：</p><script type="math/tex; mode=display">\begin{align}p_{k}^{(t+1) }&=  \text{argmax}_{p_k} \sum_{k=1}^K \sum_{i=1}^{N}  \log \left[p_{k} + \log\mathcal{N}(x_i|\mu_{k}, \Sigma_{k})\right]\cdot p(z_i=c_K|x_i, \theta^{(t)})\\&=\text{argmax}_{p_k} \sum_{k=1}^K \sum_{i=1}^{N}  \log p_{k} \cdot p(z_i=c_K|x_i, \theta^{(t)})\\ \quad \quad \quad &s.t. \ \sum_{k=1}^Kp_k=1 \end{align}\tag{14}</script><p>这里将<script type="math/tex">p_k</script>无关项忽略掉了，再利用拉格朗日乘子法有(转换为求极小值，加个负号)：</p><script type="math/tex; mode=display">L(p_k, \lambda)= - \sum_{k=1}^K \sum_{i=1}^{N}  \log p_{k} \cdot p(z_i=c_K|x_i, \theta^{(t)}) + \lambda(\sum_{k=1}^Kp_k-1 )\tag{15}</script><p>式15对<script type="math/tex">p_k</script>求偏导有:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial p_k}&= - \sum_{i=1}^{N} \frac{1}{p_k}\cdot p(z_i=c_K|x_i, \theta^{(t)}) + \lambda =0\\&\Longrightarrow p_{k}\lambda - \sum_{i=1}^{N}  p(z_i=c_K|x_i, \theta^{(t)})=0\end{align}\tag{16}</script><p>因为对于所有的<script type="math/tex">p_k</script>都要满足，那么有：</p><script type="math/tex; mode=display">\sum_{k=1}^Kp_{k}\lambda - \sum_{k=1}^K\sum_{i=1}^{N}  p(z_i=c_K|x_i, \theta^{(t)})=0 \tag{17}</script><p>而<script type="math/tex">\sum_{k=1}^Kp(z_i=c_K|x_i, \theta^{(t)})=1</script>,因此式17有：</p><script type="math/tex; mode=display">\lambda - N=0 \tag{18}</script><p>再代入是式16中有：</p><script type="math/tex; mode=display">p_k^{(t+1)} = \frac{1}{N}\sum_{i=1}^{N}  p(z_i=c_K|x_i, \theta^{(t)}) \tag{19}</script><p>这就是EM算法求解GMM模型参数的全部过程。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM算法 </tag>
            
            <tag> GMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 10. EM 算法</title>
      <link href="2020/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%2010.%20EM%20%E7%AE%97%E6%B3%95/"/>
      <url>2020/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%2010.%20EM%20%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-10-EM-算法"><a href="#机器学习-白板推导-10-EM-算法" class="headerlink" title="机器学习-白板推导 10. EM 算法"></a>机器学习-白板推导 10. EM 算法</h3><h4 id="1-EM算法导出"><a href="#1-EM算法导出" class="headerlink" title="1. EM算法导出"></a>1. EM算法导出</h4><p>本文中所有的参数定义如下：</p><script type="math/tex; mode=display">\begin{align}&X: \text{  observed data  }\\&Z: \text{  unobserved data  }\\&(X, Z): \text{  complete data  }\\&\theta: \text{ parameter  }\\\end{align}</script><p>EM是<strong>Expectation Maximization</strong>简写，意为期望最大, 用于含有隐变量的概率模型参数的极大似然估计。我们经常说的极大似然估计是对于概率分布<script type="math/tex">P(X|\theta)</script>,求解使得其最大的算法：</p><script type="math/tex; mode=display">\theta_{MLE} = \text{argmax}_{\theta} \log P(X|\theta) \tag{1}</script><p>对于式1如果引入隐变量<script type="math/tex">Z</script>，要满足<script type="math/tex">P(X) = \int_Z P(X|Z)P(Z)</script>.如果是离散量就改成求和。那么可改写成：</p><script type="math/tex; mode=display">L(\theta) = \log P(X|\theta) = \log \int_Z P(X, Z|\theta) =\log \left(\int_Z P(X|Z, \theta)P(Z|\theta) \right) \tag{2}</script><p>实际上式2是非常难计算，因为有未观测数据并含有积分的对数。 </p><p>而对<script type="math/tex">\log P(X|\theta)</script>利用条件概率有：</p><script type="math/tex; mode=display">\begin{align}\log P(X|\theta) &= \log \frac{P(X, Z|\theta)}{P(Z|X, \theta)}\\&= \log P(X, Z|\theta)- \log P(Z|X, \theta)\\&= \left ( \log P(X, Z|\theta) -\log Q(Z)\right )- \left(\log P(Z|X, \theta)-\log Q(Z)\right)\\&= \log\frac{ P(X, Z|\theta)}{ Q(Z)} - \log \frac{P(Z|X, \theta)}{Q(Z)}\end{align}\tag{3}</script><p>其中，引入的<script type="math/tex">Q(Z) \neq 0</script>是关于<script type="math/tex">Z</script>的分布。</p><p>现在对式3左右两边同时求关于<script type="math/tex">Q(Z)</script>的期望：</p><script type="math/tex; mode=display">\begin{align}\text{左边} &= \int_Z Q(Z) \cdot \log P(X|\theta) dZ\\&=  \log P(X|\theta) \int_Z Q(Z) dZ\\&=\log P(X|\theta)\end{align} \tag{4}</script><p>右边变成：</p><script type="math/tex; mode=display">\begin{align}\text{右边} &=\color{red}{\underbrace{\int_Z Q(Z) \cdot \log\frac{ P(X, Z|\theta)}{ Q(Z)}dZ}_{ELBO}} \color{green}{\underbrace{-\int_Z Q(Z)  \cdot \log \frac{ P(Z|X, \theta)}{ Q(Z)}dZ}_{KL}}\\\end{align} \tag{5}</script><p>其中，前半部分是ELBO，evidence lower bound。后半部分是<script type="math/tex">KL(Q(Z)||P(Z|X, \theta))</script>。</p><blockquote><p>KL散度是用来衡量两个分布相似度(距离)：</p><script type="math/tex; mode=display">\begin{aligned}\text{KL} (p||q) &=  E_q[ -\text{log}\frac{p(x)}{q(x)}] \ge -\text{log} E_q[\frac{p(x)}{q(x)}]\\&=-\text{log} [\int q(x) \frac{p(x)}{q(x)}dx]\\&= -\text{log}\int_{x} p(x)dx = -\text{log} 1= 0\end{aligned}</script></blockquote><p>因此，<script type="math/tex">KL(Q(Z)||P(Z|X, \theta)) \ge 0, \quad 当且仅当 Q(Z) = P(Z|X, \theta)时取等号</script>。根据式3， 4， 5可知，当KL部分取等号时，<script type="math/tex">\log P(X|\theta) = \text{ELBO}</script>，也就是说ELBO是其下界。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855016.png" alt="image-20210615155230999" style="zoom:25%;" /></p><p>如上图所示，<script type="math/tex">L(\theta)</script>和ELBO都是关于<script type="math/tex">\theta</script>的函数，且从定义上看，<script type="math/tex">L(\theta) \ge \text{ELBO}</script>,也就是<script type="math/tex">L(\theta)</script>图像总在ELBO上面。我们用迭代法来寻找最优解，当前仅当<script type="math/tex">Q(Z) = P(Z|X, \theta^t)</script>时，能使得<script type="math/tex">L(\theta) = \text{ELBO}</script>，并且因为迭代法要求<script type="math/tex">\theta^{t+1}</script>使得<script type="math/tex">L(\theta^{t+1}) \ge L(\theta^{t})</script>。实际上，先赋予初值<script type="math/tex">\theta^0</script>,求出ELBO的期望，然后求下一个<script type="math/tex">\theta^1</script>,直到ELBO值大于<script type="math/tex">\theta^0</script>对应的，用<script type="math/tex">\theta^1</script>替换掉<script type="math/tex">\theta^0</script>，直到找到<script type="math/tex">ELBO</script>最大的参数，这时也是<script type="math/tex">L(\theta)</script>最大的参数。其公式就是：</p><script type="math/tex; mode=display">\begin{align}\theta^{t+1} &= \text{argmax}_\theta \text{ ELBO}\\&= \text{argmax}_\theta \int_Z Q(Z) \cdot \log \frac{P(X, Z|\theta)}{ Q(Z)}dZ\\&= \text{argmax}_\theta \int_Z P(Z|X, \theta^t) \cdot \log \frac{ P(X, Z|\theta)}{ P(Z|X, \theta^t)}dZ\\&=\text{argmax}_\theta \int_Z P(Z|X, \theta^t) \cdot \log P(X, Z|\theta) dZ -\underbrace{\text{argmax}_\theta \int_Z P(Z|X, \theta^t) \cdot \log P(Z|X, \theta^t)}_{与\theta无关} dZ\\&= \text{argmax}_\theta \int_Z P(Z|X, \theta^t) \cdot \log P(X, Z|\theta) dZ\\&= \text{argmax}_\theta E_{P(Z|X, \theta^t)}[\log P(X, Z|\theta)]\end{align}\tag{6}</script><p>总结：EM算法适用于含有隐变量的极大似然估计，由于含隐变量的的积分和对数难于计算，转化为求ELBO的最大值，<strong>EM算法公式</strong>如下：</p><script type="math/tex; mode=display">\begin{align}\theta^{t+1} &=\text{argmax}_\theta \int_Z \log P(X, Z|\theta) P(Z|X, \theta^t)  dZ\\&= \text{argmax}_\theta E_{P(Z|X, \theta^t)}[\log P(X, Z|\theta)]\end{align} \tag{7}</script><p>最重要的两步：</p><ol><li>E step： 计算<script type="math/tex">\log P(X,Z|\theta)</script>在概率分布<script type="math/tex">P(Z|X, \theta^t)</script>下的期望。</li><li>M step:  计算使得这个期望最大化的参数<script type="math/tex">\theta</script>。</li></ol><h4 id="2-EM-公式导出另外一种方法-Jensen-不等式"><a href="#2-EM-公式导出另外一种方法-Jensen-不等式" class="headerlink" title="2. EM 公式导出另外一种方法 (Jensen 不等式)"></a>2. EM 公式导出另外一种方法 (Jensen 不等式)</h4><p>对公式2引入隐变量分布<script type="math/tex">Q(Z)</script>:</p><script type="math/tex; mode=display">\begin{align}\log P(X|\theta) &= \log\int_Z P(X, Z|\theta)dZ\\&= \log\int_Z \frac{P(X, Z|\theta)}{Q(Z)}Q(Z)dZ\\&= \log E_{Q(Z)} [\frac{P(X, Z|\theta)}{Q(Z)}]\\&\ge  \underbrace{E_{Q(Z)} [\log\frac{P(X, Z|\theta)}{Q(Z)}]}_{ELBO}\\\end{align}\tag{8}</script><p>其中，最后一步是利用Jensen不等式，对于凹函数<script type="math/tex">f(x), 有 f(E[x]) \ge E[f(x)])</script>。其实最后结果就是ELBO。当且仅当<script type="math/tex">\frac{P(X, Z|\theta)}{Q(Z)}= C(常数)</script>时，可取等号。</p><script type="math/tex; mode=display">\begin{align}&\frac{P(X, Z|\theta)}{Q(Z)} = C \\&\Longrightarrow Q(Z) = \frac{1}{C} P(X,Z|\theta)\\&\Longrightarrow \int_ZQ(Z) dZ= \int_Z\frac{1}{C} P(X,Z|\theta)dZ\\&\Longrightarrow 1 = \frac{1}{C}P(X|\theta)\\&\Longrightarrow P(X|\theta) = C\\&\Longrightarrow Q(Z) = \frac{P(X, Z|\theta)}{P(X|\theta)}=P(Z|X, \theta)\end{align}\tag{9}</script><p>这表明引入<script type="math/tex">Q(Z)</script>要等于后验分布<script type="math/tex">P(Z|X, \theta)</script>。而这时，优化目标<script type="math/tex">\log P(X|\theta = \text{ELBO}</script>，其它迭代优化过程跟第一小节一样。</p><h4 id="3-EM算法的收敛性"><a href="#3-EM算法的收敛性" class="headerlink" title="3. EM算法的收敛性"></a>3. EM算法的收敛性</h4><p>使用迭代算法来求<script type="math/tex">L(\theta)</script>的最大值，即我们希望新的估计值<script type="math/tex">\theta</script>能使得<script type="math/tex">L(\theta)</script>变大，就是<script type="math/tex">L(\theta^{t+1}) \ge L(\theta^t)</script>,其中<script type="math/tex">L(\theta^t)</script>是t次迭代后的值，这跟第一小节推导非常类似。</p><p>而要证明<script type="math/tex">P(X|\theta^{t+1}) \ge P(X|\theta^{t})</script>，我们先用条件概率公式转换下：</p><script type="math/tex; mode=display">L(\theta) = \log P(X|\theta) = \log \frac{P(X, Z|\theta)}{P(Z|X, \theta)} = \log P(X, Z|\theta)- \log P(Z|X, \theta)\tag{10}</script><p>我们对式10两边同时求关于<script type="math/tex">P(Z|X, \theta^t)</script>的期望：</p><script type="math/tex; mode=display">\begin{align}\text{左边} &= \int_Z P(Z|X, \theta^t) \cdot \log P(X|\theta) dZ\\&=  \log P(X|\theta) \int_Z P(Z|X, \theta^t) dZ\\&=\log P(X|\theta)\end{align} \tag{11}</script><p>其中，<script type="math/tex">\int_Z P(Z|X, \theta^t) dZ = 1</script>。而右边有:</p><script type="math/tex; mode=display">\begin{align}\text{右边} &= \int_Z P(Z|X, \theta^t) \cdot\log P(X, Z|\theta)dZ- \int_Z P(Z|X, \theta^t) \cdot \log P(Z|X, \theta) dZ\\\end{align} \tag{12}</script><p>我们将前半部分记为<script type="math/tex">Q(\theta, \theta^t) = \int_Z P(Z|X, \theta^t) \cdot\log P(X, Z|\theta)dZ</script>在EM算法中称为<strong>Q function</strong>, 其实就是上面ELBO，也就是EM算法迭代目标式。</p><p>后半部分记为<script type="math/tex">H(\theta, \theta^t) = \int_Z P(Z|X, \theta^t) \cdot \log P(Z|X, \theta) dZ</script>。那么根据式10、11和式12有：</p><script type="math/tex; mode=display">\begin{align}\log P(X|\theta) &= \int_Z P(Z|X, \theta^t) \cdot\log P(X, Z|\theta)dZ- \int_Z P(Z|X, \theta^t) \cdot \log P(Z|X, \theta) dZ\\&=Q(\theta, \theta^t) - H(\theta, \theta^t) \end{align}\tag{13}</script><p>对于<script type="math/tex">Q(\theta, \theta^t)</script>，由小节1可知<script type="math/tex">Q(\theta^{t+1}, \theta^{t}) \ge Q(\theta^t, \theta^{t})</script>，那么<script type="math/tex">H(\theta, \theta^t)</script>会怎么变化？</p><script type="math/tex; mode=display">\begin{align}&H(\theta^{t+1}, \theta^{t})  -  H(\theta^t, \theta^{t})  \\ &= \int_Z P(Z|X, \theta^{t}) \cdot \log P(Z|X, \theta^{t+1}) dZ- \int_Z P(Z|X, \theta^t) \cdot \log P(Z|X, \theta^{t}) dZ\\ &= \int_Z P(Z|X, \theta^t) \log\frac{ P(Z|X, \theta^{t+1})}{ P(Z|X, \theta^{t})} dZ \\ &\le \log \int_Z P(Z|X, \theta^t)\frac{ P(Z|X, \theta^{t+1})}{ P(Z|X, \theta^{t})} dZ\\ &= \log  \int_Z  P(Z|X, \theta^{t+1}) dZ\\ &= \log 1=0 \end{align} \tag{14}</script><p>其中，不等号变换使用Jensen不等式，如下式：</p><script type="math/tex; mode=display">\log \sum_{j} \lambda_{j} y_{j} \geq \sum_{j} \lambda_{j} \log y_{j}, \text { 其中 } \lambda_{j} \geq 0, \quad \sum_{j} \lambda_{j}=1 \tag{15}</script><p>如果用KL散度易知， </p><script type="math/tex; mode=display">\int_Z P(Z|X, \theta^t) \log\frac{ P(Z|X, \theta^{t+1})}{ P(Z|X, \theta^{t})} dZ\\= -\text{KL}(P(Z|X, \theta^t)||P(Z|X, \theta^{t+1})) \le 0 \tag{16}</script><p>那么根据式14可知，<script type="math/tex">H(\theta^{t+1}, \theta^{t}) \le H(\theta^t, \theta^{t})</script>，也就是说<script type="math/tex">H(\theta, \theta^t)</script>是递减的。综上根据式13可知，<script type="math/tex">L(\theta) = \log P(X|\theta)</script>是递增的，即<script type="math/tex">\log P(X|\theta^t) \le \log P(X|\theta^{t+1})</script>,因此EM算法是<strong>收敛</strong>。</p><h4 id="4-广义EM算法"><a href="#4-广义EM算法" class="headerlink" title="4. 广义EM算法"></a>4. 广义EM算法</h4><p>还是回到最开始的问题：最大化似然函数如式1所示。EM算法是为了解决参数估计中的问题，也就是learning问题：</p><script type="math/tex; mode=display">\hat \theta = \text{argmax}_\theta P(X|\theta) \tag{17}</script><p>实际过程中，由于难于计算，我们利用条件概率公式将其转化为式3所示。从第一小节，式3， 4， 5可得：</p><script type="math/tex; mode=display">L(\theta) = \log P(X|\theta)=\text{ELBO} + KL(Q||P) \tag{18}</script><p>这里我们定义ELBO为：</p><script type="math/tex; mode=display">L(Q, \theta) = \text{ELBO} = \int_Z Q(Z) \cdot \log\frac{ P(X, Z|\theta)}{ Q(Z)}dZ = E_{Q(Z)}[\log\frac{ P(X, Z|\theta)}{ Q(Z)}] \tag{19}</script><p>另外，再写出KL散度部分：</p><script type="math/tex; mode=display">KL(Q||P) = \int_Z Q(Z)  \cdot \log \frac{ Q(Z)}{ P(Z|X, \theta)}dZ = E_{Q(Z)}[ \log \frac{ Q(Z)}{ P(Z|X, \theta)}] \tag{20}</script><p>前面部分，我们直接假定<script type="math/tex">Q(Z) = P(Z|X, \theta)</script>，实际上这个后验<script type="math/tex">P(Z|X, \theta)</script>也是难以处理的。广义EM算法思路是：</p><ol><li><p>先把<script type="math/tex">\theta</script>固定住，若Q越接近P，KL部分越小，由于<script type="math/tex">\log P(X|\theta)</script>这时是定值，那么ELBO就会越大。即根据下式，求出<script type="math/tex">Q(Z)</script></p><script type="math/tex; mode=display">\hat Q(Z) = \text{argmin}_Q \ KL(Q||P) = \text{argmax}_Q \ L(Q, \theta) \tag{21}</script></li><li><p>固定<script type="math/tex">\hat Q(Z)</script>, 就是第一步求解的值，按照求解<script type="math/tex">\hat \theta = \text{argmax}_\theta \ L(\hat Q(Z), \theta)</script>.</p></li></ol><p>综上，广义EM可表示为:</p><ol><li><strong>E-step:</strong>  <script type="math/tex">Q^{t+1} = \text{argmax}_Q L(Q, \theta^t)</script></li><li><strong>M-step:</strong> <script type="math/tex">\theta^{t+1} = \text{argmax}_\theta L(Q^{t+1}, \theta)</script></li></ol><p>对ELBO进一步化简有：</p><script type="math/tex; mode=display">\begin{align}L(Q, \theta) &= E_{Q}[\log\frac{ P(X, Z|\theta)}{ Q(Z)}] \\&= E_Q[\log P(X, Z)] - E_Q[\log Q(Z)]\\&= E_Q[\log P(X, Z)] + H[Q(Z)]\end{align}\tag{22}</script><p>在前面的狭义EM算法中，只对<script type="math/tex">E_Q[\log P(X, Z)]</script>进行迭代优化，是因为我们假定<script type="math/tex">Q(Z)</script>是已知的，优化与<script type="math/tex">H[Q(Z)]</script>部分无关。</p><h5 id="5-EM的变种"><a href="#5-EM的变种" class="headerlink" title="5. EM的变种"></a>5. EM的变种</h5><p>上节介绍了广义EM算法，其中E步和M步都是求极大，也称为MM算法。这两步求解中，<strong>先固定一个参数，再优化另外一个参数；完成后，再固定优化后得到的参数，然后优化开始固定的参数</strong>，这叫做<strong>坐标上升法</strong>。SVM中求解算法SMO就是用该方法求解的。</p><p>实际过程中, <script type="math/tex">P(Z|X, \theta)</script>会采用变分推断(Variable Inference)或MCMC，结合EM算法就变成了VBEM(Variable Bayes)/VEM 和MCEM。</p><blockquote><p><strong>总结：</strong></p><ul><li>狭义EM算法：迭代求解算法目标式即极大似然函数<script type="math/tex">L(\theta)</script>的下界ELBO:</li></ul><script type="math/tex; mode=display">\theta^{t+1} =\text{argmax}_\theta \int_Z \log P(X, Z|\theta) P(Z|X, \theta^t)  dZ\\= \text{argmax}_\theta E_{P(Z|X, \theta^t)}[\log P(X, Z|\theta)]</script><p>其中，E步是计算<script type="math/tex">\log P(X,Z|\theta)</script>在概率分布<script type="math/tex">P(Z|X, \theta^t)</script>下的期望，M步是最大化该期望。</p><ul><li>广义EM算法：不假定<script type="math/tex">Q(Z)=P(Z|X, \theta)</script>, KL散度部分不可忽略，求解极为困难，我们使用坐标上升法求解E步和M步：<ul><li>E步，固定 <script type="math/tex">\theta</script>,最大化<script type="math/tex">L(Q, \theta)</script>,这时得到Q</li><li>M步，固定Q，再最大化<script type="math/tex">L(Q, \theta)</script>,得到<script type="math/tex">\theta</script>。</li></ul></li></ul></blockquote><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://www.bilibili.com/video/BV1Wp411R7am?p=3&amp;t=1">EM 算法</a></p><p>[2] <a href="https://www.zybuluo.com/galaxy-0/note/1517681">EM算法推导与三硬币模型</a></p><p>[3] <a href="https://kexue.fm/archives/5239/comment-page-1">从最大似然到EM算法：一致的理解方式</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/36331115">人人都懂EM算法</a></p><p>[5] <a href="https://www.zybuluo.com/zsh-o/note/1077331">EM</a></p><p>[6] <a href="https://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf">What is the expectation maximization algorithm?</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM算法 </tag>
            
            <tag> ELBO </tag>
            
            <tag> 坐标上升法 </tag>
            
            <tag> Jensen不等式 </tag>
            
            <tag> KL散度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导8 指数簇分布</title>
      <link href="2020/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC8%20%E6%8C%87%E6%95%B0%E7%B0%87%E5%88%86%E5%B8%83/"/>
      <url>2020/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC8%20%E6%8C%87%E6%95%B0%E7%B0%87%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导8-指数簇分布"><a href="#机器学习-白板推导8-指数簇分布" class="headerlink" title="机器学习-白板推导8 指数簇分布"></a>机器学习-白板推导8 指数簇分布</h3><h4 id="1-指数簇分布背景"><a href="#1-指数簇分布背景" class="headerlink" title="1. 指数簇分布背景"></a>1. 指数簇分布背景</h4><p>指数簇分布(Exponential Family Distribution)需要满足以下形式：</p><script type="math/tex; mode=display">p(x|\eta) =h(x)\exp(\eta^T\phi(x) - A(\eta)) \tag{1}</script><p>其中：</p><ul><li><p><script type="math/tex">\eta</script> : 自然参数(natural parameter，也叫<strong>正则参数 canonical parameter</strong>)</p></li><li><p><script type="math/tex">\phi(x)</script> : <strong>充分统计量（sufficient statistic）</strong>,常使用的有<script type="math/tex">\phi(x) = x</script>，一般是一个对样本的统计函数表示。</p></li><li><p>接下来会重点介绍的<script type="math/tex">A(\eta)</script> 是一个<strong>对数配分函数（log partition function）</strong>。</p><blockquote><p>​        如<script type="math/tex">P(x| \theta) = \frac{1}{z} \hat P(x|\theta)</script>这里的z就是归一化因子，又叫配分函数。</p><p>​        对两边同时取积分：</p><p>​        </p><script type="math/tex; mode=display">\int_{x} P(x \mid \theta)=\int_{x} \frac{1}{z} \hat{P}(x \mid \theta) \Rightarrow 1=\int_{x} \frac{1}{z} \hat{P}(x \mid \theta) \Rightarrow z=\int_{x} \hat{P}(x \mid \theta)</script><p>​        那么<script type="math/tex">A(\eta)</script>为什么叫log配分函数呢？</p><script type="math/tex; mode=display">\begin{align}P(x|\eta) &= h(x) \exp(\eta^T\phi(x)-A(\eta))\\&=h(x) \exp(\eta^T\phi(x))\exp(-A(\eta))\\ &=\frac{1}{\exp(A(\eta))}h(x) \exp(\eta^T\phi(x))\end{align}</script><p>又因为<script type="math/tex">P(x|\theta) = \frac{1}{z} \hat P(x|\theta)</script></p><p>所以<script type="math/tex">A(\eta) = \log z</script>因此<script type="math/tex">z</script>称为配分函数 ,<script type="math/tex">A(\eta)</script>称为对数配分函数。</p></blockquote><p> <script type="math/tex">\exp(−A(\eta))</script> 本质上扮演了归一化常数（normalization constant）的角色，也就是确保 $p(x| \eta)$ 的和或者积分等于$1$。</p></li></ul><p>如果给定<script type="math/tex">\phi(x)，h(x), A(\eta)</script>那么就定义了被参数$\eta$控制的一个分布簇(或者集)。改变$\eta$，我们能得到这簇中的不同分布。</p><p>指数簇分布有：</p><ul><li>Gaussian 分布</li><li>伯努利分布</li><li>二项分布</li><li>泊松分布</li><li>Beta分布</li><li>Dirichlet分布</li><li>Gamma分布</li></ul><p>指数簇分布有如下性质和应用：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857963.png" alt="image-20210602212723792" style="zoom:30%;" /></p><p><strong>性质</strong>：</p><ol><li><p>充分统计量</p><p>举例来说就是，对于一些从高斯分布中抽取出来的样本<script type="math/tex">x_1, x_2, \cdots, x_n</script>,设其充分统计量为,</p></li></ol><script type="math/tex; mode=display">\phi(x)=\left(\begin{array}{c}&\sum_{i=1}^{N} x_{i} &\\&\sum_{i=1}^{N} x_{i}^{2}&\end{array}\right)</script><p>​    那么我们就可以通过这个充分统计量来计算出均值和方差，从而明确其分布。这时就可以将样本丢掉，来节省空间，对online learning有重要作用。</p><ol><li><p>共轭：因为贝叶斯公式中分母如下：</p><p>​    </p><script type="math/tex; mode=display">P(z|x) = \frac{P(x|z)P(z)}{\int _zP(x|z)P(z)dz}</script><p>计算积分复杂或者得到的<script type="math/tex">P(z|x)</script>形式复杂太难计算，因此求<script type="math/tex">E_{P(z|x)}[f(z)]</script>也是很困难的，所以人们想了很多办法，如近似推断(变分推断，MCMC等)都是因为上述计算难。</p><p>共轭的概念是指给定一个特殊的似然函数<script type="math/tex">P(x|z)</script>的情况下，后验<script type="math/tex">P(z|x)</script>与先验<script type="math/tex">P(z)</script>会有一个形式相同的分布，这就解决了上述积分困难的问题，避免求分母的积分项，在<strong>后验概率正比于似然 × 先验概率</strong>概念下有如<script type="math/tex">\underbrace{P(z \mid x)}_{\text {Beta }} \propto \underbrace{P(x \mid z)}_{\text {二项式分布 }} \underbrace{P(z)}_{\text {Beta }}</script>。  </p></li><li><p>最大熵</p><blockquote><p>​    最大熵原理：</p><p>当给定一个限制条件的情况下，对于未知部分，我们假设它们等可能发生，但我们无法定量分析。而熵可以进行定量分析，我们去求解最大熵，熵越大则随机性越强。</p><p>无信息先验：</p><p>在贝叶斯估计中，我们往往需要给先验一个参数，有如下方法：</p><p>①共轭：为了计算方便</p><p>②利用最大熵思想：从最大熵的角度给予先验的参数（无信息先验）</p><p>③Jeffreys’s prior</p></blockquote></li></ol><h4 id="2-高斯分布指数簇形式"><a href="#2-高斯分布指数簇形式" class="headerlink" title="2. 高斯分布指数簇形式"></a>2. 高斯分布指数簇形式</h4><p>将高斯分布转换成式1指数簇这种形式：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857964.png" alt="MommyTalk1622785415491" style="zoom:18%;" /></p><p>由式2中我们可以得到<script type="math/tex">\phi(x), \eta</script>两个式子：</p><script type="math/tex; mode=display">\begin{align}&\phi(x) = \left(\begin{array}{l} x \\x^2\end{array}\right)\\&\eta=\left(\begin{array}{l}\eta_{1} \\\eta_{2}\end{array}\right)=\left(\begin{array}{c}\frac{\mu}{\sigma^{2}} \\-\frac{1}{2 \sigma^{2}}\end{array}\right)\end{align} \tag{3}</script><p>现在<script type="math/tex">A(\eta)</script>也要用<script type="math/tex">\eta</script>来表示，由式3可得：</p><script type="math/tex; mode=display">\sigma^2 = -\frac{1}{2\eta_2} \quad \quad \mu = -\frac{\eta_1}{2\eta_2} \tag{4}</script><p>式4代入到<script type="math/tex">A(\eta)</script>有：</p><script type="math/tex; mode=display">A(\eta) = -\frac{\eta_1^2}{4\eta_2} +\frac{1}{2}\log(-\frac{\pi}{\eta_2}) \tag{5}</script><p>现在就得到了高斯分布的指数簇形式：</p><script type="math/tex; mode=display">p(x|\eta) = \exp (\eta^T\phi(x)-A(\eta)) \tag{6}</script><p>更进一步来理解指数簇表示就是：<script type="math/tex">e</script>的指数形式的表示，其中前半部分<script type="math/tex">\exp(\eta^T\phi(x))</script>是一个参数和对于该分布的充分统计量，后半部分<script type="math/tex">\exp(-A(\eta))</script>就是再减去一个对数配分函数，并且在1小节中，我们知道如果将其转换到指数外面就是除以一个配分函数，实际上，后半部分本质上是一个归一化因子。</p><h4 id="3-对数配分函数与充分统计量的关系"><a href="#3-对数配分函数与充分统计量的关系" class="headerlink" title="3. 对数配分函数与充分统计量的关系"></a>3. 对数配分函数与充分统计量的关系</h4><p>按照式1中指数簇分布表示有：</p><script type="math/tex; mode=display">\begin{align}&p(x|\eta) = h(x) \exp (\eta^T\phi(x)-A(\eta))  = \frac{h(x) \exp (\eta^T\phi(x))}{\exp(A(\eta))}\\ &\Longrightarrow p(x|\eta)\exp(A(\eta)) = h(x) \exp (\eta^T\phi(x))\\  &\Longrightarrow \int p(x|\eta)\exp(A(\eta)) dx = \int h(x) \exp (\eta^T\phi(x))dx \\&\because \int p(x|\eta) dx = 1 \\&\Longrightarrow \exp(A(\eta)) =\int h(x) \exp (\eta^T\phi(x))dx\end{align} \tag{7}</script><p>对式7两边变量<script type="math/tex">\eta</script>求导得:</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial \exp(A(\eta))}{\partial \eta} = \frac{\int h(x) \exp (\eta^T\phi(x))dx}{\partial \eta}\\&\Longrightarrow \exp(A(\eta)) A^\prime(\eta) = \int h(x) \exp (\eta^T\phi(x))\phi(x)dx\\&\Longrightarrow A^\prime(\eta) = \frac{\int h(x) \exp (\eta^T\phi(x))\phi(x)dx}{\exp(A(\eta))}\\&\Longrightarrow A^\prime(\eta) = \int h(x) \exp (\eta^T\phi(x)-A(\eta))\phi(x)dx\\&\Longrightarrow  A^\prime(\eta) = \int p(x|\eta) \phi(x)dx\\&\Longrightarrow  A^\prime(\eta) =  E_{p(x|\eta)} [\phi(x)]\end{align} \tag{8}</script><p>那么就得到了对数配分函数<script type="math/tex">A(\eta)</script>和充分统计量的关系：</p><script type="math/tex; mode=display">A^\prime(\eta) =  E_{p(x|\eta)} [\phi(x)] = \int p(x|\eta) \phi(x)dx\tag{9}</script><p>继续对式9中的<script type="math/tex">\eta</script>求导得：</p><script type="math/tex; mode=display">\begin{align}A^{\prime \prime}(\eta)&=\frac{\partial\left(\int h(x) \exp \left\{\eta^{T} \phi(x)-A(\eta)\right\} \phi(x) \mathrm{d} x\right)}{\partial \eta} \\&=\int {\color{Red}\underbrace{h(x) \exp \left\{\eta^{T} \phi(x)-A(\eta)\right\}}_{P(x \mid \eta)}}\left(\phi(x)-A^{\prime}(\eta)\right) \phi(x) \mathrm{d} x \\&=\int P(x \mid \eta)\left(\phi(x)-E_{P(x \mid \eta)}[\phi(x)]\right) \phi(x) \mathrm{d} x \\&=\int P(x \mid \eta) \phi^{2}(x)-E_{P(x \mid \eta)}[\phi(x)] P(x \mid \eta) \phi(x) \mathrm{d} x \\&=\int P(x \mid \eta) \phi^{2}(x) \mathrm{d} x-E_{P(x \mid \eta)}[\phi(x)] \int P(x \mid \eta) \phi(x) \mathrm{d} x \\&=E_{P(x \mid \eta)}\left[\phi^{2}(x)\right]-E_{P(x \mid \eta)}^{2}[\phi(x)] \\&=\operatorname{Var}_{P(x \mid \eta)}[\phi(x)]\end{align} \tag{10}</script><p>由于方差不为负，所以<script type="math/tex">A(\eta)</script>是凸函数。</p><hr><p>使用第2小节高斯分布来验证：</p><script type="math/tex; mode=display">\begin{align}\sigma^2 = -\frac{1}{2\eta_2} \quad \quad \mu = -\frac{\eta_1}{2\eta_2} \\A(\eta) = -\frac{\eta_1^2}{4\eta_2} +\frac{1}{2}\log(-\frac{\pi}{\eta_2})\end{align}</script><p>直接可证明：</p><script type="math/tex; mode=display">E(\phi(x))= \left[\begin{array}{cc}E(x) \\E(x^2)\end{array}\right]= \left[\begin{array}{cc}\mu \\\mu^2+\sigma^2\end{array}\right]</script><p>因为<script type="math/tex">E(x^2) = E(x)^2+Var(x)= \mu^2+\sigma^2</script>,<a href="https://zhuanlan.zhihu.com/p/64859161">关于方差期望公式</a>。</p><p>而我们对<script type="math/tex">A(\eta)</script>中<script type="math/tex">\eta</script>求导可得:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial A(\eta)}{\partial \eta_1} &= -\frac{\eta_1}{2\eta_2} = \mu \\\frac{\partial A(\eta)}{\partial \eta_2}&= \frac{\eta_1^2}{4\eta_2^2} - \frac{1}{2\eta_2}  = \mu^2 + \sigma^2\end{align}</script><p>这就验证了式8和式10，我们可以看到对数配分函数的一阶导就是充分统计量量中的期望，二阶导就是方差。</p><hr><h4 id="4-极大似然估计与充分统计量"><a href="#4-极大似然估计与充分统计量" class="headerlink" title="4. 极大似然估计与充分统计量"></a>4. 极大似然估计与充分统计量</h4><p>若我们有数据集<script type="math/tex">D</script>如下：</p><script type="math/tex; mode=display">D = \{x_1, x_2, \cdots, x_n \}</script><p>那么我们可以用极大似然估计来估计:</p><script type="math/tex; mode=display">\begin{align}\eta_{MLE} &= \text{argmax}_\eta \ \log  P(D|\eta)\\&= \text{argmax}_\eta \ \log \prod_{i=1}^N p(x_i|\eta)\\&= \text{argmax}_\eta \ \sum_{i=1}^N \log p(x_i|\eta)\\&= \text{argmax}_\eta \ \sum_{i=1}^N \log \{h(x_i) \exp [\eta^T\phi(x_i)-A(\eta)] \}\\&= \text{argmax}_\eta \ \sum_{i=1}^N  \{\log h(x_i) + \eta^T\phi(x_i)-A(\eta) \} \quad \text{与}\eta\text{无关项可忽略} \\&= \text{argmax}_\eta \ \sum_{i=1}^N [\eta^T\phi(x_i)-A(\eta)]\end{align} \tag{11}</script><p>对式11最后一步求导得：</p><script type="math/tex; mode=display">\begin{align}&\sum_{i=1}^N [\phi(x_i) - A^{\prime}(\eta)] =0  \\\Longrightarrow  &A^{\prime}(\eta_{MLE}) = \frac{1}{N}\sum_{i=1}^N \phi(x_i) \end{align}\tag{12}</script><p>那么<script type="math/tex">\eta_{MLE}</script>就可以通过求反函数来实现。这也说明<script type="math/tex">\phi(x)</script>就是充分统计量，我们得到<script type="math/tex">\frac{1}{N}\sum_{i=1}^N \phi(x_i)</script> 就可以求出<script type="math/tex">\eta</script>,进一步就得到所有的需要统计参数。</p><h4 id="5-最大熵"><a href="#5-最大熵" class="headerlink" title="5. 最大熵"></a>5. 最大熵</h4><p>在信息论中，信息量就是概率的导数取对数，即<script type="math/tex">-\log p(x)</script>。熵就是对信息量的期望，因此，</p><script type="math/tex; mode=display">\begin{align}H[p(x)] &= E_{p(x)}[-\log p(x)] \\&= \int -p(x)\log p(x) dx \quad \quad 连续\\&= -\sum_{i=1}^{N}p(x_i) \log p(x_i) \quad \quad 离散\end{align} \tag{13}</script><p>我们先看看<strong>离散变量的最大熵</strong>：</p><p>对于离散变量要求其熵最大，即：</p><script type="math/tex; mode=display">\begin{align}\max H[p(x)] &= \max -\sum_{i=1}^{N}p_i \log p_i \\&s.t. \quad \sum_{i=1}^N p_i= 1\end{align} \tag{14}</script><p>用拉格朗日乘执法求解：</p><script type="math/tex; mode=display">\text{argmin } L(p_i, \lambda) = \sum_{i=1}^{N}p_i \log p_i + \lambda(1 - \sum_{i=1}^N p_i) \tag{15}</script><p>对<script type="math/tex">p_i</script>求导得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial p_i} = \log p_i + p_i \frac{1}{p_i} - \lambda = 0</script><p>那么<script type="math/tex">\hat p_i = \exp (\lambda -1)</script>. 其总和概率为1，因此离散概率分布是均匀分布且<script type="math/tex">\hat p_1  = \hat p_2 = \cdots = \hat p_k = \frac{1}{k}</script>.</p><p>从上述推导中看出，在离散随机变量中，均匀分布熵最大。即正在没有任何已知约束的情况下，均匀分布会使得熵最大。</p><p><strong>最大熵原理</strong>：</p><p>上面是没有任何已知信息情况下最大熵的推导，结果是要满足均匀分布。若我们已知数据集呢？比如数据集D：</p><script type="math/tex; mode=display">D = \{ x_1, x_2, \cdots, x_N\} \tag{16}</script><p>然后根据数据我们可以得到其经验分布：</p><script type="math/tex; mode=display">\hat p(x= n) = \hat p(n) = \frac{\text{count}(n)}{N} \tag{16}</script><p>接下来我们就可以得到其期望<script type="math/tex">E_{\hat p } [x]</script>, 方差<script type="math/tex">\text{Var}_{\hat p} [x]</script>, ….。把情况一般化，变量<script type="math/tex">x</script>推广到函数向量，如：</p><script type="math/tex; mode=display">\begin{aligned}&E_{\hat{p}}[\mathbf{f}(x)] =\mathbf{\Delta} \\&\text { 其中 } \mathbf{f}(x)=\left(\begin{array}{c}f_{1}(x) \\f_{2}(x) \\\vdots \\f_{Q}(x)\end{array}\right) & \mathbf{\Delta}=\left(\begin{array}{c}\Delta_{1} \\\Delta_{2} \\\vdots \\\Delta_{Q}\end{array}\right)\end{aligned} \tag{17}</script><p>其中，<script type="math/tex">\mathbf{\Delta}</script>是假定的已知量。</p><p>现在，我们就可以在满足上述条件下，求其最大熵，这个优化问题如下：</p><script type="math/tex; mode=display">\left\{\begin{array}{c}\min \sum_x p(x) \log p(x)\\\text {s.t. } \sum_x p(x)=1 \\E_{p}[\mathbf{f}(x)]=E_{\hat{p}}[\mathbf{f}(x)]=\mathbf{\Delta}\end{array}\right. \tag{18}</script><p>用拉格朗日乘子法：</p><script type="math/tex; mode=display">L(p, \lambda_o, \mathbf{\lambda_1}) = \sum_x p(x) \log p(x) + \lambda_0 (1 - \sum_x p(x)) + \mathbf{\lambda_1}^T(\mathbf{\Delta} - E_{p}[\mathbf{f}(x)]) \tag{19}</script><p>对每个<script type="math/tex">p(x_i)</script>求导， 并且<script type="math/tex">E_{p}[\mathbf{f}(x)] =\sum_x p(x_i)\mathbf{f}(x)</script>,那么：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial p(x)} = &\log p(x) + p(x) \frac{1}{p(x)} - \lambda_0 -\mathbf{\lambda_1}^T \mathbf{f}(x) \\= &\log p(x) + 1 -\lambda_0 -\mathbf{\lambda_1}^T \mathbf{f}(x) \\\Longrightarrow &\log p(x) = \lambda_0 + \mathbf{\lambda}_1^T \mathbf{f}(x)  - 1 \\\Longrightarrow  &p(x) = \exp [ \mathbf{\lambda}_1^T \mathbf{f}(x) - (1 - \lambda_0) ]\end{align} \tag{19}</script><p>注：在求导时，只是对单独的<script type="math/tex">x_i</script>求导，遇到<script type="math/tex">x_j (i \neq j)</script>时，结果为0， 可忽略。</p><p>式19中，我们看到最大熵要求分布<script type="math/tex">p(x)</script>满足指数簇分布，在满足既定数据的情况下，<strong>最大熵</strong>对应着要求满足<strong>指数簇分布</strong>。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/56414312">最大熵模型理论及NLP应用总结</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/103854460">“共轭分布”是什么？</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 指数簇分布 </tag>
            
            <tag> 对数配分函数 </tag>
            
            <tag> 充分统计量 </tag>
            
            <tag> 最大熵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 7 核方法</title>
      <link href="2020/07/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC7%20%E6%A0%B8%E6%96%B9%E6%B3%95/"/>
      <url>2020/07/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC7%20%E6%A0%B8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-7-核方法"><a href="#机器学习-白板推导-7-核方法" class="headerlink" title="机器学习-白板推导 7 核方法"></a>机器学习-白板推导 7 核方法</h3><h4 id="1-线性不可分问题"><a href="#1-线性不可分问题" class="headerlink" title="1. 线性不可分问题"></a>1. 线性不可分问题</h4><p>对于如下图中的异或问题，是线性不可分的。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857924.png" alt="image-20210531213728143" style="zoom:25%;" /></p><p>但是将数据映射到高维空间后就可以实现线性可分。如果将异或问题中的数据通过一个映射<script type="math/tex">\phi(x)</script>将低维空间中的数据<script type="math/tex">x</script>映射成高维空间中的<script type="math/tex">z</script>来实现数据的线性可分。如：</p><script type="math/tex; mode=display">x = (x_1, x_2) \overset{\phi(x)} \Longrightarrow z=(x_1, x_2, (x_1-x_2)^2)</script><p>就可以得到如下数据分布：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261857925.png" alt="image-20210601090713029" style="zoom:25%;" /></p><p>这时就可以轻松将两类点分开。</p><h4 id="2-核函数的引入"><a href="#2-核函数的引入" class="headerlink" title="2. 核函数的引入"></a>2. 核函数的引入</h4><p><a href="https://aigonna.com/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC6%20SVM%20%E4%B8%8A/">机器学习-白板推导6 SVM </a>小节3中对偶问题的拉格朗日乘子法目标函数为：</p><script type="math/tex; mode=display">\begin{align}\max_{\lambda} L(\boldsymbol{w}, b, \lambda) & = -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N\lambda_i\lambda_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j +\sum_{i=1}^N\lambda_i \\ &\text{s.t. } \quad \lambda_i \ge 0, \quad\sum_{i=1}^N\lambda_iy_i = 0, \quad i=1, \cdots, N\end{align}\tag{1}</script><p>其中，含有<script type="math/tex">\boldsymbol{x}_i^T\boldsymbol{x}_j</script>项，如果我们定义一个映射<script type="math/tex">\phi(\boldsymbol{x})</script>,将低维的特征映射到高维特征空间中，将会使得数据线性可分。但是这个映射过程如果维度过高，计算量将非常大。为此引入核函数，kernel function的作用之一就是省去计算<script type="math/tex">\phi(\boldsymbol{x})</script>，直接得到<script type="math/tex">\phi(\boldsymbol{x})^T\phi(\boldsymbol{z})</script>。</p><p><strong>核函数定义</strong>: （《统计机器学习 》P144定义）</p><p>设<script type="math/tex">\mathcal{X}</script>是输入空间(<script type="math/tex">\mathbb{R}^n</script>),又设<script type="math/tex">\mathcal{H}</script>是希尔伯特空间，如果存在：</p><script type="math/tex; mode=display">\phi(\boldsymbol{x}):\mathcal{X} \mapsto \mathcal{H} \tag{2}</script><p>使得对所有的<script type="math/tex">\boldsymbol{x}, \boldsymbol{z} \in \mathcal{X}</script>，函数<script type="math/tex">K(\boldsymbol{x}, \boldsymbol{z})</script>满足条件：</p><script type="math/tex; mode=display">K(\boldsymbol{x}, \boldsymbol{z}) = <\boldsymbol{x}, \boldsymbol{z}> \tag{3}</script><p>其中<script type="math/tex"><\boldsymbol{x}, \boldsymbol{z}>  = \boldsymbol{x}^T\boldsymbol{z}</script>就是内积，希尔伯特空间<script type="math/tex">\mathcal{H}</script>在下一节介绍。</p><p>例子：</p><blockquote><p>  $k(x,x’)=\exp(-\frac{(x-x’)^2}{2\sigma^2})$ 是一个核函数。</p><p>  证明：</p><script type="math/tex; mode=display">  \begin{align}  \exp(-\frac{(x-x')^2}{2\sigma^2})&=\exp(-\frac{x^2}{2\sigma^2})\exp(\frac{xx'}{\sigma^2})\exp(-\frac{x'^2}{2\sigma^2})\nonumber\\  &=\exp(-\frac{x^2}{2\sigma^2})\sum\limits_{n=0}^{+\infty}\frac{x^nx'^n}{\sigma^{2n}n!}\exp(-\frac{x'^2}{2\sigma^2})\nonumber\\  &=\exp(-\frac{x^2}{2\sigma^2})\varphi(x)\varphi(x')\exp(-\frac{x'^2}{2\sigma^2})\nonumber\\  &=\phi(x)\phi(x')  \end{align}</script></blockquote><p>实际上，我们一般使用的都是正定核函数。那么什么是正定核函数呢？</p><h4 id="3-正定核函数的两个定义"><a href="#3-正定核函数的两个定义" class="headerlink" title="3. 正定核函数的两个定义"></a>3. 正定核函数的两个定义</h4><p><strong>定义一</strong>：</p><script type="math/tex; mode=display">\begin{align}&若存在一个映射K:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}，并且\forall x,z \in \mathcal{X}。如果存在一个\phi:\mathcal{X}\mapsto \mathbb{R}^p，\\&并且\phi(x)\in\mathcal{H}，使得K(x,z) = <\phi(x),\phi(z)>，那么称K(x,z)为正定核函数。\end{align} \tag{4}</script><p><strong>定义二</strong>：</p><script type="math/tex; mode=display">\begin{align}&对于一个映射K:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}，对于\forall x,z\in \mathcal{X}，都有K(x,z)。\\&如果K(x,z)满足，1. 对称性；2. 正定性；那么称K(x,z)为一个正定核函数。\end{align} \tag{5}</script><p><strong>对称性，正定性解释</strong>：</p><ol><li>对称性：<script type="math/tex">K(x, z) = K(z, x)</script></li><li>正定性:  任取N个元素, <script type="math/tex">x_1, x_2, \cdots, x_N \in \mathcal{X}</script>,其对应的Gram Matrix (也叫Kernel Matrix) <script type="math/tex">[ K(x_i, x_j)]_{n \times n}</script>，就是对应元素的kernel矩阵是半正定的</li></ol><p>先介绍<strong>Hilbert Space</strong>: 希尔伯特空间是一个完备的，可能是无限维的，被赋予内积运算的线性空间。</p><ol><li><p><strong>线性空间</strong>：空间中任意两个向量都可以由基向量线性表示</p></li><li><p><strong>完备的</strong>：对极限操作的封闭性。希尔伯特空间是一个函数空间，空间中每个元素都是函数，因此：</p><p>​    </p><script type="math/tex; mode=display">\lim_{n \to +\infty} K_N = K \in \mathcal{H} \tag{6}</script></li><li><p><strong>内积</strong> 内积要满足3个条件：正定性， 对称性， 线性</p></li></ol><ul><li>对称性：<script type="math/tex">f, g \in \mathcal{H}</script>, 就有<script type="math/tex"><f, g> = <g, f></script>,这里<script type="math/tex">f, g</script>都是函数。</li><li>正定性：<script type="math/tex">f, f \ge 0</script>， 只有当<script type="math/tex">f =0</script> 时取等号。</li><li>线性：<script type="math/tex"><a_1f_1 + a_2f_2, g> = a_1<f_1, g> + a_2<f_2, g></script>。</li></ul><p>如果使用定义一，很难找到<script type="math/tex">K(x, z)</script>,我们一般用定义二来判断<script type="math/tex">K(x, z)</script>是否为正定核函数。</p><h4 id="4-正定核函数充要条件"><a href="#4-正定核函数充要条件" class="headerlink" title="4. 正定核函数充要条件"></a>4. 正定核函数充要条件</h4><p>问题是：</p><script type="math/tex; mode=display">K(x, z) = <\phi(x), \phi(z)> \Longleftrightarrow K<x, z> 是对称半正定矩阵</script><ol><li><strong>对称性</strong>：</li></ol><script type="math/tex; mode=display">\begin{equation}    K(x,z)=<\phi(x),\phi(z)> \qquad K(z,x) = <\phi(z),\phi(x)>\end{equation}</script><p>又因为：</p><script type="math/tex; mode=display">\begin{equation}    <\phi(x),\phi(z)> = <\phi(z),\phi(x)>\end{equation}</script><p>所以<script type="math/tex">K(x,z)=K(z,x)</script>。</p><ol><li><strong>正定性</strong>：令<script type="math/tex">K_{ij} = K(x_i, x_j)</script>有<script type="math/tex; mode=display">\begin{align}    \alpha^TK\alpha = &     \begin{bmatrix}        \alpha_1 & \alpha_2 & \cdots & \alpha_N    \end{bmatrix}    \begin{bmatrix}        K_{11} & K_{12} & \cdots & K_{1N} \\        K_{21} & K_{22} & \cdots & K_{2N} \\        \vdots & \vdots & \ddots & \vdots \\        K_{N1} & K_{N2} & \cdots & K_{NN} \\    \end{bmatrix}    \begin{bmatrix}        \alpha_1 \\        \alpha_2 \\         \vdots \\         \alpha_N    \end{bmatrix} \\    = & \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_jK_{ij} \\    = & \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j<\phi(x_i),\phi(x_j)> \\    = & \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\phi(x_i)^T\phi(x_j) \\     = & \sum_{i=1}^N\phi(x_i)^T\sum_{j=1}^N\phi(x_j) \\    = & \left[  \sum_{i=1}^N\phi(x_i) \right]^T \left[  \sum_{j=1}^N\phi(x_j) \right] \\    = & \left|\left| \sum_{i=1}^N \alpha_i\phi(x_i) \right|\right|^2 \geq 0\end{align}</script>所有K是半正定的。</li></ol><h4 id="5-非线性支持向量机"><a href="#5-非线性支持向量机" class="headerlink" title="5. 非线性支持向量机"></a>5. 非线性支持向量机</h4><p>式1引入核函数就能应用到非线性分类中：</p><script type="math/tex; mode=display">\begin{align}\min_{\lambda} L(\lambda) & = \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N\lambda_i\lambda_jy_iy_jK(\boldsymbol{x}_i,\boldsymbol{x}_j) -\sum_{i=1}^N\lambda_i \\ &\text{s.t. } \quad \lambda_i \ge 0, \quad\sum_{i=1}^N\lambda_iy_i\ = 0, \quad i=1, \cdots, N\end{align}\tag{7}</script><p>现在SVM算法选择合适的核函数<script type="math/tex">K(x, z)</script>就可以对非线性可分数据集分类了。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://www.cnblogs.com/pinard/p/6103615.html">支持向量机原理(三)线性不可分支持向量机与核函数</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kernel method </tag>
            
            <tag> 希尔伯特空间 </tag>
            
            <tag> 非线性SVM </tag>
            
            <tag> 正定核函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导6 SVM 上</title>
      <link href="2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC6%20SVM%20%E4%B8%8A/"/>
      <url>2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC6%20SVM%20%E4%B8%8A/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导6-SVM-上"><a href="#机器学习-白板推导6-SVM-上" class="headerlink" title="机器学习-白板推导6 SVM 上"></a>机器学习-白板推导6 SVM 上</h3><h4 id="1-SVM-核心知识点"><a href="#1-SVM-核心知识点" class="headerlink" title="1.SVM 核心知识点"></a>1.SVM 核心知识点</h4><p>Support Vector Machine(SVM) 有三宝，间隔，对偶，核技巧。其分类有：</p><ol><li><strong>Hard-margin SVM</strong> </li><li><strong>Soft-margin SVM</strong> </li><li><strong>Kernel SVM</strong></li></ol><p>本文将介绍软硬间隔部分, 核技巧等将在下篇中介绍。</p><h4 id="2-SVM模型引入"><a href="#2-SVM模型引入" class="headerlink" title="2. SVM模型引入"></a>2. SVM模型引入</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855960.png" alt="image-20210528173538769" style="zoom:20%;" /></p><p>SVM模型就是找到一个超平面将两类数据点分隔开，这个超平面记为<script type="math/tex">f(\boldsymbol{w}) = \boldsymbol{w}^Tx + b</script>。如上图所示，这样的超平面很多，如<script type="math/tex">L_1, L_2, L_3</script>，那么哪个最好呢？直觉上，离这些点距离越远越好。间隔就是相对距离最近的点与超平面的距离，首先先将问题数学化定义。</p><p>对于数据集<script type="math/tex">D=\{(\boldsymbol{x}_i,y_i)\}^{N}_{i=1}</script>，其中<script type="math/tex">\boldsymbol{x}_i\in\mathbb{R}^{p}, \ y_i\in\{1,-1\}</script>。要将数据点分隔开的间隔最大化，并且要将所有点分类正确：</p><script type="math/tex; mode=display">\max \text{ margin}  (\boldsymbol{w},b) \\       s.t. \        \left\{        \begin{array}{ll}            \boldsymbol{w}^T_ix+b>0  \quad y_i = +1 \\            \boldsymbol{w}^T_ix+b<0 \quad  y_i = -1 \\        \end{array}         \right. \tag{1}</script><p>因为约束条件可改写为：</p><script type="math/tex; mode=display">s.t. \quad y_i(\boldsymbol{w}_i^Tx + b) > 0, \quad (i=1, 2, \cdots, N) \tag{2}</script><p>对于间隔，即样本空间点到超平面<script type="math/tex">(\boldsymbol{w}, b)</script>的最小距离可写作：</p><script type="math/tex; mode=display">\begin{align}&\min \frac{1}{\lVert \boldsymbol{w} \rVert} |\boldsymbol{w}^T\boldsymbol{x}_i + b| \quad \quad  (i=1, 2, \cdots, N )\\= &\min \frac{1}{\lVert \boldsymbol{w} \rVert} y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)\\=&\frac{1}{\lVert \boldsymbol{w} \rVert} \min\ y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)\end{align}\tag{3}</script><p>进一步简化问题，式2中约束是<script type="math/tex">y_i(\boldsymbol{w}_i^Tx + b) > 0</script>,那么<script type="math/tex">\exists \gamma \gt 0, \ 令\min\ y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) = \gamma</script>。这时最小间隔就是<script type="math/tex">\gamma</script>， 对于所有点都应该大于等于最小间隔，<script type="math/tex">y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge \gamma</script>。目标函数整理后：</p><script type="math/tex; mode=display">\max_{\boldsymbol{w}, b} \frac{\gamma}{\lVert \boldsymbol{w} \rVert } \quad \quad s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge \gamma \tag{4}</script><p>但是有个问题<script type="math/tex">\gamma</script>是<strong>函数间隔</strong>，就是<script type="math/tex">\boldsymbol{w}， b</script>放大为2倍时，<strong>函数间隔</strong>也会扩大。因此要约束。不妨令<script type="math/tex">\gamma = 1</script>, 并且这时式4中<script type="math/tex">\max_{\boldsymbol{w}, b} \frac{1}{\lVert \boldsymbol{w} \rVert }</script>等价于<script type="math/tex">\min_{\boldsymbol{w}, b} \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2 = \min_{\boldsymbol{w}, b} \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}</script> （最大导数变为最小其互导数，并且为了简化计算改写下）。这时目标函数改写为：</p><script type="math/tex; mode=display">\left \{ \begin{array}{ll}\min_{\boldsymbol{w}, b} \ \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}\\\text{s.t} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1, \quad i=1, \cdots, N\end{array}\right. \tag{5}</script><h4 id="3-SVM-模型求解-硬间隔"><a href="#3-SVM-模型求解-硬间隔" class="headerlink" title="3. SVM 模型求解(硬间隔)"></a>3. SVM 模型求解(硬间隔)</h4><p>上节中式5 是带约束的优化问题，可以用拉格朗日乘子法来求解。</p><p>其拉格朗日函数为：</p><script type="math/tex; mode=display">L(\boldsymbol{w}, b, \lambda) = \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} + \sum_{i=1}^{N} \lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)) \tag{6}</script><p>其中<script type="math/tex">\lambda_i \ge 0</script>. 如果其等于0，就没有约束条件了，变为直接取最小值。</p><p><strong>关于<script type="math/tex">\lambda_i \gt 0</script>原因</strong>：因为要目标函数的梯度和可行域的梯度要平行且同向，<script type="math/tex">-\nabla_{\boldsymbol{x}}f(x) = \lambda \nabla_{\boldsymbol{x}}g(x) \quad 且 \lambda \gt 0</script>。如果小于0，就反向了。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855962.png" alt="image-20210530105847439" style="zoom:25%;" /></p><p><a href="https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf">具体看这个KKT</a>。</p><p>那么式6和式5约束问题为什么等价呢？</p><p>令<script type="math/tex">\Delta =1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)</script>有：</p><ul><li><p>若<script type="math/tex">\Delta \gt 0</script>,那么 <script type="math/tex">\sum_{i=1}^{N} \lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b))</script>将取到无穷大，所以<script type="math/tex">\max_{\lambda} L(\boldsymbol{w}, b, \lambda) = \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} + \infty =\infty</script></p></li><li><p>若<script type="math/tex">\Delta \le 0</script>, 当<script type="math/tex">\lambda_i = 0</script>时, 则<script type="math/tex">\sum_{i=1}^{N} \lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b))</script>取最大值0，因此<script type="math/tex">\max_{\lambda} L(\boldsymbol{w}, b, \lambda) = \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} + 0=\frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}</script></p><p>综上所述，<script type="math/tex">\min_{\boldsymbol{w}, b} \max_{\lambda} L(\boldsymbol{w}, b, \lambda)  = \min_{\boldsymbol{w}, b} \{\infty, \ \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}\}</script>将取<script type="math/tex">\frac{1}{2} \boldsymbol{w}^T\boldsymbol{w}</script>。这样隐式地保证了 <script type="math/tex">\Delta =1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \le 0</script> 来满足约束条件。</p></li></ul><p>现在<strong>原问题：</strong></p><script type="math/tex; mode=display">\left \{ \begin{array}{ll}\min_{\boldsymbol{w}, b } \max_{\lambda }  \ L(\boldsymbol{w}, b, \lambda)  \\\text{s.t} \quad \lambda_i \ge 0\end{array}\right. \tag{7}</script><p>将其转化为<strong>对偶问题是</strong>：</p><script type="math/tex; mode=display">\left \{ \begin{array}{ll}\max_{\lambda }  \min_{\boldsymbol{w}, b }  \ L(\boldsymbol{w}, b, \lambda)  \\\text{s.t} \quad \lambda_i \ge 0\end{array}\right. \tag{8}</script><p>简单介绍下对偶问题，具体看《统计学习方法》 P448。</p><p>式7中拉格朗日乘子法的极小极大问题的对偶问题是极大极小问题。</p><p>并满足<script type="math/tex">\min  \max L \ge \max \min L</script>，因为对于该不等式，<script type="math/tex">\min  \max L</script>最优解在最大值里面取最小的，而<script type="math/tex">\max \min L</script>是最小值里面取最大的。类似于“宁为鸡头不为凤尾”。</p><p>弱对偶关系就是：<script type="math/tex">\min\max \geq \max\min L</script>。</p><p>强对偶关系就是：<script type="math/tex">\min\max L  = \max\min L</script>。</p><p><strong>求解参数值</strong>：</p><p>对于目标函数 (转换为求解其对偶问题):</p><script type="math/tex; mode=display">\max_{\lambda} \min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \lambda) = \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} + \sum_{i=1}^{N} \lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)) \quad \quad i=1, \cdots, N \tag{9}</script><p>简化下方便计算：</p><script type="math/tex; mode=display">\begin{align}   L(\boldsymbol{w},b,\lambda) = & \frac{1}{2}\boldsymbol{w}^T\boldsymbol{w} + \sum_{i=1}^N\lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \\    = & \frac{1}{2}\boldsymbol{w}^T\boldsymbol{w} + \sum_{i=1}^N\lambda_i - \sum_{i=1}^N\lambda_iy_i\boldsymbol{w}^T\boldsymbol{x}_i - \sum_{i=1}^N\lambda_iy_ib \\\end{align} \tag{10}</script><ol><li>对参数<script type="math/tex">b</script>求导：</li></ol><script type="math/tex; mode=display">\frac{\partial L}{\partial b} = -\sum_{i=1}^N\lambda_iy_i \tag{11}</script><ol><li>对参数<script type="math/tex">\boldsymbol{w}</script>求导，先将式11代入式10，然后求导。</li></ol><script type="math/tex; mode=display">L(\boldsymbol{w},b,\lambda) = \frac{1}{2}\boldsymbol{w}^T\boldsymbol{w} + \sum_{i=1}^N\lambda_i - \sum_{i=1}^N\lambda_iy_i\boldsymbol{w}^T\boldsymbol{x}_i \tag{12}</script><p>对其求导得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{w}} = \boldsymbol{w} -\sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i = 0 \tag{13}</script><p>这里<script type="math/tex">\boldsymbol{w}^\star = \sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i</script>是样本数据的线性组合。</p><ol><li>求解 <script type="math/tex">\min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \lambda)</script>, 依旧将<script type="math/tex">\boldsymbol{w}^\star</script>代入式12得：</li></ol><script type="math/tex; mode=display">\begin{align}\min_{\boldsymbol{w}, b} L(\boldsymbol{w}, b, \lambda)  &= \frac{1}{2} (\sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i)^T\sum_{j=1}^N\lambda_jy_j\boldsymbol{x}_j +\sum_{i=1}^N\lambda_i - \sum_{i=1}^N\lambda_iy_i(\sum_{j=1}^N\lambda_jy_j\boldsymbol{x}_j)^T\boldsymbol{x}_i\\ &= -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N\lambda_i\lambda_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j +\sum_{i=1}^N\lambda_i \end{align}\tag{14}</script><p>现在问题简化为：</p><script type="math/tex; mode=display">\begin{align}\max_{\lambda}  L(\boldsymbol{w}, b, \lambda) & = -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N\lambda_i\lambda_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j +\sum_{i=1}^N\lambda_i \\ &\text{s.t. } \quad \lambda_i \ge 0, \quad\sum_{i=1}^N\lambda_iy_i = 0, \quad i=1, \cdots, N\end{align}\tag{15}</script><h4 id="4-SVM-KKT条件导出"><a href="#4-SVM-KKT条件导出" class="headerlink" title="4. SVM KKT条件导出"></a>4. SVM KKT条件导出</h4><p>KKT条件实际上由以下部分构成：</p><ol><li>目标函数和所有约束条件组成的拉格朗日函数的梯度为0。</li><li>互补松弛条件（complementary slackness）， 就是不等式约束和其拉格朗日系数积为0。</li><li>不等式约束条件</li><li>等式约束条件</li><li>拉格朗日系数约束</li></ol><p>其中，互补松弛条件解释跟小节3中<script type="math/tex">\lambda_i \ge 0</script>部分类似，还可以看看关于这个得解释<a href="https://www.cnblogs.com/massquantity/p/10807311.html">拉格朗日乘子法 - KKT条件 - 对偶问题</a>。</p><p>导出式15的KKT条件为：</p><script type="math/tex; mode=display">\begin{equation}    \left\{    \begin{array}{ll}          \frac{\partial L}{\partial \boldsymbol{w}} = 0,\quad          \frac{\partial L}{\partial b} = 0,\quad          \frac{\partial L}{\partial \lambda} = 0 & \\          \lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) = 0 & \\          1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \leq 0 &\\          \lambda_i \geq 0 & \\    \end{array}    \right.\end{equation} \tag{16}</script><p>其中式16中第二排等式就是互补松弛条件。满足KKT条件是原问题的对偶问题具有强对偶关系的充分条件，当然也包括目标函数<script type="math/tex">f(x)</script>和不等式约束为凸函数，等式约束为仿射函数这些必备条件。</p><p>根据小节3中，<script type="math/tex">\boldsymbol{w}^\star = \sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i</script>。</p><p>那么<script type="math/tex">b^{\star}</script>呢？这时使用互补松弛条件来求解，<script type="math/tex">\lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) = 0</script>。</p><p>再解释下具体SVM中这个条件是怎么来的。如下图，图来自于 <a href="https://en.wikipedia.org/wiki/Support-vector_machine">Wiki SVM</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855963.png" alt="image-20210528170838009" style="zoom:20%;" /></p><p>图中距离分割超平面最近的点都落在<script type="math/tex">y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) = \pm 1</script>上，这些点叫<strong>支持向量</strong>。SVM算法最核心部分就是找支持向量。</p><p>对于支持向量，必然有<script type="math/tex">1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)=0</script>，那么<script type="math/tex">\lambda_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) = 0</script>中<script type="math/tex">\lambda_i</script>才有可能取值(<script type="math/tex">\lambda_i \neq 0</script>)。其它点的话，<script type="math/tex">\lambda_i = 0</script>。</p><p>不妨假设一个支持向量为<script type="math/tex">(\boldsymbol{x}_k, y_k)</script>那么，</p><script type="math/tex; mode=display">\begin{align}&1-y_k(\boldsymbol{w}^T\boldsymbol{x}_k+b)=0 \\&\Longrightarrow y_kb^{\star} = 1-y_k\boldsymbol{w^{\ast ^T}}\boldsymbol{x}_k\\&\Longrightarrow  b^{\star} = y_k-\boldsymbol{w^{\ast ^T}}\boldsymbol{x}_k \quad \quad 因为y_k= \pm 1 \\& \Longrightarrow  b^{\star} =  y_k -\sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i\boldsymbol{x}_k \end{align} \tag{17}</script><p><strong>总结</strong>：</p><p>到此为止，我们求出了硬间隔的SVM的最优解，模型为：</p><script type="math/tex; mode=display">f(\boldsymbol{x}) = \text{sign }(\boldsymbol{w^{\ast ^T}}\boldsymbol{x} + b^{\star} ) \tag{18}</script><p>其中，<script type="math/tex">\boldsymbol{w}^\star = \sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i, \quad b^{\star} = y_k -\sum_{i=1}^N\lambda_iy_i\boldsymbol{x}_i\boldsymbol{x}_k</script></p><p>并且KKT条件是原问题的对偶问题有强对偶关系的充要条件，利用支持向量求解SVM计算相对简单。支持向量寻找过程是通过代入数据点，从满足式15中当<script type="math/tex">\lambda_i=0</script>时，对应数据点就是是支持向量。具体计算算法看SVM SMO这节。</p><h4 id="5-SVM-软间隔"><a href="#5-SVM-软间隔" class="headerlink" title="5. SVM 软间隔"></a>5. SVM 软间隔</h4><p>上面介绍的SVM要求数据集能完全被分开，不能有数据点噪声落在硬间隔区间内，但是数据集必然有噪声等落在这个区间。Soft Margin就是允许一些点落在margin内。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855964.png" alt="image-20210531155653799" style="zoom:25%;" /></p><p>如上图中，对于要分类的两类数据点，落入蓝色实线之间的点就是被误分类的，图中A点，那么<script type="math/tex">y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \lt 1</script>。如果有多个呢？我们引入指示函数有：</p><script type="math/tex; mode=display">\text{loss} =\sum_{i=1}^{N} I \{y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \lt 1\}</script><p>其中<script type="math/tex">I</script>是指示函数，不连续，不可导。</p><p>不妨将其转换为距离来看损失函数。</p><ul><li>若<script type="math/tex">y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \ge 1</script>,正确分类，损失为0</li><li>若<script type="math/tex">y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \lt 1</script>，距离为图中<script type="math/tex">\epsilon</script>,  <script type="math/tex">1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)</script></li></ul><p>用max来合并下这些距离，<script type="math/tex">\text{loss} =   \max\{0, 1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\}</script>，这个函数叫hinge loss。</p><p>现在式5，加上合页损失就是目标函数：</p><script type="math/tex; mode=display">\left \{ \begin{array}{ll}\min_{\boldsymbol{w}, b} \ \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} + C\sum_{i=1}^N \max\{0, 1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\}\\\text{s.t} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1, \quad i=1, \cdots, N\end{array}\right. \tag{19}</script><p>其中，C是惩罚系数，有点像正则化中的系数。</p><p>令<script type="math/tex">\xi_i =  1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)</script>, 因为其是距离<script type="math/tex">\xi_i \ge 0</script>。式19就可以简化为：</p><script type="math/tex; mode=display">\left \{ \begin{array}{ll}\min_{\boldsymbol{w}, b} \ \frac{1}{2} \boldsymbol{w}^T\boldsymbol{w} +  C\sum_{i=1}^N \xi_i \\\text{s.t} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \ge 1-\xi_i, \quad \xi_i \ge 0, \ i=1, \cdots, N\end{array}\right. \tag{20}</script><p>如上图中，<script type="math/tex">1- \xi_i</script>就是允许误分类点到分类边界的距离，也可以认为是允许误分类点的范围。接下来也可以用拉格朗日乘子法求解。</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/335017540">支持向量机SVM（Support Vector Machine）笔记</a></p><p>[3] <a href="https://www.jianshu.com/p/eeb33ce6bc6d">支持向量机</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/38182879">拉格朗日对偶性</a> </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 硬间隔 </tag>
            
            <tag> 软间隔 </tag>
            
            <tag> 对偶问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 5 降维</title>
      <link href="2020/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC5%20%E9%99%8D%E7%BB%B4/"/>
      <url>2020/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC5%20%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-5-降维"><a href="#机器学习-白板推导-5-降维" class="headerlink" title="机器学习-白板推导 5 降维"></a>机器学习-白板推导 5 降维</h3><h4 id="1-降维-Dimension-Reduction"><a href="#1-降维-Dimension-Reduction" class="headerlink" title="1. 降维 Dimension Reduction"></a>1. 降维 Dimension Reduction</h4><p>机器学习中，最关心的是<strong>泛化误差</strong>，降低泛化误差是非常关键的。<a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id7">泛化误差和过拟合等关系图如下</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855404.png" alt="image-20210526151702695" style="zoom:35%;" /></p><p>解决过拟合的方法有：</p><ol><li>增加<strong>数据量</strong></li><li><strong>正则化</strong>，如<strong>Lasso</strong>和<strong>Ridge</strong>本质都是降维，Lasso会让部分<script type="math/tex">w</script>趋于0，来消除一些特征。</li><li><strong>降维</strong></li></ol><p><strong>维度灾难</strong>：</p><ol><li><p>高纬度有更大的特征空间，需要更多数据才可以拟合模型。若特征是二值的，则每增加一个特征，数据列都在以2的指数级增长，更何况很多特征不是二值的</p></li><li><p>几何角度：</p><ul><li>如下图所示，假定正方形边长为1，则其内切圆半径为0.5，正方形面积为1，圆面积为<script type="math/tex">\pi(0.5)^2</script>。拓展到3维时，正方形体积为1，内切球体积为<script type="math/tex">\frac{4}{3}\pi(0.5)^3</script>。</li></ul></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855406.png" alt="image-20210526153724477" style="zoom:20%;" /></p><ul><li><p>如下图所示，假设外圆半径为<script type="math/tex">r=1</script>，则内圆半径为<script type="math/tex">r- \epsilon = 1- \epsilon</script>。在高维情况下，外圆体积为<script type="math/tex">V_外 = k1^D = k</script>, 中间圆环体积为<script type="math/tex">V_环= k - k(1-\epsilon)^D</script>。</p><p>那么有：</p><script type="math/tex; mode=display">\lim_{D \to 0}  \frac{V_环}{V_外} = \lim_{D \to 0} \frac{k - k(1-\epsilon)^D}{k} = \lim_{D \to 0}1-(1-\epsilon)^D = 1</script></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855407.png" alt="image-20210526155434035" style="zoom:20%;" /></p><p>​            从两个例子可以看出，高维情况下，数据占据着空间表明，内部趋于0，数据分布稀疏。</p><p><strong>常用的降维方法</strong>：</p><ol><li>直接降维：特征选择，将不重要的特征扔掉</li><li>线性降维：PCA， MDS(multidimensional scaling)</li><li>非线性降维：流形（将数据嵌入到高维空间），局部线性嵌入</li></ol><h4 id="2-样本均值和方差的矩阵表示"><a href="#2-样本均值和方差的矩阵表示" class="headerlink" title="2. 样本均值和方差的矩阵表示"></a>2. 样本均值和方差的矩阵表示</h4><p>后续PCA和SVD都是在矩阵上操作，所以先将均值和方差矩阵表示。</p><p>假设数据集为：</p><script type="math/tex; mode=display">\begin{equation}    X=(x_1, x_2, \cdots, x_N)^T=    \begin{pmatrix}    x_1^T \\     x_2^T \\    \vdots\\    x_N^T \\    \end{pmatrix} =    \begin{pmatrix}    x_{11} & x_{12} & \dots & x_{1p}\\    x_{21} & x_{32} & \dots & x_{2p}\\    \vdots & \vdots & \ddots & \vdots\\    x_{N1} & x_{N2} & \dots & x_{Np}\\    \end{pmatrix}_{N\times P}\end{equation}</script><p>其中<script type="math/tex">x_i \in \mathbb{R}^p, \ i=1,2, \cdots, N</script>。</p><p>其样本均值为：</p><script type="math/tex; mode=display">\bar X_{p \times 1} = \frac{1}{N} \sum_{i=1}^N x_i \tag{1}</script><p>样本方差为：</p><script type="math/tex; mode=display">S_{p \times p} = \frac{1}{N} \sum_{i=1}^N (x_i- \bar X)(x_i- \bar X)^T \tag{2}</script><p>对式1， 2简化表示：</p><p><strong>1. 均值</strong></p><script type="math/tex; mode=display">\bar X = \frac{1}{N}\sum_{i=1}^N x_i=\frac{1}{N}(x_1 \quad x_2 \quad \cdots \quad x_N)    \begin{pmatrix}    1\\     1 \\    \vdots\\   1 \\    \end{pmatrix} = \frac{1}{N}X^T1_{N \times 1}= \frac{1}{N}X^T1_{N }\tag{3}</script><p>其中， <script type="math/tex">1_{N}=1_{N \times 1} = \begin{pmatrix}    1\\     1 \\    \vdots\\   1 \\    \end{pmatrix}</script> 。</p><p><strong>2. 方差</strong></p><script type="math/tex; mode=display">\begin{align}S_{p \times p} &=\frac{1}{N} \sum_{i=1}^N (x_i- \bar X)(x_i- \bar X)^T\\&=\frac{1}{N}(x_1 - \bar X\quad x_2- \bar X \quad \cdots \quad x_N- \bar X)    \begin{pmatrix}    (x_1 - \bar X)^T\\     (x_2 - \bar X)^T\\    \vdots\\   (x_N - \bar X)^T\\    \end{pmatrix}\\ &=\frac{1}{N}(X^T(I_N- \frac{1}{N}1_{N }1_{N }^T))(X^T(I_N- \frac{1}{N}1_{N }1_{N }^T))^T\\ &= \frac{1}{N}X^T(I_N- \frac{1}{N}1_{N }1_{N }^T)(I_N- \frac{1}{N}1_{N }1_{N }^T)^TX\\ &= \frac{1}{N}X^THH^TX\\ \end{align} \tag{4}</script><p>其中，</p><script type="math/tex; mode=display">\begin{align}&(x_1 \quad x_2 \quad \cdots \quad x_N) - (\bar X\quad \bar X \quad \cdots \quad \bar X)\\&= (x_1 \quad x_2 \quad \cdots \quad x_N) - \bar X(1 \quad 1 \quad \cdots \quad 1)\\&= X^T-\bar X1_{1 \times N}\\&= X^T-\frac{1}{N}X^T1_{N }1_{N }^T\\\end{align}</script><p>而<script type="math/tex">H = I_N- \frac{1}{N}1_{N }1_{N }^T</script>，叫做 centering matrxi。并且有 <script type="math/tex">H^T=H, H^2=H</script>，那么式4进一步化简得：</p><script type="math/tex; mode=display">S =\frac{1}{N}X^THX \tag{5}</script><h4 id="3-PCA"><a href="#3-PCA" class="headerlink" title="3. PCA"></a>3. PCA</h4><p>PCA基本思想是将所有数据投影到一个子空间取，从而达到降维的目的。这个子空间要满足：</p><ol><li>最大可分性：样本点在这个超平面上的投影尽可能分开，所有数据在子空间中更为分散。</li><li>最近重构性：样本点到这个平面的距离足够近，那么损失的信息最少，即在补空间上的分量小。</li></ol><p>关键思想:</p><ul><li>一个中心：将原始特征空间重构，将线性相关的特征转化为线性无关的特征。</li><li>两个基本点：最大投影方差和最小重构距离.</li></ul><p>如下图所示，将原始数据分别投影到<script type="math/tex">\mathbf{u}_1, \mathbf{u}_2</script>,在<script type="math/tex">\mathbf{u}_1</script>方向更为分散，因而选择往这个方向投影。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261855408.png" alt="image-20210526234306440" style="zoom:25%;" /></p><p>PCA降维步骤：</p><ol><li><strong>数据去中心化</strong> ，就是减去均值，一定得去中心化。</li><li>投影到一个新的方向上，这个新的方向就是重构的特征空间的坐标轴，并且要满足投影后得到的数据的方差最大，即最大投影方差，这样同时也保证了数据重构的距离最小。</li></ol><h4 id="4-最大投影方差理解PCA"><a href="#4-最大投影方差理解PCA" class="headerlink" title="4. 最大投影方差理解PCA"></a>4. 最大投影方差理解PCA</h4><p>假设投影方向为<script type="math/tex">\mathbf{u}</script>, 并且将其模设为1，<script type="math/tex">u^Tu =1</script>,那么原数据去中心化后的数据在<script type="math/tex">\mathbf{u}</script>方向上的投影为<script type="math/tex">(x_i - \bar x)^Tu</script>，又因为去中心化后均值为0，所以投影方差为<script type="math/tex">((x_i - \bar x)^Tu)^2</script>。</p><p>那么目标函数为：</p><script type="math/tex; mode=display">\begin{align}J &= \frac{1}{N} \sum_{i=1}^N((x_i - \bar x)^Tu)^2 \quad s.t. \ u^Tu=1 \\ &= \frac{1}{N} \sum_{i=1}^Nu^T(x_i -\bar x)(x_i - \bar x)^Tu\\ &= u^T(\frac{1}{N} \sum_{i=1}^N(x_i -\bar x)(x_i - \bar x)^T)u\\ &= u^T Su\end{align}\tag{6}</script><p>其中<script type="math/tex">S = \frac{1}{N} \sum_{i=1}^N(x_i -\bar x)(x_i - \bar x)^T</script>是协方差矩阵。</p><p>使用拉格朗日乘子法：</p><script type="math/tex; mode=display">L(u, \lambda) = u^TSu + \lambda(1-u^Tu) \tag{7}</script><p>对<script type="math/tex">u</script>求导得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial u} = 2Su - 2\lambda u=0 \Longrightarrow Su = \lambda u\tag{8}</script><p>其中<script type="math/tex">S</script>是协方差矩阵，<script type="math/tex">\lambda</script>是特征值，<script type="math/tex">u</script>是特征向量。那么只要将协方差矩阵S进行特征值分解，将求得的特征值排序，取需要的前p个，在计算出对应的特征向量，就是PCA的解。</p><h4 id="5-最小重构距离理解PCA"><a href="#5-最小重构距离理解PCA" class="headerlink" title="5. 最小重构距离理解PCA"></a>5. 最小重构距离理解PCA</h4><p>去中心化后数据为<script type="math/tex">x_i ^\prime=x_i - \bar x</script>, 假设我们将其投影到对应空间后，取前p个特征有：</p><script type="math/tex; mode=display">{\hat x^\prime}_i = \sum_{k=1}^p ({x^\prime_i}^Tu_k)u_k \tag{9}</script><p>假定<script type="math/tex">u_i</script>已经按照特征值<script type="math/tex">\lambda_i</script>从大到小排列。</p><p>那么目标函数为(去中心数据点到投影点的距离最小),本质上源空间有p个特征，如果降维后选取了q个特征，降维的数据也就是舍弃掉第q+1到第p这几个方向上的信息。</p><script type="math/tex; mode=display">\begin{align}J &=\frac{1}{N} \sum_{i=1}^N  \lVert x_i ^\prime-{\hat x^\prime}_i\rVert^2 \quad \ s.t. \ u_k^Tu_k=1\\ &= \frac{1}{N} \sum_{i=1}^N \sum_{k=q+1}^p ((x_i-\bar x)^Tu_k)^2\\ &=  \sum_{k=q+1}^p\frac{1}{N} \sum_{i=1}^N ((x_i-\bar x)^Tu_k)^2\\ &=\sum_{k=q+1}^p u^TSu_k \end{align} \tag{10}</script><p>式10跟式7一样有：</p><script type="math/tex; mode=display">Su_k = \lambda_ku_k \tag{11}</script><p>这里<script type="math/tex">u_k</script>为协方差矩阵的前k个特征向量。</p><h4 id="6-SVD角度看PCA"><a href="#6-SVD角度看PCA" class="headerlink" title="6. SVD角度看PCA"></a>6. SVD角度看PCA</h4><p> <a href="https://aigonna.com/2020/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture8/">SVD介绍</a>，如果我们先对数据<script type="math/tex">X</script>去中心化，左乘中心矩阵H(在上面第3部分)，然后进行SVD分解，因此有：</p><script type="math/tex; mode=display">HX = U\Sigma V^T \tag{12}</script><p>其中，</p><ul><li><script type="math/tex">U</script>是<script type="math/tex">N \times N</script>的矩阵，并且 <script type="math/tex">U^TU=UU^T=I</script>。</li><li><script type="math/tex">V</script>是<script type="math/tex">P \times P</script>的矩阵，并且 <script type="math/tex">V^TV=VV^T=I</script>。</li><li><script type="math/tex">\Sigma</script>为<script type="math/tex">N \times P</script>的矩阵，且是对角阵。</li></ul><p>式5中<script type="math/tex">S =\frac{1}{N}X^THX</script>, 因此：</p><script type="math/tex; mode=display">\begin{align}S &=\frac{1}{N}X^THX  \qquad \text{由于}\frac{1}{N}\text{是常数简化计算忽略}\\ &= X^TH^THX\\&= (HX)^THX \\&=  (U\Sigma V^T)^T U\Sigma V^T\\&= V\Sigma^T U^T  U\Sigma V^T\\&=  V\Sigma^T \Sigma V^T\\&=  V\Sigma^2 V^T\end{align} \tag{13}</script><p>那么<script type="math/tex">V\Sigma^2 V^T</script>是<script type="math/tex">S</script>的特征值分解，<script type="math/tex">\Sigma^2</script>是特征值矩阵。</p><p>若我们构造矩阵<script type="math/tex">T_{N \times N}</script>如下：</p><script type="math/tex; mode=display">T_{N \times N} = HXX^TH^T =U\Sigma V^T V\Sigma^T U^T=U\Sigma \Sigma^T U^T \tag{14}</script><ul><li><p>将<script type="math/tex">S</script>进行特征分解后得到主成分，将<script type="math/tex">X</script>投影到主成分的方向后，其坐标为<script type="math/tex">HX \cdot V = U\Sigma V^TV = U\Sigma</script></p></li><li><p>而直接对<script type="math/tex">T</script>进行特征分解，坐标矩阵<script type="math/tex">U \Sigma</script>， 这种方法叫主坐标分析 PCoA ——Principle Coordinate Analysis。</p><p>​    原因： <script type="math/tex">TU\Sigma = U\Sigma \Sigma^T U^T U \Sigma = U \Sigma \Sigma^T \Sigma</script>,  （就是将式14代入），那么有<script type="math/tex">T U \Sigma = U\Sigma (\Sigma^T\Sigma)</script>,这里 <script type="math/tex">\Sigma^T\Sigma</script> 是特征值， <script type="math/tex">U\Sigma</script>就是坐标。</p></li></ul><p>T分解和S分解的区别：</p><ul><li><script type="math/tex">S</script>是<script type="math/tex">p \times p</script>的矩阵， <script type="math/tex">T</script>是<script type="math/tex">N \times N</script>的矩阵</li><li>当数据量较少是或特征数较多时用PCoA</li></ul><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/326074168">降维</a></p><p>[2] <a href="https://www.jianshu.com/p/8f0fe4e7f371">主成分分析</a></p><p>[3] <a href="https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466/kg2npf">Bilibili-机器学习白板系列 降维</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
            <tag> 降维 </tag>
            
            <tag> PCoA </tag>
            
            <tag> SVD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导4.3 线性分类——朴素贝叶斯</title>
      <link href="2020/07/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.3%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
      <url>2020/07/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.3%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导4-3-线性分类——朴素贝叶斯"><a href="#机器学习-白板推导4-3-线性分类——朴素贝叶斯" class="headerlink" title="机器学习-白板推导4.3 线性分类——朴素贝叶斯"></a>机器学习-白板推导4.3 线性分类——朴素贝叶斯</h3><h4 id="1-算法推导"><a href="#1-算法推导" class="headerlink" title="1.算法推导"></a>1.算法推导</h4><p>在分类问题中，我们希望从数据集<script type="math/tex">X</script>中学到能从<script type="math/tex">X</script>分类到类别标签<script type="math/tex">C</script>的映射，数学描述就是：</p><script type="math/tex; mode=display">\mathbf{X} \longmapsto \mathbf{C}, \ \mathbf{C} \in \{C_1, C_2, \cdots, C_{m} \} \tag{1}</script><p>先强调下，Naive Bayes中Naive就是要求数据特征列各自独立，也就是：</p><script type="math/tex; mode=display">P(\mathbf{X}_1, \cdots,  \mathbf{X}_p) = P(\mathbf{X}_1) \cdots P(\mathbf{X}_p)</script><p>加上条件有：</p><script type="math/tex; mode=display">P(\mathbf{X}_1, \cdots,  \mathbf{X}_p | \mathbf{C})  = \prod_{j=1}^mP(\mathbf{X}_j|\mathbf{C})</script><p>利用贝叶斯公式有：</p><script type="math/tex; mode=display">P(\mathbf{C}|\mathbf{X}) = \frac{P(\mathbf{X}|\mathbf{C})P(\mathbf{C})}{P(\mathbf{X})} \tag{2}</script><p>对于式2中，<script type="math/tex">P(\mathbf{C})</script>是先验估计， <script type="math/tex">P(\mathbf{C}|\mathbf{X})</script>是后验，<script type="math/tex">P(\mathbf{X}|\mathbf{C}</script> 是似然，整个bayes最关键部分。</p><p>又因为我们要求的是给定<script type="math/tex">P(\mathbf{X})</script>对应每一类别的概率：</p><script type="math/tex; mode=display">P(\mathbf{C_j}|\mathbf{X}) = \frac{\prod_{j=1}^{m} P(\mathbf{X}|\mathbf{C_j})P(\mathbf{C_j})}{\sum_{k=1}^ P(\mathbf{X}|\mathbf{C_k)}P(\mathbf{C_k})} \tag{3}</script><p>式3中分母就是归一化因子。</p><p>因此式3可以简化成：</p><script type="math/tex; mode=display">P(\mathbf{C_j}|\mathbf{X}) \propto \prod_{j=1}^{m} P(\mathbf{X}|\mathbf{C_j})P(\mathbf{C_j}) \tag{4}</script><p>一般情况下会取对数简化计算：</p><script type="math/tex; mode=display">P(\mathbf{C_j}|\mathbf{X}) \propto \sum_{j=1}^{m}\log P(\mathbf{X}|\mathbf{C_j})P(\mathbf{C_j}) \tag{5}</script><h4 id="2-垃圾邮件分类代码实战"><a href="#2-垃圾邮件分类代码实战" class="headerlink" title="2. 垃圾邮件分类代码实战"></a>2. 垃圾邮件分类代码实战</h4><p><code>spam.py</code>部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba;</span><br><span class="line"><span class="keyword">import</span> os;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">spamEmailBayes</span>:</span></span><br><span class="line">    <span class="comment">#获得停用词表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getStopWords</span>(<span class="params">self</span>):</span></span><br><span class="line">        stopList=[]</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&quot;../data/中文停用词表.txt&quot;</span>):</span><br><span class="line">            stopList.append(line[:<span class="built_in">len</span>(line)-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> stopList;</span><br><span class="line">    <span class="comment">#获得词典</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_list</span>(<span class="params">self,content,wordsList,stopList</span>):</span></span><br><span class="line">        <span class="comment">#分词结果放入res_list</span></span><br><span class="line">        res_list = <span class="built_in">list</span>(jieba.cut(content))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> res_list:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> stopList <span class="keyword">and</span> i.strip()!=<span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> i!=<span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> wordsList:</span><br><span class="line">                    wordsList.append(i)</span><br><span class="line">                    </span><br><span class="line">    <span class="comment">#若列表中的词已在词典中，则加1，否则添加进去</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addToDict</span>(<span class="params">self,wordsList,wordsDict</span>):</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> wordsList:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> wordsDict.keys():</span><br><span class="line">                wordsDict[item]+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                wordsDict.setdefault(item,<span class="number">1</span>)</span><br><span class="line">                            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_File_List</span>(<span class="params">self,filePath</span>):</span></span><br><span class="line">        filenames=os.listdir(filePath)</span><br><span class="line">        <span class="keyword">return</span> filenames</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#通过计算每个文件中p(s|w)来得到对分类影响最大的15个词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getTestWords</span>(<span class="params">self,testDict,spamDict,normDict,normFilelen,spamFilelen</span>):</span></span><br><span class="line">        wordProbList=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word,num  <span class="keyword">in</span> testDict.items():</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> spamDict.keys() <span class="keyword">and</span> word <span class="keyword">in</span> normDict.keys():</span><br><span class="line">                <span class="comment">#该文件中包含词个数</span></span><br><span class="line">                pw_s=spamDict[word]/spamFilelen</span><br><span class="line">                pw_n=normDict[word]/normFilelen</span><br><span class="line">                ps_w=pw_s/(pw_s+pw_n) </span><br><span class="line">                wordProbList.setdefault(word,ps_w)</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> spamDict.keys() <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> normDict.keys():</span><br><span class="line">                pw_s=spamDict[word]/spamFilelen</span><br><span class="line">                pw_n=<span class="number">0.01</span></span><br><span class="line">                ps_w=pw_s/(pw_s+pw_n) </span><br><span class="line">                wordProbList.setdefault(word,ps_w)</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> spamDict.keys() <span class="keyword">and</span> word <span class="keyword">in</span> normDict.keys():</span><br><span class="line">                pw_s=<span class="number">0.01</span></span><br><span class="line">                pw_n=normDict[word]/normFilelen</span><br><span class="line">                ps_w=pw_s/(pw_s+pw_n) </span><br><span class="line">                wordProbList.setdefault(word,ps_w)</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> spamDict.keys() <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> normDict.keys():</span><br><span class="line">                <span class="comment">#若该词不在脏词词典中，概率设为0.4</span></span><br><span class="line">                wordProbList.setdefault(word,<span class="number">0.47</span>)</span><br><span class="line">        <span class="built_in">sorted</span>(wordProbList.items(),key=<span class="keyword">lambda</span> d:d[<span class="number">1</span>],reverse=<span class="literal">True</span>)[<span class="number">0</span>:<span class="number">15</span>]</span><br><span class="line">        <span class="keyword">return</span> (wordProbList)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算贝叶斯概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calBayes</span>(<span class="params">self,wordList,spamdict,normdict</span>):</span></span><br><span class="line">        ps_w=<span class="number">1</span></span><br><span class="line">        ps_n=<span class="number">1</span></span><br><span class="line">         </span><br><span class="line">        <span class="keyword">for</span> word,prob <span class="keyword">in</span> wordList.items() :</span><br><span class="line">            <span class="built_in">print</span>(word+<span class="string">&quot;/&quot;</span>+<span class="built_in">str</span>(prob))</span><br><span class="line">            ps_w*=(prob)</span><br><span class="line">            ps_n*=(<span class="number">1</span>-prob)</span><br><span class="line">        p=ps_w/(ps_w+ps_n)</span><br><span class="line"><span class="comment">#         print(str(ps_w)+&quot;////&quot;+str(ps_n))</span></span><br><span class="line">        <span class="keyword">return</span> p        </span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算预测结果正确率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calAccuracy</span>(<span class="params">self,testResult</span>):</span></span><br><span class="line">        rightCount=<span class="number">0</span></span><br><span class="line">        errorCount=<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> name ,catagory <span class="keyword">in</span> testResult.items():</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">int</span>(name)&lt;<span class="number">1000</span> <span class="keyword">and</span> catagory==<span class="number">0</span>) <span class="keyword">or</span>(<span class="built_in">int</span>(name)&gt;<span class="number">1000</span> <span class="keyword">and</span> catagory==<span class="number">1</span>):</span><br><span class="line">                rightCount+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                errorCount+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> rightCount/(rightCount+errorCount)</span><br><span class="line">    </span><br><span class="line">  </span><br></pre></td></tr></table></figure><p><code>main.py</code> 部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spam.spamEmail <span class="keyword">import</span> spamEmailBayes</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#spam类对象</span></span><br><span class="line">spam=spamEmailBayes()</span><br><span class="line"><span class="comment">#保存词频的词典</span></span><br><span class="line">spamDict=&#123;&#125;</span><br><span class="line">normDict=&#123;&#125;</span><br><span class="line">testDict=&#123;&#125;</span><br><span class="line"><span class="comment">#保存每封邮件中出现的词</span></span><br><span class="line">wordsList=[]</span><br><span class="line">wordsDict=&#123;&#125;</span><br><span class="line"><span class="comment">#保存预测结果,key为文件名，值为预测类别</span></span><br><span class="line">testResult=&#123;&#125;</span><br><span class="line"><span class="comment">#分别获得正常邮件、垃圾邮件及测试文件名称列表</span></span><br><span class="line">normFileList=spam.get_File_List(<span class="string">&quot;./../data/normal&quot;</span>)</span><br><span class="line">spamFileList=spam.get_File_List(<span class="string">&quot;./../data/spam&quot;</span>)</span><br><span class="line">testFileList=spam.get_File_List(<span class="string">&quot;./../data/test&quot;</span>)</span><br><span class="line"><span class="comment">#获取训练集中正常邮件与垃圾邮件的数量</span></span><br><span class="line">normFilelen=<span class="built_in">len</span>(normFileList)</span><br><span class="line">spamFilelen=<span class="built_in">len</span>(spamFileList)</span><br><span class="line"><span class="comment">#获得停用词表，用于对停用词过滤</span></span><br><span class="line">stopList=spam.getStopWords()</span><br><span class="line"><span class="comment">#获得正常邮件中的词频</span></span><br><span class="line"><span class="keyword">for</span> fileName <span class="keyword">in</span> normFileList:</span><br><span class="line">    wordsList.clear()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&quot;./../data/normal/&quot;</span>+fileName):</span><br><span class="line">        <span class="comment">#过滤掉非中文字符</span></span><br><span class="line">        rule=re.<span class="built_in">compile</span>(<span class="string">r&quot;[^\u4e00-\u9fa5]&quot;</span>)</span><br><span class="line">        line=rule.sub(<span class="string">&quot;&quot;</span>,line)</span><br><span class="line">        <span class="comment">#将每封邮件出现的词保存在wordsList中</span></span><br><span class="line">        spam.get_word_list(line,wordsList,stopList)</span><br><span class="line">    <span class="comment">#统计每个词在所有邮件中出现的次数</span></span><br><span class="line">    spam.addToDict(wordsList, wordsDict)</span><br><span class="line">normDict=wordsDict.copy()  </span><br><span class="line"></span><br><span class="line"><span class="comment">#获得垃圾邮件中的词频</span></span><br><span class="line">wordsDict.clear()</span><br><span class="line"><span class="keyword">for</span> fileName <span class="keyword">in</span> spamFileList:</span><br><span class="line">    wordsList.clear()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&quot;./../data/spam/&quot;</span>+fileName):</span><br><span class="line">        rule=re.<span class="built_in">compile</span>(<span class="string">r&quot;[^\u4e00-\u9fa5]&quot;</span>)</span><br><span class="line">        line=rule.sub(<span class="string">&quot;&quot;</span>,line)</span><br><span class="line">        spam.get_word_list(line,wordsList,stopList)</span><br><span class="line">    spam.addToDict(wordsList, wordsDict)</span><br><span class="line">spamDict=wordsDict.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试邮件</span></span><br><span class="line"><span class="keyword">for</span> fileName <span class="keyword">in</span> testFileList:</span><br><span class="line">    testDict.clear( )</span><br><span class="line">    wordsDict.clear()</span><br><span class="line">    wordsList.clear()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&quot;./../data/test/&quot;</span>+fileName):</span><br><span class="line">        rule=re.<span class="built_in">compile</span>(<span class="string">r&quot;[^\u4e00-\u9fa5]&quot;</span>)<span class="comment">#中文</span></span><br><span class="line">        line=rule.sub(<span class="string">&quot;&quot;</span>,line)</span><br><span class="line">        spam.get_word_list(line,wordsList,stopList)</span><br><span class="line">    spam.addToDict(wordsList, wordsDict)</span><br><span class="line">    testDict=wordsDict.copy()</span><br><span class="line">    <span class="comment">#通过计算每个文件中p(s|w)来得到对分类影响最大的15个词</span></span><br><span class="line">    wordProbList=spam.getTestWords(testDict, spamDict,normDict,normFilelen,spamFilelen)</span><br><span class="line">    <span class="comment">#对每封邮件得到的15个词计算贝叶斯概率  </span></span><br><span class="line">    p=spam.calBayes(wordProbList, spamDict, normDict)</span><br><span class="line">    <span class="keyword">if</span>(p&gt;<span class="number">0.9</span>):</span><br><span class="line">        testResult.setdefault(fileName,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        testResult.setdefault(fileName,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#计算分类准确率（测试集中文件名低于1000的为正常邮件）</span></span><br><span class="line">testAccuracy=spam.calAccuracy(testResult)</span><br><span class="line"><span class="keyword">for</span> i,ic <span class="keyword">in</span> testResult.items():</span><br><span class="line">    <span class="built_in">print</span>(i+<span class="string">&quot;/&quot;</span>+<span class="built_in">str</span>(ic))</span><br><span class="line"><span class="built_in">print</span>(testAccuracy)  </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Naive Bayes </tag>
            
            <tag> 垃圾邮件分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导4.2 线性分类——高斯判别模型</title>
      <link href="2020/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.2%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.2%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导4-2-线性分类——高斯判别模型"><a href="#机器学习-白板推导4-2-线性分类——高斯判别模型" class="headerlink" title="机器学习-白板推导4.2 线性分类——高斯判别模型"></a>机器学习-白板推导4.2 线性分类——高斯判别模型</h3><h4 id="1-高斯判别模型描述"><a href="#1-高斯判别模型描述" class="headerlink" title="1. 高斯判别模型描述"></a>1. 高斯判别模型描述</h4><p><strong>Gaussian Discriminate Analysis</strong>——GDA 模型描述。</p><p>假定数据集：</p><script type="math/tex; mode=display">\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}, \ 其中x_i\in\mathbb{R}^{p}，y_i\in\mathbb{R}，i=1, \ 2,\cdots,\ N。</script><p>数据矩阵为：</p><script type="math/tex; mode=display">\begin{equation}    X=(x_1, x_2, \cdots, x_N)^T=    \begin{pmatrix}    x_1^T \\     x_2^T \\    \vdots\\    x_N^T \\    \end{pmatrix} =    \begin{pmatrix}    x_{11} & x_{12} & \dots & x_{1p}\\    x_{21} & x_{32} & \dots & x_{2p}\\    \vdots & \vdots & \ddots & \vdots\\    x_{N1} & x_{N2} & \dots & x_{Np}\\    \end{pmatrix}_{N\times P}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}    Y=    \begin{pmatrix}    y_1 \\     y_2 \\    \vdots\\    y_N \\    \end{pmatrix}_{N\times 1}\end{equation}</script><p>这里讨论高斯判别分析解决二分类问题，那么对于数据集<script type="math/tex">\{ (x_i, y_i)\}</script>, 有。数据集分成+1, -1两部分并增加以下约定：</p><script type="math/tex; mode=display">\begin{array}{ll}            C_1 = \left\{ x_i|y_i=1, i=1,2,\cdots,N_1 \right\} & \\            C_2 = \left\{ x_i|y_i=0, i=1,2,\cdots,N_2 \right\} & \\        \end{array}</script><p>其中，<script type="math/tex">|C_1|=N_1，|C_2|=N_2，且N_1+N_2=N</script>。</p><p>现在用bayes公式来生成模型：</p><script type="math/tex; mode=display">p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{p(x,y)}{p(x)}\propto p(x,y)</script><p>其中，<script type="math/tex">p(y)</script>是先验Prior， <script type="math/tex">p(y|x)</script>是后验Posterior。</p><script type="math/tex; mode=display">\hat y = \text{argmax }_{y \in {0, 1}} p(x|y)p(y)</script><h4 id="2-高斯判别模型建立"><a href="#2-高斯判别模型建立" class="headerlink" title="2. 高斯判别模型建立"></a>2. 高斯判别模型建立</h4><p>因为是二分类问题，可以假定先验概率符合伯努利分布，<script type="math/tex">p(y)\sim \text{Bernoulli Distribution}</script>.</p><script type="math/tex; mode=display">\begin{array}{c|cc}\mathrm{y} & 1 & 0 \\\hline \mathrm{p} & \varphi & 1-\varphi\end{array}</script><p>所以，其表达式为：</p><script type="math/tex; mode=display">\begin{equation}    p(y)=    \left\{        \begin{array}{ll}            \varphi^y & y=1 \\            (1-\varphi)^{1-y} & y=0        \end{array}    \right.    \Longrightarrow    \varphi^y(1-\varphi)^{1-y}\end{equation}</script><p>我们假设两类别数据<script type="math/tex">p(x|y)</script>都符合高斯分布，其均值不同，但不同变量间的协方差矩阵一样，(其实协方差不一样结论也一样，只是计算复杂)因此有：</p><script type="math/tex; mode=display">\begin{equation}    p(x|y)=    \left\{        \begin{array}{ll}            p(x|y=1)\sim \mathcal{N}(\mu_1, \Sigma) & \\            p(x|y=0)\sim \mathcal{N}(\mu_2, \Sigma) & \\        \end{array}    \right.    \Longrightarrow    \mathcal{N}(\mu_1, \Sigma)^y\mathcal{N}(\mu_2, \Sigma)^{1-y}\end{equation}</script><p>后验估计函数为：</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        \mathcal{L}(\theta) = & \log\prod_{i=1}^Np(x_i,y_i) \\         = & \sum_{i=1}^N\log p(x_i,y_i) \\         = & \sum_{i=1}^N\log p(x_i|y_i)p(y_i) \\         = & \sum_{i=1}^N\left[ \log p(x_i|y_i)+ \log p(y_i) \right]\\         = & \sum_{i=1}^N\left[ \log \mathcal{N}(\mu_1, \Sigma)^{y_i}\mathcal{N}(\mu_2, \Sigma)^{1-y_i}+ \log \varphi^y_i(1-\varphi)^{1-y_i} \right]\\         = & \sum_{i=1}^N \log \mathcal{N}(\mu_1, \Sigma)^y_i + \sum_{i=1}^N \log \mathcal{N}(\mu_2, \Sigma)^{1-y_i}+ \sum_{i=1}^N \log \varphi^{y_i} + \sum_{i=1}^N \log (1-\varphi)^{1-y_i} \\    \end{split}\end{equation} \tag{1}</script><p>为了推导方便，记：</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}_1&  = \sum_{i=1}^N \log \mathcal{N}(\mu_1, \Sigma)^{y_i} \\\mathcal{L}_2&  =\sum_{i=1}^N \log \mathcal{N}(\mu_2, \Sigma)^{1-y_i} \\\mathcal{L}_3&  =\sum_{i=1}^N \log \varphi^{y_i} + \sum_{i=1}^N \log (1-\varphi)^{1-y_i}\end{align} \tag{2}</script><p>那么上述函数可以表示为：</p><script type="math/tex; mode=display">\theta = (\mu_1,\mu_2,\Sigma,\varphi) \qquad \hat{\theta} = \text{argmax }_{\theta} \mathcal{L}(\theta) \tag{3}</script><h4 id="3-参数求解"><a href="#3-参数求解" class="headerlink" title="3. 参数求解"></a>3. 参数求解</h4><h5 id="1-求解-varphi"><a href="#1-求解-varphi" class="headerlink" title="1. 求解\varphi"></a>1. 求解<script type="math/tex">\varphi</script></h5><p>只有<script type="math/tex">\mathcal{L}_3</script>跟参数<script type="math/tex">\varphi</script>有关，对其求导得：</p><script type="math/tex; mode=display">\begin{align}    \frac{\partial   \mathcal{L}_3}{\partial \varphi} &= \sum_{i=1}^Ny_i\frac{1}{\varphi} + \sum_{i=1}^N (1-y_i)\frac{1}{1-\varphi} = 0 \\  \Longrightarrow&  \sum_{i=1}^N y_i(1-\varphi) - (1-y_i)\varphi = 0 \\    \Longrightarrow&  \sum_{i=1}^N (y_i-\varphi) = 0\\    \Longrightarrow& \hat{\varphi} = \frac{1}{N} \sum_{i=1}^N y_i\end{align} \tag{4}</script><p>因为只有类别为<script type="math/tex">C_1</script>中<script type="math/tex">y_i = 1</script>, 式4可以简化成：</p><script type="math/tex; mode=display">\hat{\varphi} = \frac{1}{N} \sum_{i=1}^{N_1} y_i = \frac{N_1}{N} \tag{5}</script><h5 id="2-求解-mu"><a href="#2-求解-mu" class="headerlink" title="2. 求解\mu"></a>2. 求解<script type="math/tex">\mu</script></h5><p>因为<script type="math/tex">\mu_1</script>只跟<script type="math/tex">\mathcal{L}_1</script>部分有关, 展开后得：</p><script type="math/tex; mode=display">\begin{align}&\sum_{i=1}^N\log \mathcal{N}(\mu_1,\Sigma)^{y_i} \\\\    = &\sum_{i=1}^Ny_i\log \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \text{exp }\left\{ -\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1) \right\}\\    = &\sum_{i=1}^Ny_i\log \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} - \sum_{i=1}^Ny_i (\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1))\\\\    \end{align} \tag{6}</script><p>式6只有后半部分跟<script type="math/tex">\mu_1</script>有关，剔除前面部分并化简：</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}_{\mu_1} &= \sum_{i=1}^Ny_i (\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1))\\ &= \frac{1}{2}\sum_{i=1}^Ny_i (x_i^T\Sigma^{-1} - \mu_1^T\Sigma^{-1})(x_i-\mu_1)\\ &= \frac{1}{2}\sum_{i=1}^N  y_i( x_i^T\Sigma^{-1}x_i - x_i^T\Sigma^{-1}\mu_1 - \mu_1^T\Sigma^{-1}x_i +\mu_1^T\Sigma^{-1}\mu_1 )\\ \end{align} \tag{7}</script><p>因为<script type="math/tex">x_i</script>是<script type="math/tex">p \times 1</script>的，那么<script type="math/tex">\Sigma^{-1}</script>是<script type="math/tex">p \times p</script>， 所以<script type="math/tex">x_i^T\Sigma^{-1}\mu_1 = \mu_1^T\Sigma^{-1}x_i</script> （都是实数）。并且<script type="math/tex">x_i^T\Sigma^{-1}x_i</script>跟<script type="math/tex">\mu_1</script>无关</p><p>进一步简化得：</p><script type="math/tex; mode=display">\Delta = \frac{1}{2}\sum_{i=1}^Ny_i (- 2\mu_1^T\Sigma^{-1}x_i + \mu_1^T\Sigma^{-1}\mu_1) \tag{8}</script><p>对<script type="math/tex">\mu_1</script>求导得：</p><script type="math/tex; mode=display">\frac{\partial   \Delta}{\partial \mu_1} = \sum_{i=1}^N(-y_i\Sigma^{-1}x_i + y_i\Sigma^{-1}\mu_1)\tag{9}</script><p>令式9为0得：</p><script type="math/tex; mode=display">\sum_{i=1}^N y_i\mu_1 = \sum_{i=1}^Ny_ix_i \Longrightarrow \hat{\mu_1}= \frac{\sum_{i=1}^Ny_ix_i}{\sum_{i=1}^Ny_i} =\frac{\sum_{i=1}^{N_1}x_i}{N_1}\tag{10}</script><p>式10最后简化原因：<script type="math/tex">y_i \in \{0, 1\}</script>，只有当<script type="math/tex">y_i=1</script>时，累加才有用，所以</p><script type="math/tex; mode=display">\sum_{i=1}^Ny_ix_i =\sum_{i=1}^{N_1}x_i</script><p>因为对称性，同理可得<script type="math/tex">\hat{\mu_2} = \frac{\sum_{i=1}^{N_2}x_i}{N_2}</script></p><h5 id="3-求解-Sigma"><a href="#3-求解-Sigma" class="headerlink" title="3. 求解 \Sigma"></a>3. 求解 <script type="math/tex">\Sigma</script></h5><p>跟<script type="math/tex">\Sigma</script>有关的只有<script type="math/tex">\mathcal{L}_1, \mathcal{L}_2</script>, 即：</p><script type="math/tex; mode=display">\Omega = \sum_{i=1}^N \log \mathcal{N}(\mu_1, \Sigma)^{y_i} +\sum_{i=1}^N \log \mathcal{N}(\mu_2, \Sigma)^{1-y_i}\\=  \sum_{i=1}^N ({y_i}\log \mathcal{N}(\mu_1, \Sigma) + (1-y_i)\log \mathcal{N}(\mu_2, \Sigma)) \tag{11}</script><p>同样因为<script type="math/tex">y_i</script>要么0， 要么1，式11可以简化为：</p><script type="math/tex; mode=display">\Omega =  \sum_{i=1}^{N_1} \log \mathcal{N}(\mu_1, \Sigma) + \sum_{i=1}^{N_2} \log \mathcal{N}(\mu_2, \Sigma) \tag{12}</script><p>为了简化计算，我们先计算下：</p><script type="math/tex; mode=display">\begin{align}\hat\Omega &= \sum_{i=1}^{N} \log \mathcal{N}(\mu, \Sigma) \\&= \sum_{i=1}^{N} \log \frac{1}{(2\pi)^{\frac{p}{2}} |\Sigma|^{\frac{1}{2}}} \exp  (-\frac{1}{2}(x_i -\mu)^T \Sigma^{-1}(x_i-\mu)) \\&= \sum_{i=1}^{N} \left [ \log \frac{1}{(2\pi)^{\frac{p}{2}} } -\frac{1}{2} \log |\Sigma|  -\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu) \right]\\&= \sum_{i=1}^{N}\log \frac{1}{(2\pi)^{\frac{p}{2}}} + \sum_{i=1}^{N}(-\frac{1}{2} \log |\Sigma|  -\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)) \\&= C + \sum_{i=1}^{N}(-\frac{1}{2} \log |\Sigma|  -\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)  \\&= C  -\frac{1}{2} N \log |\Sigma| -\frac{1}{2}\sum_{i=1}^{N}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\\\end{align}\tag{13}</script><p>其中<script type="math/tex">\sum_{i=1}^{N}\log \frac{1}{(2\pi)^{\frac{p}{2}}} = C</script>, 简化计算。</p><p>首先，有下列公式：</p><script type="math/tex; mode=display">\begin{gather}    \text{tr}(AB) = tr(BA) \tag{14}\\    \text{tr} (ABC) = \text{tr}(CAB) = \text{tr}(BCA)\tag{15}\\    \frac{\partial tr(AB)}{\partial A} = B^T \tag{16}\\    \frac{\partial|A|}{\partial A} = |A|A^{-1} \tag{17}\\\end{gather}</script><p>式16证明：<a href="https://math.stackexchange.com/questions/2573030/gradient-of-mboxtrab">Gradient of tr(AB)</a></p><p>式13对<script type="math/tex">\Sigma</script>求导得：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial \hat \Omega}{\partial \Sigma} &= -\frac{1}{2} N\frac{1}{|\Sigma|}|\Sigma| \Sigma^{-1} - \frac{1}{2} \frac{\sum_{i=1}^{N}\text{ tr} [ (x_i-\mu)^T\Sigma^{-1}(x_i-\mu) ] }{\partial \Sigma} \\&= -\frac{1}{2} N  \Sigma^{-1}-\frac{1}{2} \frac{\sum_{i=1}^{N}\text{ tr} [ (x_i-\mu)^T(x_i-\mu) \Sigma^{-1}] }{\partial \Sigma} \\&=-\frac{1}{2} N  \Sigma^{-1}-\frac{1}{2} \frac{\text{ tr} \sum_{i=1}^{N}[ (x_i-\mu)^T(x_i-\mu) \Sigma^{-1}] }{\partial \Sigma} \\&= -\frac{1}{2} N  \Sigma^{-1}-\frac{1}{2} \frac{\text{ tr} (NS \Sigma^{-1})}{\partial \Sigma}  \end{align}\tag{18}</script><p>其中<script type="math/tex">S = \frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^T(x_i-\mu)</script>是协方差矩阵。</p><p>由式18写出式12：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial  \Omega}{\partial \Sigma}&= -\frac{1}{2} N_1 \Sigma^{-1} - \frac{1}{2} \frac{\text{ tr} (N_1S \Sigma^{-1})}{\partial \Sigma} -\frac{1}{2} N_2 \Sigma^{-1} - \frac{1}{2} \frac{\text{ tr} (N_2S \Sigma^{-1})}{\partial \Sigma} \\&= -\frac{1}{2}N \Sigma^{-1}-\frac{1}{2}(N_1S_1^T(-1)\Sigma^{-2} + N_2S_2^T(-1)\Sigma^{-2})\\&=\frac{1}{2}(N_1S_1^T\Sigma^{-2} +N_2S_2^T\Sigma^{-2} -N \Sigma^{-1} )\end{align} \tag{19}</script><p>令其为0得两边同乘以<script type="math/tex">\Sigma^{-2}</script>得：</p><script type="math/tex; mode=display">N_1S_1 + N_2S_2 = N\Sigma \Longrightarrow \Sigma = \frac{N_1S_1 + N_2S_2 }{N}</script><p>可以理解为协方差矩阵等于各自协方差矩阵的加权和。</p><p><strong>高斯判别分析总结</strong>：</p><ol><li>首先假设先验<script type="math/tex">p(y)</script> 服从伯努利分布和似然 <script type="math/tex">p(x|y)</script>服从高斯分布，利用贝叶斯公式得到条件概率<script type="math/tex">p(y|x)</script>函数。</li><li>利用极大似然估计求解最大值参数。</li></ol><script type="math/tex; mode=display">\begin{align}\hat{\varphi} &=\frac{N_1}{N} \\\hat{\mu_1} &= \frac{\sum_{i=1}^{N_1}x_i}{N_1}\\\hat{\mu_2} &= \frac{\sum_{i=1}^{N_2}x_i}{N_2}\\\Sigma &= \frac{N_1S_1 + N_2S_2 }{N}\end{align}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性分类 </tag>
            
            <tag> 高斯判别分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 4.1 线性分类</title>
      <link href="2020/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.1%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/"/>
      <url>2020/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%204.1%20%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-4-1-线性分类"><a href="#机器学习-白板推导-4-1-线性分类" class="headerlink" title="机器学习-白板推导 4.1 线性分类"></a>机器学习-白板推导 4.1 线性分类</h3><h4 id="1-从线性回归到线性分类"><a href="#1-从线性回归到线性分类" class="headerlink" title="1. 从线性回归到线性分类"></a>1. 从线性回归到线性分类</h4><p> <strong>线性回归的特性及将线性回归转为线性分类的办法</strong></p><p>线性回归模型可以写作<script type="math/tex">f(w, b) = w^Tx + b</script>。其线性体现在三个方面：</p><ol><li>属性线性：<script type="math/tex">f(w, b)</script>关于 <script type="math/tex">x</script>是线性的</li><li>全局线性：指<script type="math/tex">w^Tx + b</script>是一个线性组合，然后输出得到<script type="math/tex">f(w, b)</script></li><li>系数线性：指<script type="math/tex">f(w, b</script> 关于<script type="math/tex">w^T</script>也是线性的</li></ol><p>如果将某一种线性改为非线性就可以得到一种非线性模型。对应有：</p><ol><li>将<strong>属性改为非线性</strong>：可以用特征转换(多项式回归)，如<script type="math/tex">a_1x_1^2 + a_2x_2^2 + \cdots</script></li><li>将<strong>全局线性改为非线性</strong>：加入激活函数，让输出变成非线性，比如logistic 分类，就变成了线性分类</li><li>将<strong>系数线性改为非线性</strong>：系数会改变，比如神经网络，通过反向传播改变权重。</li></ol><p>而线性回归还有<strong>全局性</strong>和<strong>数据未加工</strong>的特性：</p><ol><li><strong>全局性</strong>：指线性回归是在整个特征空间上学习的，并没有将特征空间进行划分，然后在每个特征上学习。</li><li><strong>数据未加工</strong>：线性回归直接在给定数据上进行学习而没有对数据进行处理,如PCA，流形等。</li></ol><p>将<strong>全局性打破</strong>，即不根据所有点的情况回归，而是将数据分为一个个小的空间，对每个子空间进行回归，如决策树模型。</p><p><strong>总结</strong>：</p><script type="math/tex; mode=display">\begin{align}\text {线性回归}  \left\{\begin{array}{l}1. 线性 \stackrel{转换为非线性} \longrightarrow\left\{\begin{array}{l}1. 属性非线性：特征转换(多项式回归)\\2. 全局非线性: 线性分类(激活函数是非线性)\\3. 系数非线性: 神经网络，感知机 \\\end{array}\right.\\\text {2.全局性 }\stackrel{转换为非线性}\longrightarrow 决策树\\\text {3. 数据未加工 }\stackrel{转换为非线性}\longrightarrow PCA，流形\end{array}\right.\end{align}</script><p><strong>线性分类分类</strong>：</p><script type="math/tex; mode=display">\text { 线性分类 }\left\{\begin{array}{l}\text { 硬分类 }\left\{\begin{array}{l}\text { 线性判别分析:Fisher } \\\text { 感知机: }\text {perception }\end{array}\right. \\\text { 软分类 }\left\{\begin{array}{l}\text { 概率生成模型: 高斯判别分析(连续),} \ \ \text{Naive Bayes}(离散)  \\\text { 概率判别模型: logistic regression}\end{array}\right.\end{array}\right.</script><h4 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2. 感知机"></a>2. 感知机</h4><ol><li>概述</li></ol><p>假设有一可被线性分类的样本集<script type="math/tex">\left \{ (x_i, y_i) \right \}_{i=1}^N</script>,其中<script type="math/tex">x_i \in \mathbb{R}^p, y_i \in \{-1, +1 \}</script>.那么感知机算法可以使用SGD来在特征空间<script type="math/tex">\mathbb{R}^p</script>中找到一个超平面<script type="math/tex">w^Tx+b=0</script>可以将其分为两类，其中<script type="math/tex">w\in \mathbb{R}^p</script>,是这个超平面的法向量。</p><p>感知机算法是错误驱动的算法，可以理解为不断调整这个超平面来使得误分类错误点越少。</p><ol><li>具体算法</li></ol><p>对于所有的样本数据：</p><ul><li>在<script type="math/tex">y_i = + 1</script>的样本点中，有 <script type="math/tex">w^T \cdot x + b > 0</script></li><li>在<script type="math/tex">y_i = - 1</script>的样本点中，有 <script type="math/tex">w^T \cdot x + b < 0</script></li></ul><p>可直接改写为<script type="math/tex">y_i(w^T \cdot x + b)>0</script>代表正确分类，反之就是错误分类。</p><p>而特征空间<script type="math/tex">\mathbb{R}^p</script>中任意一点<script type="math/tex">x_0</script>到超平面的距离为 ：</p><script type="math/tex; mode=display">\frac{1}{\lVert w\rVert} \lvert w ^T\cdot x_0 +b \rvert \tag{1}</script><p>所有误分类点到超平面总距离为：</p><script type="math/tex; mode=display">\sum_{x_i \in M}\frac{1}{\lVert w\rVert} \lvert w \cdot x_i +b \rvert= -\frac{1}{\lVert w\rVert} \sum_{x_i \in M} y_i( w^T \cdot x_i +b ) \tag{2}</script><p>不考虑<script type="math/tex">\frac{1}{\lVert w\rVert}</script>，就可以得到感知机损失函数：</p><script type="math/tex; mode=display">L(w, b) = -\sum_{x_i \in M} y_i( w^T \cdot x_i +b ) \tag{3}</script><ol><li>学习算法</li></ol><p>对损失函数求<script type="math/tex">w, b</script>梯度得：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L(w, b)}{\partial w} &=-\sum_{x_{i} \in M} y_{i} x_{i} \\\frac{\partial L(w, b)}{\partial b} &=-\sum_{x_{i} \in M} y_{i}\end{aligned} \tag{4}</script><p>实际训练步骤如下( <script type="math/tex">0\lt \eta \le 1</script>)：[李航——统计学习方法P39]</p><ol><li><p>选取初值 <script type="math/tex">w_0, b_0</script></p></li><li><p>在训练集中选取数据<script type="math/tex">(x_i, y_i)</script></p></li><li><p>如果<script type="math/tex">y_i(wx_i + b) \le 0</script>， 则更新参数：</p><script type="math/tex; mode=display">w \leftarrow w+ \eta y_i w_i \\b  \leftarrow b+ \eta y_i \tag{5}</script></li><li><p>转至2，直到训练集中没有误分类点</p></li></ol><h4 id="3-线性判别分析"><a href="#3-线性判别分析" class="headerlink" title="3. 线性判别分析"></a>3. 线性判别分析</h4><ol><li>算法思想</li></ol><p>假设数据集为<script type="math/tex">\left \{ (x_i, y_i) \right \}_{i=1}^N</script>, 其中 <script type="math/tex">x_i \in \mathbb{R}^p</script>，<script type="math/tex">y_i\in\{+1,-1\}</script>，记<script type="math/tex">\{y=+1\}</script> 为 <script type="math/tex">C_1</script>类，<script type="math/tex">\{y=-1\}</script>为<script type="math/tex">C_2</script>类。那么，<script type="math/tex">X_{c_1}</script>为<script type="math/tex">\left\{ x_i|y_i=+1 \right\}</script>，<script type="math/tex">X_{c_2}</script>为<script type="math/tex">\left\{ x_i|y_i=-1 \right\}</script>,  <script type="math/tex">|X_{c_1}|=N_1,\ |X_{c_2}|=N_2</script> 且<script type="math/tex">N_1+N_2=N</script>。</p><p>LDA (Linear Discriminant Analysis) 的想法是： 设法将数据样本投影到一条直线上，使得同类别样本的投影点尽可能接近、异类样本投影点尽可能远。总结来说就是， 类内小、类间大，具体是指类内方差之和小，类间的均值之差大。具体图示如下：(出自： 周志华——《机器学习》”西瓜书“ P60)</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856491.png" alt="image-20210523173405131" style="zoom:25%;" /></p><ol><li>算法推导</li></ol><p>若将数据投影到直线<script type="math/tex">w</script>上，则样本点投影到该直线后值<script type="math/tex">z_i</script>为<script type="math/tex">w^Tx_i</script>。样本均值和方差按以下公式计算：</p><script type="math/tex; mode=display">\begin{align}\bar{z}&=\frac{1}{N}\sum_{i=1}^{N}z_i = \frac{1}{N}\sum_{i=1}^{N}w^Tx_i \\\\    S_z&=\frac{1}{N}\sum_{i=1}^N(z_i-\bar{z})(z_i-\bar{z})^T     \end{align}    \tag{6}</script><p>那么对于第一类分类点<script type="math/tex">X_{c_1}</script>和第二类分类点<script type="math/tex">X_{c_2}</script>可以表述为：</p><script type="math/tex; mode=display">\begin{align}C_1:\qquad \bar{z_1}= \frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i \qquad S_1 =\frac{1}{N_1}\sum_{i=1}^N(z_i-\bar{z_1})(z_i-\bar{z_1})^T\\    C_2:\qquad \bar{z_2}= \frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i \qquad S_2 =\frac{1}{N_2}\sum_{i=1}^N(z_i-\bar{z_2})(z_i-\bar{z_2})^T \\     \end{align}\tag{7}</script><p>那么类间的距离我们可以定义为：$(\bar{z_1}-\bar{z_2})^2$,<br>类内的距离被我们定义为$S_1+S_2$。那么我们的目标函数Target Function $\mathcal{J}(w)$，可以被定义为:</p><script type="math/tex; mode=display">\mathcal{J}(w) = \frac{(\bar{z_1}-\bar{z_2})^2}{S_1+S_2}</script><p>根据目标函数要使得分子越小越好，分母越大越好。即类间距离越大越好，类内的距离越小越好。</p><p><strong>分子</strong>化简：</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        (\bar{z_1}-\bar{z_2})^2         = & \left( \frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i - \frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i \right)^2 \\        = & \left( w^T(\frac{1}{N_1}\sum_{i=1}^{N_1}x_i - \frac{1}{N_2}\sum_{i=1}^{N_2}x_i ) \right)^2 \\        = & \left( w^T(\bar{X}_{c_1} - \bar{X}_{c_2}) \right)^2 \\        = & w^T(\bar{X}_{c_1} - \bar{X}_{c_2})(\bar{X}_{c_1} - \bar{X}_{c_2})^Tw \\    \end{split}\end{equation}</script><p><strong>分母化简</strong>：</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        S_1 = & \frac{1}{N_1}\sum_{i=1}^N(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T \\        = & \frac{1}{N_1}\sum_{i=1}^N(w^Tx_i-\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i)(w^Tx_i-\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i)^T \\        = & w^T\frac{1}{N_1}\sum_{i=1}^N(x_i-\frac{1}{N_1}\sum_{i=1}^{N_1}x_i)(x_i-\frac{1}{N_1}\sum_{i=1}^{N_1}x_i)^Tw \\        = & w^TS_{c_1}w \\    \end{split}\end{equation}</script><p>同理可得，</p><script type="math/tex; mode=display">    S_2= w^TS_{c_2}w</script><p>所以, 分母化简为<script type="math/tex">S_1+S_2=w^T(S_{c_1}+S_{c_2})w</script>。代入目标函数得：</p><script type="math/tex; mode=display"> \mathcal{J}(w)=\frac{w^T(\bar{X}_{c_1} - \bar{X}_{c_2})(\bar{X}_{c_1} - \bar{X}_{c_2})^Tw}{w^T(S_{c_1}+S_{c_2})w}</script><p>定义类间散度矩阵为(within-class scatter matrix)：</p><script type="math/tex; mode=display">S_w = (\bar{X}_{c_1} - \bar{X}_{c_2})(\bar{X}_{c_1} - \bar{X}_{c_2})^T</script><p>类内散度矩阵为(between-class scatter matrix)：</p><script type="math/tex; mode=display">S_w = (S_{c_1}+S_{c_2})</script><p>于是，目标函数改写为:</p><script type="math/tex; mode=display">\mathcal{J}(w)=\frac{w^TS_bw}{w^TS_ww}</script><p>为了方便求导，我们令<script type="math/tex">\mathcal{J}(w)=(w^TS_bw)(w^TS_ww)^{-1}</script> 。</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        \frac{\partial \mathcal{J}(w)}{\partial w} = 2S_bw(w^TS_ww)^{-1} + & (-1)(w^TS_bw)(w^TS_ww)^{-2}(2)S_ww = 0 \\\\        \Longrightarrow         S_bw(w^TS_ww)^{-1} = & (w^TS_bw)(w^TS_ww)^{-2}S_ww\\    \end{split}\end{equation}</script><p>显然，$w$的维度是$p\times 1$，$w^T$的维度是$1 \times p$，$S_w$的维度是$p\times p$，所以，$w^TS_ww$是一个实数，同理 可得，$w^TS_ww$是一个实数所以，</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        S_bw = & (w^TS_bw)(w^TS_ww)^{-1}S_ww \\\\        \Longrightarrow         S_bw = & \frac{(w^TS_bw)}{(w^TS_ww)}S_ww    \end{split}\end{equation}</script><p>我们只要方向，大小由于超平面可以放缩，可以不关注。可以忽略一些实数，</p><script type="math/tex; mode=display">w = \frac{(w^TS_bw)}{(w^TS_ww)}S_b^{-1}S_ww \propto S_b^{-1}S_ww</script><p>而代入<script type="math/tex">S_w</script>有：</p><script type="math/tex; mode=display">S_ww =  (\bar{X}_{c_1} - \bar{X}_{c_2})(\bar{X}_{c_1} - \bar{X}_{c_2})^Tw</script><p> 因为<script type="math/tex">(\bar{X}_{c_1} - \bar{X}_{c_2})^Tw</script>也是个实数，可以忽略。所以<script type="math/tex">w</script>最后正比于：</p><script type="math/tex; mode=display">S_b^{-1}S_ww \propto S_w^{-1}(\bar{X}_{c_1} - \bar{X}_{c_2})</script><p>那么，我们最后求得<script type="math/tex">w</script>方向为<script type="math/tex">S_w^{-1}(\bar{X}_{c_1} - \bar{X}_{c_2})</script>。如果对角矩阵<script type="math/tex">S_w^{-1}</script>是各项同性，那么其正比于单位矩阵，<script type="math/tex">S_w^{-1} \propto I</script>,则有<script type="math/tex">w\propto (\bar{X}_{c_1} - \bar{X}_{c_2})</script>。</p><blockquote><p>实际分类中很少用LDA了，但是其在早期是非常有代表性的算法，可以参考其思想。</p></blockquote><h4 id="3-逻辑回归"><a href="#3-逻辑回归" class="headerlink" title="3. 逻辑回归"></a>3. 逻辑回归</h4><p>逻辑回归的想法是，我们将线性回归得到的值经过一个函数映射后能不能得到一个在<script type="math/tex">[0, 1]</script>的值，这样可以将其看作一个概率值。而sigmoid恰好有这个性质，其定义为：</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><p>如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856492.png" alt="image-20210524185713567" style="zoom:35%;" /></p><p><strong>算法推导</strong>：</p><p>假设数据集为：<script type="math/tex">\left \{ (x_i, y_i) \right \}_{i=1}^N ,\ x_i \in \mathbb{R}^p, y_i \in {0, 1}</script>。</p><p>记：</p><script type="math/tex; mode=display">\begin{align}p_1&=P(y=1|x)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}}  \\p_0&=P(y=0|x)=1-p(y=1|x)=1-\sigma(w^Tx) =\frac{e^{-w^Tx}}{1+e^{-w^Tx}}  \end{align}</script><p>简化后记为：</p><script type="math/tex; mode=display">P(y|x) = p_1^y \cdot p_0^{1-y}</script><p>只要对于所有样本的概率最大就是最优参数，用MLE求解最优<script type="math/tex">\hat w</script>:</p><script type="math/tex; mode=display">\begin{align}\hat{w} =&\underset{w}{\text{argmax }} \log P(y|x) \\\            = & \underset{w}{\text{argmax }}\sum _{i=1}^N \log  P(y_i|x_i) \\            = & \underset{w}{\text{argmax }} \sum _{i=1}^N\log (p_1^{y_i} \cdot p_0^{1-y_i})  \\            = & \underset{w}{\text{argmax }} \sum _{i=1}^N (y_i\log p_1 + (1-y_i) \log p_0) \\ \end{align}</script><p>最后的<script type="math/tex">y_i\log p_1 + (1-y_i) \log p_0</script> 也可以根据交叉熵公式写出，不过要加个负号。</p><p>现在只要求解<script type="math/tex">L(w) = \sum _{i=1}^N (y_i\log p_1 + (1-y_i) \log p_0)</script>最大值。</p><p>其中，<script type="math/tex">p_1, p_0 = 1-p_1</script>表达式如上，并且对sigmoid求导有<script type="math/tex">\sigma^\prime = \sigma(1-\sigma)</script>，因此有：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L(w)}{\partial w} =&  \sum _{i=1}^N (y_i \frac{1}{p_1}p_1(1-p_1)x_i + (1-y_i)\frac{1}{1-p_1}(-1)(1-p_1)x_i)\\=& \sum _{i=1}^N (y_i(1-p_1)x_i - (1-y_i)p_1x_i)\\=& \sum _{i=1}^N (y_ix_i - p_1x_i)\\=& \sum _{i=1}^N (y_i - p_1)x_i\end{align}</script><p>令其为0时，可求出<script type="math/tex">\hat w</script>。但是由于概率是非线性的，该式无法实际求解，实际上使用SGD来求<script type="math/tex">L(w)</script>最大值。当最优解为<script type="math/tex">\hat w</script>时，</p><script type="math/tex; mode=display">\begin{align}P(y=1|x)&=\frac{1}{1+e^{- \hat w^Tx}}  \\P(y=0|x)&=\frac{e^{-\hat w^Tx}}{1+e^{-\hat w^Tx}}  \end{align}</script><p>其中,</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}=-\sum_{i=1}^n (y_i-f(x_i))x_i^* \\w:=w-\eta \frac{\partial L}{\partial w}</script><p>代码实现：引用自[zhulei227] <a href="https://github.com/zhulei227/ML_Notes/blob/master/notebooks/02_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.ipynb">02<em>线性模型</em>逻辑回归.ipynb</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, fit_intercept=<span class="literal">True</span>, solver=<span class="string">&#x27;sgd&#x27;</span>, if_standard=<span class="literal">True</span>, l1_ratio=<span class="literal">None</span>, l2_ratio=<span class="literal">None</span>, epochs=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 eta=<span class="literal">None</span>, batch_size=<span class="number">16</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.fit_intercept = fit_intercept <span class="comment">#截距</span></span><br><span class="line">        self.solver = solver <span class="comment">#求解器</span></span><br><span class="line">        self.if_standard = if_standard <span class="comment">#标准化标志</span></span><br><span class="line">        <span class="keyword">if</span> if_standard:</span><br><span class="line">            self.feature_mean = <span class="literal">None</span></span><br><span class="line">            self.feature_std = <span class="literal">None</span></span><br><span class="line">        self.epochs = epochs</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.l1_ratio = l1_ratio</span><br><span class="line">        self.l2_ratio = l2_ratio</span><br><span class="line">        <span class="comment"># 注册sign函数</span></span><br><span class="line">        self.sign_func = np.vectorize(utils.sign)</span><br><span class="line">        <span class="comment"># 记录losses</span></span><br><span class="line">        self.losses = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">self, n_features</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化参数 w</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.w = np.random.random(size=(n_features, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit_closed_form_solution</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        直接求闭式解</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self._fit_sgd(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit_sgd</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        随机梯度下降求解</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x_y = np.c_[x, y]</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.epochs):</span><br><span class="line">            np.random.shuffle(x_y)</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(x_y.shape[<span class="number">0</span>] // self.batch_size):</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                <span class="comment"># 取每批数据</span></span><br><span class="line">                batch_x_y = x_y[self.batch_size * index:self.batch_size * (index + <span class="number">1</span>)]</span><br><span class="line">                batch_x = batch_x_y[:, :-<span class="number">1</span>]</span><br><span class="line">                batch_y = batch_x_y[:, -<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">                dw = -<span class="number">1</span> * (batch_y - utils.sigmoid(batch_x.dot(self.w))).T.dot(batch_x) / self.batch_size</span><br><span class="line">                dw = dw.T</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 添加l1和l2的部分</span></span><br><span class="line">                dw_reg = np.zeros(shape=(x.shape[<span class="number">1</span>] - <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">                <span class="keyword">if</span> self.l1_ratio <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    dw_reg += self.l1_ratio * self.sign_func(self.w[:-<span class="number">1</span>]) / self.batch_size</span><br><span class="line">                <span class="keyword">if</span> self.l2_ratio <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    dw_reg += <span class="number">2</span> * self.l2_ratio * self.w[:-<span class="number">1</span>] / self.batch_size</span><br><span class="line">                dw_reg = np.concatenate([dw_reg, np.asarray([[<span class="number">0</span>]])], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                dw += dw_reg</span><br><span class="line">                self.w = self.w - self.eta * dw</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算losses</span></span><br><span class="line">            cost = -<span class="number">1</span> * np.<span class="built_in">sum</span>(</span><br><span class="line">                np.multiply(y, np.log(utils.sigmoid(x.dot(self.w)))) + np.multiply(<span class="number">1</span> - y, np.log(</span><br><span class="line">                    <span class="number">1</span> - utils.sigmoid(x.dot(self.w)))))</span><br><span class="line">            self.losses.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x: ndarray格式数据: m x n</span></span><br><span class="line"><span class="string">        :param y: ndarray格式数据: m x 1</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y = y.reshape(x.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 是否归一化feature</span></span><br><span class="line">        <span class="keyword">if</span> self.if_standard:</span><br><span class="line">            self.feature_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">            self.feature_std = np.std(x, axis=<span class="number">0</span>) + <span class="number">1e-8</span></span><br><span class="line">            x = (x - self.feature_mean) / self.feature_std</span><br><span class="line">        <span class="comment"># 是否训练bias</span></span><br><span class="line">        <span class="keyword">if</span> self.fit_intercept:</span><br><span class="line">            x = np.c_[x, np.ones_like(y)]</span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        self.init_params(x.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 更新eta</span></span><br><span class="line">        <span class="keyword">if</span> self.eta <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.eta = self.batch_size / np.sqrt(x.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.solver == <span class="string">&#x27;closed_form&#x27;</span>:</span><br><span class="line">            self._fit_closed_form_solution(x, y)</span><br><span class="line">        <span class="keyword">elif</span> self.solver == <span class="string">&#x27;sgd&#x27;</span>:</span><br><span class="line">            self._fit_sgd(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输出原始的系数</span></span><br><span class="line"><span class="string">        :return: w,b</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.fit_intercept:</span><br><span class="line">            w = self.w[:-<span class="number">1</span>]</span><br><span class="line">            b = self.w[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            w = self.w</span><br><span class="line">            b = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.if_standard:</span><br><span class="line">            w = w / self.feature_std.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            b = b - w.T.dot(self.feature_mean.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> w.reshape(-<span class="number">1</span>), b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        预测为y=1的概率</span></span><br><span class="line"><span class="string">        :param x:ndarray格式数据: m x n</span></span><br><span class="line"><span class="string">        :return: m x 1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.if_standard:</span><br><span class="line">            x = (x - self.feature_mean) / self.feature_std</span><br><span class="line">        <span class="keyword">if</span> self.fit_intercept:</span><br><span class="line">            x = np.c_[x, np.ones(x.shape[<span class="number">0</span>])]</span><br><span class="line">        <span class="keyword">return</span> utils.sigmoid(x.dot(self.w))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        预测类别，默认大于0.5的为1，小于0.5的为0</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        proba = self.predict_proba(x)</span><br><span class="line">        <span class="keyword">return</span> (proba &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        绘制前两个维度的决策边界</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :param y:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y = y.reshape(-<span class="number">1</span>)</span><br><span class="line">        weights, bias = self.get_params()</span><br><span class="line">        w1 = weights[<span class="number">0</span>]</span><br><span class="line">        w2 = weights[<span class="number">1</span>]</span><br><span class="line">        bias = bias[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        x1 = np.arange(np.<span class="built_in">min</span>(x), np.<span class="built_in">max</span>(x), <span class="number">0.1</span>)</span><br><span class="line">        x2 = -w1 / w2 * x1 - bias / w2</span><br><span class="line">        plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>)</span><br><span class="line">        plt.plot(x1, x2, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_losses</span>(<span class="params">self</span>):</span></span><br><span class="line">        plt.plot(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.losses)), self.losses)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://www.jianshu.com/p/4e7ca71a94be">线性分类|机器学习推导系列</a></p><p>[2] <a href="https://www.cnblogs.com/OldPanda/archive/2013/04/12/3017100.html">《统计学习方法》读书笔记——感知机</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性分类 </tag>
            
            <tag> 感知机 </tag>
            
            <tag> LDA </tag>
            
            <tag> Logistic Regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 3. 线性回归</title>
      <link href="2020/07/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%203%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>2020/07/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%203%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-3-线性回归"><a href="#机器学习-白板推导-3-线性回归" class="headerlink" title="机器学习-白板推导 3. 线性回归"></a>机器学习-白板推导 3. 线性回归</h3><h4 id="1-线性回归矩阵表示"><a href="#1-线性回归矩阵表示" class="headerlink" title="1. 线性回归矩阵表示"></a>1. 线性回归矩阵表示</h4><p>假定数据集：</p><script type="math/tex; mode=display">\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}, \ 其中x_i\in\mathbb{R}^{p}，y_i\in\mathbb{R}，i=1, \ 2,\cdots,\ N。</script><p>数据矩阵为：(这样可以保证每一行为一个数据点)</p><script type="math/tex; mode=display">\begin{equation}    X=(x_1, x_2, \cdots, x_N)^T=    \begin{pmatrix}    x_1^T \\     x_2^T \\    \vdots\\    x_N^T \\    \end{pmatrix} =    \begin{pmatrix}    x_{11} & x_{12} & \dots & x_{1p}\\    x_{21} & x_{32} & \dots & x_{2p}\\    \vdots & \vdots & \ddots & \vdots\\    x_{N1} & x_{N2} & \dots & x_{Np}\\    \end{pmatrix}_{N\times P}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}    Y=    \begin{pmatrix}    y_1 \\     y_2 \\    \vdots\\    y_N \\    \end{pmatrix}_{N\times 1}\end{equation}</script><p>设拟合的函数为：<script type="math/tex">f(x)=W^T x</script></p><p>如下图所示，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856647.png" alt="image-20210521173503596" style="zoom:25%;" /></p><p>对于每个样本点，比如<script type="math/tex">(x_3, y_3)</script>的残差就是红色部分的距离：<script type="math/tex">\lVert w^Tx_3 - y_3 \rVert^2</script>.因此总的损失为：</p><script type="math/tex; mode=display">\begin{align}     L(w) = & \sum_{i=1}^{N}\lVert w^T x_i-y_i\rVert^2 \\          = & (w^T x_1-y_1, w^T x_2-y_2, \dots, w^T x_N-y_N)          \begin{pmatrix}            w^T x_1-y_1\\            w^T x_2-y_2\\            \vdots\\            w^T x_N-y_N\\          \end{pmatrix}                      \end{align}</script><p>写成矩阵形式就是：</p><script type="math/tex; mode=display"> \begin{align}        L(W) = & (W^TX^T-Y^T)(W^TX^T-Y^T)^T \\        = & (W^TX^T-Y^T)(XW-Y) \\        = & W^TX^TXW - W^TX^TY - Y^TXW + Y^TY\\        = & W^TX^TXW - 2W^TX^TY + Y^TY \end{align}</script><p>对 <script type="math/tex">W</script>求导得：</p><script type="math/tex; mode=display">\frac{\partial L(W)}{\partial W} = 2X^TXW-2X^TY = 0 \Longrightarrow W=(X^TX)^{-1}X^TY</script><p>这里要求<script type="math/tex">（X^TX）^{-1}</script>满秩，也就是<script type="math/tex">X^TX \in \mathbb{R}^{p \times p}</script>是满秩的即<script type="math/tex">X</script>矩阵列向量都是独立的，即<script type="math/tex">X</script>特征列都不相关。</p><h4 id="2-最小二乘估计的几何意义"><a href="#2-最小二乘估计的几何意义" class="headerlink" title="2. 最小二乘估计的几何意义"></a>2. 最小二乘估计的几何意义</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856648.png" alt="image-20210521183837745" style="zoom:25%;" /></p><p>如上图，假设样本<script type="math/tex">\mathbf{x}_1, \mathbf{x}_2</script>张成空间为平面<script type="math/tex">\alpha</script>。 那么其通过拟合得到的函数为:</p><script type="math/tex; mode=display">\hat {\mathbf{y}}= w_1x_i + w_2x_2</script><p>当我们拟合得到的<script type="math/tex">\hat {\mathbf{y}}</script>正好是真实值<script type="math/tex">\mathbf{y}</script>在平面<script type="math/tex">\alpha</script>内的投影时，可以使得d最小。即<script type="math/tex">r \perp 平面\alpha</script>那么有：</p><script type="math/tex; mode=display">X^T(Y - X^TW) = 0</script><p>因此<script type="math/tex">W = (X^TX)^{-1}X^TY</script>.</p><h4 id="3-从概率视角看最小二乘法"><a href="#3-从概率视角看最小二乘法" class="headerlink" title="3. 从概率视角看最小二乘法"></a>3. 从概率视角看最小二乘法</h4><p>假设<script type="math/tex">\mathbf{y} = \mathbf{w}^T\mathbf{x} + \mathbf{\epsilon}</script>, 其中<script type="math/tex">\mathbf{\epsilon} \sim \mathcal{N}(0, \sigma^2)</script>。那么有<script type="math/tex">\mathbf{y} \sim \mathcal{N}(\mathbf{w}^T\mathbf{x} , \sigma^2)</script>。在该条件下，目标是求<script type="math/tex">\mathbf{w}</script>使得观察值<script type="math/tex">\mathbf{y}</script>出现的概率最大，使用MLE求解下式：</p><script type="math/tex; mode=display"> P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma} \text{ exp}\left(-\frac{(y-w^Tx)^2}{2\sigma^2}\right)</script><p>即对于每个样本<script type="math/tex">\mathbf{x}_i</script>都能式上面概率最大：</p><script type="math/tex; mode=display">\begin{align}    L(w) = & \text{ ln}\ p(y|x;w) = \text{ ln}\prod_{i=1}^Np(y_i|x_i;w) \\         = & \sum_{i=1}^N\text{ ln}\ p(y_i|x_i;w) \\          = & \sum_{i=1}^N \left( \text{ ln}\frac{1}{\sqrt{2\pi}\sigma} + \text{ ln}\ \text{ exp}\left( -\frac{(y_i - w^Tx)^2}{2\sigma^2} \right) \right)\\         = &\sum_{i=1}^N \left( \text{ ln}\frac{1}{\sqrt{2\pi}\sigma} +  -\frac{(y_i - w^Tx)^2}{2\sigma^2} \right) \end{align}</script><p>因此，只要<script type="math/tex">(y_i - w^Tx)^2</script>最大即可。这与矩阵求解表达式一样，因此，最小二乘法隐含着一个条件，噪声符合高斯分布。</p><h4 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4. 正则化"></a>4. 正则化</h4><p>对损失函数正则化后，表达式变为<script type="math/tex">L(w)+\lambda P(w)</script>。</p><ol><li>Lasso，其中<script type="math/tex">P(w) = \lVert w \rVert_1 = \sum_{i=1}^Nw_i</script></li><li>Ridge，岭回归，也就是<script type="math/tex">P(w)=\lVert w\lVert_2^2=\sum_{i=1}^Nw_i^2</script></li></ol><h5 id="岭回归频率派角度"><a href="#岭回归频率派角度" class="headerlink" title="岭回归频率派角度"></a>岭回归频率派角度</h5><p>损失函数改写为 <script type="math/tex">L(W)=\sum_{i=1}^N\lVert w^Tx_i-y_i \rVert^2 + \lambda W^TW</script></p><script type="math/tex; mode=display">\begin{align}    L(W) = & \sum_{i=1}^N \lVert w^Tx_i-y_i \rVert^2 + \lambda W^TW \\    \nonumber = & (W^TX^T - Y^T)(XW-Y)+\lambda W^TW \\    \nonumber = & W^TX^TXW - W^TX^TY - Y^TXW - Y^TY + \lambda W^TW \\     \nonumber = & W^TX^TXW - 2W^TX^TY - Y^TY + \lambda W^TW \\    \nonumber = & W^T(X^TX + \lambda I)W - 2W^TX^TY - Y^TY \end{align}</script><p>然后 <script type="math/tex">L(W)</script>对<script type="math/tex">W</script>求导得：</p><script type="math/tex; mode=display">\frac{\partial J(W)}{\partial W} = 2(X^TX + \lambda I)W - 2X^TY = 0</script><p>可得： </p><script type="math/tex; mode=display">W = (X^TX + \lambda I)^{-1}X^TY</script><p>这里，因为<script type="math/tex">X^T X</script>是半正定矩阵,  <script type="math/tex">X^TX + \lambda I</script>必然可逆。</p><h5 id="岭回归Bayes估计角度"><a href="#岭回归Bayes估计角度" class="headerlink" title="岭回归Bayes估计角度"></a>岭回归Bayes估计角度</h5><p>根据第3小节所说， <script type="math/tex">\mathbf{\epsilon} \sim \mathcal{N}(0, \sigma^2)，\  \mathbf{y} \sim \mathcal{N}(\mathbf{w}^T\mathbf{x} , \sigma^2)</script> 可以得到 <script type="math/tex">P(w), P(y|w)</script>:</p><script type="math/tex; mode=display">P(y|w) = \frac{1}{\sqrt{2\pi}\sigma} \text{ exp}\left( -\frac{(y - w^Tx)^2}{2\sigma^2} \right) \\P(w) = \frac{1}{\sqrt{2\pi}\sigma_0}\text{ exp}\left( -\frac{\lVert w\rVert^2}{2\sigma_0^2} \right)</script><p>目标是求<script type="math/tex">w</script>的最大后验估计MAP, 也就是<script type="math/tex">\hat{w} = \text{ argmax}_w  \ P(w|y)</script>。由bayes公式得：</p><script type="math/tex; mode=display">P(w|y) = \frac{P(y|w)P(w)}{P(y)}</script><p>因为<script type="math/tex">y</script>是由数据集给定的，可以看做常量，再加入对数操作简化计算，可以简化成：</p><script type="math/tex; mode=display">\text{ argmax}_w \ P(w|y)= \log P(y|w)P(w)</script><p>即：</p><script type="math/tex; mode=display">\begin{align}    \text{ argmax}_w \ P(w|y) = & \sum_{i=1}^{N}\log \frac{1}{\sqrt{2\pi}\sigma}\text{ exp}\left( -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}  \right) \frac{1}{\sqrt{2\pi}\sigma_0}\text{ exp}\left( -\frac{||w||^2}{2\sigma_0^2} \right) \\    = & \sum_{i=1}^{N}\log \frac{1}{2\pi\sigma\sigma_0}\text{ exp}\left( -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}  -\frac{||w||^2}{2\sigma_0^2} \right) \\    = & \sum_{i=1}^{N} \log \frac{1}{2\pi\sigma\sigma_0} + \log \text{ exp}\left( -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}  -\frac{||w||^2}{2\sigma_0^2} \right) \end{align}</script><p>因为<script type="math/tex">\log \frac{1}{2\pi\sigma\sigma_0}</script>与 最大化<script type="math/tex">P(w|y)</script>无关，可以舍去，即：</p><script type="math/tex; mode=display">\begin{align}    \text{ argmax}_w \ P(w|y)     = & \sum_{i=1}^{N}  \log \text{ exp}\left( -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}  -\frac{||w||^2}{2\sigma_0^2} \right) \\    = & \sum_{i=1}^{N}  -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}  -\frac{||w||^2}{2\sigma_0^2} \\\end{align}</script><p>进一步简化的：</p><script type="math/tex; mode=display">\begin{equation}    \text{ argmax}_w  P(w|y) =  \sum_{i=1}^{N} (y_i - w^Tx_i)^2  + \frac{\sigma^2}{\sigma_0^2}||w||^2\end{equation}</script><p>这样跟从频率角度得到的岭回归结果一样。即最小二乘估计中隐藏一个假设：噪声服从高斯分布。即正则化的最小二乘法等价于最大后验估计MAP，其中噪声为高斯分布，并且假定<script type="math/tex">w</script>也服从高斯分布。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
            <tag> 最小二乘法 </tag>
            
            <tag> Ridge </tag>
            
            <tag> Lasso </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 2 高斯分布</title>
      <link href="2020/07/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%202%20%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/"/>
      <url>2020/07/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%202%20%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-2-高斯分布"><a href="#机器学习-白板推导-2-高斯分布" class="headerlink" title="机器学习-白板推导 2 高斯分布"></a>机器学习-白板推导 2 高斯分布</h3><h4 id="1-从概率密度函数看高斯分布"><a href="#1-从概率密度函数看高斯分布" class="headerlink" title="1. 从概率密度函数看高斯分布"></a>1. 从概率密度函数看高斯分布</h4><p>高斯分布的pdf：</p><script type="math/tex; mode=display">P(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\operatorname{ exp }\left\{ -\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu}) \right\}</script><p>后面式子<script type="math/tex">-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})</script>是二次型，其中<script type="math/tex">\mathbf{x} \in \mathbb{R}^p</script>，是<script type="math/tex">p</script>维随机变量。</p><script type="math/tex; mode=display">\begin{equation}    \mathbf{x}=    \begin{pmatrix}        x_1 \\        x_2 \\        \vdots \\        x_p    \end{pmatrix} \qquad     \mathbf{u}=    \begin{pmatrix}        u_1 \\        u_2 \\        \vdots \\        u_p    \end{pmatrix} \qquad     \Sigma =     \begin{pmatrix}        \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p} \\        \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p} \\        \vdots      & \vdots      & \ddots & \vdots      \\        \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp}        \end{pmatrix}\end{equation}</script><p>通常来说<script type="math/tex">\Sigma</script> 是半正定的(对称的)，这里假设其是正定的，<script type="math/tex">\lambda \gt 0</script>, 因为<script type="math/tex">\mathbf{x}</script>是自变量，所以这个<script type="math/tex">P(\mathbf{x})</script>只跟<script type="math/tex">(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})</script>有关。这是个数，也叫马氏距离 (<script type="math/tex">\mathbf{x}</script> 与 <script type="math/tex">\mathbf{\mu}</script> 之间的距离)。</p><h5 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h5><p><strong>当<script type="math/tex">\Sigma = I</script>时，就是欧氏距离。</strong></p><p>在高斯分布中，$(X-\mu)^T\Sigma^{-1}(X-\mu)$的计算结果是一个数，这个数被称为马氏距离。设<script type="math/tex">$z_1=(z_{11}, z_{12})^T</script>，<script type="math/tex">z_1=(z_{21}, z_{22})^T</script>。那么<script type="math/tex">z_1</script>和<script type="math/tex">z_2</script>之间的马氏距离为，</p><script type="math/tex; mode=display">\begin{equation}    (z_1-z_2)^T\Sigma^{-1}(z_1-z_2)=    \begin{pmatrix}        z_{11}-z_{12} & z_{21}-z_{22}    \end{pmatrix}    \Sigma^{-1}    \begin{pmatrix}        z_{11}-z_{12} \\        z_{21}-z_{22}    \end{pmatrix}\end{equation}</script><p>显然，当<script type="math/tex">\Sigma^{-1}=I</script>时，马氏距离等于欧式距离<script type="math/tex">(z_1-z_2)^T\Sigma^{-1}(z_1-z_2)=(z_{11}-z_{12})^2+(z_{21}-z_{22})^2</script>。</p><h5 id="对标准差进行特征分解"><a href="#对标准差进行特征分解" class="headerlink" title="对标准差进行特征分解"></a>对标准差进行特征分解</h5><p>由于$\Sigma$为实对称矩阵，那么可以对$\Sigma$进行特征分解，那么有<script type="math/tex">\Sigma = U\Lambda U^T</script>，并且<script type="math/tex">UU^T=U^TU=I</script>，所以<script type="math/tex">U^{-1}=U^T</script>，<script type="math/tex">\Lambda=diag(\lambda_i)\quad(i=1,2,\cdots,N)</script>，并且<script type="math/tex">U=(U_1,U_2,\cdots,U_p)_{p\times p}</script>。</p><script type="math/tex; mode=display">\begin{align}    \Sigma = & U\Lambda U^T \\    = & (U_1,U_2,\cdots,U_p)    \begin{pmatrix}        \lambda_1 & & & \\        & \lambda_2 & & \\        & & \ddots & \\        & & & \lambda_p \\    \end{pmatrix}    \begin{pmatrix}        U_1^T  \\        U_2^T  \\        \vdots \\        U_p^T  \\    \end{pmatrix} \\     = & (U_1\lambda_1,U_2\lambda_2,\cdots,U_p\lambda_p)     \begin{pmatrix}        U_1^T  \\        U_2^T  \\        \vdots \\        U_p^T  \\    \end{pmatrix} \\    = & \sum_{i=1}^{p}U_i\lambda_i U_i^T\end{align}</script><p>而<script type="math/tex">\Sigma^{-1}</script>的求解过程如下所示：</p><script type="math/tex; mode=display">\begin{equation}    \Sigma^{-1} = (U \Lambda U^T)^{-1} = (U^T)^{-1} \Lambda^{-1} U^{-1} = U \Lambda^{-1} U^T\end{equation}</script><p>代入可以解得：</p><script type="math/tex; mode=display">\begin{equation}    \Sigma^{-1} = \sum_{i=1}^{p}U_i\frac{1}{\lambda_i} U_i^T\end{equation}</script><p>那么，</p><script type="math/tex; mode=display">\begin{align}    (X-\mu)^T\Sigma^{-1}(X-\mu) = & (X-\mu)^T\sum_{i=1}^{p}U_i\frac{1}{\lambda_i} U_i^T(X-\mu) \\    = & \sum_{i=1}^{p} (X-\mu)^TU_i\frac{1}{\lambda_i} U_i^T(X-\mu)\end{align}</script><p>令<script type="math/tex">y_i=(X-\mu)^TU_i</script>，这是一个典型的投影算法，其中<script type="math/tex">U_i</script>是特征值为<script type="math/tex">\lambda_i</script>的特征向量，那么</p><script type="math/tex; mode=display">\begin{equation}    (X-\mu)^T\Sigma^{-1}(X-\mu) = \sum_{i=1}^{p} y_i\frac{1}{\lambda_i} y_i^T\end{equation}</script><p>当<script type="math/tex">p = 2</script>时， 有：</p><script type="math/tex; mode=display">\begin{align}\Delta = \frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2}\end{align}</script><p>每当 <script type="math/tex">\Delta</script> 取不同值，椭圆就相当于对这一高度的等高线，也对应一个固定的概率值，若<script type="math/tex">\lambda_i=c</script>) (常量)时，图中椭圆便是一个圆。</p><p>具体如下面两图所示，高度是概率值的话，<script type="math/tex">0 \le P(\mathbf{x}) \le 1</script> ,就是去不同概率值时切出一个个椭圆。图出自 [Machine Learning: a Probabilistic Perspective P48 Joint probability distributions ]</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856537.png" alt="image-20210520151856348" style="zoom:40%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856539.png" alt="image-20210520151941866" style="zoom:50%;" /></p><h4 id="2-高斯分布的局限性"><a href="#2-高斯分布的局限性" class="headerlink" title="2. 高斯分布的局限性"></a>2. 高斯分布的局限性</h4><p>来自 <a href="https://github.com/ws13685555932/machine_learning_derivation/blob/master/02%20%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf">数学基础.pdf</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856540.png" alt="image-20210520160029485" style="zoom:30%;" /></p><script type="math/tex; mode=display">\Sigma  要计算元素个数为\frac{(1+p)p}{2}，复杂度O(p^2)。</script><h4 id="3-已知联合概率求边缘概率及条件概率"><a href="#3-已知联合概率求边缘概率及条件概率" class="headerlink" title="3. 已知联合概率求边缘概率及条件概率"></a>3. 已知联合概率求边缘概率及条件概率</h4><p>高斯分布的一个定理：</p><script type="math/tex; mode=display">若 \mathbf{x} \sim N(\mathbf{\mu}, \Sigma), \mathbf{y} = A\mathbf{x} + B,那么：\mathbf{y} \sim N(A\mathbf{\mu}+B, \ A\Sigma A^T)</script><p>证明:</p><script type="math/tex; mode=display">    E[y]=E[A x+B]=A E[x]+E[B]=A \mu+B \\    Var[y]=Var[A x+B]=Var[A x]+D[B]=A \ Var[x] A^{T}=A \Sigma A^{T}</script><p>假定多元高斯分布：</p><script type="math/tex; mode=display">P(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\operatorname{ exp }\left\{ -\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu}) \right\}</script><p>其中，</p><script type="math/tex; mode=display">\begin{equation}    \mathbf{x}=    \begin{pmatrix}        x_1 \\        x_2 \\        \vdots \\        x_p    \end{pmatrix} \qquad    \Sigma =     \begin{pmatrix}        \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p} \\        \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p} \\        \vdots      & \vdots      & \ddots & \vdots      \\        \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp}        \end{pmatrix}_{p\times p}\end{equation}</script><p>若已知联合概率密度求条件概率密度和边缘概率密度，可描述为已知：(就是将变量x分为两部分， <script type="math/tex">\mathbf{x}_a, 和\mathbf{x}_b</script>)</p><script type="math/tex; mode=display">\begin{equation}    X=     \begin{pmatrix}        x_a \\        x_b    \end{pmatrix}    \quad m+n=p \quad    \mu=    \begin{pmatrix}        \mu_a \\        \mu_b    \end{pmatrix} \quad    \Sigma=    \begin{pmatrix}    \Sigma_{aa} & \Sigma_{ab} \\    \Sigma_{ba} & \Sigma_{bb}     \end{pmatrix}\end{equation}</script><p>求：<script type="math/tex">P(x_a)</script>和<script type="math/tex">P(x_b|x_a)</script></p><p>而</p><script type="math/tex; mode=display">x_a=\begin{pmatrix}\mathbb{I}_{m\times m}&\mathbb{O}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}</script><p>就是<script type="math/tex">y = Ax + B</script>这种形式，按照式3可以有：</p><script type="math/tex; mode=display">\mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\Var[x_a]=\begin{pmatrix}\mathbb{I}&\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}</script><p>同样地， <script type="math/tex">x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})</script>.</p><p>下面是最关键的3个构造式，(没有为什么，就是这样构造能解决问题，不然用PRML上配方法)。</p><script type="math/tex; mode=display">x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}</script><p>式5中最后一个式子叫<script type="math/tex">\Sigma_{bb}</script>舒尔补，补充下 <a href="https://zh.wikipedia.org/wiki/%E8%88%92%E5%B0%94%E8%A1%A5">舒尔补</a>：</p><blockquote><p>假设有分别属于<script type="math/tex">\mathbb{R}^n</script>以及’<script type="math/tex">\mathbb{R}^m</script>的随机列向量<script type="math/tex">\mathbf{x, y}</script>，并且<script type="math/tex">\mathbb{R}^{n+m}</script>中的向量对 <script type="math/tex">\mathbf{(x, y)}</script>具有<strong>多维正态分布</strong>，其方差矩阵是对称的正定矩阵</p><script type="math/tex; mode=display">V=\left[\begin{matrix} A & B \\ B^T & C \end{matrix}\right].</script><p>那么’’X’’在’’Y’’给定时的[[条件方差]]是矩阵’’C’’在’’V’’中的舒尔补：</p><script type="math/tex; mode=display">\operatorname{var}(X\mid Y) = A-BC^{-1}B^T.</script></blockquote><p>由式5中第一个式子得到：</p><script type="math/tex; mode=display">x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}</script><p>同样按照式3有：</p><script type="math/tex; mode=display">\mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times n}\end{pmatrix}=\Sigma_{bb\cdot a}</script><p>同样由式5中第一个式子得到 <script type="math/tex">x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a</script>， 在求条件概率<script type="math/tex">P(x_b|x_a)</script>时，<script type="math/tex">x_a</script>对于<script type="math/tex">x_b</script>可以看做是已知，因此：</p><script type="math/tex; mode=display">\mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\Var[x_b|x_a]=\Sigma_{bb\cdot a}</script><p>同理可得：</p><script type="math/tex; mode=display">\mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\Var[x_a|x_b]=\Sigma_{aa\cdot b}</script><p>即：<script type="math/tex">P(x_b|x_a) \sim \mathcal{N}( \mathbf{\mu_{b\cdot a}} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a, \Sigma_{bb\cdot a}) = \mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})</script>。同理可得 <script type="math/tex">P(x_a|x_b)</script></p><h4 id="4-求联合概率分布"><a href="#4-求联合概率分布" class="headerlink" title="4. 求联合概率分布"></a>4. 求联合概率分布</h4><p>上面是已知联合概率分布求条件概率分布和边缘概率分布。</p><p>而下面内容是：</p><p>已知： </p><ul><li><script type="math/tex">P(x) = \mathcal{N} (\mu, \Lambda^{-1})</script>, 这里 <script type="math/tex">\Lambda^{-1}</script> 是精度矩阵，就是协方差矩阵的导数 <script type="math/tex">\text{precision matrix}</script>，详见PRML中文版P68.只是为了计算，写成这样。</li><li><script type="math/tex; mode=display">P(y|x) = \mathcal{N} (Ax+b, L^{-1})</script></li></ul><p>求：<script type="math/tex">P(y), P(x|y)</script></p><p>证明：</p><p>假设<script type="math/tex">y=Ax+b+\epsilon, 其中\ \epsilon\sim\mathcal{N}(0,L^{-1})</script>, 这里<script type="math/tex">x, y, \epsilon都是随机变量，且 \epsilon \perp x(独立)</script>。</p><ol><li><p>求解<script type="math/tex">P(y)</script></p><script type="math/tex; mode=display">\begin{align}&\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b\\&Var[y] = Var[Ax+b+\epsilon] = Var[Ax+b] + L^{-1} = A\Lambda^{-1}A^T+L^{-1}\end{align}</script><p>所以<script type="math/tex">y \sim \mathcal{N}(A\mu+b， A\Lambda^{-1}A^T+L^{-1})</script></p></li><li><p>求解 <script type="math/tex">P(x|y)</script></p></li></ol><p>先引入一个变量<script type="math/tex">z = \begin{pmatrix}        x \\        y    \end{pmatrix}</script></p><p>那么可得：</p><script type="math/tex; mode=display">\begin{equation}    z=    \begin{pmatrix}        x \\         y    \end{pmatrix} \sim    \mathcal{N}    \left(    \begin{pmatrix}        \mu \\        A\mu + b \\    \end{pmatrix}    \quad    \begin{pmatrix}        \Lambda^{-1} & \Delta \\        \Delta   & A\Lambda^{-1}A^T +L^{-1} \\    \end{pmatrix}    \right)\end{equation}</script><p>这里:</p><script type="math/tex; mode=display">\begin{align}\Delta = \text{Conv}(x, y)= & \mathbb{E}[(x-\mu)(Ax-A\mu)^T] \\       = & \mathbb{E}[(x-\mu)(x-\mu)^T]\cdot A^T \\       = & Var[x]\cdot A^T \\       = & \Lambda^{-1} A^T\end{align}</script><h4 id="5-Jensen’s-Inequality"><a href="#5-Jensen’s-Inequality" class="headerlink" title="5. Jensen’s Inequality"></a>5. Jensen’s Inequality</h4><p>Jensen’s Inequality 是在给定<script type="math/tex">f(x)</script>是凸函数的条件下，过凸函数上任意两点所作割线一定在两点函数图像上方，即对于<script type="math/tex">t \in [a, b]</script>, 有：</p><script type="math/tex; mode=display"> tf(a)+(1-t)f(b)\geq f\left(ta+(1-t)b \right),\ 0\leq t\leq 1.</script><p>如下图所示，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261856541.png" alt="image-20210521145026124" style="zoom:20%;" /></p><p>在概率论中，其中<script type="math/tex">L(x)</script>是该点的切线，设其为<script type="math/tex">L(x) = ax+b</script>有</p><script type="math/tex; mode=display"> \begin{align} \mathbb{E}[f(x)]\geq & \mathbb{E}[L(x)] \\        = & \mathbb{E}[a\mathbb{E}[x]+b] \\        = & a\mathbb{E}[x] + b \\        = & f(\mathbb{E}[x]) \\ \end{align}</script><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p>[1] <a href="https://zhuanlan.zhihu.com/p/290876484">特征值分解</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/88116062">高斯分布</a></p><p>[3] <a href="https://www.jianshu.com/p/709a12a3abf0">高斯分布|机器学习推导系列（二）</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高斯分布 </tag>
            
            <tag> Jensen’s Inequality </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-白板推导 1 概率基础</title>
      <link href="2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%201%20%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/"/>
      <url>2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC%201%20%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习-白板推导-1-概率基础"><a href="#机器学习-白板推导-1-概率基础" class="headerlink" title="机器学习-白板推导 1 概率基础"></a>机器学习-白板推导 1 概率基础</h3><h4 id="1-MLE"><a href="#1-MLE" class="headerlink" title="1. MLE"></a>1. MLE</h4><p>一元高斯分布：</p><script type="math/tex; mode=display">P(x) =   {\frac {1}{ {\sqrt {2\pi }\sigma}}}\operatorname{ exp } \left(-{\frac {\left(x-\mu \right)^{2}}{2\sigma ^{2}}}\right) \tag{1}</script><p>多元高斯分布：</p><script type="math/tex; mode=display">P(X) = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}\operatorname{ exp }\left\{ -\frac{1}{2}(X-\mu)^T\Sigma^{-1}(X-\mu) \right\} \tag{2}</script><p>用极大似然估计参数<script type="math/tex">\theta</script> 有：</p><script type="math/tex; mode=display">\begin{align}    L(\theta) = \operatorname{ \log } P(x | \theta) = &\operatorname{ \log } \prod_{i=1}^N P(x_i|\theta)=\sum_{i=1}^N \operatorname{ \log } P(x_i|\theta) \\    = & \sum_{i=1}^N \operatorname{ \log } \frac{1}{\sqrt{2\pi}} + \operatorname{ \log } \frac{1}{\sigma} - \frac{(x_i-\mu)^2}{2\sigma^2} \end{align} \tag{3}</script><p>求解<script type="math/tex">\mu_{\text{MLE}}</script>，就是对式3求 <script type="math/tex">\mu</script>的偏导，</p><script type="math/tex; mode=display">\frac{\partial L(\theta)}{ \partial \mu} = \sum_{i=1}^N(x_i-\mu)=0 \tag{4}</script><p>即得到：</p><script type="math/tex; mode=display">\sum_{i=1}^{N} x_i - N\mu=0 \Longrightarrow \mu_{MLE}=\frac{1}{N}\sum_{i=1}^{N} x_i</script><p>求解<script type="math/tex">\sigma^2_{MLE}</script>时，计算目标为<script type="math/tex">\frac{\partial\log p(x|\theta)}{\partial \sigma}</script>，推导公式如下：</p><script type="math/tex; mode=display">\begin{gather}    \frac{\partial\log p(x|\theta)}{\partial \sigma}      = \sum_{i=1}^N - \frac{1}{\sigma} - \frac{1}{2}(x_i-\mu)^2(-2)\sigma^{-3} = 0 \\     \sum_{i=1}^N  \sigma^2 = \sum_{i=1}^N (x_i-\mu)^2 \\     \sigma^2_{MLE} = \frac{1}{N} \sum_{i=1}^N (x_i-\mu)^2 \end{gather}</script><p>实际上这里的$\mu$是$\mu_{MLE}$，所以，</p><script type="math/tex; mode=display">\begin{equation}    \sigma^2_{MLE} = \frac{1}{N} \sum_{i=1}^N (x_i-\mu_{MLE})^2 \end{equation}</script><h4 id="2-无偏有偏"><a href="#2-无偏有偏" class="headerlink" title="2. 无偏有偏"></a>2. 无偏有偏</h4><p>首先需要明确什么是无偏估计，所谓无偏估计也就是，<script type="math/tex">\mathbb{E}(\hat{x})=x</script>。那么利用这个性质我们就可以很方便的判断一个估计是否为无偏估计。验证<script type="math/tex">\sigma^2_{MLE}</script>有偏无偏</p><script type="math/tex; mode=display">\begin{equation}    \begin{split}        \mathbb{E}[\sigma^2_{MLE}] = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i-\mu_{MLE})^2] \\        = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i^2-2\mu_{MLE} x_i + \mu_{MLE}^2)] \\        = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i^2- \mu_{MLE}^2)]\\        = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i^2-\mu^2)-(\mu_{MLE}^2-\mu^2)] \\        = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i^2-\mu^2)]-\mathbb{E}[(\mu_{MLE}^2-\mu^2)]\\        = & \mathbb{E}[ \frac{1}{N}\sum_{i=1}^N (x_i^2-(\frac{1}{N}\sum_{i=1}^Nx_i)^2)]-\mathbb{E}[(\mu_{MLE}^2-\mu^2)]\\        = & \frac{1}{N}\sum_{i=1}^{N}(\mathbb{E}[x_i^2]-\mathbb{E}[x]^2)-\mathbb{E}[(\mu_{MLE}^2-\mu^2)] \\        = & \sigma^2 - (\mathbb{E}[\mu_{MLE}^2] - \mathbb{E}[\mu^2]) \\        = & \sigma^2 - (\mathbb{E}[\mu_{MLE}^2] - \mathbb{E}[\mathbb{E}[\mu_{MLE}]^2]) \\        = & \sigma^2 - (\mathbb{E}[\mu_{MLE}^2] - \mathbb{E}[\mu_{MLE}]^2] \\        = & \sigma^2 - Var[\mu_{MLE}] \\        = & \sigma^2 - Var[\frac{1}{N}\sum_{i=1}^Nx_i] \\        = & \sigma^2 - \frac{1}{N^2}Var[\sum_{i=1}^Nx_i] \\        = & \sigma^2 - \frac{1}{N^2}\sum_{i=1}^NVar[x_i] \\         = & \sigma^2 - \frac{1}{N^2} N \sigma^2 \\        = & \frac{N-1}{N}\sigma^2    \end{split}\end{equation}</script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261848100.png" alt="image-20210520101036278" style="zoom:30%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MLE </tag>
            
            <tag> 高斯分布 </tag>
            
            <tag> 有偏无偏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 11  PageRank and Ridge Regression</title>
      <link href="2020/06/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture11/"/>
      <url>2020/06/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture11/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-11-PageRank-and-Ridge-Regression"><a href="#Lecture-11-PageRank-and-Ridge-Regression" class="headerlink" title="Lecture 11: PageRank and Ridge Regression"></a>Lecture 11: PageRank and Ridge Regression</h2><h3 id="1-PageRank"><a href="#1-PageRank" class="headerlink" title="1. PageRank"></a>1. PageRank</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909373.png" alt="PageRank" style="zoom:150%;" /></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="E:\systemshare\Pic\notion\gitee\pic\img/image-20201103131844700.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Google PageRank</div> </center><p><a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">Google原始论文</a></p><ul><li>Imagine surfing web by randomly clicking links. This is called a “random walk” over graph. If you do this long enough eventually reach “steady state” where Probability that you are at site <script type="math/tex">i = \pi_i</script></li></ul><p>让$\tilde A$ 表示这个图的邻近矩阵，其中：</p><script type="math/tex; mode=display">\tilde A = \begin{cases}1, & \text{if   links   from  j  to  i},\\0, & otherwise\end{cases}\tag{1}</script><p>具体地有：</p><script type="math/tex; mode=display">\tilde A =\left[\begin{array}{r} 0 & 1 & 1 & 1 \\   0 & 0 & 1 & 1 \\   1 & 0 &0 & 0 \\   1 & 0 & 1& 0 \\\end{array} \right]\tag{2}</script><p>接下来，让$A = \tilde A $ column-normalized version.</p><script type="math/tex; mode=display">\tilde A =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\tag{3}</script><p>定义：$e_{i} = i^{th}$单位矩阵$I$的第$i$列。</p><p>那么，</p><script type="math/tex; mode=display">Ae_3 =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\left[\begin{array}{r} 0  \\ 0  \\1\\0\end{array} \right]=\left[\begin{array}{r}\frac {1}{3}  \\\frac {1}{3} \\0\\\frac {1}{3}\end{array} \right]\tag{4}</script><script type="math/tex; mode=display">Ae_4 =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\left[\begin{array}{r} 0  \\ 0  \\0\\1\end{array} \right]=\left[\begin{array}{r}\frac {1}{2}  \\\frac {1}{2} \\0\\0\end{array} \right]\tag{5}</script><p>$A_ij= Pro(\text{go  to  site  i  }| \text{  at  site  j})$.我们从$j$到$i$的可能性。</p><p>令：</p><script type="math/tex; mode=display">\underline{\pi}=\left[\begin{array}{r}\pi_{1}  \\\pi_{2}  \\\pi_{3} \\\pi_{4} \end{array} \right]\tag{6}</script><p>$\underline{\pi} $是到下一个网站的可能性，而且有：</p><script type="math/tex; mode=display">A\underline{\pi} =\underline{\pi}</script><p>目标是找到 <script type="math/tex">\underline{\pi}</script> 。</p><script type="math/tex; mode=display">\begin{align}&\underline{\pi}  是 \  A  \ 的特征向量， 令 A的特征向量\  \boldsymbol{v}_k \ 且\ \lVert  \boldsymbol{v}_k\rVert_{2}=1\  。如果\ A\boldsymbol{v}_k = \lambda_k \boldsymbol{v}_{k} ,   \\&对一些标量\ \lambda_k\ 成立。其中\ \lambda_k \ 叫作特征值。 \end{align}</script><p>令 <script type="math/tex">V = [v_1, v_2, \cdots, v_n]</script>是<script type="math/tex">A</script>的特征向量。</p><script type="math/tex; mode=display">AV=A[v_1, v_2, \cdots, v_n]=[\lambda_1v_1, \lambda_2v_2, \cdots, \lambda_nv_n]\\= V\begin{bmatrix}    \lambda_1 & 0 & \cdots & 0 \\    0 & \lambda_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \lambda_n \end{bmatrix}=V\Lambda</script><p>对于：</p><script type="math/tex; mode=display">X \in \mathbb{R}^{n \times p}, A = X^TX \in \mathbb{p \times p}.若 X= U \Sigma V^T,\ \ 那么 A = U \Sigma V^TU\Sigma V^T \Rightarrow A = V\Sigma^2 V^T \Rightarrow AV=V\Sigma^2</script><p>$X$的$\sigma$的平方就是$A$的特征值。$X$右奇异向量就是$A$的特征向量。</p><p>PageRank 矩阵$A$有$\lambda_1=1 &gt; \lambda_2 \ge\lambda_3 \ge \cdots\ge \lambda_n$.</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909374.png" alt="算法流程" style="zoom:125%;" /></p><p><strong>算法流程</strong></p><script type="math/tex; mode=display">Let \ \ \underline{\Pi}^{0} \ \ be \  initial \ guess \  of \ \underline{\Pi} \ \ for \ k=1, 2, \cdots \\\underline{\Pi}^{k} = \frac {A\underline{\Pi}^{(k-1)}}{\lVert  A\underline{\Pi}^{(k-1)}\rVert_{2}}</script><script type="math/tex; mode=display">if \ \lVert \underline{\Pi}^{(k)} - \underline{\Pi}^{(k-1)}\rVert_{2}< \epsilon, \ \ then \ stop</script><p>又有：</p><script type="math/tex; mode=display">\begin{aligned}\underline{\Pi}^{1} &\propto A \ \underline{\Pi}^{0} \\\underline{\Pi}^{2}& \propto A \ \underline{\Pi}^{1} \propto A^2 \ \underline{\Pi}^{0}\\\vdots &\\\underline{\Pi}^{k}  &\propto A^k \ \underline{\Pi}^{0}\\\end{aligned}\tag{7}</script><p>和</p><script type="math/tex; mode=display">\begin{aligned}A &= V\Lambda V^T\\A^2 &= V\Lambda V^T V\Lambda V^T = V\Lambda\Lambda V^T\\\vdots &\\A^k &= V\Lambda^{k} V^T\end{aligned}\tag{8}</script><p>接下来证明算法为什么会有效？</p><p>假定特征向量是<strong>orthonormal</strong>($V^TV=VV^T=I$)。</p><p>相应地：</p><script type="math/tex; mode=display">\underline{\Pi}^{O} = c_1v_1 + c2v_2 +\cdots+c_nv_n \ \text{for} \ c_1, c_2, \cdots,c_n.assume c_1 \ne 0.\tag{9}</script><p>其中，$v_1就等于\underline{\Pi}$.那么，</p><script type="math/tex; mode=display">\underline{\Pi}^{k} \propto A^k \underline{\Pi}^{0}=V\Lambda^k V^T( c_1v_1 + c2v_2 +\cdots+c_nv_n)\tag{10}</script><p>而</p><script type="math/tex; mode=display">V^Tv_i =\left[\begin{array}{ll}- &\boldsymbol{v}_{1}^{\top}&- \\- & \boldsymbol{v}_{2}^{\top}&- \\- &    \vdots &-        \\- & \boldsymbol{v}^T_{n}&-\end{array}\right]v_i=\boldsymbol{e}_{i}, Ve_i=v_i\tag{11}</script><p>那么式10等于：</p><script type="math/tex; mode=display">\underline{\Pi}^{k} \propto A^k \underline{\Pi}^{0} = V(\lambda^kc_1e_1+ \cdots + \lambda^nc_ne_n)=\lambda^k_1c_1(v_1 + \frac{\lambda^k_2c_2}{\lambda_1^kc_1}v_2+\cdots+ \frac{\lambda^k_nc_n}{\lambda_1^kc_1}v_n)\tag{12}</script><p>当$k \to \infty$,$\frac{\lambda^k_i}{\lambda_1^k} \to 0 \ for \ i \ne 1$，那么</p><script type="math/tex; mode=display">当 k \to \infty, \lambda^k_1c_1(v_1 + \frac{\lambda^k_2c_2}{\lambda_1^kc_1}v_2+\cdots+ \frac{\lambda^k_nc_n}{\lambda_1^kc_1}v_n) \to \lambda^k_1c_1v_1\tag{13}</script><p>那么式12等于</p><script type="math/tex; mode=display">\frac{\lambda^k_1c_1v_1}{|\lambda^k_1c_1|\lVert \boldsymbol{v}_1\rVert} \to \boldsymbol{v}_1 = \Pi \tag{14}</script><p><a href="https://zh.wikipedia.org/wiki/PageRank">Wiki证明版本</a></p><h3 id="Classical-Ridge-Regression"><a href="#Classical-Ridge-Regression" class="headerlink" title="Classical Ridge Regression"></a>Classical Ridge Regression</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909375.png" alt="Classical Ridge Regression 1" style="zoom:125%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909376.png" alt="Classical Ridge Regression 2" style="zoom:125%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
            <tag> PageRank </tag>
            
            <tag> Ridge Regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 10 More on the SVD in Machine Learning</title>
      <link href="2020/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture10/"/>
      <url>2020/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture10/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-10-More-on-the-SVD-in-Machine-Learning-including-matrix-completion"><a href="#Lecture-10-More-on-the-SVD-in-Machine-Learning-including-matrix-completion" class="headerlink" title="Lecture 10: More on the SVD in Machine Learning including matrix completion"></a>Lecture 10: More on the SVD in Machine Learning including matrix completion</h2><h3 id="1-SVD-in-machine-learning"><a href="#1-SVD-in-machine-learning" class="headerlink" title="1. SVD in machine learning"></a>1. SVD in machine learning</h3><ol><li>Dimensionality Reduction(PCA)</li><li>Principal Components Regression</li><li>Least Squares </li><li>Matrix Completion</li><li>PageRank</li></ol><h3 id="2-Least-Squares-amp-SVD"><a href="#2-Least-Squares-amp-SVD" class="headerlink" title="2. Least Squares &amp; SVD"></a>2. Least Squares &amp; SVD</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261908172.png" alt="理解最小二乘" style="zoom:100%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261908173.png" alt="理解最小二乘续" style="zoom:100%;" /></p><p>跟前面提到的一样有：</p><p>假设$n \ge p$ &amp; $X$有$p$个线性无关的列，那么</p><script type="math/tex; mode=display">\hat w = (X^TX)^{-1}X^Ty \tag{1}</script><h4 id="情形1："><a href="#情形1：" class="headerlink" title="情形1："></a>情形1：</h4><p>如果$X= U \Sigma V^T$,其中$U : n\times n ,\ \ \Sigma: n \times p ,\ \ V^T: p \times p$, 那么</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T = (V \Sigma^T U^T U \Sigma V^T)^{-1}V\Sigma^TU^T=(V \Sigma^T \Sigma V^T)^{-1}V\Sigma^TU^T\tag{2}</script><p>由下面两式,</p><script type="math/tex; mode=display">VV^T = V^TV= I \Rightarrow V^{-1}=V^T\\ (AB)^{-1}=B^{-1}A^{-1}</script><p>可得:</p><script type="math/tex; mode=display">(AV^T)^{-1} = (V^T)^{-1}A^{-1}=VA^{-1}\tag{3}</script><script type="math/tex; mode=display">(VB)^{-1}=B^{-1}V^{T} \tag{4}</script><script type="math/tex; mode=display">(VAV^T)^{-1}= VA^{-1}V^T \tag{5}</script><p>继续化简式2，[利用式5，把  <script type="math/tex">\Sigma^T \Sigma</script>   看作  <script type="math/tex">A</script> ]</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T = V(\Sigma^T \Sigma)^{-1}V^TV\Sigma^TU^T=V(\Sigma^T \Sigma)^{-1}\Sigma^TU^T\tag{6}</script><p>pseudo-inverse:</p><script type="math/tex; mode=display">\Sigma^+ = (\Sigma^T \Sigma)^{-1}\Sigma^T \tag{7}</script><p>并且有上节课里提到的：</p><script type="math/tex; mode=display">\underbrace{ (\Sigma^T \Sigma)^{-1}\Sigma^T}_{p \times n}=\left[\begin{array}{cccc|c}    1/\sigma^{2}_1 & 0 & \cdots & 0  &\\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{array}\right]=\Sigma^+</script><p>由式7得：</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T =V\Sigma^+U^T\tag{8}</script><p>跟上节课一样：</p><script type="math/tex; mode=display">\hat w \approx V\Sigma^+U^Ty\tag{9}</script><h4 id="情形2："><a href="#情形2：" class="headerlink" title="情形2："></a>情形2：</h4><p>$p \ge n$, $X$ 有$n$个独立的行，</p><p>那么有无数解[       无穷 个   <script type="math/tex">w</script>   满足   <script type="math/tex">\hat y = Xw</script>  ].</p><p><strong>Claim</strong>: $\hat w = V \Sigma^+ U^T$ has the smallest norm of any w $s.t: \ y = Xw$。我们从上面的$w$中挑出一个$\hat w$来使其有最小的范数，并且满足约束。</p><script type="math/tex; mode=display">V\Sigma^+U^T = X^T (X^TX)^{-1}\tag{10}</script><p>因为$X$是“瘦长型”矩阵，那么其伪逆为$\Sigma^+= \Sigma^T(\Sigma\Sigma^T)^{-1}$。</p><p>对于任意$w$要有$y = Xw$.我们想要得到   </p><script type="math/tex; mode=display">\lVert  w \rVert^2_{2} \ge \lVert  \hat w \rVert^2_{2}</script><p>而，</p><script type="math/tex; mode=display">\lVert  w \rVert^2_{2} = \lVert  w - \hat w + \hat w \rVert^2_{2}\tag{11}</script><p>回忆一下，</p><script type="math/tex; mode=display">\lVert  a +b \rVert^2_{2} = (a+b)^T(a+b)=a^Ta-2a^Tb+b^Tb=\lVert a\rVert^2_{2}-2a^Tb+\lVert  b \rVert^2_{2}\tag{12}</script><p>式11,变为：</p><script type="math/tex; mode=display">\lVert  w - \hat w + \hat w \rVert^2_{2} = \lVert  w - \hat w \rVert^2_{2} -2(w-\hat w)^T\hat w + \lVert  \hat w \rVert^2_{2} \tag{13}</script><p>我们现在只看中间项，它等价于, 利用 <script type="math/tex">\hat w=X^T(XX^T)^{-1}y</script> 跟伪逆一样理由写成这样化简下 ：</p><script type="math/tex; mode=display">(w-\hat w)^T\hat w =(w-\hat w)^TX^T(XX^T)^{-1}y=\underbrace{(X(w-\hat w))^{T}}(XX^T)^{-1}y\tag{14}</script><p>又因为  <script type="math/tex">y = X w = X\hat w \Rightarrow X\hat w -Xw =X(w-\hat w) =\boldsymbol{0}</script> 。只能是  <script type="math/tex">(w-\hat w) =\boldsymbol{0}</script> .</p><p>所以式12等于：</p><script type="math/tex; mode=display">\begin{aligned}\lVert  w - \hat w + \hat w \rVert^2_{2} &=  \lVert  w - \hat w \rVert^2_{2} +\lVert  \hat w \rVert^2_{2}\\ &\Rightarrow \lVert w \rVert^2_{2} \ge \underbrace{\lVert  w - \hat w \rVert^2_{2}}_{\ge 0} +\lVert  \hat w \rVert^2_{2}\\ &\Rightarrow \lVert  w \rVert^2_{2} \ge \lVert  \hat w \rVert^2_{2} \end{aligned} \tag{15}</script><h3 id="3-Why-is-minimum-norm-good"><a href="#3-Why-is-minimum-norm-good" class="headerlink" title="3.  Why is minimum norm good?"></a>3.  Why is minimum norm good?</h3><p>一些问题</p><p>Why can’t we set $\hat w = X^{-1}y$?</p><ul><li>$X^{-1}$does not exist。</li></ul><p>When $\hat w = (X^TX)^{-1}X^Ty$ vs $\hat w = X^T(XX^T)^{-1}y$？</p><ul><li>$\hat w = (X^TX)^{-1}X^Ty$ :   $n \ge p, \ X \ has \ p \ LI \ cols. $</li><li>$\hat w = X^T(XX^T)^{-1}y$ :   $p \ge n, \ X \ has \ n \ LI \ rows. $</li></ul><p>In both cases,</p><script type="math/tex; mode=display">\hat w = V\Sigma^+U^Ty \tag{16}</script><p>is good.</p><h3 id="4-Matrix-Completion"><a href="#4-Matrix-Completion" class="headerlink" title="4. Matrix Completion"></a>4. Matrix Completion</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261908174.png" alt="Matrix Completionimage-20201106225306125" style="zoom: 100%;" /></p><p>$X \in \mathbb{R}^{n \times p}$,assume $rank(X) = r \leq min(n, p)$, we observe $X_ij \ for (i, j)\in \Omega$</p><p><a href="https://web.stanford.edu/~hastie/TALKS/SVD_hastie.pdf">stanford slides</a></p><h3 id="5-Singular-Value-Thresholding"><a href="#5-Singular-Value-Thresholding" class="headerlink" title="5. Singular Value Thresholding"></a>5. Singular Value Thresholding</h3><p><a href="https://statweb.stanford.edu/~candes/software/svt/">stanford讲义</a></p><p><a href="https://arxiv.org/abs/0810.3286">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 9 The SVD in Machine Learning</title>
      <link href="2020/06/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture9/"/>
      <url>2020/06/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture9/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-9-The-SVD-in-Machine-Learning"><a href="#Lecture-9-The-SVD-in-Machine-Learning" class="headerlink" title="Lecture 9 The SVD in Machine Learning"></a>Lecture 9 The SVD in Machine Learning</h2><p><img src="https://i.loli.net/2020/10/28/6b5vyHQzVTOha4C.png" alt="降维" style="zoom:125%;" /></p><h3 id="矩阵表示"><a href="#矩阵表示" class="headerlink" title="矩阵表示"></a>矩阵表示</h3><p><img src="https://i.loli.net/2020/10/28/ip5EFuTVCfxMIkP.png" alt="示例" style="zoom:125%;" /></p><script type="math/tex; mode=display">X= \left[\begin{array}{r}  1 & -1 & -1 & 1 \\   -1 & 1 & -1 & 1 \\   1 & -1 & -1 & 1 \\   -1 & 1 & -1 & 1 \\1 & -1 & 0 & 0 \end{array} \right]=  \left[\begin{array}{r}   1 & -1  \\   -1 & 1  \\   1 & -1  \\   -1 & 1 \\  1 & -1  \end{array} \right]\left[\begin{array}{r}   1 & -1 &0&0 \\   0 & 0&-1&1  \\   \end{array} \right]\tag{1}</script><p>其实是利用从列向量乘法来分解的,$X$第一列是由：</p><script type="math/tex; mode=display">X[1]=\left[\begin{array}{r}   1 \\   -1 \\   1  \\   -1 \\  1 \end{array} \right]=1*\left[\begin{array}{r}   1 \\   -1 \\   1  \\   -1 \\  1 \end{array} \right] + 0*\left[\begin{array}{r}   -1 \\   1 \\   -1  \\   1 \\  -1 \end{array} \right]</script><p>这不是SVD，不是奇异值。</p><p>$X$的秩是2，列向量、行向量最大为2.</p><p>上面接下来才是SVD。</p><p>Python numpy SVD结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [-<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [-<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line"></span><br><span class="line">U: [[-<span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span>  <span class="number">7.11769812e-01</span>  <span class="number">2.08287624e-01</span></span><br><span class="line">   <span class="number">1.00420616e-17</span>]</span><br><span class="line"> [ <span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span> -<span class="number">3.37852315e-02</span> -<span class="number">1.24600260e-01</span></span><br><span class="line">   <span class="number">7.30296743e-01</span>]</span><br><span class="line"> [-<span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span> -<span class="number">7.00888060e-01</span>  <span class="number">2.34632102e-01</span></span><br><span class="line">  -<span class="number">6.08580619e-02</span>]</span><br><span class="line"> [ <span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span>  <span class="number">2.29034795e-02</span> -<span class="number">3.18319466e-01</span></span><br><span class="line">  -<span class="number">6.69438681e-01</span>]</span><br><span class="line"> [-<span class="number">4.47213595e-01</span> -<span class="number">5.55111512e-17</span> -<span class="number">2.17635041e-02</span> -<span class="number">8.85839452e-01</span></span><br><span class="line">   <span class="number">1.21716124e-01</span>]] </span><br><span class="line"> Sigma: [<span class="number">3.16227766e+00</span> <span class="number">2.82842712e+00</span> <span class="number">2.95877601e-16</span> <span class="number">4.03361097e-17</span>] </span><br><span class="line">    </span><br><span class="line"> VT: [[-<span class="number">7.07106781e-01</span>  <span class="number">7.07106781e-01</span>  <span class="number">0.00000000e+00</span>  <span class="number">0.00000000e+00</span>]</span><br><span class="line"> [-<span class="number">7.02166694e-17</span> -<span class="number">7.02166694e-17</span>  <span class="number">7.07106781e-01</span> -<span class="number">7.07106781e-01</span>]</span><br><span class="line"> [ <span class="number">2.98602000e-01</span>  <span class="number">2.98602000e-01</span> -<span class="number">6.40965557e-01</span> -<span class="number">6.40965557e-01</span>]</span><br><span class="line"> [ <span class="number">6.40965557e-01</span>  <span class="number">6.40965557e-01</span>  <span class="number">2.98602000e-01</span>  <span class="number">2.98602000e-01</span>]]</span><br></pre></td></tr></table></figure><p>$X^T=\tilde V \tilde \Sigma \tilde U^T $</p><h3 id="最小二乘的SVD"><a href="#最小二乘的SVD" class="headerlink" title="最小二乘的SVD"></a>最小二乘的SVD</h3><p><img src="https://i.loli.net/2020/10/28/uN4JBCmSzZceHiP.png" alt="SVD最小二乘" style="zoom:125%;" /></p><script type="math/tex; mode=display">\tilde w =(X^TX)^{-1}X^Ty=(V\Sigma^TU^TU\Sigma V^T)^{-1}V\Sigma^TU^T \tag{2}</script><p>对于均是方阵$(AB)=B^{-1}A^{-1}$。另外$U^TU=U^TU=I=UU^{-1}$。</p><p>式2变为：</p><script type="math/tex; mode=display">\tilde w =(V\Sigma^T\Sigma V^T)^{-1}V\Sigma^TU^T \tag{3}</script><p>$V^TV=VV^T=I=VV^{-1} \Rightarrow V^T=V^{-1}$</p><script type="math/tex; mode=display">\begin{aligned}\tilde w &=(V^T)^{-1}(V\Sigma^T \Sigma)^{-1}V \Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}V^{-1}V\Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}V^{T}V\Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}\Sigma^TU^T\\\end{aligned}</script><p>因为$n \ge p$,$\Sigma$矩阵是瘦长型，下面全是0.$\Sigma$是$n \times p$,那么$\Sigma^T\Sigma$是$p \times p$。即</p><script type="math/tex; mode=display">\Sigma^T \Sigma=\begin{bmatrix}    \sigma^{2}_1 & 0 & \cdots & 0 \\    0 & \sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \sigma^{2}_p \end{bmatrix}</script><script type="math/tex; mode=display">(\Sigma^T \Sigma)^{-1}=\begin{bmatrix}    1/\sigma^{2}_1 & 0 & \cdots & 0 \\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{bmatrix}</script><p>那么，竖线后面全是0</p><script type="math/tex; mode=display">\underbrace{ (\Sigma^T \Sigma)^{-1}\Sigma^T}_{p \times n}=\left[\begin{array}{cccc|c}    1/\sigma^{2}_1 & 0 & \cdots & 0  &\\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{array}\right]=\Sigma^+</script><p>又叫做$\Sigma$的pseudo-inverse。</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T=V\Sigma^{+}U^T</script><script type="math/tex; mode=display">\hat y= V\Sigma U^T\hat w</script><p>$ w$估计为：</p><script type="math/tex; mode=display">\hat w = V\Sigma^{+}U^Ty</script><p>证明：</p><script type="math/tex; mode=display">\begin{aligned}y &\approx  \hat y = U \ Sigma V^T \hat w\\\\&\Rightarrow U^Ty \approx U^TU \Sigma V^T \hat w \quad 而UU^T=I\\\\&\Rightarrow \Sigma^+U^Ty \approx \Sigma^+\Sigma V^T \hat w \quad 因为\Sigma^+\Sigma=I\\\\&\Rightarrow V \Sigma^+U^Ty  \approx VV^T \hat w \quad 而VV^T=I\\\\&\Rightarrow V \Sigma^+U^Ty  \approx \hat w\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 8 The Singular Value Decomposition</title>
      <link href="2020/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture8/"/>
      <url>2020/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture8/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-8-The-Singular-Value-Decomposition"><a href="#Lecture-8-The-Singular-Value-Decomposition" class="headerlink" title="Lecture 8 The Singular Value Decomposition"></a>Lecture 8 The Singular Value Decomposition</h2><h3 id="1-SVD"><a href="#1-SVD" class="headerlink" title="1. SVD"></a>1. SVD</h3><p>$X \in \mathbb{R}^{n \times p}$有SVD $U\Sigma V^T$,并且满足：</p><ul><li>$U \in \mathbb{R}^{n \times n}$是正交的$UU^T=U^TU=I$ [   $U$的列=左边奇异向量]</li><li>$V \in \mathbb{R}^{p \times p}$是正交的$VVT=V^TV=I$ [   $V$的列=右边奇异向量]</li><li>$\Sigma \in \mathbb{R}^{n \times p}$是对角阵，并且对角元素满足$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p$。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/szpP8XFyVHhESN4.png?=raw" width=50% height=50%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">SVD示意图</div> </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line">U: [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">    [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">    [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: </span><br><span class="line"> [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">OUT：</span><br><span class="line">U: </span><br><span class="line"> [[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: </span><br><span class="line"> [[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[-<span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">U: [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: [[-<span class="number">1.</span> -<span class="number">0.</span> -<span class="number">0.</span>]</span><br><span class="line">   [-<span class="number">0.</span> -<span class="number">1.</span> -<span class="number">0.</span>]</span><br><span class="line">   [-<span class="number">0.</span> -<span class="number">0.</span> -<span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><p>注意上面，SVD 中$U, \Sigma, V$变化</p><p>对于上面第三个例子，$SVD(X)=(U\Sigma V^T)^T=V\Sigma U^T$</p><p>奇异值会按照从大到小排，这跟最小化投影有关。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/9Ysda1k6mMwDcQ3.png?=raw" width=60% height=60%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">奇异值变化</div> </center><p>等价地，我们可以用rank-1的矩阵的和表示矩阵$X$</p><script type="math/tex; mode=display">X= \sum^{min(n, p)}_{i=1}\sigma_iU_iV_i^T \tag{1}</script><p>其中，$U_i$是$U$的第$i$列，$V_i^T$是$V$的第$i$列。</p><p><img src="https://i.loli.net/2020/10/28/TRa64gqme1uKYd8.png" alt="SVD分解" style="zoom:100%;" /></p><h3 id="2-The-subspace-approximation-Theorem"><a href="#2-The-subspace-approximation-Theorem" class="headerlink" title="2. The subspace approximation Theorem"></a>2. The subspace approximation Theorem</h3><p>给定$X \in \mathbb{R}^{n \times p}$，$rank (X) = r \le min(p, n)$。矩阵的秩等于奇异值大于0的个数$num(\sigma_i \ge 0)$。</p><p>找一个$X_k \in \mathbb{R}^{n \times p}$,并且$rank (X_k) = k &lt; r$，作为尽可能接近$X$。(as “close “ as possible to X).</p><script type="math/tex; mode=display">min_{Z \in \mathbb{R}^{n \times p}} \lVert  Z-X \rVert^2_{F} \ \text{with rank(Z)=k} \tag{2}</script><p><strong>Frobenius norm</strong> :</p><script type="math/tex; mode=display">\lVert  Z-X \rVert^2_{F} = \left( \sum_{(i, j)}A^2_{i, j} \right)^{\frac{1}{2}} \tag{3}</script><script type="math/tex; mode=display">If A = [A_1, A_2, \cdots, A_p], \ then \ \lVert  A \rVert^2_{F} = \lVert  A \rVert^2_{2} \tag{4}</script><p>近似矩阵$\tilde X$的SVD有：</p><ul><li>$\tilde \Sigma \in \mathbb{R}^{r \times r}对角元素 \tilde \sigma_1 \ge \tilde \sigma_2 \ge \dots \ge \tilde \sigma_p.$</li><li>$\tilde U \in \mathbb{R}^{n \times r}, \tilde U^T\tilde U=I, but \ \tilde U\tilde U^T \ne I $</li><li>$\tilde V \in \mathbb{R}^{p \times r}, \tilde V^{T}\tilde V=I, but \ \tilde V\tilde V^T \ne I $</li></ul><p><img src="https://i.loli.net/2020/10/28/iYI2ZeNqgyufBXJ.png" alt="The subspace approximation Theorem" style="zoom:100%;" /></p><p><strong>Singular Value Spectrum</strong>:</p><h3 id="3-The-“-Economy-SVD”"><a href="#3-The-“-Economy-SVD”" class="headerlink" title="3. The “ Economy SVD”"></a>3. The “ Economy SVD”</h3><p><img src="https://i.loli.net/2020/10/28/rU3scDBv1nh9tyC.png" alt="SVD的作用" style="zoom:100%;" /></p><h3 id="4-降维"><a href="#4-降维" class="headerlink" title="4. 降维"></a>4. 降维</h3><p><img src="https://i.loli.net/2020/10/28/Wp35fln1gKMvG7e.png" alt="降维" style="zoom:100%;" /></p><p>给定$X_i \in \mathbb{R}^p \ for \ i=1, \cdots, n$，找到对于$k &lt; p \ for \  i=1, \dots, n 的Z_i \in \mathbb{R}^k $，有跟$X_i$同样的性质。</p><script type="math/tex; mode=display">X^T_k = V_k{\Sigma}U^T_k</script><p>其中，</p><ul><li>$V_k$: $p \times k$ 对于最好的k-dim子空间的基。</li><li>${\Sigma}U^T_k$ 第i列是对$\tilde X_i$k组基的系数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 7 Introduction to the Singular Value Decomposition</title>
      <link href="2020/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/"/>
      <url>2020/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Introduction-to-the-Singular-Value-Decomposition"><a href="#Lecture-7-Introduction-to-the-Singular-Value-Decomposition" class="headerlink" title="Lecture 7 Introduction to the Singular Value Decomposition"></a>Lecture 7 Introduction to the Singular Value Decomposition</h2><h3 id="1-问题引入和投影矩阵的性质"><a href="#1-问题引入和投影矩阵的性质" class="headerlink" title="1. 问题引入和投影矩阵的性质"></a>1. 问题引入和投影矩阵的性质</h3><p>Goal:对于观测值$X_1, X_2,\cdots, X_p \in \mathbb{R}^n$,找到一个一维子空间(可以理解为一根直线)能”最好的拟合数据”</p><p>Solution： </p><script type="math/tex; mode=display">\text{ 把每一个 }X_i \text{ 投影到 } \overrightarrow a上， Proj^{X_i}_{\overrightarrow a} \ , \text{ 投影到让距离 } d^2_i=\lVert  X_i- Proj^{X_i}_{\overrightarrow a}\rVert^2_{2} 和最小。</script><p>复习<strong>Projection Matrices</strong>：</p><p>If $A \in \mathbb{R}^{n \times p}$张成的子空间，那么Projection of $X$ onto $span(cols(A))=Proj_A X$，如果$A$的每列都是线性无关的，并且有：</p><script type="math/tex; mode=display">Proj_A X = A(A^TA)^{-1}X \tag{1}</script><p>其中，$P_A=A(A^TA)^{-1}A^T$。</p><p>还有性质：</p><ol><li><script type="math/tex; mode=display">P_A = P^{2}_A=P_AP_A=P^T_{A}=P^T_{A}P_A \tag{2}</script></li><li><p>If $A = \boldsymbol{a}$,</p><script type="math/tex; mode=display">P_{\boldsymbol{a}} = \boldsymbol{a}(\boldsymbol{a}^T\boldsymbol{a})^{-1}\boldsymbol{a}^T=\frac{\boldsymbol{a}\boldsymbol{a}^T}{\boldsymbol{a}^T\boldsymbol{a}}\tag{3}</script><p>因为$(\boldsymbol{a}^T\boldsymbol{a})^{-1}$是一个标量(可以看详细投影矩阵的证明)。</p></li></ol><ol><li><p>The orthofonal complement of a subspace is the set of all vectors orthogonal to the subspace.</p><p>子空间的正交补 <strong>orthofonal complement</strong> 是所有正交于子空间的向量的集合(这个子空间是正交子空间)。并且：$A^TB=0$（正交）</p><p>(不是这个图的示意，是)如果$A \in \mathbb{R}^{n \times r0}$,那么$B \in \mathbb{R}^{n \times (n-r)}$。维度上和不变，正交补的维度是其差。</p></li></ol><p>   对于任意的$X \in \mathbb{R}^n$,能写作：</p><script type="math/tex; mode=display">   IX = P_AX+P_BX =(P_A + P_B)X \Rightarrow I=P_A+P_B \ \Rightarrow P_B=I-P_A\tag{4}</script>   <center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/4aoHTGlmw6BF7N3.png?=raw" width=25% height=25%>        <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">正交空间</div> </center><p><img src="https://i.loli.net/2020/10/27/oGJK8lOet1m3fIy.png" alt="SVD几何表示" style="zoom:100%;" /></p><h3 id="2-证明最小距离和、引入奇异向量奇异值"><a href="#2-证明最小距离和、引入奇异向量奇异值" class="headerlink" title="2. 证明最小距离和、引入奇异向量奇异值"></a>2. 证明最小距离和、引入奇异向量奇异值</h3><p><img src="https://i.loli.net/2020/11/06/zwhjaEtkxWKfeid.png" alt="SVD" style="zoom:125%;" /></p><p>将$P_a$用式3代入，然后相当于做了提取公因子$X_i$ [拓展作单位矩阵 $IX_i$ ]，那么$I-\frac{aa^T}{a^Ta}$可以看作投影矩阵$I-P_a$,由式4可得这也是投影矩阵$P_B$,这就是正交补。再写作内积展开式，由性质2公式3可以知道$(I-\frac{aa^T}{a^Ta})^T(I-\frac{aa^T}{a^Ta})=I-\frac{aa^T}{a^Ta}$.</p><p>​    而$X_i^TX$是常数跟$a$无关。即只要最大化$\frac{X_iaa^TX_i}{a^Ta}$.</p><p>​    $a^TX_i$是标量。</p><p>​    </p><script type="math/tex; mode=display">\begin{aligned}XX^T&=\left[\begin{array}{ll}| & |&|&|\\X_{1} & X_{2}&\cdots&X_{p} \\| &| &|&|\end{array}\right]\left[\begin{array}{ll}- & X_{1}^{\top}&- \\- & X_{2}^{\top}&- \\- &    \vdots &-        \\- & X_{p}^{\top}&-\end{array}\right]\\&=X_1X_1^T+X_2X_2^T+\cdots+X_pX_p^T\\&=\sum^{p}_{1}X_iX_i^T \end{aligned}\tag{5}</script><p>​    用式5矩阵的外积，写作矩阵外积去掉求和，只要求$argmax_{a}(\frac{a^TXX^Ta}{a^Ta})$。</p><p>现在定义一些名词：</p><ol><li><p>使得上是取得最大值的向量$\hat{\boldsymbol{a}}$称作$\boldsymbol{X}$的第一个左奇异向量。</p></li><li><p>$\frac{a^TXX^Ta}{a^Ta}=\sigma^{2}_1$(这里没用黑粗体懒得写了)的值称作$X$第一个奇异值。</p><script type="math/tex; mode=display">\begin{aligned}\frac{a^TXX^Ta}{a^Ta}&=\sigma^{2}_1\\&= \lVert X \rVert^2_{op} \ \text{"operator norm"}\\&=\lVert X \rVert^2_{2} (跟向量2范数有别)\end{aligned}\tag{6}</script></li></ol><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3. 奇异值分解"></a>3. 奇异值分解</h3><p><img src="https://i.loli.net/2020/11/03/GTAeDhXVw1gbL7K.png" alt="SVD降维" style="zoom:100%;" /></p><p>$U$的列向量是$X$的列向量的正交基。</p><p>$\Sigma$是没有负元素的对角矩阵，$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p$。</p><p>令$U = [U_1, U_2, \cdots, U_n]$,  $U_1$is the best 1d subspace fit to $X_i’$s。也是上面式6的解。</p><p>令 <script type="math/tex">\tilde X_i^{(1)} = X_i - Proj_{U_1}{X_i}</script> ，</p><p>​    </p><script type="math/tex; mode=display">U_2= \text{ the best 1d subspace fit to } \tilde X_i' s.</script><p>若$X_{n \times p} $有秩$r &lt; min(n, p)$。</p><p>thin $\Sigma$。</p><h3 id="4-PCA"><a href="#4-PCA" class="headerlink" title="4. PCA"></a>4. PCA</h3><p><img src="https://i.loli.net/2020/10/27/bz3rqiYtBOsEHwF.png" alt="PCA-1" style="zoom:100%;" /></p><p><img src="https://i.loli.net/2020/10/28/ewo8Ipvx4tluJbj.png" alt="PCA-2" style="zoom:100%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 6 Finding Orthogonal Bases</title>
      <link href="2020/06/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture6/"/>
      <url>2020/06/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture6/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-Finding-Orthogonal-Bases"><a href="#Lecture-6-Finding-Orthogonal-Bases" class="headerlink" title="Lecture 6 Finding Orthogonal Bases"></a>Lecture 6 Finding Orthogonal Bases</h2><h3 id="1-怎么求得U"><a href="#1-怎么求得U" class="headerlink" title="1.怎么求得U?"></a>1.怎么求得U?</h3><ol><li>Gram Schemidt Orthogonalization</li><li>Singular Value Decomposition</li></ol><p>有$X$怎么找到$U$?</p><p><img src="https://i.loli.net/2020/10/26/USF1erTsbyMqVJa.png" alt="施密特正交化"></p><p>残差 <script type="math/tex">X^{\prime}_2=X_2-aU_1</script> ,所以 <script type="math/tex">U_2=\frac {X^{\prime}_2} {\lVert  X^{\prime}_2 \rVert^2_{2}}=\left[\begin{array}{c}0 \\b \end{array}\right]/b=\left[\begin{array}{c}0 \\1 \end{array}\right]</script> </p><h3 id="2-斯密特正交化的几何示例"><a href="#2-斯密特正交化的几何示例" class="headerlink" title="2. 斯密特正交化的几何示例"></a>2. 斯密特正交化的几何示例</h3><p><img src="https://i.loli.net/2020/10/26/5WoS1ITcDa3iJ2A.png" alt="施密特正交化计算" style="zoom:100%;" /></p><script type="math/tex; mode=display">\begin{align}&X^{\prime}_2=X_2-Projection(X_2 \ onto \ U_1) \ , \text{best fit of } X_2 \text{ as weighted  }  U_1  。\\ &\text{best  fit} = U_1w = U_1U^{T}_1X_2=P_{U_1}(X_2)\end{align}</script><p>示例，施密特正交化具体过程和想法。</p><script type="math/tex; mode=display">U_1=\left[\begin{array}{c}1/\sqrt{2} \\1/\sqrt{2}\\0\end{array}\right],X_2=\left[\begin{array}{c}1 \\3\\0\end{array}\right]</script><p>  那么，</p><script type="math/tex; mode=display">U_1U_1^TX_2=\left[\begin{array}{c}1/2 & 1/2& 0\\1/2 & 1/2& 0\\0   & 0  & 0\end{array}\right]\left[\begin{array}{c}1\\3\\0\end{array}\right]=\left[\begin{array}{c}2\\2\\0\end{array}\right]</script><script type="math/tex; mode=display">X_2-X_2-U_1U_1^TX_2=\left[\begin{array}{c}1\\3\\0\end{array}\right]-\left[\begin{array}{c}2\\2\\0\end{array}\right]=\left[\begin{array}{c}-1\\1\\0\end{array}\right]\tag{1}</script><script type="math/tex; mode=display">U_2=\left[\begin{array}{c}-1\\1\\0\end{array}\right]/ \sqrt{2}=\left[\begin{array}{c}-\sqrt{2}/2\\\sqrt{2}/2\\0\end{array}\right]</script><h3 id="3-斯密特正交化算法"><a href="#3-斯密特正交化算法" class="headerlink" title="3. 斯密特正交化算法"></a>3. 斯密特正交化算法</h3><p><img src="https://i.loli.net/2020/10/26/9HBc6F81seotMRW.png" alt="施密特正交化算法"></p><p><a href="https://zh.wikipedia.org/wiki/%E6%A0%BC%E6%8B%89%E5%A7%86-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96">施密特正交化wiki</a></p><p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://upload.wikimedia.org/wikipedia/commons/5/56/GSO.png?=raw" width=25% height=25%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">斯密特正交化 引用自wiki https://upload.wikimedia.org/wikipedia/commons/5/56/GSO.png</div> </center><br><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261910088.png" alt="施密特正交化计算" style="zoom:100%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
            <tag> 正交基 </tag>
            
            <tag> 斯密特正交化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 5 Subspaces, Bases, and Projections</title>
      <link href="2020/06/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture5/"/>
      <url>2020/06/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture5/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-5-Subspaces-Bases-and-Projections"><a href="#Lecture-5-Subspaces-Bases-and-Projections" class="headerlink" title="Lecture 5 Subspaces, Bases, and Projections"></a>Lecture 5 Subspaces, Bases, and Projections</h2><h3 id="1-回想最小二乘的几何意义引入span"><a href="#1-回想最小二乘的几何意义引入span" class="headerlink" title="1.回想最小二乘的几何意义引入span"></a>1.回想最小二乘的几何意义引入span</h3><p><img src="https://i.loli.net/2020/10/26/2K3OkpZTwcEmt4A.png" alt="最小二乘几何表示" style="zoom: 33%;" /></p><p>上面的平面可有$X$的列向量的张成空间$X$。</p><script type="math/tex; mode=display">span(cols(X)) =  { v \in \mathbb{R}^n \ v=w_1X_1 + w_2X_2+\cdots+ w_pXp \quad for \ some \ w_1, w_2, \cdots, w_p  }</script><h3 id="2-subspace"><a href="#2-subspace" class="headerlink" title="2. subspace"></a>2. subspace</h3><p><img src="https://i.loli.net/2020/10/26/vau9GQDrsZqHVRL.png" alt="子空间" style="zoom: 33%;" /></p><p>If the cols of $X$ are Linearly Independent, $X$ is a subspace. Then  they form a basis for $X$.</p><p>上上图中的(绿色点)$\hat {\underline y}$是$\underline{y}$在$X$上的投影。</p><p><img src="https://i.loli.net/2020/10/26/WtPgoJCYe1KbcNS.png" alt="空间" style="zoom: 33%;" /></p><p>vertical plane</p><p>horizontal plane</p><p>子空间永远包含原点或者说$\overrightarrow{0}$。</p><p><img src="https://i.loli.net/2020/10/26/CSrXLAnMBOyw8xY.png" alt="span" style="zoom: 33%;" /></p><h3 id="3-怎么表示一个子空间"><a href="#3-怎么表示一个子空间" class="headerlink" title="3. 怎么表示一个子空间?"></a>3. 怎么表示一个子空间?</h3><p><img src="https://i.loli.net/2020/10/26/iOnAXw7rGBfvN4T.png" alt="子空间的表示" style="zoom: 33%;" /></p><ul><li>用一组向量的集合作为一个张成子空间</li><li>用一组线性无关的向量张成子空间，这组向量叫一组基</li><li>用一组正交单位向量(orthonormal vector)张成子空间,这组向量又叫正交单位化基，正交基。</li></ul><p>orthonormal: </p><ul><li>ortho : orthogonal</li><li>normal: norm(length=1)</li></ul><p>理解正交、单位向量</p><script type="math/tex; mode=display">如果 \ <u_1, u_2>=u_1^Tu_2=u_2^Tu_1=0\tag{1}</script><p>那么，两个向量$u_1$和$u_2$都是正交的。</p><script type="math/tex; mode=display">如果 \ \lVert  u\rVert_{2} = \lVert  u\rVert^2_{2} =<u_1, u_2>=u^Tu=1 \tag{2}</script><p>那么$u$是单位化的。</p><script type="math/tex; mode=display">如果u_1, u_2, \cdots, u_p满足：<u_i, u_j>={\begin{cases}1,&{\mbox{if }}i=j\\0&{\mbox{if }}i \neq j\end{cases}}</script><p>这组向量$u_1, u_2, \cdots, u_p$是正交单位向量</p><h3 id="4-正交矩阵及其性质"><a href="#4-正交矩阵及其性质" class="headerlink" title="4. 正交矩阵及其性质"></a>4. 正交矩阵及其性质</h3><p><img src="https://i.loli.net/2020/10/26/OgLB42xKds3c1If.png" alt="正交基" style="zoom: 33%;" /></p><p>The matrix $U$ is orthogonal in this setting if it is a $p \times p$ square matrix. If it’s an $n \times p$ matrix with $n&gt;p$, then $U $gives a basis for a subspace but it is not an orthogonal matrix. In this case, $U^TU=I$,  but $UU^T$ is not = $I$.</p><p>如果$u_1, \cdots, u_p$是正交单位化的，且$\delta = span (u_1, \cdots, u_p)$，那么</p><script type="math/tex; mode=display">U=\left[\begin{array}{ll}| & | &| \\u_1 & u_2 ...&u_p \\| & | & |\end{array}\right]是正交基矩阵。\tag{3}</script><p>性质1：</p><script type="math/tex; mode=display">U^TU = I\tag{4}</script><p>性质2:</p><script type="math/tex; mode=display">\lVert  Uv\rVert^2_{2} = \lVert  v\rVert^2_{2}\tag{5}</script><p>记个记号：dimension of subspace , $dim(\delta)$=子空间基向量的个数。</p><p>如果一个子空间有矩阵$X$的列向量张成，那么$dim(\delta)=rank(X)$。</p><h3 id="5-矩阵中含有线性无关-列或行-向量的数目"><a href="#5-矩阵中含有线性无关-列或行-向量的数目" class="headerlink" title="5. 矩阵中含有线性无关(列或行)向量的数目?"></a>5. 矩阵中含有线性无关(列或行)向量的数目?</h3><p><img src="https://i.loli.net/2020/10/26/vmoji1UF9BubpIs.png" alt="无线向量" style="zoom: 33%;" /></p><p>$X \in \mathbb{R}^{n\times p}$，$r=rank(X)\leq min(n,p)$ , 假设$n \geq p \Rightarrow r \leq p$。</p><p>我们能在$\mathbb{R}^n$上有多于n个线性无关的向量?——不能。</p><h3 id="6-Projection"><a href="#6-Projection" class="headerlink" title="6. Projection"></a>6. Projection</h3><p>一个点的投影到一个集上是这个集上离这个点最近的点。</p><p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/26/lRCy7ZQw3t5D1YA.png?=raw" width="50%" height="50%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Projection</div> </center><br><img src="https://i.loli.net/2020/10/26/HNhJmEaA9eZicOM.png" alt="Projection" style="zoom: 25%;" /></p><p>$(X^TX)^{-1}X^T$又叫作投影矩阵, 记作$P_X$。</p><p>用投影的定义来最优化解决问题。</p><p>$P_X$的性质：</p><ol><li>方阵</li><li>$P_X=P_XP_X=P^2_X$</li></ol><p>从定义很容易证明，投影的投影就是本身。</p><h3 id="7-正交基子空间和最小二乘法"><a href="#7-正交基子空间和最小二乘法" class="headerlink" title="7. 正交基子空间和最小二乘法"></a>7. 正交基子空间和最小二乘法</h3><p><img src="https://i.loli.net/2020/10/26/rAndYw2IyQNDxlV.png" alt="正交基和最小二乘" style="zoom: 25%;" /></p><p>这里用全新的角度来解决上述中非常难计算的$P_X=(X^TX)^{-1}X^T$。</p><p>找到一组正交基向量使得$span(X_1, X_2, \cdots, X_p)=span(U_1, U_2, \cdots, U_p)$即$span(cols(U))=span(cols(X))$。</p><p>那么$P_Xy=P_Uy$，就有$\hat y = UU^Ty$。不要矩阵的逆了。</p><p><img src="https://i.loli.net/2020/10/26/FPwCGDrkfWv7MKR.png" alt="线性表示子空间" style="zoom: 25%;" /></p><p><img src="https://i.loli.net/2020/10/26/sSxTEZRGKgA4twh.png" alt="线性表示子空间-2" style="zoom: 25%;" /></p><p>注意：最后一行。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
            <tag> 子空间 </tag>
            
            <tag> 正交基 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6. 基本句法</title>
      <link href="2020/06/12/6.%20%E5%9F%BA%E6%9C%AC%E5%8F%A5%E6%B3%95/"/>
      <url>2020/06/12/6.%20%E5%9F%BA%E6%9C%AC%E5%8F%A5%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="6-基本句法"><a href="#6-基本句法" class="headerlink" title="6. 基本句法"></a>6. 基本句法</h2><h3 id="1-基本结构"><a href="#1-基本结构" class="headerlink" title="1.基本结构"></a>1.基本结构</h3><ol><li>顺序语句</li><li>分支语句</li><li>循环语句</li></ol><h4 id="1-if-switch-语句"><a href="#1-if-switch-语句" class="headerlink" title="1. if, switch 语句"></a>1. if, switch 语句</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isLeapYear</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> year)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ((year % <span class="number">4</span> == <span class="number">0</span> &amp;&amp; year % <span class="number">100</span> !=<span class="number">0</span>) || (year % <span class="number">400</span> == <span class="number">0</span>))&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">enum</span> _<span class="title">COLOR</span>&#123;</span></span><br><span class="line">    RED,</span><br><span class="line">    GREEN,</span><br><span class="line">    BLUE,</span><br><span class="line">    UNKNOWN</span><br><span class="line">&#125;color;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="built_in">isLeapYear</span>(<span class="number">2000</span>) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">isLeapYear</span>(<span class="number">2001</span>) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">isLeapYear</span>(<span class="number">2020</span>) &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    color color0;</span><br><span class="line">    color0 = BLUE;</span><br><span class="line">    <span class="keyword">if</span> (color0==RED)&#123;cout &lt;&lt; <span class="string">&quot;red&quot;</span> &lt;&lt; endl;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (color0==GREEN)&#123;cout &lt;&lt; <span class="string">&quot;green&quot;</span> &lt;&lt; endl;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (color0==BLUE)&#123;cout &lt;&lt; <span class="string">&quot;blue&quot;</span> &lt;&lt; endl;&#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;cout &lt;&lt; <span class="string">&quot;unknown&quot;</span> &lt;&lt; endl;&#125;</span><br><span class="line"></span><br><span class="line">    color color1;</span><br><span class="line">    color1 = GREEN;</span><br><span class="line">    <span class="built_in"><span class="keyword">switch</span></span> (color1) &#123;</span><br><span class="line">        <span class="keyword">case</span> RED: <span class="comment">// case 后面接常数值</span></span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;red&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> GREEN:</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;green&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> BLUE:</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;blue&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;unknown&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>if 和switch 比较</strong>：</p><ul><li>使用场景<ol><li>switch 只支持常量值固定相等的分支判断</li><li>if还可用于区间判断</li><li>用switch能做的，用if都能做，反过来则不行</li></ol></li><li>性能比较<ol><li>分支较少是，差别不是很多；分支多是，switch性能较高</li><li>if开始处几个分支效率很高，之后递减</li><li>switch所有case的速度几乎一样</li></ol></li></ul><h3 id="2-自定义结构——枚举enum"><a href="#2-自定义结构——枚举enum" class="headerlink" title="2. 自定义结构——枚举enum"></a>2. 自定义结构——枚举enum</h3><ul><li>使用<code>#define</code> 和 <code>const</code>创建符号常量，使用<code>enum</code>不仅能够创建符号常量，还能定义新的数据类型。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">wT</span>&#123;</span>Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday&#125;;</span><br><span class="line">    wT weekday;</span><br><span class="line">    weekday = Monday;</span><br><span class="line">    weekday = Wednesday;</span><br><span class="line">    <span class="comment">// weekday = 1 非法 不能直接给int值，只能赋值成wT定义好的类型值 不能做左值</span></span><br><span class="line">    cout &lt;&lt; weekday &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用细节：</strong></p><ol><li>枚举值不可以做左值</li><li>非枚举变量不可以赋值给枚举变量</li><li>枚举变量可以赋值给非枚举变量</li></ol><h3 id="3-自定义结构——结构体与联合体"><a href="#3-自定义结构——结构体与联合体" class="headerlink" title="3. 自定义结构——结构体与联合体"></a>3. 自定义结构——结构体与联合体</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">union</span> <span class="title">Score</span>&#123;</span></span><br><span class="line">        <span class="keyword">double</span> ds; <span class="comment">// 8</span></span><br><span class="line">        <span class="keyword">char</span> level; <span class="comment">// 1 联合体使用同一个空间</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">Student</span>&#123;</span></span><br><span class="line">        <span class="keyword">char</span> name[<span class="number">6</span>]; <span class="comment">// 6 * 1 bytes</span></span><br><span class="line">        <span class="keyword">int</span> age;<span class="comment">// 4 bytes</span></span><br><span class="line">        Score s;<span class="comment">// 8 bytes</span></span><br><span class="line">    &#125;;</span><br><span class="line">    cout &lt;&lt; <span class="built_in"><span class="keyword">sizeof</span></span>(Score) &lt;&lt; endl; <span class="comment">// 8</span></span><br><span class="line">    cout &lt;&lt; <span class="built_in"><span class="keyword">sizeof</span></span>(Student) &lt;&lt; endl; <span class="comment">//24</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结构体数据对齐问题：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">s1</span>&#123;</span></span><br><span class="line">        <span class="keyword">char</span> x;<span class="comment">//1byte 不满 4个字节， 在32位cpu中，直接补成4字节</span></span><br><span class="line">        <span class="keyword">int</span> z;<span class="comment">// 4</span></span><br><span class="line">        <span class="keyword">short</span> y;<span class="comment">//2 不满 4个字节， 在32位cpu中，直接补成4字节 共12个字节</span></span><br><span class="line">    &#125;;</span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">s2</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> x;<span class="comment">//1 </span></span><br><span class="line">    <span class="keyword">short</span> y;<span class="comment">//2 先一个字节 加 2个字节凑成4字节</span></span><br><span class="line">    <span class="keyword">int</span> z;<span class="comment">//4 共8个字节</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">s3</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> x;<span class="comment">//1</span></span><br><span class="line">    <span class="keyword">short</span> y;<span class="comment">//2</span></span><br><span class="line">    <span class="keyword">double</span> z;<span class="comment">//8 以最大的size考虑，上面1+2会补成8，共16bytes</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line">cout &lt;&lt; <span class="built_in"><span class="keyword">sizeof</span></span>(s1) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in"><span class="keyword">sizeof</span></span>(s2) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in"><span class="keyword">sizeof</span></span>(s3) &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>对于32位CPU：</p><ul><li><code>char</code>： 任何地址</li><li><code>short</code>：偶数地址</li><li><code>int</code>： 4的整数倍地址</li><li><code>double</code>： 8的整数倍地址</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Student stu;</span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(stu.name, <span class="string">&quot;lili&quot;</span>, <span class="number">6</span>);</span><br><span class="line">stu.age = <span class="number">16</span>;</span><br><span class="line">stu.s.ds = <span class="number">95.5</span>;</span><br><span class="line">stu.s.level = <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line">cout &lt;&lt; stu.name &lt;&lt; endl; <span class="comment">// lili</span></span><br></pre></td></tr></table></figure><h3 id="4-循环语句"><a href="#4-循环语句" class="headerlink" title="4. 循环语句"></a>4. 循环语句</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>, index = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (index &lt;= <span class="number">100</span>)&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">        index ++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// for循环</span></span><br><span class="line">    <span class="comment">// index = 1;</span></span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span>  index = <span class="number">1</span>; index &lt;= <span class="number">100</span>; ++index)&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//do-while语句</span></span><br><span class="line">    sum = <span class="number">0</span>, index = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">do</span>&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">        index += <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">while</span> (index &lt;= <span class="number">100</span>);</span><br><span class="line">    cout &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Q:输出所有形如aabb的四位完全平方数。</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// 实现 aabb的完全平方数</span></span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">double</span> m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> a = <span class="number">0</span>; a &lt; <span class="number">10</span>; ++a) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> b = <span class="number">0</span>; b &lt; <span class="number">10</span>; ++b) &#123;</span><br><span class="line">            num = a * <span class="number">1100</span> + b * <span class="number">11</span>;</span><br><span class="line">            m = <span class="built_in">sqrt</span>(num);</span><br><span class="line">            <span class="keyword">if</span> ((m - <span class="built_in"><span class="keyword">int</span></span>(m)) &lt; <span class="number">0.000001</span>)&#123;</span><br><span class="line">                cout &lt;&lt; num &lt;&lt; endl;</span><br><span class="line">                cout &lt;&lt; m &lt;&lt; endl;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> high, low;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>;; ++index) &#123;</span><br><span class="line">        n = index * index;</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">1000</span>)&#123;</span><br><span class="line">            <span class="keyword">continue</span>; <span class="comment">// 继续下一次循环，这一次不再往下继续</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n &gt; <span class="number">9999</span>)&#123;</span><br><span class="line">            <span class="keyword">break</span>;<span class="comment">//退出循环</span></span><br><span class="line">        &#125;</span><br><span class="line">        high = n / <span class="number">100</span>; <span class="comment">// 4567 / 100 = 45</span></span><br><span class="line">        low = n % <span class="number">100</span>; <span class="comment">// 4567 % 100 = 67</span></span><br><span class="line">        <span class="keyword">if</span> ((high / <span class="number">10</span> == high % <span class="number">10</span>) &amp;&amp; (low / <span class="number">10</span> == low % <span class="number">10</span>))&#123;</span><br><span class="line">            cout &lt;&lt; n &lt;&lt; endl;</span><br><span class="line">            cout &lt;&lt; index &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">___________________________</span><br><span class="line"><span class="number">7744</span></span><br><span class="line"><span class="number">88</span></span><br></pre></td></tr></table></figure><h3 id="5-函数"><a href="#5-函数" class="headerlink" title="5. 函数"></a>5. 函数</h3><p>一个C++程序是由若干个源程序文件构成，而一个源程序是由若干个函数构成，函数将一段逻辑封装起来，便于复用</p><p>从用户角度看，可以分为：</p><ul><li>库函数： 标准函数， 由c++系统提供</li><li>用户自定义函数：需要用户定义后使用</li></ul><p>从组成角度看：</p><ul><li>返回类型： 一个函数可以返回一个值</li><li>函数名称：函数名和参数列表一起构成了函数签名</li><li>参数： 参数列表包括函数参数的类型、顺序、数量。参数是可选的，函数可以不包含参数。</li></ul><h4 id="1-overload-函数重载"><a href="#1-overload-函数重载" class="headerlink" title="1. overload 函数重载"></a>1. overload 函数重载</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> a)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">double</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in"><span class="keyword">int</span></span>(b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">double</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in"><span class="keyword">int</span></span> (*p)(<span class="keyword">int</span>);</span><br><span class="line">    p = test;<span class="comment">// 函数指针</span></span><br><span class="line">    <span class="keyword">int</span> result = (*p)(<span class="number">1</span>);</span><br><span class="line">    cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line"><span class="comment">//    int result = test(1);</span></span><br><span class="line"><span class="comment">//    cout &lt;&lt; result &lt;&lt; endl;</span></span><br><span class="line">    result = <span class="built_in">test</span>(<span class="number">2.0</span>);</span><br><span class="line">    cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    result = <span class="built_in">test</span>(<span class="number">1</span>, <span class="number">2.0</span>);</span><br><span class="line">    cout &lt;&lt; result &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-指数函数的指针与返回指针的函数"><a href="#2-指数函数的指针与返回指针的函数" class="headerlink" title="2. 指数函数的指针与返回指针的函数"></a>2. 指数函数的指针与返回指针的函数</h4><p>每一个函数都占用一段内存单元，它们有一个起始地址，指向函数入口地址的指针称为函数指针。</p><p>一般形式： 数据类型 （*指针变量名）(参数表)；</p><p><code>int (*p)(int)</code></p><p><strong>区别：</strong></p><ul><li><code>int (*p)(int);</code>是指针，指向一个函数入口地址</li><li><code>int* p(int);</code>是函数，返回值是一个指针</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MaxValue</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &gt; y)? x:y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MinValue</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &lt; y)? x:y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ProcessNum</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span>(*p)(<span class="keyword">int</span> a, <span class="keyword">int</span> b))</span></span>&#123; <span class="comment">//回调函数</span></span><br><span class="line">    cout &lt;&lt; <span class="built_in">p</span>(x, y) &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">10</span>, y= <span class="number">20</span>;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">ProcessNum</span>(x, y, MaxValue) &lt;&lt; endl; </span><br><span class="line">    cout &lt;&lt; <span class="built_in">ProcessNum</span>(x, y, MinValue) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">ProcessNum</span>(x, y, add) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-命名空间"><a href="#3-命名空间" class="headerlink" title="3. 命名空间"></a>3. 命名空间</h4><ul><li>命名空间，可以作为附加信息来区分不同库中名称的函数、类、变量等，命名空间即定义了上下文。本质上，命名空间就是定义了一个范围。</li><li>关键字：using 和namespace。 <code>std::name</code></li></ul><h4 id="4-函数体"><a href="#4-函数体" class="headerlink" title="4. 函数体"></a>4. 函数体</h4><p>函数主体包含一组定义函数执行任务的语句。</p><h4 id="5-内联函数-inline-function"><a href="#5-内联函数-inline-function" class="headerlink" title="5.内联函数 inline function"></a>5.内联函数 inline function</h4><ul><li>如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方</li><li>引入内联函数的目的是为了解决程序中函数调用的效率问题</li><li>注意：内联函数内部不能有太复杂的逻辑，编译器有时会有自己的优化策略，所以内联不一定起作用。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">Fib</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( n == <span class="number">0</span> )&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ( n == <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">Fib</span>(n<span class="number">-1</span>) + <span class="built_in">Fib</span>(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 有可能没用</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">10</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">Fib</span>(n) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-数学归纳法"><a href="#6-数学归纳法" class="headerlink" title="6. 数学归纳法"></a>6. 数学归纳法</h3><p>递归的四个基本法则：</p><ol><li>基准情形：无须递归就能解出；</li><li>不断推进：每一次递归调用都必须是求解状况朝接近基准情形的方向推进</li><li>设计法则：假设所有递归调用都能运行</li><li>合成效益法则——compound interest rule:求解一个问题的同一个实例是，切勿在不同的递归调用中做重复性的工作。</li></ol><p><strong>递归recursion的缺陷：</strong></p><ol><li>递归是一种重要的编程思想：<ul><li>很多重要的算法都包含递归的思想</li><li>递归最大的缺陷：<ul><li>空间删需要开辟大量的栈空间1</li><li>时间上可能需要有大量重复运算</li></ul></li></ul></li><li>递归的优化<ul><li>尾递归： 所有递归形式的调用都出现在函数的末尾</li><li>使用循环替代</li><li>使用动态规划：时间换空间</li></ul></li></ol><p>分别使用递归，循环，尾递归、动态规划来实现斐波那契数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> g_a[<span class="number">1000</span>];<span class="comment">//全局数组，记录fib计算的值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fib</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( n == <span class="number">0</span> )&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ( n == <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">Fib</span>(n<span class="number">-1</span>) + <span class="built_in">Fib</span>(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fibloop</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> n0=<span class="number">0</span>, n1=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> tmp;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i&lt;=n; i++)&#123;</span><br><span class="line">        tmp = n0;</span><br><span class="line">        n0 = n1;</span><br><span class="line">        n1 = n0 + tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n1;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 尾递归</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fiblast</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> ret0, <span class="keyword">int</span> ret1)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">0</span>)</span><br><span class="line">    &#123;<span class="keyword">return</span> <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> ret1;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Fiblast</span>(n<span class="number">-1</span>, ret1, ret0 + ret1);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 动态规划</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fibd</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    g_a[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    g_a[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (g_a[i]==<span class="number">0</span>)&#123;</span><br><span class="line">            g_a[i] = g_a[i<span class="number">-1</span>] + g_a[i<span class="number">-2</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> g_a[n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">10</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">Fib</span>(n) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">Fibloop</span>(n) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">Fiblast</span>(n, <span class="number">0</span>, <span class="number">1</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">Fibd</span>(n) &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">assert</span>(<span class="built_in">Fib</span>(<span class="number">10</span>)==<span class="number">55</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Cpp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cpp </tag>
            
            <tag> 函数体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 4 Least squares and  Optimization</title>
      <link href="2020/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture4/"/>
      <url>2020/06/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture4/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-4-Least-squares-and-Optimization"><a href="#Lecture-4-Least-squares-and-Optimization" class="headerlink" title="Lecture 4 Least squares and  Optimization"></a>Lecture 4 Least squares and  Optimization</h2><h3 id="1-最小二乘估计"><a href="#1-最小二乘估计" class="headerlink" title="1. 最小二乘估计"></a>1. 最小二乘估计</h3><p><img src="https://i.loli.net/2020/10/25/yDWUbxNtIqdOlhP.png" alt="最小二乘估计" style="zoom:80%;" /></p><p>最小二乘估计(上节课没讲的)：</p><p>如果列向量都是线性无关的找到最小化残差和的$w$是：</p><script type="math/tex; mode=display">\begin{aligned}\hat {\underline w}&=(X^TX)^{-1}X^Ty\\ \Rightarrow \hat {\underline y}&=X \hat {\underline w} =X(X^TX)^{-1}X^Ty\end{aligned}\tag{1}</script><p>其中，$(X^TX)^{-1}X^T$又叫pseudo-inverse， 伪逆。</p><p>$P_{Xy}$投影$y$到$X$上。</p><h3 id="2-最小二乘法分类"><a href="#2-最小二乘法分类" class="headerlink" title="2. 最小二乘法分类"></a>2. 最小二乘法分类</h3><p><img src="https://i.loli.net/2020/10/25/ujR5nvF3pdfcDwH.png" alt="最小二乘分类" style="zoom:80%;" /></p><p>二分类就是把$<X_{new},  \underline{\hat w}>$输入到sign函数得到其输出。</p><h3 id="3-优化方法"><a href="#3-优化方法" class="headerlink" title="3. 优化方法"></a>3. 优化方法</h3><p><img src="https://i.loli.net/2020/10/25/4BMdthxRajYJWUX.png" alt="优化方法" style="zoom:80%;" /></p><p>2范数或者欧几里得范数：</p><script type="math/tex; mode=display">\lVert  x\rVert^2_{2}  = \sum^n_{i=1}x_i^2=x^Tx=<x, x>\tag{2}</script><p>所以：</p><script type="math/tex; mode=display">\begin{aligned}arg \ min_w\lVert  y-Xw\rVert^2_2 &=arg \ min_w \ (y-Xw)^T(y-Xw)\\ &=arg \ min_w (y^Ty-y^TXw-w^TX^Ty+w^TX^TXw)\\ &=arg \ min_w (y^Ty-2w^TX^Ty+w^TX^TXw)\end{aligned}\tag{3}</script><p>$y^TXw和w^TX^Ty$都是标量，而且是一样的。</p><p>Warmup：$f(w) = \frac{1}{2}w^2-w-\frac{1}{2}$。求导求极值点。</p><h3 id="4-正定矩阵"><a href="#4-正定矩阵" class="headerlink" title="4. 正定矩阵"></a>4. 正定矩阵</h3><p><img src="https://i.loli.net/2020/10/25/jfe3tGv7lCqd6IT.png" alt="正定矩阵" style="zoom:80%;" /></p><p>我们需要$X^TX$是可逆的。</p><p>从优化的角度来看这个问题。$X^TX$是正定的</p><p>正定矩阵定义：</p><script type="math/tex; mode=display">\begin{aligned}& \text{给定一个大小为}n \times n \text{的实对称矩阵}Q ，\text{若对于任意长度为}n\text{的非零向量}x ，\text{有}x^TQx>0\text{恒成立，则矩阵}Q\text{是一个正定矩阵}\\&(positive \ definite(p.d)。\text{简记}Q>0)。\\\\&\text{给定一个大小为}n \times n\text{的实对称矩阵}Q ，\text{若对于任意长度为}n\text{的非零向量}x ，\text{有}x^TQx \ge 0\text{恒成立，则矩阵}Q是\text{一个半正定矩}\\&\text{阵}(positive \ semi-definite(p.s.d)。\text{简记}Q \ge 0)。\end{aligned} \tag{4}</script><h4 id="1-详解正定矩阵的作用和凸优化"><a href="#1-详解正定矩阵的作用和凸优化" class="headerlink" title="1. 详解正定矩阵的作用和凸优化"></a>1. 详解正定矩阵的作用和凸优化</h4><p><img src="https://i.loli.net/2020/10/26/la6VyLTRGbF7rj5.png" alt="凸优化" style="zoom:80%;" /></p><p><a href="https://zhuanlan.zhihu.com/p/44860862">正定矩阵详解</a></p><h4 id="2-正定矩阵的性质"><a href="#2-正定矩阵的性质" class="headerlink" title="2. 正定矩阵的性质"></a>2. 正定矩阵的性质</h4><p><img src="https://i.loli.net/2020/10/26/2yfkJQabOdANUjD.png" alt="正定矩阵的性质" style="zoom:80%;" /></p><p>性质3：</p><p>对于任意矩阵$A$，那么$A^TA \ge 0 $和$AA^T \ge 0 $</p><h3 id="5-最优化最小二乘法"><a href="#5-最优化最小二乘法" class="headerlink" title="5. 最优化最小二乘法"></a>5. 最优化最小二乘法</h3><p><img src="https://i.loli.net/2020/10/26/GkB7YAjedviDqp9.png" alt="Least squares optimization problem" style="zoom:80%;" /></p><p>假设$X^TX &gt; 0$,那么：$f(w)=y^Ty-2w^TX^Ty+w^TX^TXw$是凸的。</p><p>计算其导数，让其等于0来求最小值。</p><p>例如$f(w) = c^Tw=c_1w_1+c_2w_2+ \cdots+c_pw_p$</p><p>其梯度为：</p><script type="math/tex; mode=display">\nabla_{w} f=\left[\begin{array}{c}c_{1} \\c_{2} \\\vdots \\c_{p}\end{array}\right]=c\tag{5}</script><p>例如$f(w) =w^Tw=\lVert  w\rVert^2_{2}=w^2_1 + w^2_2 + \cdots+ w^2_p$</p><script type="math/tex; mode=display">\nabla_{w} f=\left[\begin{array}{c}2w_{1} \\2w_{2} \\\vdots \\2w_{p}\end{array}\right]\tag{6}</script><p>总结，</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{w} <c, w> &= c\\\\\nabla_{w} \lVert  w\rVert^2_{2} &= 2w\end{aligned} \tag{7}</script><p><img src="https://i.loli.net/2020/10/26/DW1o93EOgzFs4Yc.png" alt="对称矩阵的性质" style="zoom:80%;" /></p><p>例子：</p><script type="math/tex; mode=display">f(w) = w^TQw=\sum^p_{i=1} \sum^p_{i=1}w_iQ_ijw_j\tag{8}</script><p>那么对$w$的梯度有下面四种情况:</p><script type="math/tex; mode=display">\frac{d\left(w_{i} Q_{i j} w_{j}\right)}{d w_{k}}=\left\{\begin{array}{ll}2Q_{i i} w_{i} & \text { if } k=i=j \\Q_{i j} w_{j} & \text { if } i=k \neq j \\Q_{i j} w_{i} & \text { if } i \neq k=j \\0 & \text { if } k \neq i, k \neq j\end{array}\right.\tag{9}</script><p>这需要把$\boldsymbol{w}^TQ\boldsymbol{w}$展开来理解，下面用一个具体的例子,其中：</p><script type="math/tex; mode=display">Q=\left[\begin{array}{ccc}2 & -1 & 0 \\-1 & 2 & -1 \\0 & -1 & 2\end{array}\right] \in \mathbb{R}^{3 \times 3}</script><script type="math/tex; mode=display">\boldsymbol{w}=\left[\begin{array}{l}w_{1} \\w_{2} \\w_{3}\end{array}\right] \in \mathbb{R}^{3}</script><p>其部分展开得：</p><script type="math/tex; mode=display">\boldsymbol{w}^{T} Q \boldsymbol{w}=\left[\left(2 w_{1}-w_{2}\right) \quad\left(-w_{1}+2 w_{2}-w_{3}\right) \quad-w_{2}+2 w_{3}\right]\left[\begin{array}{l}w_{1} \\w_{2} \\w_{3}\end{array}\right]</script><p>上式,</p><ul><li>第一种情况，可以结合式7中第二个式子理解，$\boldsymbol{w}_k都是变量$。</li><li>第二种情况，可以结合式7中第一个式子理解，$\boldsymbol{w}_k$是变量,$\boldsymbol{w}_j$是常量。</li><li>第三种种情况，可以结合式7中第一个式子理解，$\boldsymbol{w}_k$是变量,$\boldsymbol{w}_i$是常量。</li><li>不含变量$\boldsymbol{w}_k$都是常量。</li></ul><p>所以：</p><script type="math/tex; mode=display">\frac{d f}{d w_{k}}=\sum_{i=1}^{p} \sum_{j=1}^{p} \frac{d\left(w_{i} Q_{i j} w_{j}\right)}{d w_{k}}\\=\sum_{i=1}^{p} \frac{d\left(w_{i} Q_{i j} \right)}{d w_{k}} + \sum_{j=1}^{p} \frac{d\left( Q_{i j} w_{j}\right)}{d w_{k}}\tag{10}</script><p>回想下简单的公式：</p><script type="math/tex; mode=display">\sum_{i=1}^{N} x_{i} y_{i}=\left[\begin{array}{c}x_{1} \\\vdots \\x_{n}\end{array}\right]^{T}\left[\begin{array}{c}y_{1} \\\vdots \\y_{n}\end{array}\right]=\mathbf{x}^{T} \mathbf{y}</script><p>那么：</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}} f=\frac{d f}{d w_{k}}=Q\boldsymbol{w}+Q^T\boldsymbol{w}\tag{11}</script><p>如果$Q$是对称矩阵($Q=Q^T$):</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}} f=2Q\boldsymbol{w}\tag{12}</script><p><a href="https://math.stackexchange.com/questions/312077/differentiate-fx-xtax">一些其它方法的证明</a></p><p>而$f(w)=y^Ty-2w^TX^Ty+w^TX^TXw$,其梯度为：</p><script type="math/tex; mode=display">\nabla_{w} f = -2X^Ty+2X^TXw\tag{13}</script><p>令其为0，同样可得：</p><script type="math/tex; mode=display">\begin{array}{l}X^{\top} X \hat{\underline{w}}=X^{\top} y \\\Rightarrow\hat{w}=\left(X^{\top} X\right)^{-1} X^{\top} y\end{array}\tag{14}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> 最小二乘法 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
            <tag> 凸优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 3 Least squares and geometry</title>
      <link href="2020/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture3/"/>
      <url>2020/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture3/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-3-Least-squares-and-geometry"><a href="#Lecture-3-Least-squares-and-geometry" class="headerlink" title="Lecture 3 Least squares and geometry"></a>Lecture 3 Least squares and geometry</h2><h3 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h3><p>目标：从一些特征向量$x$学到预测标签$y$。建立一个线性模型，</p><script type="math/tex; mode=display">\begin{aligned}\hat y_i &= \left \langle X_i, w\right \rangle=X_i^Tw=w^TX_i\\&=w_1X_{i1} + w_2X_{i2}+\cdots+w_pX_{ip}\end{aligned}\tag{1}</script><p>需要找到$w$来使得$\hat y_i = y_i \quad for i = 1, 2, \cdots, n$ </p><p>1.最小二乘法怎么来的?</p><p>因为残差residual error为：</p><script type="math/tex; mode=display">r_i=r_i(\underline{w})=y_i - <w, x_i></script><p>找到最小化残差和的$w$。（式2中第一个式子，第二个式欧式距离）</p><script type="math/tex; mode=display">\begin{aligned}\lVert r_i \rVert^2_{2} &= \sum ^n _{i=1}r_i^2 \\  \lVert r_i \rVert_{2}&=\left(\sum ^n _{i=1}r_i^2 \right)^{1/2}  \end{aligned}\tag{2}</script><p>提到$l_p$Norm，对于向量$r$,对其每一个元素的$p$次幂求和再开$p$次方：</p><script type="math/tex; mode=display">\lVert r \rVert_{p} = \left(\sum ^n _{i=1}\lVert r_i \rVert^{p} \right)^{1/p}\tag{3}</script><p>2.为什么用最小二乘法？</p><ul><li>正负残差同样处理</li><li>数学上便利</li><li>好的几何表示</li><li>放大了大误差影响</li><li>和高斯模型噪声一致</li></ul><p><img src="https://i.loli.net/2020/10/25/jbzIwkhWrQfq61p.png" alt="最小二乘估计" style="zoom:125%;" /></p><h4 id="1-1-张成空间-Span"><a href="#1-1-张成空间-Span" class="headerlink" title="1.1 张成空间 Span"></a>1.1 张成空间 Span</h4><p><img src="https://i.loli.net/2020/10/25/VZcLYajxIuOAEzJ.png" alt="Span" style="zoom:125%;" /></p><h4 id="1-2-最小二乘的几何意义"><a href="#1-2-最小二乘的几何意义" class="headerlink" title="1.2 最小二乘的几何意义"></a>1.2 最小二乘的几何意义</h4><p><img src="https://i.loli.net/2020/10/25/6YtRJuFihnfg9wV.png" alt="最小二乘的几何意义" style="zoom:125%;" /></p><p>最小二乘的几何意义：</p><p>$n=3, p = 2$即3个样本，2个特征，那么：</p><script type="math/tex; mode=display">\hat y = Xw=w_1X_1+w_2X_2 \in \mathbb{R}^3即\hat y \in Span(col(X)) \hat y 在X的列向量的张成空间里。</script><p>图中的蓝色$\underline y$就是真实值，考虑对$X$的列向量的张成空间，一些$\tilde{ \underline y}$不是$\hat {\underline  y}$（残差不是垂线或向量不正交)。而且有：</p><script type="math/tex; mode=display">\lVert \tilde{ r} \rVert^2_{2} = \lVert  r \rVert^2_{2} + \lVert d \rVert^2_{2}\tag{4}</script><p>那么$\lVert \tilde{ r} \rVert^2&gt;\lVert  r \rVert^2$,即这不是最优的一组$w_1, w_2$。</p><h4 id="1-3-最小化残差向量"><a href="#1-3-最小化残差向量" class="headerlink" title="1.3 最小化残差向量"></a>1.3 最小化残差向量</h4><p><img src="https://i.loli.net/2020/10/25/3UkMWbDzO7TojVe.png" alt="最小化残差向量" style="zoom:80%;" /></p><p>由上面的几何证明可以知道残差向量是要与$X$的列向量张成空间垂直才最小化。那么有：</p><script type="math/tex; mode=display">X^{T} \hat {\underline r}=0\\\Rightarrow X^{T}(y-X \hat {\underline w})=0\Rightarrow X^Ty=X^TX\hat {\underline w}</script><p>接下来我们想想是不是存在$\hat {\underline w}$使得线性方程组$X^Ty=X^TX\hat {\underline w}$成立?它是不是唯一?</p><h4 id="1-4-线性方程组"><a href="#1-4-线性方程组" class="headerlink" title="1.4 线性方程组"></a>1.4 线性方程组</h4><p><img src="https://i.loli.net/2020/10/25/3InKEOy27iHwArf.png" alt="有解无解?" style="zoom:125%;" /></p><p>线性方程组是否有解？</p><p>有唯一解，无穷解，无解？</p><h4 id="1-5-线性无关"><a href="#1-5-线性无关" class="headerlink" title="1.5 线性无关"></a>1.5 线性无关</h4><p><img src="https://i.loli.net/2020/10/25/bTFMwJ7zDCjdlA8.png" alt="LI" style="zoom:100%;" /></p><p>线性无关的定义：</p><p>若向量 <script type="math/tex">v_1, v_2,\cdots,v_p \in \mathbb{R}^{n}</script>  是线性无关的，那么    <script type="math/tex">\sum^p _ {i=1}a_iv_i=0</script>  当且仅当对于所有的  <script type="math/tex">i</script> ，  <script type="math/tex">a_i=0</script>  时成立。</p><p><img src="https://i.loli.net/2020/10/25/VgasiG9fokCIbKW.png" alt="LI的判定" style="zoom:125%;" /></p><p>线性相关</p><p>这里讲了一个非常重要的概念，矩阵的秩rank。</p><p>矩阵的秩：线性无关列向量的数目=线性无关行向量的数目</p><p>如果$X^T = [x_1, x_2, \cdots, x_n] \in \mathbb{R}^{p \times n}$,那么$rank(X) \leq min(p, n)$。如果$rank(X) = min(p, n)$,那么矩阵满秩full rank。</p><p><img src="https://i.loli.net/2020/10/25/fNoOD2IRlXT7rFs.png" alt="matrix product" style="zoom:100%;" /></p><h4 id="1-6-矩阵的逆-只有方阵有逆"><a href="#1-6-矩阵的逆-只有方阵有逆" class="headerlink" title="1.6 矩阵的逆(只有方阵有逆)"></a>1.6 矩阵的逆(只有方阵有逆)</h4><p><img src="https://i.loli.net/2020/10/25/glp2dxraBH7ItPe.png" alt="matrix inverse" style="zoom:80%;" /></p><p>不是所有矩阵都有逆矩阵。只有矩阵是满秩矩阵，那么它才可能有逆矩阵。</p><p>若$X \in \mathbb{R}^{n \times p}$,假设$n \geq  p , rank(X)=p$($X$有$p$个线性无关的列或者说特征), 就有$rank(X^TX)=p \Rightarrow  X^T X$ 有逆矩阵。</p><p>理由：</p><ol><li>因为$rank(X^TX)\leq min(p, p)=p$,那么$X^TX$有$p$个线性无关的列或者说行，所以矩阵有逆。</li><li>$rank(X^TX)\leq min(p, p)=p$, $X^TX$是满秩矩阵，就有逆矩阵</li></ol><h4 id="1-7-最小二乘线性方程组是否有解"><a href="#1-7-最小二乘线性方程组是否有解" class="headerlink" title="1.7 最小二乘线性方程组是否有解?"></a>1.7 最小二乘线性方程组是否有解?</h4><p><img src="https://i.loli.net/2020/10/25/k9SChvXzWo6UYqa.png" alt="最小二乘线性方程组是否有解-投影矩阵" style="zoom:125%;" /></p><p>$X^{T}(y-X \hat {\underline w})=0<br>\Rightarrow X^Ty=X^TX\hat {\underline w}$中，由于$n \geq p$,那么$rank(X^TX)\leq min(p, p)=p$是满秩的，$X^TX$存在逆。</p><p>可以推出：</p><script type="math/tex; mode=display">\begin{aligned}\hat {\underline w}&=(X^TX)^{-1}X^Ty\\ \Rightarrow \hat {\underline y}&=X \hat {\underline w} =X(X^TX)^{-1}X^Ty\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 最小二乘法 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 2 Vectors and Matrices in Machine learning</title>
      <link href="2020/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture2/"/>
      <url>2020/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture2/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-2-机器学习中的向量和矩阵"><a href="#Lecture-2-机器学习中的向量和矩阵" class="headerlink" title="Lecture 2 机器学习中的向量和矩阵"></a>Lecture 2 机器学习中的向量和矩阵</h2><h3 id="1-线性模型"><a href="#1-线性模型" class="headerlink" title="1. 线性模型"></a>1. 线性模型</h3><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909823.png" alt="t5ajyXsZi7zUKx9" style="zoom:30%;" /></p><p>我们可以理解$X_i \in \mathbb{R}^p$为$p$个一系列的数值特征，一般写作一个列向量。</p><script type="math/tex; mode=display">{X}_{i}=\left[\begin{array}{c}X_{i 1} \\x_{i 2} \\\vdots \\X_{i p}\end{array}\right] \in \mathbb{R}^{p}\tag{1}</script><p>从训练数据，对于新的新样本$X_0$学习如何预测标签(或者目标值)$\hat{y}$。</p><p>例如线性模型，</p><script type="math/tex; mode=display">\begin{array}{l}\hat{y}=w_{1} x_{01}+w_{2} x_{02}+\cdots w_{p} x_{0 p} \\w_{1}, \ldots, w_{p}=\text { weights to be learned from data }\end{array}\tag{2}</script><p>using training data to find $W$, such that} $\hat{y_i}\approx y_i, \ for \ i=1, \ldots, n $ . want $L(\hat{y_i}, y_i) $smallest </p><p>权重向量weights vector：</p><script type="math/tex; mode=display">\underline W=\left[\begin{array}{c}\mathbb{w}_{1} \\\mathbb{w}_{2} \\\vdots \\\mathbb{w}_{p}\end{array}\right] \in \mathbb{R}^{p}\tag{3}</script><p>特征向量feature vector：</p><script type="math/tex; mode=display">\underline X=\left[\begin{array}{c}\mathbb{w}_{o 1} \\\mathbb{w}_{o 2} \\\vdots \\\mathbb{w}_{o p}\end{array}\right] \in \mathbb{R}^{p}\tag{4}</script><p>我们的模型就可以写作内积的形式。</p><h4 id="例子1-线性模型"><a href="#例子1-线性模型" class="headerlink" title="例子1 线性模型"></a>例子1 线性模型</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909825.png" alt="hn9C8TiKjH4M6av" style="zoom:30%;" /></p><p>竖轴是第二个特征维度，横轴是第一个特征维度。蓝色直线代表所有在</p><script type="math/tex; mode=display"><X, W>=-2\times x_1+ x_2=0</script><p> 的点。</p><p>我们也可以看作： <script type="math/tex">\hat y_i = x_{i1}w_1+ x_{i2}w_2 + w_0</script> 。我们不防把第一个元素<script type="math/tex">x_1</script>看作1，那么就有：</p><script type="math/tex; mode=display">\underline{x}_{0}=\left[\begin{array}{c}1 \\x_{01} \\x_{02} \\\vdots \\x_{0 p}\end{array}\right] , \underline{w}=\left[\begin{array}{c}w_{0} \\w_{1} \\w_{2} \\\vdots \\w_{p}\end{array}\right] \in \mathbb{R}^{p+1} \tag{5}</script><p>也可以像前面一样写成内积形式。</p><p>接下来就是最小化损失函数。</p><h4 id="线性模型的矩阵表示矩阵表示"><a href="#线性模型的矩阵表示矩阵表示" class="headerlink" title="线性模型的矩阵表示矩阵表示"></a>线性模型的矩阵表示矩阵表示</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909826.png" alt="TOANckqixvmW8E5" style="zoom:30%;" /></p><script type="math/tex; mode=display">\begin{aligned}X&=\left[\begin{array}{ll}- & x_{1}^{\top}&- \\- & x_{2}^{\top}&- \\- & x_{n}^{\top}&-\end{array}\right]\\&=\left[\begin{array}{cccc}x_{11} & x_{12} & \cdots & x_{1 p} \\x_{21} & x_{22} & \cdots & x_{2 p} \\\vdots & & \\x_{n 1} & x_{n 2} & \cdots & x_{n p}\end{array}\right]\end{aligned}\tag{6}</script><p>例如$x<em>{21}$表示第二个训练样本的第1个特征。$x</em>{12}$表示第一个训练样本的第二个特征。</p><script type="math/tex; mode=display">X^T=  \begin{bmatrix}    x_{11}& x_{21}& \cdots  & x_{n1} \\    x_{12}& x_{22}& \cdots  & x_{n2} \\    \vdots & \vdots & \ddots & \vdots \\    x_{1p}& x_{2p}& \cdots  & x_{np}  \end{bmatrix}  \tag{7}</script><p>$X^T$变成$p$行$n$ 列。</p><p>式6，矩阵$X$的第$i $行<strong>row</strong>代表： 第$i$训练样本的$p$个特征。</p><p>​         矩阵$X$的第$j $列<strong>col</strong>代表：所有 $n$个训练样本的第$j$个特征。</p><p>这就是不同角度看待$X$矩阵。</p><p>计算$Xw$意味着：作$X$每一行和$w$作内积然后把结果用向量$\hat y$保存下来。</p><p>教授Rebecca Willett非常详细讲了矩阵行列之间的变化关系。体会矩阵shape的变化，这对代码中间的debug非常有用.</p><h4 id="例子-2：深刻理解Xw"><a href="#例子-2：深刻理解Xw" class="headerlink" title="例子 2：深刻理解Xw"></a>例子 2：深刻理解Xw</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909827.png" alt="hG8q5SWUlVxbP3s" style="zoom:30%;" /></p><p>上面其实是从2个角度来看矩阵乘法：行向量和列向量。</p><p>行向量角度是我们线代教材常说的，列向量有时候会让计算非常简单。(MIT线代课程，Gilbert Strang讲的非常详细。)</p><p>列向量角度：$Xw$表示$X$的列的权重之和。</p><h4 id="例子3：复杂结构的线性模型"><a href="#例子3：复杂结构的线性模型" class="headerlink" title="例子3：复杂结构的线性模型"></a>例子3：复杂结构的线性模型</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909828.png" alt="GbnMQkD5A1ujYa8" style="zoom:30%;" /></p><p>This doesn’t look like a straight line,but linear models can still help!</p><p>cubic polynomial：三次多项式。</p><p>范德蒙矩阵：</p><h3 id="2-矩阵与矩阵相乘"><a href="#2-矩阵与矩阵相乘" class="headerlink" title="2. 矩阵与矩阵相乘"></a>2. 矩阵与矩阵相乘</h3><h4 id="1-例子：推荐系统"><a href="#1-例子：推荐系统" class="headerlink" title="1. 例子：推荐系统"></a>1. 例子：推荐系统</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909829.png" alt="GbnMQkD5A1ujYa8" style="zoom:30%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909830.png" alt="OjI7EQWYwJzaHqb" style="zoom:30%;" /></p><p>推荐系统</p><p>电影名 观众名 数字是评分</p><p>把矩阵$X$写作矩阵$U*V$,其中$X \in n\times p , U \in n\times r,V \in r\times p $。</p><p>矩阵$U$是对于$r$个示例电影的顾客$r$不同评分向量；$v$是每个实际顾客对$r$示例电影的相似度。</p><p>If $X= UV$，那么</p><script type="math/tex; mode=display">X_{ij} = <i^{th} \text{  row  of  U}, j^{th}\text{ col  of  V}></script><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909831.png" alt="fvtUYeNTCkxadsh" style="zoom:30%;" /></p><h4 id="2-内积和外积表示"><a href="#2-内积和外积表示" class="headerlink" title="2. 内积和外积表示"></a>2. 内积和外积表示</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909832.png" alt="1fRAWToulPIBMrQ" style="zoom:30%;" /></p><h4 id="3-怎么找到最小的r"><a href="#3-怎么找到最小的r" class="headerlink" title="3.怎么找到最小的r"></a>3.怎么找到最小的r</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261909833.png" alt="AdVpme5LGSlt8HO" style="zoom:30%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
            <tag> 线性模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resnet 网络</title>
      <link href="2020/06/05/2.ResNet%20%E7%BD%91%E7%BB%9C/"/>
      <url>2020/06/05/2.ResNet%20%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Resnet-架构"><a href="#1-Resnet-架构" class="headerlink" title="1. Resnet 架构"></a>1. Resnet 架构</h3><p>ResNet原文： <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p><h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>在<code>Abstract</code>中, 就说到更深的网络更难训练，并且会出现退化现象。而残差网络更容易优化，并且增加网络深度能提高准确率。</p><blockquote><p> We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261738504.png" alt="image-20210221234022732" style="zoom:50%;" /></p><p>如上图所示，当模型深度增加时，模型的test error却变大了，这就是“退化”问题。</p><p>当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差。</p><blockquote><p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error.</p></blockquote><p>对于退化问题，简单的想法是，如果我们直接把后面的层和浅层直接相连，从效果上来说不应该比浅层网络差。</p><blockquote><p>There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).</p></blockquote><p>通过引入<strong>deep residual learning</strong>学习框架解决了退化问题。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261738621.png" alt="image-20210221235801547" style="zoom: 50%;" /></p><script type="math/tex; mode=display">F(\mathbf{x}) = H(\mathbf{x}) - \mathbf{x}</script><p>在卷积神经网络中， $F(\mathbf{x})$输出维度可能和$\mathbf{x}$不一样。文中按下面两种方式处理：</p><ol><li><p>当维度一致是，二者直接相加，公式：</p><script type="math/tex; mode=display">\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x} \tag{1}</script></li><li><p>当维度不一致是， $\mathbf{x}$做一次矩阵变换， 公式：</p></li></ol><script type="math/tex; mode=display">\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + W_s\mathbf{x} \tag{2}</script><p>下图中，左边是VGG-19,中间是仿VGG19堆叠的34层网络，记为plain-34，网络更深，但<a href="https://aigonna.com/2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/">FLOPs</a>(有计算公式) 仅为VGG-19的18%，VGG-19两层全连接层计算量太大。最右边是针对中间加入了跨层连接即残差结构，注意实线就是直接恒等变换和后面的feature map直接相加，就是用公式1，虚线就是由于维度不匹配，用公式2。三个模型计算量分别为：<code>19.6 billion FLOPs</code>、<code>3.6 billion FLOPs</code>、 <code>3.6 billion FLOPs</code>。 <strong>残差结构既不增加计算复杂度（除了几乎可以忽略的元素相加），又不增加模型的参数量，同时这也为模型间的比较提供了方便</strong></p><p><strong>升维有两种方式：</strong>第一种是直接全补0，这样做优势是不会增加网络的参数；第二种是1 x 1卷积升维，后面实验部分会进行比较。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261737146.png" alt="image-20210222000426907" style="zoom:50%;" /></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>Res18、Res34、Res50、Res101、Res152网络结构如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261737993.png" alt="image-20210222112028710" style="zoom:33%;" /></p><p>Res50、Res101、Res152采用的是被称为<strong>bottleneck</strong>的残差结构：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261737685.png" alt="image-20210222112446810" style="zoom: 50%;" /></p><p>bottleneck结构就是前面先用1 x 1卷积降维，后面再用1 x 1卷积升维以符合维度大小，这样做可以大大减少计算量。<strong>注意bottleneck中3 x 3的卷积层只有一个，而不是普通结构的两个。</strong></p><p>1x1 小卷积作用：</p><ol><li>升维或降维</li><li>通道融合 / 跨通道信息交互</li><li>保持feature map尺寸不变（不损失分辨率）的情况下增加网络的非线性特性（虽然1 x 1卷积是线性的，但ReLU是非线性的）</li></ol><p>下图比较：在ImageNet上训练。细曲线表示训练误差，粗曲线表示中心裁剪图像的验证误差。左：18层和34层的简单网络。右：18层和34层的ResNet。在本图中，残差网络与对应的简单网络相比没有额外的参数。ResNet网络更深，验证误差也能降低，而不跟简单网络一样出现深层网络的退化问题。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261737696.png" alt="image-20210222112820209" style="zoom:33%;" /></p><p><strong>训练技巧</strong>：</p><ul><li>图像水平翻转，减去均值，224x224随机裁剪</li><li>对于跳跃结构，当输入与输出的维度一样时，不需要进行任何处理，二者直接相加；当输入与输出维度不同时，输入要进行变换去匹配输出的维度：zero-padding或1x1卷积</li><li><p>设计网络的规则：对于输出特征图大小相同的层，有相同数量的Filters，即channel数相同。当特征图大小减半时（池化），Filters数量翻倍。</p></li><li><p>每个卷积后和激活前采用BN</p></li><li><code>batchsize =256，lr = 0.1</code>，当误差稳定时更新<code>lr = lr * 0.1</code>，SGD优化函数，<code>weight_decay = 0.0001，momentum = 0.9</code>。</li><li>未使用Dropout</li><li>网络末端以全局平均池化层结束，后接Softmax输出</li></ul><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p>简化的Resnet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 包含一个主干分支和串联分支</span></span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channel, out_channel,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(out_channel, out_channel,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> in_channel != out_channel <span class="keyword">or</span> stride &gt; <span class="number">1</span>:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channel, out_channel,</span><br><span class="line">                          kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channel),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out1 = self.layer(x)</span><br><span class="line">        out2 = self.shortcut(x)</span><br><span class="line">        out = out1 + out2</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span>(<span class="params">self, block, out_channel, stride, num_block</span>):</span></span><br><span class="line">        layer_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_block):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span>:</span><br><span class="line">                in_stride = stride</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_stride = <span class="number">1</span></span><br><span class="line">            layer_list.append(block(self.in_channel, out_channel, in_stride))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将in_channel变为out_channel</span></span><br><span class="line">            self.in_channel = out_channel</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layer_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ResBlock</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.in_channel = <span class="number">32</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.layer1 = self.make_layer(ResBlock, out_channel=<span class="number">64</span>, stride=<span class="number">1</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer2 = self.make_layer(ResBlock, out_channel=<span class="number">128</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(ResBlock, out_channel=<span class="number">256</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(ResBlock, out_channel=<span class="number">512</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.layer1(out)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = self.layer3(out)</span><br><span class="line">        out = self.layer4(out)</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">4</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet</span>():</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(ResBlock)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://niecongchong.github.io/2019/06/11/ResNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">ResNet论文翻译——中文版</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/56961832">ResNet论文笔记及代码剖析</a></p><p>[3] <a href="https://www.cnblogs.com/aiblbns/p/11143978.html">ResNet论文总结</a></p><p>[4] <a href="https://www.jianshu.com/p/bb479421de64">ResNet论文和代码解读</a></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 1 Introduction</title>
      <link href="2020/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture1/"/>
      <url>2020/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture1/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-1-Introduction"><a href="#Lecture-1-Introduction" class="headerlink" title="Lecture 1 Introduction"></a>Lecture 1 Introduction</h2><p>本节最主要就是引入机器学习的元素。教授Rebecca Willett 用一个区分笑脸的任务介绍了机器学习的基本步骤。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261906521.png" alt="E8ql7RHS9sbWGxu" style="zoom:30%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261906523.png" alt="qiHJnFjKYhmzUCo" style="zoom:30%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261906524.png" alt="DvCm1GEyBgfo9VJ" style="zoom:30%;" /></p><p>想象我们想用一张脸部图片来决定这个人是否微笑。</p><p>关键想法：我们用一个模型表示脸是否在笑——一种数据的数学表示。</p><p>基本步骤：</p><ol><li><p>收集原始数据——例如， 脸部图像</p></li><li><p>预处理——不丢失相关信息的条件下，做数据变换来简化后面操作。例如，剪裁图像到标准尺寸，一张图一张脸，脸部在图像中间。</p></li><li><p>特征提取——通过提取模型相关的特征或特性来简化数据(基于深度神经网络的现代图像识别系统这个步骤不是必须的)。例如，每对脸部坐标的距离。</p></li><li><p>生成训练样本——我们用来学习模型的大量样本的集合。</p><ul><li>​    $(x_i, y_I) \quad for \quad i=1, \ldots, n$  其中，<script type="math/tex; mode=display">n ： 训练样本数目 \\y_i ： 第i个样本标签\\x_i ： 第i个样本特征\\x_{ij} ：第个样本的第j个特征</script></li></ul></li><li><p>选择loss function——衡量我们模型预测跟真实值之间的接近程度。 <script type="math/tex">loss = \sum^n_{i=1}\left|y_{i}-\hat{y}_{i}\right|^{2}</script></p></li><li><p>学习模型——遍历搜索候选模型或模型参数集合来找到一个能在训练集上损失最小的。</p></li><li><p>泛化误差——我们在新数据集上的的预测误差，不是开始用来训练的数据。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> CMSC35300 Mathematics Foundations of ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5. 指针</title>
      <link href="2020/06/02/5.%20%E6%8C%87%E9%92%88/"/>
      <url>2020/06/02/5.%20%E6%8C%87%E9%92%88/</url>
      
        <content type="html"><![CDATA[<h2 id="5-指针"><a href="#5-指针" class="headerlink" title="5. 指针"></a>5. 指针</h2><h3 id="1-指针的引用"><a href="#1-指针的引用" class="headerlink" title="1. 指针的引用"></a>1. 指针的引用</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">112</span>, b = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">float</span> c = <span class="number">3.14f</span>;</span><br><span class="line">    <span class="keyword">int</span>* d = &amp;a;</span><br><span class="line">    <span class="keyword">float</span>* e = &amp;c;</span><br><span class="line">    cout &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; e &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; (*d )&lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; (*e) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line"><span class="number">0xa1fe04</span></span><br><span class="line"><span class="number">0xa1fe00</span></span><br><span class="line"><span class="number">112</span></span><br><span class="line"><span class="number">3.14</span></span><br></pre></td></tr></table></figure><h3 id="2-左值和右值"><a href="#2-左值和右值" class="headerlink" title="2. 左值和右值"></a>2. 左值和右值</h3><p>一般说法，编译器为其单独分配了一块存储空间，可以取其地址的，左值可以放在赋值运算符左边；</p><p>右值是数据本身，不能取到其自身地址，右值只能赋值运算右边。</p><p>具体来说：</p><ul><li>左值最常见的情况如函数和数据成员的名字；</li><li>右值是没有标识符、不可以取地址的表达式，一般称之为“临时对象”</li><li><code>&amp;a</code>是允许的，而<code>&amp;(b+c)</code>不能通过编译，因此a是一个左值，(b+c)是一个右值</li></ul><h3 id="3-指针的数组、数组的指针"><a href="#3-指针的数组、数组的指针" class="headerlink" title="3. 指针的数组、数组的指针"></a>3. 指针的数组、数组的指针</h3><ul><li>指针的数组 array of pointers : <code>T* t[]</code></li><li>数组的指针 a pointer to an array : <code>T (*t) []</code> <code>[]</code>优先级比较高</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">int</span>* iP = &amp;i;</span><br><span class="line">    cout &lt;&lt; (*iP) &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">double</span> d = <span class="number">3.14</span>;</span><br><span class="line">    <span class="keyword">double</span> * dP = &amp;d;</span><br><span class="line">    cout &lt;&lt; (*dP) &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">char</span> c = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    <span class="keyword">char</span> * cP = &amp;c;</span><br><span class="line">    cout &lt;&lt; (*cP) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-OUT:</span><br><span class="line">    <span class="number">4</span></span><br><span class="line">    <span class="number">3.14</span></span><br><span class="line">    a</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// array of pointers 和 a pointer to an array</span></span><br><span class="line">    <span class="keyword">int</span> c[<span class="number">4</span>] = &#123;<span class="number">0x40000000</span>, <span class="number">0x4FFFFFFF</span>, <span class="number">0x00000000</span>, <span class="number">0x7FFFFFFF</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span>* a[<span class="number">4</span>]; <span class="comment">//array of pointers</span></span><br><span class="line">    <span class="built_in"><span class="keyword">int</span></span>(*b) [<span class="number">4</span>]; <span class="comment">// a pointer to an array</span></span><br><span class="line">    b = &amp;c; <span class="comment">//这里数组个数得匹配，</span></span><br><span class="line">    <span class="comment">// 将数组c中元素赋给数组a</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i&lt;<span class="number">4</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = &amp;(c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; *(a[<span class="number">0</span>]) &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; *(b) [<span class="number">3</span>] &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-OUT:</span><br><span class="line"><span class="number">1073741824</span></span><br><span class="line"><span class="number">2147483647</span></span><br></pre></td></tr></table></figure><h3 id="4-const-pointer-与-pointer-to-const"><a href="#4-const-pointer-与-pointer-to-const" class="headerlink" title="4. const pointer 与 pointer to const"></a>4. const pointer 与 pointer to const</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>  strHelloworld[] = &#123;<span class="string">&quot;helloworld&quot;</span>&#125;; </span><br><span class="line"><span class="keyword">char</span> <span class="keyword">const</span>  *pStr1 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">// 修饰char 说明内容是不可改变的</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span> *pStr1 = <span class="string">&quot;helloworld&quot;</span>; </span><br><span class="line"><span class="comment">// 修更常见的写法</span></span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span>  pStr2 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">// 修饰指针， 说明不变的是地址</span></span><br><span class="line"><span class="keyword">char</span> <span class="keyword">const</span> * <span class="keyword">const</span> pStr3 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">//两个const分别修饰char和指针，说明内容和地址都不可改变</span></span><br><span class="line">pStr1 = strHelloworld;</span><br><span class="line"><span class="comment">//pStr2 = strHelloworld; //pStr2不可改</span></span><br><span class="line"><span class="comment">//pStr3 = strHelloworld; //pStr3不可改</span></span><br></pre></td></tr></table></figure><ul><li>关于<code>const</code>修饰部分：<ol><li>看左侧最近的部分</li><li>如果左侧没有，则看右侧</li></ol></li></ul><h3 id="5-指针的指针"><a href="#5-指针的指针" class="headerlink" title="5. 指针的指针"></a>5. 指针的指针</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">12</span>;</span><br><span class="line"><span class="keyword">int</span>* b = &amp;a;</span><br><span class="line"><span class="keyword">int</span>** c = &amp;b;</span><br></pre></td></tr></table></figure><ul><li><code>*</code>操作符具有从右向左的结合性， <code>**</code>相当于<code>*(*c)</code>,必须从里向外逐层求值，<code>*c</code>是c指向的位置，即b；<code>**c</code>相当于 <code>*b</code>，得到变量a的值。</li></ul><h3 id="6-NULL"><a href="#6-NULL" class="headerlink" title="6. NULL"></a>6. NULL</h3><p><code>int* a = NULL</code></p><ul><li>一个特殊的指针变量，表示不指向任何东西</li><li>它给了一种方法，来表示特定的指针目前未指向任何东西</li><li>使用注意事项：<ul><li>对于一个指针，如果已经知道江北初始化为什么样的地址，那么请赋给它这个地址值，否则把它设为<strong>NULL</strong></li><li>在对一个指针进行间接引用前，请先判断这个指针的值是否为<strong>NULL</strong></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">123</span>;</span><br><span class="line">    <span class="keyword">int</span> *b = &amp;a;</span><br><span class="line">    <span class="keyword">int</span> **c = &amp;b;</span><br><span class="line">    <span class="comment">// NULL 的使用</span></span><br><span class="line">    <span class="keyword">int</span> *pA = <span class="literal">NULL</span>;</span><br><span class="line">    pA = &amp;a;</span><br><span class="line">    <span class="keyword">if</span> (pA != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        cout &lt;&lt; (*pA) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    pA = <span class="literal">NULL</span>; <span class="comment">// 不用时一定又置为NULL</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br><span class="line">OUT：</span><br><span class="line"><span class="number">123</span></span><br></pre></td></tr></table></figure><h4 id="杜绝“-野-”指针"><a href="#杜绝“-野-”指针" class="headerlink" title="杜绝“ 野 ”指针"></a>杜绝“ 野 ”指针</h4><ul><li>指向“垃圾”内存的指针。if判断对其不起作用，因为不为NULL。</li></ul><p>一般有三种情况：</p><ol><li>指针变量没有初始化</li><li>已经释放不用的指针没有置NULL，如delete和free之后的指针</li><li>指针操作超越了变量的作用范围</li></ol><p><strong>Note：</strong> <strong>没有初始化的，不用的或者超出范围的指针请把值置为NULL。</strong></p><h3 id="7-指针相关操作符"><a href="#7-指针相关操作符" class="headerlink" title="7. 指针相关操作符"></a>7. 指针相关操作符</h3><p><code>&amp; 与 *</code> 的比较：</p><p>左值取地址，右值取地址里的内容。</p><p><strong>左值和右值？？</strong></p><h3 id="8-与-操作符"><a href="#8-与-操作符" class="headerlink" title="8. ++ 与  - - 操作符"></a>8. ++ 与  - - 操作符</h3><ul><li><code>++ i</code> : 先++， 再赋值给<code>i</code></li><li><code>i++</code>: 先赋值给i, 再++</li></ul><p><strong>++i和i++区别？？？</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">1</span>; b = <span class="number">2</span>; c;</span><br><span class="line">c = a+++b; <span class="comment">// 相当于a++ +b</span></span><br><span class="line">d = a++++b; <span class="comment">// 相当于a++ ++b, error</span></span><br></pre></td></tr></table></figure><h3 id="9-内存分析"><a href="#9-内存分析" class="headerlink" title="9. 内存分析"></a>9. 内存分析</h3><h4 id="1-栈和队列"><a href="#1-栈和队列" class="headerlink" title="1.栈和队列"></a>1.栈和队列</h4><ol><li><p>栈区（stack）——由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p></li><li><p>堆区（heap）—— 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</p></li><li>全局区（静态区）（static）——，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放</li><li>文字常量区 ——常量字符串就是放在这里的。 程序结束后由系统释放</li><li>程序代码区——存放函数体的二进制代码。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;                                                <span class="comment">//(GVAR)全局初始化区 </span></span><br><span class="line"><span class="keyword">int</span>* p1;                                                   <span class="comment">//(bss)全局未初始化区 </span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>                                               <span class="comment">//(text)代码区</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">int</span> b=<span class="number">1</span>;                                              <span class="comment">//(stack)栈区变量 </span></span><br><span class="line">     <span class="keyword">char</span> s[] = <span class="string">&quot;abc&quot;</span>;                                 <span class="comment">//(stack)栈区变量</span></span><br><span class="line">     <span class="keyword">int</span>*p2=<span class="literal">NULL</span>;                                     <span class="comment">//(stack)栈区变量</span></span><br><span class="line">     <span class="keyword">char</span> *p3 = <span class="string">&quot;123456&quot;</span>;                         <span class="comment">//123456\0在常量区, p3在(stack)栈区</span></span><br><span class="line">     <span class="keyword">static</span> <span class="keyword">int</span> c = <span class="number">0</span>;                                   <span class="comment">//(GVAR)全局(静态)初始化区 </span></span><br><span class="line">     p1 = <span class="keyword">new</span> <span class="built_in"><span class="keyword">int</span></span>(<span class="number">10</span>);                               <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     p2 = <span class="keyword">new</span> <span class="built_in"><span class="keyword">int</span></span>(<span class="number">20</span>);                               <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     <span class="keyword">char</span>* p4 = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">7</span>];                     <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     <span class="built_in">strcpy_s</span>(p4, <span class="number">7</span>, <span class="string">&quot;123456&quot;</span>);                  <span class="comment">//(text)代码区</span></span><br><span class="line"> </span><br><span class="line">     <span class="comment">//(text)代码区</span></span><br><span class="line">     <span class="keyword">if</span> (p1 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span> p1;</span><br><span class="line">         p1 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (p2 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span> p2;</span><br><span class="line">         p2 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (p4 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span>[ ] p4;</span><br><span class="line">         p4 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">//(text)代码区</span></span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;                                            <span class="comment">//(text)代码区</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/cppstorage.png" alt="cppstorage" style="zoom: 80%;" /></p><p><strong>堆heap——动态分配资源</strong>：</p><ol><li>从现代语言的编程角度看，使用堆，或者说使用内存分配是很自然的；</li><li>动态内存带来了不确定性：内存分配耗时要多久？失败了怎么办？在实时性比较高的场合，如嵌入式控制器和电信设备。</li><li>一般地，当我们在堆上分配内存时，很多语言使用new这样的关键字，有些语言则是隐式分配。在c++中new的对应词是delete，因为c++是可以让程序员完全结果内存的分配释放的。</li></ol><h4 id="2-资源管理方案——RAII"><a href="#2-资源管理方案——RAII" class="headerlink" title="2. 资源管理方案——RAII"></a>2. 资源管理方案——RAII</h4><p><strong>Resource Acquisition Is Initialization:</strong></p><ul><li>C++所特有的资源管理方式。主流语言中，C++是唯一一个依赖RAII做资源管理的</li><li>RAII依托栈和析构函数，来对所有资源——包括堆内存在内进行管理。</li><li>RAII有些比较成熟的智能指针代表：<code>std::auto_ptr</code>和<code>boost::shared_ptr</code></li></ul><h4 id="3-C-中栈和堆的对比"><a href="#3-C-中栈和堆的对比" class="headerlink" title="3. C++ 中栈和堆的对比"></a>3. C++ 中栈和堆的对比</h4><div class="table-container"><table><thead><tr><th></th><th>stack</th><th>heap</th></tr></thead><tbody><tr><td>作用域</td><td>函数体内，语句块{}作用域；</td><td>整个程序范围内，由new，malloc开始，delete， free结束</td></tr><tr><td>编译期间大小确定</td><td>变量大小范围确定</td><td>变量大小范围不确定，需要运行期确定</td></tr><tr><td>大小范围</td><td>win：1M，linux默认8M或10M，可以用ulimit -s查询</td><td>所有系统的堆空间大小是接近内存（虚拟内存）的总大小的（一部分被OS占用）</td></tr><tr><td>内存分配方式</td><td>地址由高到低减少</td><td>地址由低到高增加</td></tr><tr><td>内容是否可变</td><td>可变</td><td>可变</td></tr></tbody></table></div><h4 id="4-内存泄漏"><a href="#4-内存泄漏" class="headerlink" title="4. 内存泄漏"></a>4. 内存泄漏</h4><p><strong>定义</strong>：</p><p>​    程序中已动态分配的<strong>堆内存由于某种原因程序未释放或无法释放</strong>，造成系统内存的浪费，导致<strong>程序运行速度减慢甚至系统崩溃等严重后果</strong>。</p><p><strong>原因和排查方式</strong>：</p><ol><li>内存泄漏主要发生在堆内存分配方式中，即“配置内存后，所有指向该内存的指针都遗失了”。若缺乏语言的垃圾回收机制，这样的内存就无法归还系统。</li><li>因为内存泄漏属于程序运行中的问题，无法通过编译识别，所以只能在程序运行过程中来判别和诊断。</li></ol><h3 id="10-C-的智能指针"><a href="#10-C-的智能指针" class="headerlink" title="10. C++的智能指针"></a>10. C++的智能指针</h3><p>C++中推出了四种常用的智能指针：</p><ul><li><code>unique_ptr</code></li><li><code>shared_ptr</code></li><li><code>weak_ptr</code>: C++ 11中已经被遗弃</li><li><code>auto_ptr</code>：C++ 17中正式删除。</li></ul><p>从应用方面来分析这几种智能指针：</p><ol><li>应用场景<ul><li>对象所有权</li><li>生命周期</li></ul></li><li>性能分析</li></ol><h4 id="1-auto-ptr"><a href="#1-auto-ptr" class="headerlink" title="1. auto_ptr"></a>1. auto_ptr</h4><p>由new expression 获得对象，在auto_ptr 对象销毁是，其所管理的对象也会被自动delete掉。</p><p><strong>所有权转移</strong>：不小心把它传递给另外的智能指针，原来的指针就不在拥有这个对象了。<strong>转交给新对象，然后再将原对象指针置为nullptr。</strong></p><p>示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    &#123;<span class="comment">// 这个大括号只是为了确定auto_ptr作用范围 int</span></span><br><span class="line">        <span class="function">auto_ptr&lt;<span class="keyword">int</span>&gt; <span class="title">pl</span><span class="params">(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">10</span>))</span></span>;</span><br><span class="line">        cout &lt;&lt; *pl &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// auto_ptr C++ 17中移除 拥有严格对象所有权语义的智能指针</span></span><br><span class="line">        <span class="comment">// auto_ptr 原理： 在拷贝和赋值过程中，直接剥夺原对象对内存的控制权，转交给新对象</span></span><br><span class="line">        <span class="comment">// 然后再将原对象指针置为nullptr，这种做法也叫管理权转移</span></span><br><span class="line">        <span class="comment">// 缺点：当我们再次去访问原对象是，程序就会报错，所以auto_ptr实现原理就不好</span></span><br><span class="line">        auto_ptr&lt;string&gt; languages[<span class="number">5</span>] =&#123;</span><br><span class="line">                auto_ptr&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;C&quot;</span>)),</span><br><span class="line">                auto_ptr&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;Java&quot;</span>)),</span><br><span class="line">                auto_ptr&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;C++&quot;</span>)),</span><br><span class="line">                auto_ptr&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;Python&quot;</span>)),</span><br><span class="line">                auto_ptr&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;GO&quot;</span>))</span><br><span class="line">        &#125;;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;There are some computer languagues here: \n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)&#123;</span><br><span class="line">        cout &lt;&lt; *languages[i] &lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    auto_ptr&lt;string&gt; pC;</span><br><span class="line">    pC = languages[<span class="number">2</span>];<span class="comment">//languages[2] 失去所有权，将languages[2]转交给pC</span></span><br><span class="line">    <span class="comment">// 此时languages[2]不再引用该字符串从而变成空指针</span></span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;There are some computer languagues here: \n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)&#123;</span><br><span class="line">        cout &lt;&lt; *languages[i] &lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;The winner is &quot;</span> &lt;&lt; *pC &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">——————————————————————————————</span><br><span class="line"><span class="number">10</span></span><br><span class="line">There are some computer languagues here:</span><br><span class="line">C</span><br><span class="line">Java</span><br><span class="line">There are some computer languagues here:</span><br><span class="line">C</span><br><span class="line">Java</span><br><span class="line">The winner is C++</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-unique-ptr"><a href="#2-unique-ptr" class="headerlink" title="2. unique_ptr"></a>2. unique_ptr</h4><p><code>unique_ptr</code>:是专属所有权，所以<code>unique_ptr</code>管理的内存，只能被一个对象持有，不支持复制和赋值。</p><p>移动语义：<code>unique_ptr</code>禁止了拷贝语义，但有时我们也需要能够转移所有权，用<code>std::move()</code>进行控制所有权的转移。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>    </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span>    </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;    </span><br><span class="line">    <span class="keyword">auto</span> w = std::make_unique&lt;<span class="keyword">int</span>&gt;(<span class="number">10</span>);    </span><br><span class="line">    cout &lt;&lt; *(w.<span class="built_in">get</span>()) &lt;&lt; endl;    </span><br><span class="line"></span><br><span class="line">    <span class="comment">//auto w2 = w;//编译错误，无法讲w复制给w2    </span></span><br><span class="line">    <span class="comment">//因为复制从语义上来说，两个对象将共享同一块内存    </span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// unique_ptr 只支持移动语义</span></span><br><span class="line">    <span class="keyword">auto</span> w2 = std::<span class="built_in">move</span>(w);   <span class="comment">// w2获得内存所有权，w此时等于nullptr                                                </span></span><br><span class="line">    cout &lt;&lt; ((w.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)? (*w.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl;                    </span><br><span class="line">    cout &lt;&lt; ((w2.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)?(*w2.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl;                   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;                                                                 </span><br><span class="line">&#125;      </span><br><span class="line">——————————————————————————————————————</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">-1</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>cout &lt;&lt; ((w.get() != nullptr)? (*w.get()):-1) &lt;&lt; endl</code> 三目运算符， <code>(w.get() != nullptr)?</code>成立吗？成立取：前面，否则取：后面。</p><h4 id="3-shared-ptr"><a href="#3-shared-ptr" class="headerlink" title="3. shared_ptr"></a>3. shared_ptr</h4><p><code>shared_ptr</code>通过一个引用计数共享一个对象。</p><p><code>shared_ptr</code>是为了解决<code>auto_ptr</code>在对象所有权上的局限性，在使用<strong>引用计数的机制</strong>上提供了可以共享所有权的智能指针，当然也需要额外的开销。</p><p><strong>当引用计数为0时，该对象没有被使用，可以进行析构</strong>。</p><h4 id="4-weak-ptr"><a href="#4-weak-ptr" class="headerlink" title="4. weak_ptr"></a>4. weak_ptr</h4><p><code>weak_ptr</code>被设计为与 <code>shared_ptr</code>共同工作，用一种观察这模式工作。</p><p>作用：协作 <code>shared_ptr</code>工作，可获得资源的观测权，像旁观者那样观测资源使用情况。</p><p>意味着 <code>weak_ptr</code>只对 <code>shared_ptr</code>进行引用，而不改变其引用计数，当被观察的<code>shared_ptr</code>失效后，相应的 <code>weak_ptr</code>也相应失效。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span>    </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span>    </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">//shared_ptr</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// shared_ptr 代表的是共享所有权， 即多个shared_ptr 可以共享同一块内存</span></span><br><span class="line">        <span class="keyword">auto</span> wA = shared_ptr&lt;<span class="keyword">int</span>&gt;(<span class="keyword">new</span> <span class="built_in"><span class="keyword">int</span></span>(<span class="number">20</span>));</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> wA2 = wA;</span><br><span class="line">            cout &lt;&lt; ((wA2.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)? (*wA2.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl; <span class="comment">//20</span></span><br><span class="line">            cout &lt;&lt; ((wA.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)? (*wA.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl;<span class="comment">//20</span></span><br><span class="line">            cout &lt;&lt; wA2.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">//2 wA2.use_count() 引用计数</span></span><br><span class="line">            cout &lt;&lt; wA.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">//2</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//cout &lt;&lt; wA2.use_count() &lt;&lt; endl; //</span></span><br><span class="line">        cout &lt;&lt; wA.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">// 1</span></span><br><span class="line">        <span class="comment">// shared_ptr内部是利用计数来实现内存的自动管理，每当赋值一个shared_ptr</span></span><br><span class="line">        <span class="comment">// 引用计数会+1.当shared_ptr离开作用域时，引用计数会-1</span></span><br><span class="line">        <span class="comment">// 当引用计数为0的时候，则delete内存</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// move 语法</span></span><br><span class="line">    <span class="keyword">auto</span> wAA = std::make_shared&lt;<span class="keyword">int</span>&gt;(<span class="number">30</span>);</span><br><span class="line">    <span class="keyword">auto</span> wAA2 = std::<span class="built_in">move</span>(wAA); <span class="comment">// 此时wAA等于nullptr, wAA2.use_count()等于1</span></span><br><span class="line">    cout &lt;&lt; ((wAA.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)? (*wAA.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl; <span class="comment">//-1</span></span><br><span class="line">    cout &lt;&lt; ((wAA2.<span class="built_in">get</span>() != <span class="literal">nullptr</span>)? (*wAA2.<span class="built_in">get</span>()):<span class="number">-1</span>) &lt;&lt; endl;<span class="comment">//30</span></span><br><span class="line">    cout &lt;&lt; wAA.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">//0 wA2.use_count() 引用计数</span></span><br><span class="line">    cout &lt;&lt; wAA2.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">//1</span></span><br><span class="line">    <span class="comment">// 将wAA 对象move给wAA2，意味着wAA放弃了对内存的所有权和管理，此时wAA对象等于nullptr</span></span><br><span class="line">    <span class="comment">// 而wAA2获得了对象所有权，但因为此时wAA已不再持有对象，因此wAA2引用计数减1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">----------------------------------------</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">-1</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="11-C-的引用"><a href="#11-C-的引用" class="headerlink" title="11. C++的引用"></a>11. C++的引用</h3><h4 id="1-引用"><a href="#1-引用" class="headerlink" title="1.引用"></a>1.引用</h4><p>引用： 是一种特殊的指针，不允许修改的指针。</p><ul><li><p>使用指针的坑：</p><ul><li>空指针</li><li>野指针</li><li>不知不觉改变了指针的值，却继续引用</li></ul></li><li><p>引用：</p><ul><li>不存在空引用</li><li>必须初始化</li><li>一个引用永远指向它初始化的对象</li></ul></li></ul><h4 id="2-基本使用"><a href="#2-基本使用" class="headerlink" title="2. 基本使用"></a>2. 基本使用</h4><p>引用：可以认为是指定变量的别名，使用是可以认为是变量本身。</p><p><code>int x = 1;</code></p><p><code>int &amp;rx = x;</code>这两句中&amp;是引用，注意与指针的区别</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> &amp;a, <span class="keyword">int</span> &amp;b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    temp = a;</span><br><span class="line">    a = b;</span><br><span class="line">    b = temp;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap2</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    temp = *a;</span><br><span class="line">    *a = *b;</span><br><span class="line">    *b = temp;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">1</span>, x2 = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">int</span> &amp;rx = x;</span><br><span class="line">    rx = <span class="number">2</span>;</span><br><span class="line">    cout &lt;&lt; x &lt;&lt; endl; <span class="comment">// 2</span></span><br><span class="line">    cout &lt;&lt; rx &lt;&lt; endl;<span class="comment">//2</span></span><br><span class="line">    rx = x2;</span><br><span class="line">    cout &lt;&lt; x &lt;&lt; endl;<span class="comment">//3</span></span><br><span class="line">    cout &lt;&lt; rx &lt;&lt; endl;<span class="comment">//3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">3</span>, b = <span class="number">4</span>;</span><br><span class="line">    <span class="built_in">swap</span>(a, b);</span><br><span class="line">    <span class="built_in">assert</span>(a==<span class="number">4</span>&amp;&amp;b==<span class="number">3</span>);</span><br><span class="line">    <span class="built_in">swap2</span>(&amp;a, &amp;b);</span><br><span class="line">    <span class="built_in">assert</span>(a==<span class="number">3</span>&amp;&amp;b==<span class="number">4</span>);</span><br><span class="line">    <span class="built_in">swap</span>(rx, x);</span><br><span class="line">    cout &lt;&lt; x &lt;&lt; endl;<span class="comment">//3</span></span><br><span class="line">    cout &lt;&lt; rx &lt;&lt; endl;<span class="comment">//3</span></span><br><span class="line">    rx = x2 + <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">swap</span>(rx, x);</span><br><span class="line">    cout &lt;&lt; x &lt;&lt; endl;<span class="comment">//4</span></span><br><span class="line">    cout &lt;&lt; rx &lt;&lt; endl;<span class="comment">//4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-C-的引用的作用"><a href="#3-C-的引用的作用" class="headerlink" title="3. C++的引用的作用"></a>3. C++的引用的作用</h4><ul><li>有了指针为什么还要引用？——为了支持运算符重载；</li><li>有了引用为什么还需要指针？——为了兼容c语言</li></ul>]]></content>
      
      
      <categories>
          
          <category> Cpp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 指针 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VGG 网络</title>
      <link href="2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/"/>
      <url>2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="1-VGG结构："><a href="#1-VGG结构：" class="headerlink" title="1. VGG结构："></a>1. VGG结构：</h3><p>VGG 原文在：<a href="https://arxiv.org/pdf/1409.1556.pdf">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a></p><p>在<code>ABSTRACT</code>  中：</p><blockquote><p> Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. </p></blockquote><p>就说了VGG最大特点是用小卷积滤波器(3x3)，16-19层, <strong>Small filters, Deeper networks</strong></p><p>VGG16 结构示意图(对应论文Config D)：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210221231111.png?=raw" width=50% height=50%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">VGG16结构示意图</div> </center><p>论文中VGG不同结构如下(C和D区别在有没有1x1卷积)：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261728322.png" alt="image-20210221200541135" style="zoom:25%;" /></p><h4 id="conv-3x3-优势"><a href="#conv-3x3-优势" class="headerlink" title="conv 3x3 优势"></a>conv 3x3 优势</h4><p>在VGG中，采用3个3x3的小卷积能够代替一个7x7的大卷积，要快速计算感受野，用这个 <a href="https://fomoro.com/research/article/receptive-field-calculator#3,1,1,SAME;3,1,1,SAME;3,1,1,SAME"><a href="https://fomoro.com/">Fomoro AI</a></a></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="E:\systemshare\Pic\notion\gitee\bpic\img\blog\20210219155841.png?=raw" width=400 height=220>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">3层layer感受野</div> </center><p>先计算特征图。带<code>dilation</code>的卷积操作，特征图size为：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261728323.png" alt="image-20210219154251782" style="zoom: 33%;" /></p><p>像cs231n中，(不带dilation)计算特征图公式：</p><script type="math/tex; mode=display">W_{out} = \frac{W - K + 2P }{S} + 1</script><p>其中, $W$是输入尺寸， $K$是kernel_size, $P$是padding大小， $S$是步长</p><p>那么，经过第一层卷积后， (K=3, S=1, padding=1), 输入224x224，输出为224x224,对应vgg 图中第一层卷积。</p><p>感受野计算code，详细见 <a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807">A guide to receptive field arithmetic for Convolutional Neural Networks</a></p><script type="math/tex; mode=display">l_k = l_{k-1}+ [(f_k -1) * \prod_{i=1}^{k-1}s_i]</script><p>其中， </p><div class="table-container"><table><thead><tr><th style="text-align:left">No.</th><th style="text-align:center">Layers</th><th style="text-align:center">Kernel Size</th><th style="text-align:center">Stride</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:center">Conv1</td><td style="text-align:center">3*3</td><td style="text-align:center">1</td></tr><tr><td style="text-align:left">2</td><td style="text-align:center">Pool1</td><td style="text-align:center">2*2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:left">3</td><td style="text-align:center">Conv2</td><td style="text-align:center">3*3</td><td style="text-align:center">1</td></tr></tbody></table></div><p>对于初始感受野$l_0=1$，各层感受野分别为:</p><script type="math/tex; mode=display">\begin{aligned}&l_0 = 1\\&l_1 = 1 + (3-1)=3\\&l_2 = 3 + (2-1)*1=4\\&l_3 = 4+(3-1)*1*2=8\end{aligned}</script><p>那么像图3层layer感受野中，Layer1中感受野为1，Layer2中感受野为(按VGG16 中卷积层 $f_k=3, s_i=1$ )：3,  Layer3为5， Layer4为7.也是7x7的感受野。</p><p>更重要的是<strong>参数量</strong>大大降低，</p><p>一个7x7卷积核的参数量为49， 3个3x3的卷积核计算量为3x3x3.</p><p><strong>如何理解卷积层的参数量和计算量</strong>？</p><ul><li>参数量：参于计算参数的个数，占用内存空间(考虑bias)</li></ul><script type="math/tex; mode=display">(C_{in} * (K * K) + 1)* C_{out} \tag{1}</script><ul><li>FLOPS: 每秒浮点运算次数，理解为计算速度。是衡量<strong>硬件性能</strong>的指标</li><li>FLOPs: 浮点运算数，理解为计算量。可以用来衡量算法、模型的复杂度, H、W是输出feature map的尺寸（宽和高）</li></ul><script type="math/tex; mode=display">(C_{in} *2* K * K) * H_{out} * W_{out} * C_{out} \tag{2}</script><ul><li>MAC:乘加次数，用来衡量计算量<script type="math/tex; mode=display">C_{in} * K * K * H_{out} * W_{out} * C_{out} \tag{3}</script></li></ul><p>计算实例, 如下图，来自于 <a href="http://cs231n.stanford.edu/slides/2020/lecture_9.pdf">cs231n lecture9</a></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261728324.png" alt="image-20210221220738811" style="zoom: 50%;" /></p><p>原文对3x3的解释：</p><blockquote><p>First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer $3 × 3$convolution stack has C channels, the stack is parametrised by $3 (3^2C^2) = 27C^2$ weights; at the same time, a single $7 × 7$ conv. layer would require $7^2C^2 = 49C^2 $parameters, i.e. 81%more. This can be seen as imposing a regularisation on the $7 × 7 $conv. filters, forcing them to have a decomposition through the $3 × 3$ filters (with non-linearity injected in between)</p></blockquote><p>文章认为 LRN(local Response Normalization) 并没有提升模型在 ILSVRC 数据集上的表现，反而增加了内存消耗和计算时间。</p><p>模型 C 和 D 的层数一样，但 C 层使用了 1×1 的卷积核，用于对输入的线性转换，增加非线性决策函数，而不影响卷积层的接受视野。后面的评估阶段也有证明，使用增加的 1×1 卷积核不如添加 3×3 的卷积核。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="http://deanhan.com/2018/07/26/vgg16/">VGG16学习笔记</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/42233779">VGG 论文阅读记录</a></p><p>[3] [<a href="https://www.cnblogs.com/wangguchangqing/p/10338560.html">卷积神经网络之VGG</a></p><p>[4] <a href="http://noahsnail.com/2017/08/17/2017-08-17-VGG%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/">VGG论文翻译——中文版</a></p><p>[5] <a href="https://www.plob.org/article/22077.html">感受野计算公式</a></p><p>[6]  <a href="https://www.cnblogs.com/shine-lee/p/12069176.html">感受野</a></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> VGG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. 树和森林</title>
      <link href="2020/05/21/DS_2.%E6%A0%91%E5%92%8C%E6%A3%AE%E6%9E%97/"/>
      <url>2020/05/21/DS_2.%E6%A0%91%E5%92%8C%E6%A3%AE%E6%9E%97/</url>
      
        <content type="html"><![CDATA[<h3 id="2-树和森林"><a href="#2-树和森林" class="headerlink" title="2. 树和森林"></a>2. 树和森林</h3><h4 id="1-树和森林定义"><a href="#1-树和森林定义" class="headerlink" title="1. 树和森林定义"></a>1. 树和森林定义</h4><p>树：是$n(n \ge 0)$ 个结点的有限集。若$n=0$, 称为空树；</p><ul><li><p>若$n=0$, 则是空树</p></li><li><p>若$n \gt 0$ , 有且仅有一个特定的称为根(root)的结点；</p><p>​                    其余结点可以分为$m(m\ge0)$个互不相交的有限集$T1, T2, \cdots, Tm$</p></li></ul><p>森林： 是$m (m \ge 0)$棵互不相交的树的集合。</p><h4 id="2-树的存储结构"><a href="#2-树的存储结构" class="headerlink" title="2. 树的存储结构"></a>2. 树的存储结构</h4><ol><li><strong>双亲表示法</strong>，找双亲简单，找孩子难。</li></ol><p>实现：定义结构数组，存放树的结点，每个结点含两个域</p><ul><li>数据域：存放结点本身信息</li><li>双亲域：指示本结点的双亲节点在数组中的位置</li></ul><p>如下表所示，表示出树的结构如图所示。其中 <code>-1</code>代表没有双亲节点</p><div class="table-container"><table><thead><tr><th>数组索引</th><th>存储数据</th><th>双亲节点在数组中的位置</th></tr></thead><tbody><tr><td>0</td><td>R</td><td>-1</td></tr><tr><td>1</td><td>A</td><td>0</td></tr><tr><td>2</td><td>B</td><td>0</td></tr><tr><td>3</td><td>C</td><td>0</td></tr><tr><td>4</td><td>D</td><td>1</td></tr><tr><td>5</td><td>E</td><td>1</td></tr><tr><td>6</td><td>F</td><td>3</td></tr><tr><td>7</td><td>G</td><td>6</td></tr><tr><td>8</td><td>H</td><td>6</td></tr><tr><td>9</td><td>K</td><td>6</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261819213.png" alt="image-20210605231331579" style="zoom:25%;" /></p><ol><li><strong>孩子链表</strong></li></ol><p>把每个节点的孩子节点排列起来，看成是一个线性表，用单链表存储n个节点有n个孩子链表（叶子的孩子链表为空表）。而n个头指针又组成一个线性表，用顺序表（含n个元素的结构数组）存储。特点是：<strong>找孩子任意，找双亲难。</strong></p><ul><li>孩子节点结构由：<code>child</code>自身和下一个节点位置 <code>next</code>组成</li><li>双亲节点结构由： <code>data</code>本身和第一个孩子表头指针 <code>firstchild</code></li></ul><p>跟上表一样的树用孩子链表表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261819208.png" alt="image-20210606134739350" style="zoom:20%;" /></p><ol><li><p><strong>带双亲的孩子链表</strong></p></li><li><p><strong>孩子兄弟表示法</strong></p></li></ol><p>实现：用<strong>二叉链表</strong>做树的存储结构，链表中每个节点的两个指针域分布指向其<strong>第一个孩子节点</strong>和<strong>下一个兄弟节点</strong>。</p><p>图示该表示法的树的存储结构，其中左边指向第一个孩子节点，右边指向相邻的第一个兄弟节点， <code>^</code>表示空。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261819158.png" alt="image-20210606135457927" style="zoom:17%;" /></p><h4 id="3-树与二叉树的转换"><a href="#3-树与二叉树的转换" class="headerlink" title="3. 树与二叉树的转换"></a>3. 树与二叉树的转换</h4><ul><li>将树转化为二叉树处理，利用二叉树的算法来实现对数的操作</li><li>由于树和二叉树都可以用二叉链表作存储结构，则以二叉链表作媒介可以导出数与二叉树直接的一个对应关系。</li></ul><p>如下图中，对于一棵树，我们可以将其用二叉链表存储，按照上面规则解释，也可将其按照二叉树规则解释。这样左边的树就有与之对应的右边二叉树。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261819616.png" alt="image-20210606141843381" style="zoom:17%;" /></p><p><strong>直接将树转化为二叉树</strong>：</p><ol><li>加线：在兄弟间加一条线</li><li>抹线：对每个节点，除了其左孩子外，去除其余其余孩子之间的关系</li><li>旋转：以数的根节点为轴心，将整树顺时针旋转45°。</li></ol><p>树变二叉树口诀：<strong>兄弟相连留长子</strong>。示例如下图</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820462.png" alt="image-20210606142626402" style="zoom:25%;" /></p><p>逆过程：<strong>二叉树变树</strong>，记下口诀： <strong>左孩右右连双亲，去掉原来右孩线</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820541.png" alt="image-20210606143239862" style="zoom:31%;" /></p><h4 id="4-森林与二叉树转化"><a href="#4-森林与二叉树转化" class="headerlink" title="4. 森林与二叉树转化"></a>4. 森林与二叉树转化</h4><p><strong>森林转化为二叉树</strong>：</p><ol><li>将每棵树分别转换为二叉树</li><li>将每棵树的根节点用线相连</li><li>以第一棵树的根节点为二叉树的根，再以根节点为轴心，顺时针旋转，构成二叉树结构</li></ol><p>简单来说，就是先将树变为二叉树，再将其根相连，口诀：<strong>树变二叉根相连</strong>。</p><p>具体看如下图例子，我们看到根为AEG三棵树组成的森林。</p><ol><li>按照树转化为二叉树规则，将其根连接起来，去兄弟，只保留长子<ul><li>如BCD连起来，再去掉兄弟节点连续，只保留长子</li><li>EF就变成E为根，F为左子树</li><li>GHIJ就连接HI，去掉GI连线</li></ul></li><li>将形成的3棵二叉树的根AEG连接，顺时针旋转得到二叉树结构</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820444.png" alt="image-20210606163536777" style="zoom:35%;" /></p><p><strong>二叉树转化为森林</strong>：</p><ol><li>抹线：将二叉树中根节点与其右孩子连线，及沿着有分支搜索到的所有右孩子间连线全部抹掉，使其变为孤立的二叉树</li><li>还原：将孤立二叉树还原为树</li></ol><p>如下图所示，</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820751.png" alt="image-20210606164912286" style="zoom:25%;" /></p><h4 id="5-树与森林的遍历"><a href="#5-树与森林的遍历" class="headerlink" title="5. 树与森林的遍历"></a>5. 树与森林的遍历</h4><p><strong>树的遍历</strong>（三种方式，没有中序遍历）</p><ul><li>先根遍历：若树不空，先访问根节点，然后依次先根遍历各棵子树</li><li>后根遍历：若树不空，先依次后根遍历各棵子树，然后访问根节点</li><li>层次遍历：若树不空，则自上而下自左而右访问树中的每个节点</li></ul><p>如下图所示树结构，</p><ol><li>先根遍历：ABCDE</li><li>后根遍历：BDCEA</li><li>层次遍历：ABCED</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820045.png" alt="image-20210606170254360" style="zoom:45%;" /></p><p><strong>森林的遍历</strong>：</p><p>森林遍历可以分为3个部分组成：</p><ol><li>森林中第一棵树的根节点</li><li>森林中第一棵树的子树森林</li><li>森林中其他树构成的森林</li></ol><p>比如，<strong>先序遍历</strong>：</p><ol><li>若森林不空，先访问森林中第一棵树的根节点</li><li>先序遍历森林中第一棵树的子树森林</li><li>先序遍历森林中除第一棵树之外其余树构成的森林。</li></ol><p><strong>中序遍历：</strong>若森林不空，</p><ol><li>中序遍历第一棵树的子树森林</li><li>访问第一棵树的根节点</li><li>中序遍历森林中除第一棵树之外其余树构成的森林。</li></ol><p>如下图中森林遍历如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261820339.png" alt="image-20210606180536311" style="zoom:25%;" /></p><p>先序遍历：ABCDEFGHIJ</p><p>中序遍历：BCDAFEHJIG</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树和森林 </tag>
            
            <tag> 双亲表示法 </tag>
            
            <tag> 孩子链表表示法 </tag>
            
            <tag> 树和二叉树转化 </tag>
            
            <tag> 森林与二叉树转化 </tag>
            
            <tag> 树与森林的遍历 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. 二叉树</title>
      <link href="2020/05/20/DS_1.%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>2020/05/20/DS_1.%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h3 id="1-二叉树"><a href="#1-二叉树" class="headerlink" title="1. 二叉树"></a>1. 二叉树</h3><h4 id="1-术语"><a href="#1-术语" class="headerlink" title="1.术语"></a>1.术语</h4><ol><li>根节点：非空书中无前驱节点的节点</li><li>节点的度：节点拥有的子树数</li><li>树的度：树内各节点的度的最大值</li><li>节点的子树的根称为该节点的孩子，该节点称为孩子的双亲；</li><li>节点的祖先：从该节点到根所经分支上的所有节点</li><li>节点的子孙：以某一节点为根的子树中的任一节点。</li><li>树的深度：树中节点的最大高度</li></ol><h4 id="2-二叉树应用"><a href="#2-二叉树应用" class="headerlink" title="2.二叉树应用"></a>2.二叉树应用</h4><ol><li><strong>数据压缩</strong></li><li><strong>二叉树求解表达式的值</strong></li></ol><h4 id="3-二叉树的定义"><a href="#3-二叉树的定义" class="headerlink" title="3. 二叉树的定义"></a>3. 二叉树的定义</h4><p>二叉树是n(n≥0)个节点的有限集，它或者是<strong>空集</strong>(n=0)，或者由一个<strong>根节点及两颗互不相交的</strong>分别称作这个跟的左子树和右子树的二叉树组成。</p><p><strong>特点</strong>：</p><ol><li>每个节点最多有俩孩子(<strong>二叉树中不存在度大于2的节点</strong>)</li><li>子树有左右之分，其词性不能颠倒</li><li>二叉树可以是<strong>空集合</strong>，<strong>根可以有空的左子树或空的右子树</strong></li></ol><p>注： <strong>二叉树不是树的特殊情况，它们是两个概念。</strong></p><p>二叉树要区分左子树和右子树。树的话只有一个孩子是，无须区分左右。</p><h4 id="4-二叉树的性质和存储结构"><a href="#4-二叉树的性质和存储结构" class="headerlink" title="4.二叉树的性质和存储结构"></a>4.二叉树的性质和存储结构</h4><p>性质1：在二叉树的第i层上制度有$2^{i-1}$个节点($ i \ge 1$)。第i层上至少有<strong>1个</strong>节点</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818100.png" alt="image-20210226155729156" style="zoom:33%;" /></p><p>性质2：深度为k的二叉树至多有$2^k - 1= \sum_{i=1}^k 2^{i-1}$个节点, 至少要有k个节点.</p><p>性质3： 对于任何一棵二叉树T，如果其叶子树为$n_0$, 度为2的节点数为$n_2$,则$ n_0 = n_2 + 1 $</p><p>证明：</p><p>总边数设为E， 那么$E = n-1 \ 其中n 为节点总数$, 从下往上看</p><p>那么，$E = n_1 + 2n_2$ 度为1的有两条边，度为2的有两条边。从上往下看</p><p>那么，$n = n_1 + 2n_2 +1 = n_1 + n_2 + n_0$</p><h4 id="5-完全二叉树"><a href="#5-完全二叉树" class="headerlink" title="5. 完全二叉树"></a>5. 完全二叉树</h4><ul><li>满二叉树：一棵深度为k且有$2^k - 1$个节点的二叉树是满二叉树。跟上图一样，就是每个节点都有2个分支，度都为2.<ul><li>特点：1.每一层上的节点数都是最大节点数(即每层都是满的)；2. 叶子节点全部都在最底层</li><li>编号规则：从根节点开始，从上而下，自左向右。</li><li>满二叉树在同样深度的二叉树中节点个数最多</li><li>满二叉树在同样深度的二叉树中叶子节点个数最多</li></ul></li><li>完全二叉树： 深度为k的具有n个节点的二叉树，当且仅当其每一个节点都与深度为k的满二叉树中编号为1~n的节点一一对应时，称为完全二叉树。 </li></ul><p>下图是完全二叉树。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818102.png" alt="image-20210226161603642" style="zoom:25%;" /></p><p>注：在满二叉树中，从最后一个节点开始，<strong>连续去掉</strong>任意个节点，就构成了一个完全二叉树</p><p>特点：</p><ol><li>叶子只能分布在层次最大的两层上</li><li>对任一节点，如果其右子树最大层次为i，则其左子树的最大层次必为$i或i+1$</li></ol><p>则其左子树的最大层次必为$i或i+1$</p><p><strong>完全二叉树性质</strong>：</p><p>性质：</p><ul><li>具有n个节点的我完全二叉树的深度为 $ \text{floor }(\text{log}_2n)+ 1 $.得到深度与节点数的关系</li><li>如歌对一棵有n个节点的完全二叉树(深度为$ \text{floor }(\text{log}_2n)+ 1 $)的节点按层序编号，从第一层到第$ \text{floor }（ \text{log}_2n）+ 1 $层，每层从左到右，则对任一节点$1 \le i \le n$有：(父节点和孩子节点编号关系)<ul><li>如果$i=1$,则节点$i$是二叉树的根，无父节点，如下图节点1；如果$i \gt 1$,则其父节点是节点 $ \text{floor }(i/2) $，如节点2或3</li><li>如果$2i \gt n$,则节点$i$为叶子节点，无左孩子；否则其<strong>左孩子是节点$2i$</strong></li><li>如果$2i+1 \gt n$， 则节点$i$无右孩子；否则其<strong>右孩子节点是$2i+1$</strong></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818103.png" alt="image-20210226195047996" style="zoom:33%;" /></p><h4 id="6-二叉树的存储结构"><a href="#6-二叉树的存储结构" class="headerlink" title="6. 二叉树的存储结构"></a>6. 二叉树的存储结构</h4><ol><li><strong>二叉树的顺序存储：</strong></li></ol><p>实现：按满二叉树的节点层次编号，依次存放二叉树中的数据元素。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818104.png" alt="image-20210226201931215" style="zoom:33%;" /></p><p>其编号和存储元素为：</p><div class="table-container"><table><thead><tr><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th><th style="text-align:center">7</th><th style="text-align:center">8</th><th style="text-align:center">9</th><th style="text-align:center">10</th><th style="text-align:center">11</th></tr></thead><tbody><tr><td style="text-align:center">31</td><td style="text-align:center">23</td><td style="text-align:center">12</td><td style="text-align:center">66</td><td style="text-align:center">94</td><td style="text-align:center">5</td><td style="text-align:center">17</td><td style="text-align:center">70</td><td style="text-align:center">62</td><td style="text-align:center">49</td><td style="text-align:center">55</td><td style="text-align:center">88</td></tr></tbody></table></div><p>但如果不是完全二叉树呢?如下图，依旧按满二叉树编号，没有的存储元素为空。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818105.png" alt="image-20210226202812192" style="zoom: 25%;" /><br>其编号和存储元素为,</p><div class="table-container"><table><thead><tr><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th><th style="text-align:center">7</th><th style="text-align:center">8</th><th style="text-align:center">9</th><th style="text-align:center">10</th></tr></thead><tbody><tr><td style="text-align:center">31</td><td style="text-align:center">23</td><td style="text-align:center">12</td><td style="text-align:center">66</td><td style="text-align:center">94</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">49</td><td style="text-align:center">55</td></tr></tbody></table></div><p>顺序存储的缺点：</p><ol><li>元素大小固定</li><li>出现空节点时，相应位置浪费存储空间。最坏情况：深度为k的且只有k个节点的单支树需要长度为$2^k - 1 $的一维数组。(右单支树)</li></ol><p>特点：节点间关系蕴含在其存储位置中，浪费空间，适于<strong>满二叉树和完全二叉树</strong>。</p><ol><li><strong>二叉树的链式存储结构</strong></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818107.png" alt="image-20210226213510575" style="zoom: 50%;" /></p><p><strong>注:在n个节点的二叉链表中，有n+1个空指针域</strong></p><p>因为n个节点，那么有2n个链域。(这是从根往叶子节点看)。反过来看，每个节点必有一个指针，除了根节点。那么空指针域有$2n-(n-1)=n+1$</p><h3 id="2-二叉树的遍历"><a href="#2-二叉树的遍历" class="headerlink" title="2. 二叉树的遍历"></a>2. 二叉树的遍历</h3><p>遍历的定义：顺着某一条搜索路径访问二叉树中的节点，使得每个节点均被访问异常，而且仅被访问一次.</p><p>遍历的目的： 得到树中所有节点的一个线性排列</p><p>遍历的用途： 它是树的结构插入、删除、修改、查找的基础</p><p>共有6种遍历方式：D：根节点 L:左节点 R：右节点</p><p>DLR、LDR、LRD、DRL、RDL、RLD</p><p>前序遍历：DLR</p><p>中序遍历：LDR</p><p>后序遍历： LRD</p><p>递归进行遍历。</p><p>前序遍历：若二叉树为空，则空操作；否则：</p><ol><li>访问根节点</li><li>前序遍历左子树</li><li>前序遍历右子树</li></ol><p>下图二叉树的3中遍历顺序：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818108.png" alt="image-20210226234519487" style="zoom: 25%;" /></p><p>前序遍历： ABELDHMIJ（根左右）</p><p>中序遍历： ELBAMHIDJ  (左根右)</p><p>后序遍历： LEBMIHJDA  (左右根)</p><p>再练习下图二叉树3种顺序遍历：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818109.png" alt="image-20210226235820740" style="zoom: 25%;" /></p><p>二叉树表示算术表达式：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818110.png" alt="image-20210227001137085" style="zoom: 25%;" /></p><p>前序遍历中运算符在前面，称作<strong>前缀表示</strong></p><p>中序遍历中运算符在中间，称作<strong>中缀表示</strong></p><p>后序遍历中运算符在后面，称作<strong>后缀表示</strong></p><p><strong>前序和中序确定二叉树</strong></p><p>如果要根据前序和中序遍历结果求二叉树，关键点是：<strong>前序确定根，中序确定左右子树</strong>。</p><p>例如： 已知</p><ul><li>先序： A B C D E F G H I J</li><li>中序： C D B F E A I H G J</li></ul><p>由先序确定A是整个的根，根据中序： CDBFE是左子树部分， IHGJ是右子树。</p><p>在CDBFE中，先序先遍历B，那么B是这部分的根；再看中序 CD是左， FE是右；再对CD而言，先序遍历是CD，中序是CD，(根左右， 左根右)那么只能是C为B的左子树，D为C的右子树。</p><p>在IHGJ中，先序先遍历G，那么G是这部分的根；再看中序， IH是左， J是右。结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818111.png" alt="image-20210227003733801" style="zoom: 25%;" /></p><p><strong>中序和后序确定二叉树</strong></p><p>中序： BDCEAFHG</p><p>后序： DECBHGFA</p><p>根据后序最后是根来确定根，根据中序确定左右。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818112.png" alt="image-20210227005448341" style="zoom: 22%;" /></p><h4 id="1-中心遍历非递归算法"><a href="#1-中心遍历非递归算法" class="headerlink" title="1. 中心遍历非递归算法"></a>1. 中心遍历非递归算法</h4><p>二叉树中序遍历的非递归算法的关键：在中序遍历过某个节点的整个左子树后，如何找到该节点的根以及右子树</p><p><strong>基本思想</strong>:(遇到根时先不访问，先入栈；先访问左子树，等访问完；再访问根，就把根节点出栈；最后访问右子树。）</p><ol><li>建立一个栈</li><li>根节点进栈，遍历左子树</li><li>根节点出现，输出根节点，遍历右子树</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        st, ans = [], []  <span class="comment"># 节点暂时存储的栈，结果</span></span><br><span class="line">        cur = root  <span class="comment"># 当前指针</span></span><br><span class="line">        <span class="keyword">while</span> st <span class="keyword">or</span> cur:  <span class="comment"># 当栈或当前节点不为空</span></span><br><span class="line">            <span class="keyword">if</span> cur:  <span class="comment"># 如果当前节点不为空</span></span><br><span class="line">                st.append(cur)  <span class="comment"># 栈压入当前节点</span></span><br><span class="line">                cur = cur.left  <span class="comment"># 当前指针指向左子树</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 栈不空</span></span><br><span class="line">                cur = st.pop()  <span class="comment"># 当前指针指向栈顶</span></span><br><span class="line">                ans.append(cur.val)  <span class="comment"># 栈顶值加入ans</span></span><br><span class="line">                cur = cur.right  <span class="comment"># 当前指针指向右子树</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h4 id="2-二叉树的层次遍历"><a href="#2-二叉树的层次遍历" class="headerlink" title="2. 二叉树的层次遍历"></a>2. 二叉树的层次遍历</h4><p>设计思路： 使用一个队列</p><ol><li><p>将根节点入队</p></li><li><p>队不空时循环；从队列中出列一个节点*p， 访问它；</p><ol><li>若它有左孩子节点，将左孩子节点入队</li><li>若它有右孩子节点，将右孩子节点入队</li></ol></li></ol><p><strong>稀碎的理解：</strong></p><ol><li>先入队根节点，再出队；</li><li>如果有左右孩子，将左右孩子都入队；</li><li>左孩子出队，如果左孩子还有左右孩子，继续将左右孩子入队；</li><li>右孩子也一样</li><li>直到队列为空</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        ans = []</span><br><span class="line">        que = [root] <span class="comment"># 根节点入队</span></span><br><span class="line">        <span class="keyword">while</span> que: <span class="comment"># 队列不空</span></span><br><span class="line">            tmp = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(que)):</span><br><span class="line">                cur = que.pop(<span class="number">0</span>) <span class="comment">#指向队列第一个元素</span></span><br><span class="line">                tmp.append(cur.val) <span class="comment">#将当前值加入ans</span></span><br><span class="line">                <span class="comment">#开始访问其左右子树</span></span><br><span class="line">                <span class="keyword">if</span> cur.left:</span><br><span class="line">                    que.append(cur.left)</span><br><span class="line">                <span class="keyword">if</span> cur.right:</span><br><span class="line">                    que.append(cur.right)</span><br><span class="line">                </span><br><span class="line">            ans.append(tmp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h4 id="3-二叉树遍历算法的应用"><a href="#3-二叉树遍历算法的应用" class="headerlink" title="3. 二叉树遍历算法的应用"></a>3. 二叉树遍历算法的应用</h4><h4 id="1-二叉树的建立"><a href="#1-二叉树的建立" class="headerlink" title="1. 二叉树的建立"></a>1. 二叉树的建立</h4><p>如先序遍历得到字符如ABC##DE#G##F###；</p><ol><li>先判断输入字符是不是#,如果是，将当前指针置为空</li><li>不是#时， 先生成根节点</li><li>再依次生成左右子树</li></ol><p>题目：<a href="https://leetcode-cn.com/problems/construct-binary-search-tree-from-preorder-traversal/">1008. 前序遍历构造二叉搜索树</a></p><p>稀碎的想法：</p><ol><li>如果前序列表为空，直接返回None</li><li>先将树添加前序的第一个元素，并将root指向Tree，这样就形成了root为整个树的根节点</li><li>遍历前序列表中剩余元素，如果小于root值就是左子树，大于就是右子树</li><li>但上面左右子树是一个大的左子树，右子树也是，将其分别看作一个小的子树，再遍历就好了</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bstFromPreorder</span>(<span class="params">self, preorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; TreeNode:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> preorder:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 当前指针置为树节点的根节点，前序遍历第一个元素为整棵树的根</span></span><br><span class="line">        root = TreeNode(preorder[<span class="number">0</span>])</span><br><span class="line">        left, right = [], [] <span class="comment">#两个列表分别放左右子树</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> preorder[<span class="number">1</span>:]: <span class="comment">#遍历前序列表1:n</span></span><br><span class="line">            <span class="keyword">if</span> x &lt; root.val: <span class="comment">#如果元素小于根节点，左子树</span></span><br><span class="line">                left.append(x) <span class="comment">#左子树元素加入左子树列表</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                right.append(x)</span><br><span class="line">        root.left = self.bstFromPreorder(left) <span class="comment">#递归遍历左子树</span></span><br><span class="line">        root.right = self.bstFromPreorder(right) </span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><h4 id="2-复制二叉树"><a href="#2-复制二叉树" class="headerlink" title="2. 复制二叉树"></a>2. 复制二叉树</h4><p>基本思路：</p><ol><li><p>如果是空树，返回None</p></li><li><p>不是； 申请新节点空间，再复制根节点</p><ul><li><p>递归复制左子树</p></li><li><p>递归复制右子树</p></li></ul></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">treenode</span>//结点的定义</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    treenode* left;</span><br><span class="line">    treenode* right;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> treenode* Tree;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">CopyNode</span><span class="params">(Tree t)</span><span class="comment">//复制结点t</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Tree newT = (Tree)<span class="built_in">malloc</span>(<span class="built_in"><span class="keyword">sizeof</span></span>(treenode));</span><br><span class="line">    newT-&gt;data = t-&gt;data;</span><br><span class="line">    newT-&gt;left = t-&gt;left;</span><br><span class="line">    newT-&gt;right = t-&gt;right;</span><br><span class="line">    <span class="keyword">return</span> newT;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Tree <span class="title">CopyTree</span><span class="params">(Tree t)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!t)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;<span class="comment">//树为空，直接返回NULL</span></span><br><span class="line"></span><br><span class="line">    Tree newLeft = <span class="literal">NULL</span>,newRight = <span class="literal">NULL</span>,newTree = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(t-&gt;left)<span class="comment">//左子树不为空则复制左子树</span></span><br><span class="line">        newLeft = <span class="built_in">CopyTree</span>(t-&gt;left);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        newLeft = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(t-&gt;right)<span class="comment">//右子树不为空则复制右子树</span></span><br><span class="line">        newRight = <span class="built_in">CopyTree</span>(t-&gt;right);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        newRight = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    Tree NewTree = <span class="built_in">CopyNode</span>(t);<span class="comment">//复制左右子树的根结点</span></span><br><span class="line">    NewTree-&gt;left = newLeft;</span><br><span class="line">    NewTree-&gt;right = newRight;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> NewTree;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-二叉树的深度"><a href="#3-二叉树的深度" class="headerlink" title="3. 二叉树的深度"></a>3. 二叉树的深度</h4><p>思路：</p><ul><li>如果是空树，则深度为0；</li><li>否则，递归计算左子树的深度为m，递归计算右子树的深度为n，则二叉树的深度为m与n较大者再加1，$max(m, n) + 1$</li></ul><p>题目：<a href="https://leetcode-cn.com/problems/er-cha-shu-de-shen-du-lcof/">剑指 Offer 55 - I. 二叉树的深度</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        思路：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - 如果是空树，则深度为0；</span></span><br><span class="line"><span class="string">            - 否则，递归计算左子树的深度为m，递归计算右子树的深度为n，</span></span><br><span class="line"><span class="string">                则二叉树的深度为m与n较大者再加1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>    </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">max</span>(self.maxDepth(root.left), self.maxDepth(root.right))</span><br></pre></td></tr></table></figure><p><strong>c++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxDepth</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">depth</span>(root);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">depth</span><span class="params">(TreeNode* root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root== <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="built_in">depth</span>(root-&gt;left);</span><br><span class="line">        <span class="keyword">int</span> right = <span class="built_in">depth</span>(root-&gt;right);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(left, right)+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="4-计算二叉树节点总数"><a href="#4-计算二叉树节点总数" class="headerlink" title="4. 计算二叉树节点总数"></a>4. 计算二叉树节点总数</h4><ol><li>如果空树，节点为0</li><li>否则，节点个数为左子树的节点个数+右子树节点个数+1</li></ol><p>题目：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="comment"># *   1. 如果空树，节点为0</span></span><br><span class="line"><span class="comment"># *   2. 否则，节点个数为左子树的节点个数+右子树节点个数+1</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">countNodes</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> self.countNodes(root.left) + self.countNodes(root.right) + <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>c++</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">countNodes</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (root)&#123;</span><br><span class="line">            count ++;</span><br><span class="line">            <span class="built_in">countNodes</span>(root-&gt;left);</span><br><span class="line">            <span class="built_in">countNodes</span>(root-&gt;right);           </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="3-线索二叉树"><a href="#3-线索二叉树" class="headerlink" title="3. 线索二叉树"></a>3. 线索二叉树</h3><h4 id="1-为什么研究线索二叉树？"><a href="#1-为什么研究线索二叉树？" class="headerlink" title="1. 为什么研究线索二叉树？"></a>1. 为什么研究线索二叉树？</h4><p>当二叉链表作为二叉树的存储结构时，可以很方便地找到某个节点的左右孩子；但是一般情况下，无法直接找到该节点在某种遍历序列中的<strong>前驱和后继节点。</strong></p><p><strong>Q:那么如何寻找特定遍历序列中的二叉树节点的前驱和后继？</strong></p><p><strong>A:</strong></p><ol><li>通过遍历寻找——费时间</li><li>再增设前驱、后继指针与——增加了存储负担</li><li>利用二叉链表中的<strong>空的指针域</strong></li></ol><p>利用<strong>二叉链表中的空的指针域的数量: </strong></p><p>具有n个节点的二叉链表中，一共有2n个指针域；</p><p>因为n个节点中有n-1个孩子，即2n个指针域中有n-1个用来指示节点左右孩子，其余n+1个指针域为空。</p><h4 id="2-线索二叉树"><a href="#2-线索二叉树" class="headerlink" title="2. 线索二叉树"></a>2. 线索二叉树</h4><p><strong>利用二叉链表中的空指针域：</strong></p><ul><li><p>​    如果某个节点的<strong>左</strong>孩子为空，则将空的<strong>左孩子指针域</strong>改为指向其<strong>前驱</strong>；</p></li><li><p>​    如果某个节点的<strong>右</strong>孩子为空，则将空的<strong>右孩子指针域</strong>改为指向其<strong>后驱</strong></p></li></ul><p>这种<strong>改变指向的指针</strong>称为“线索”</p><p>加上线索的二叉树称为<strong>线索二叉树 （Threaded Binary Tree）</strong>。</p><p>对二叉树按某种遍历次序使其变为线索二叉树的过程叫线索化。</p><p>但是我们无法区分一个节点到底是指向左右孩子还是前驱后继节点，为此引入<strong>标志位</strong>。</p><p>为了区分<code>lchild</code>和<code>rchild</code>指针到底是指向孩子的指针，还是指向前驱或者后继的指针，我们增加了两个标志与<code>ltag</code>和<code>rtag</code>。并约定：</p><ol><li><code>ltag</code> = 0 : <code>lchild</code>指向该节点的左孩子</li><li><code>ltag</code> = 1 : <code>lchild</code>指向该节点的前驱</li><li><code>rtag</code> = 0 : <code>lchild</code>指向该节点的右孩子</li><li><code>rtag</code> = 1 : <code>lchild</code>指向该节点的后继</li></ol><p>再增加一个头结点：</p><p><code>ltag</code>=0, <code>lchild</code>指向根节点</p><p><code>rtag</code>=1,<code>rchild</code>指向遍历序列中的最后一个节点</p><p>遍历序列中第一个节点的<code>lc</code>与后最后一个节点<code>rc</code>与都指向头结点。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818113.png" alt="image-20210605225445837" style="zoom:25%;" /></p><h4 id="3-线索二叉树分类"><a href="#3-线索二叉树分类" class="headerlink" title="3. 线索二叉树分类"></a>3. 线索二叉树分类</h4><p><strong>先序线索二叉树：ABCDE</strong></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261818114.png" alt="image-20210605223945860" style="zoom:25%;" /></p><p>如上图中，整个线索化过程如下</p><ul><li>A的左右孩子为BD，A的标志位都置为0，并分别指向BD</li><li>B的左孩子为空，标志位<code>ltag=1</code>，指向前驱A，右孩子为C，标志位<code>rtag=0</code>，指向C</li><li>C的左右孩子为空，标志位都置为1，并分别指向前驱B后后继D</li><li>D的孩子为E，左标志位置为0，指向E；右标志位置为1指向后继E</li></ul><p>同样<strong>中序线索二叉树</strong>和 <strong>后续线索二叉树</strong></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二叉树 </tag>
            
            <tag> 二叉树遍历 </tag>
            
            <tag> B线索二叉树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. 二叉树</title>
      <link href="2020/05/15/DS_4.%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>2020/05/15/DS_4.%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="2-二叉树"><a href="#2-二叉树" class="headerlink" title="2. 二叉树"></a>2. 二叉树</h2><h3 id="1-二叉树基本概念"><a href="#1-二叉树基本概念" class="headerlink" title="1. 二叉树基本概念"></a>1. 二叉树基本概念</h3><h4 id="1-节点"><a href="#1-节点" class="headerlink" title="1. 节点"></a>1. 节点</h4><ol><li>节点</li><li>根节点</li><li>父节点</li><li>子节点</li><li>兄弟节点</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821065.png" alt="树结构示意图" style="zoom:25%;" /></p><p>上图中，A 节点就是 B 节点的<strong>父节点</strong>，B 节点是 A 节点的<strong>子节点</strong>。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为<strong>兄弟节点</strong>。我们把没有父节点的节点叫作<strong>根节点</strong>，也就是图中的节点 E。我们把没有子节点的节点叫作<strong>叶子节点</strong>或者<strong>叶节点</strong>，比如图中的 G、H、I、J、K、L 都是叶子节点。</p><p>一棵树没有任何节点，称为空树</p><p>一棵树可以只有一个节点，也就是根节点</p><p>子树， 左子树， 右子树</p><h4 id="2-度"><a href="#2-度" class="headerlink" title="2. 度"></a>2. 度</h4><ul><li><p>节点的度(degree): 子树的个数</p></li><li><p>树的度： 所有节点度中的最大值</p></li><li><p>叶子节点(leaf): 度为0的节点</p></li><li><p>非叶子节点：度不为0的节点</p></li></ul><h4 id="3-层数-和-深度"><a href="#3-层数-和-深度" class="headerlink" title="3. 层数 和 深度"></a>3. 层数 和 深度</h4><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821066.png" alt="高度、深度、层"  style="zoom:25%;" /></p><ul><li><p>层数：根节点在第一层， 根节点的子节点在第二层，以此类推(也有从0层计算)</p></li><li><p>节点的深度：从根节点到当前节点的唯一路径上的节点总数 </p></li><li><p>节点的高度：从当前节点到最远叶子节点的路径上的节点总数(边数)</p></li><li><p>树的深度：所有节点深度中的最大值</p></li><li><p>树的高度：所有节点高度的最大值</p></li><li><p>树的深度等于树的高度</p></li></ul><h4 id="4-有序树、无序树、森林"><a href="#4-有序树、无序树、森林" class="headerlink" title="4. 有序树、无序树、森林"></a>4. 有序树、无序树、森林</h4><ul><li>有序树<ul><li>树种任意节点的子节点之间有顺序关系</li></ul></li><li>无序树<ul><li>树中任意节点的子节点之间没有顺序关系</li><li>也称为“自由树”</li></ul></li><li>森林<ul><li>由$m \ge 0$棵互不相交的树组成的集合</li></ul></li></ul><h4 id="5-二叉树-Binary-Tree"><a href="#5-二叉树-Binary-Tree" class="headerlink" title="5. 二叉树 Binary Tree"></a>5. 二叉树 Binary Tree</h4><ul><li><p>二叉树的特点</p><ul><li><p>每个节点的度最大为2(最多有2棵子树)</p></li><li><p>左子树和右子树是由顺序的</p></li><li><p>即使某节点只有一棵子树，也要分左右子树</p></li><li><p>二叉树是有序树</p></li></ul></li><li><p>二叉树的性质</p><ul><li><p>非空二叉树的第$i$层，最多有$2^{i-1} \quad (i  \ge 1)$个节点</p></li><li><p>在高度为$h$的二叉树上最多有$2^h - 1$个节点(1, 2, 4…的等比数列求和)</p></li><li><p>对于一棵非空二叉树，如果叶子节点个数为$n_0$，度为2的节点个数为$n_2$，那么$n_0 = n_2 + 1$</p><ul><li><p>证明：</p><p>​        假设度为1的节点个数为$n_1$，那么二叉树的节点总数为$n = n_0 + n_1 + n_2$；        </p><p>​        二叉树的边数为：$n_1 + 2 * n_2= \text{除了根节点，每个节点都有一条边} = n - 1= n_0+n_1+n_2$即$n_0 = n_2+1$.</p></li></ul></li></ul></li></ul><h4 id="6-真二叉树-Proper-Binary-Tree"><a href="#6-真二叉树-Proper-Binary-Tree" class="headerlink" title="6. 真二叉树     Proper Binary Tree"></a>6. 真二叉树     Proper Binary Tree</h4><pre class="mermaid">graph TB;1((1))==>2((2))1((1))==>3((3))2((2))==>4((4))2((2))==>5((5))5((5))==>6((6))5((5))==>7((7))</pre><p>所有节点的度要么为0要么为2，才为真二叉树。</p><h4 id="7-满二叉树-Full-Binary-Tree"><a href="#7-满二叉树-Full-Binary-Tree" class="headerlink" title="7. 满二叉树  Full Binary Tree"></a>7. 满二叉树  Full Binary Tree</h4><p>所有节点的度要么为0，要么为2，且所有叶子节点都在最后一层</p><pre class="mermaid">graph TB;1((1))==>2((2))1((1))==>3((2))2((2))==>4((4))2((2))==>5((5))3((3))==>6((6))3((3))==>7((7))</pre><ul><li>在同样高度的二叉树中，满二叉树的叶子节点数量最多、总结点数量最多</li><li>满二叉树一定是真二叉树，真二叉树不一定是满二叉树</li><li>假设满二叉树的高度为$h(h \ge 1)$，那么：<ul><li>第$i$层的节点数量为：$2^{i-1}$</li><li>叶子节点数量: $2^{h-1}$</li><li>总结点数量：$2^0+2^1 + \cdots + 2^{h-1}=2^{h}-1$</li></ul></li></ul><h4 id="8-完全二叉树-Complete-Binary-Tree"><a href="#8-完全二叉树-Complete-Binary-Tree" class="headerlink" title="8. 完全二叉树  Complete Binary Tree"></a>8. 完全二叉树  Complete Binary Tree</h4><p>叶子节点只会出现最后2层，且最后一层的叶子节点都靠左对齐</p><pre class="mermaid">graph TB;1((A))==>2((B))1((A))==>3((C))2((B))==>4((D))2((B))==>5((E))3((C))==>6((F))3((C))==>7((G))4((D))==>8((H))4((D))==>9((I))5((E))==>10((J))</pre><p>性质：</p><ol><li>度为1的节点只有左子树</li><li>度为1的节点要么是1个要么是0个</li><li>同样节点数量的二叉树，完全二叉树的高度最小</li><li><p>假设完全二叉树的高度为$h(h \ge 1)$，那么：</p><ul><li>至少有$2^{h-1}$个节点  $2^0+2^1+\cdots+2^{h-1}+1$</li><li>最多有$2^h-1$个节点  $2^0+2^1+ \cdots + 2^{h-1}$满二叉树  </li><li>总结点数量为$n$。其中$2^{h-1} \le n \le 2^{h}$，即$h-1 \le log_2n \le h$，<strong>因为$h$是整数，所以$h=log_2 n向下取整+1=floor(log_2 n) + 1$</strong></li></ul></li><li><p>一棵有$n$个节点的完全二叉树 $n &gt; 0$ ，从上到下，从左到右对节点从1开始进行编号，对任意第$i$个节点：<br><pre class="mermaid">  graph TB;1((1))==>2((2))1((1))==>3((3))2((2))==>4((4))2((2))==>5((5))3((3))==>6((6))3((3))==>7((7))4((4))==>8((8))4((4))==>9((9))5((5))==>10((10))</pre></p><ul><li>如果$i=1$，它是根节点</li><li>如果$i&gt;1$,它的父节点编号为$floor(1/2)$</li><li>如果$2i \le n$,它的左子节点编号为$2i$</li><li>如果$2i &gt; n$，它无左子节点</li><li>如果$2i+1 \le n$,它的右子节点编号为$2i+1$</li><li>如果$2i+1 &gt; n$,它无右子节点</li></ul></li><li><p>一棵有$n$个节点的完全二叉树 $n &gt; 0$ ，从上到下，从左到右对节点从0开始进行编号，对任意第$i$个节点：<br><pre class="mermaid">  graph TB;1((0))==>2((1))1((0))==>3((2))2((1))==>4((3))2((1))==>5((4))3((2))==>6((5))3((2))==>7((6))4((3))==>8((7))4((3))==>9((8))5((4))==>10((9))</pre></p><ul><li>如果$i=0$，它是根节点</li><li>如果$i&gt;0$,它的父节点编号为$floor((i-1)/2)$</li><li>如果$2i  + 1\le n-1$,它的左子节点编号为$2i+1$</li><li>如果$2i +1&gt; n-1$，它无左子节点</li><li>如果<script type="math/tex">2i+2 \le n -1</script>,它的右子节点编号为<script type="math/tex">2i+2</script></li><li>如果<script type="math/tex">2i+2 > n</script>,它无右子节点</li></ul></li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821067.png" alt="满二叉树和完全二叉树"  style="zoom:25%;" /></p><p>上图中，编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作<strong>满二叉树</strong>。</p><p>编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作<strong>完全二叉树</strong>。</p><p>满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821068.png" alt="完全二叉树?"  style="zoom:25%;" /></p><p><strong>题目：</strong>一棵完全二叉树有768个节点，求叶子节点数?</p><ul><li>假设叶子节点的个数为<script type="math/tex">n_0</script>, 度为1的节点个数为<script type="math/tex">n_1</script>， 度为2的节点个数为<script type="math/tex">n_2</script><ul><li>​    那么总结点个数为<script type="math/tex">n = n_0+n_1+n_2</script>,而<script type="math/tex">n_0=n_2+1</script>。所以<script type="math/tex">n = 2n_0+n_1-1</script>。</li></ul></li><li>完全二叉树的<script type="math/tex">n_1</script>，要么为0，要么为1；<ul><li>当<script type="math/tex">n_1</script>为1时， <script type="math/tex">n=2n_0</script>, <script type="math/tex">n</script>必然是偶数。此时叶子节点个数为<script type="math/tex">n_0=n/2</script>， 非叶子节点个数为<script type="math/tex">n_1 + n_2=n/2</script>。</li><li>当<script type="math/tex">n_1</script>为0时， <script type="math/tex">n=2n_0-1</script>, <script type="math/tex">n</script>必然是奇数。此时叶子节点个数为<script type="math/tex">n_0=(n+1)/2</script>， 非叶子节点个数为<script type="math/tex">n_1 + n_2=(n-1)/2</script>。</li></ul></li></ul><p><strong>总结：</strong></p><ul><li>叶子节点个数<script type="math/tex">n_0 = floor((n+1)/2)=ceiling(n/2)</script></li><li>非叶子节点个数<script type="math/tex">n_1+n_2 = floor(n/2)=ceiling((n-1)/2)</script> <strong>floor向下取整， ceiling向上取整</strong></li></ul><p>因此上题中叶子节点的个数为384</p><h3 id="2-二叉搜索树"><a href="#2-二叉搜索树" class="headerlink" title="2. 二叉搜索树"></a>2. 二叉搜索树</h3><h4 id="1-二叉搜索树-Binary-Search-Tree"><a href="#1-二叉搜索树-Binary-Search-Tree" class="headerlink" title="1. 二叉搜索树  Binary Search Tree"></a>1. 二叉搜索树  Binary Search Tree</h4><p>二叉搜索树是二叉树的一种，是应用非常广泛的一种二叉树，简称<strong>BST</strong></p><ol><li>又称作：二叉查找树、二叉排序树树</li><li>任意一个节点的值都大于其左子树所有节点</li><li>任意一个节点的值都小于其右子树所有节点</li><li>它的左右子树也是一棵二叉搜索树</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821069.png" alt="二叉搜索树"  style="zoom:25%;" /></p><p><a href="http://btv.melezinek.cz/binary-search-tree.html">可视化网址</a></p><p>二叉搜索树可以提高搜索效率，但其存储元素必须是可比较对象。不允许其元素为None</p><h4 id="2-二叉树设计"><a href="#2-二叉树设计" class="headerlink" title="2. 二叉树设计"></a>2. 二叉树设计</h4><ol><li>添加  遇到值相等，覆盖或者直接<code>return</code><ul><li>找到父节点<code>parent</code></li><li>创建新节点<code>node</code></li><li><code>parent.left = node</code> 或  <code>parent.right= node</code></li></ul></li></ol><h4 id="3-二叉树的遍历"><a href="#3-二叉树的遍历" class="headerlink" title="3. 二叉树的遍历"></a>3. 二叉树的遍历</h4><h5 id="1-前序遍历-Preorder-Traversal"><a href="#1-前序遍历-Preorder-Traversal" class="headerlink" title="1.  前序遍历 (Preorder Traversal)"></a>1.  前序遍历 (Preorder Traversal)</h5><p><pre class="mermaid">graph TB;1((7))==>2((4))1((7))==>3((9))2((4))==>4((2))2((4))==>5((5))3((9))==>6((8))3((9))==>7((11))4((2))==>8((1))4((2))==>9((3))7((11))==>10((10))7((11))==>11((12))</pre><br>访问顺序：</p><ol><li>根节点</li><li>前序遍历左子树</li><li>前序遍历右子树</li></ol><p>上面树的访问顺序为：<script type="math/tex">7 \to \underbrace{ (4\to2\to1\to3\to 5)}_{左子树}\to \underbrace{ 9\to8\to11\to10\to12}_{右子树}</script></p><h5 id="2-中序遍历-Inorder-Traversal"><a href="#2-中序遍历-Inorder-Traversal" class="headerlink" title="2. 中序遍历(Inorder Traversal)"></a>2. 中序遍历(Inorder Traversal)</h5><p>访问顺序：(根节点必须放中间，可以先右后左)</p><ol><li>中序遍历左子树</li><li>中序遍历根节点</li><li>中序遍历右子树</li></ol><p>上面树的访问顺序为：<script type="math/tex">1 \to 2\to 3\to 4\to 5\to 7\to 8\to 9\to 10\to 11\to12</script>。对于二叉搜索树，从小到大顺序排列，升序。如果先右后左，就变为降序。</p><h5 id="3-后序遍历-Postorder-Traversal"><a href="#3-后序遍历-Postorder-Traversal" class="headerlink" title="3. 后序遍历(Postorder Traversal)"></a>3. 后序遍历(Postorder Traversal)</h5><p>访问顺序：(根节点放最后，左右子树可以互换)</p><ol><li>后序遍历左子树</li><li>后序遍历右子树</li><li>后序遍历根节点</li></ol><p>上面树的访问顺序为：<script type="math/tex">1 \to 3\to 2\to 5\to 4\to 8\to 10\to 12\to 11\to 9\to7</script>。</p><h5 id="4-层序遍历-Level-Order-Traversal"><a href="#4-层序遍历-Level-Order-Traversal" class="headerlink" title="4. 层序遍历(Level Order Traversal)"></a>4. 层序遍历(Level Order Traversal)</h5><p>访问顺序：<br>            从上到下从左到右。<br>            上面树的访问顺序为：<script type="math/tex">7 \to 4\to 9\to 2\to 5\to 8\to 11\to 1\to 3\to 10\to12</script>。</p><h5 id="5-遍历的作用"><a href="#5-遍历的作用" class="headerlink" title="5. 遍历的作用"></a>5. 遍历的作用</h5><ol><li>前序遍历：树状结构的展示</li><li>中序遍历：二叉搜索树的中序遍历按升序或降序处理节点</li><li>后续遍历：适用于一些先子后父的操作</li><li>层序遍历:  计算二叉树高度，判断二叉树为完全二叉树</li></ol><h4 id="4-二叉搜索树的复杂度分析"><a href="#4-二叉搜索树的复杂度分析" class="headerlink" title="4. 二叉搜索树的复杂度分析"></a>4. 二叉搜索树的复杂度分析</h4><p>如果按照7、4、9、2、5、8、11的顺序添加节点，形成满二叉树如下图</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821070.png" alt="image-20201209205738371" style="zoom: 25%;" /></p><p>那么搜索，删除，添加复杂度为$O(h)=O(logn)$等于树的高度。</p><p>如果按照2、5、4、7、8、9、11添加节点，形成链表：$O(h)=O(n)$。</p><h3 id="3-平衡二叉树"><a href="#3-平衡二叉树" class="headerlink" title="3. 平衡二叉树"></a>3. 平衡二叉树</h3><ol><li>平衡<br>当节点数量固定是，左右子树的高度越接近，这棵树就越平衡。</li><li>理想平衡<br>最理想的平衡就是像完全二叉树、满二叉树那样，高度是最小的</li></ol><h4 id="1-平衡二叉搜索树-Balanced-Binary-Search-Tree"><a href="#1-平衡二叉搜索树-Balanced-Binary-Search-Tree" class="headerlink" title="1. 平衡二叉搜索树(Balanced Binary Search Tree)"></a>1. 平衡二叉搜索树(Balanced Binary Search Tree)</h4><ol><li><p>经典常见的平衡二叉树搜索树有:</p><ul><li><p>AVL， Windows NT内核中广泛应用</p></li><li><p>红黑树：</p><ul><li><p>C++ STL中map set</p></li><li><p>Java中TreeMap、TreeSet、HashMap、HashSet</p></li><li>Linux的进程调度</li></ul></li></ul></li></ol><h4 id="2-AVL树"><a href="#2-AVL树" class="headerlink" title="2. AVL树"></a>2. AVL树</h4><ol><li>平衡因子(Balance Factor)：某节点的左右子树的高度差</li></ol><ol><li>AVL树的特点:<ul><li>每个节点的平衡因子只能是1、0、-1(故其绝对值$\le1$，如果超过1，称之为“失衡”)</li><li>每个节点的左右子树高度差不超过1</li><li>添加、搜索、删除时间复杂度是<code>O(logn)</code></li></ul></li><li>最小不平衡树:<ul><li>距离插入节点最近的，且平衡因子的绝对值大于1的节点为根的子树，我们称为最小不平衡树。</li></ul></li></ol><h4 id="3-左单旋-Left-Rotation"><a href="#3-左单旋-Left-Rotation" class="headerlink" title="3. 左单旋 Left Rotation"></a>3. 左单旋 Left Rotation</h4><p>平衡二叉树构建的基本思想就是在构建二叉排序树的过程中，每当插入一个结点时，先检查是否因插入而破坏了树的平衡性，若是，则找出最小不平衡子树。在保持二叉排序树特性的前提下，调整最小不平衡子树中各结点之间的链接关系，进行相应的旋转，使之成为新的平衡子树。</p><ol><li><strong>左旋</strong>：将右子树的左子树链接到父节点的右子树节点， 父节点作为新根节点的左子树节点。如下三图所示，</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821071.png" alt="image-20210201122958880"  style="zoom: 25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821072.png" alt="image-20210201123030650"  style="zoom: 25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821073.png" alt="image-20210201123100308"  style="zoom: 25%;" /></p><h4 id="4-右单旋-Right-Rotation"><a href="#4-右单旋-Right-Rotation" class="headerlink" title="4. 右单旋 Right Rotation"></a>4. 右单旋 Right Rotation</h4><ol><li><strong>右旋</strong>：右单旋是左单旋的镜像旋转.<br>将左子树的右子树链接到父节点的左子树节点，父节点作为新根节点的右子树节点。如下三图所示，</li></ol><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821074.png" alt="image-20210201123326326"  style="zoom: 25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821075.png" alt="image-20210201123237836"  style="zoom: 25%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261821076.png" alt="image-20210201123403582"  style="zoom: 25%;" /></p><p>左旋——自己变为右孩子的左孩子；右旋——自己变为左孩子的右孩子</p><p>[1] <a href="https://baozoulin.gitbook.io/-data-structure/di-8-zhang-cha-zhao/86ping-heng-er-cha-shu-ff08-avl-shu-ff09">大话数据结构笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> tree </tag>
            
            <tag> BST </tag>
            
            <tag> AVL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter 06 Probability and Distributions</title>
      <link href="2020/04/04/M4ML_Chapter06-Probability-and-Distributions/"/>
      <url>2020/04/04/M4ML_Chapter06-Probability-and-Distributions/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-06-Probability-and-Distributions"><a href="#Chapter-06-Probability-and-Distributions" class="headerlink" title="Chapter 06: Probability and Distributions"></a>Chapter 06: Probability and Distributions</h2><ul><li><p>中英名词对照：</p><p>PMF：Probability Mass Function(概率质量函数)</p><p>PDF： Probability Density Function(概率密度函数)</p><p>CDF：Cumulative Distribution Function (累积分布函数)</p><p>i.i.d：Independent and identically distributed</p><p>median: 中位数</p><p>mode： 众数</p></li></ul><h3 id="1-Sum-Rule-Product-Rule-and-Bayes’-Theorem"><a href="#1-Sum-Rule-Product-Rule-and-Bayes’-Theorem" class="headerlink" title="1. Sum Rule, Product Rule, and Bayes’ Theorem"></a>1. Sum Rule, Product Rule, and Bayes’ Theorem</h3><ul><li><p><strong>sum rule</strong>:</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\left\{\begin{array}{ll}\sum_{\boldsymbol{y} \in \mathcal{Y}} p(\boldsymbol{x}, \boldsymbol{y}) & \text { if } \boldsymbol{y} \text { is discrete } \\\int_{\mathcal{Y}} p(\boldsymbol{x}, \boldsymbol{y}) \mathrm{d} \boldsymbol{y} & \text { if } \boldsymbol{y} \text { is continuous }\end{array}\right. \tag{1}</script></li><li><p><strong>product rule</strong></p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y}) = P \left( \boldsymbol{y} \left|\boldsymbol{x}\right. \right) p(\boldsymbol{x}) \tag{2}</script></li></ul><ul><li><strong>Bayes’ theorem:</strong></li></ul><script type="math/tex; mode=display">\underbrace{p(\boldsymbol{x} \mid \boldsymbol{y})}_{\text {posterior }}=\frac{\overbrace{p(\boldsymbol{y} \mid \boldsymbol{x})}^{\text {likelihood }} \overbrace{p(\boldsymbol{x})}^{\text {prior }}}{\underbrace{p(\boldsymbol{y})}_{\text {evidence }}} \tag{3}</script><p><strong>后验概率正比于似然 $\times$ 先验概率</strong></p><p><strong>likelihood 也可称作measurement model</strong></p><p>  假设我们有一些关于隐变量$\boldsymbol{x}$先验$p(\boldsymbol{x})$，以及关于$\boldsymbol{x}$和第二个变量$\boldsymbol{y}$之间的关系$p \left( \boldsymbol{y} \left|\boldsymbol{x}\right. \right)$。如果我们观察$\boldsymbol{y}$,我们可以使用Bayes’理论来得到公式3.</p><p>  $p(\boldsymbol{x})$，封装了我们在观察任何数据之前对隐变量$\boldsymbol{x}$主观先验知识。我们可以选择任何对我有意义的先验知识，但至关重要的是确保先验在所有可能的$\boldsymbol{x}$有非零的pdf,即使其非常罕见。</p><p>  似然${p(\boldsymbol{y} \mid \boldsymbol{x})}$描述$\boldsymbol{x}$和$\boldsymbol{y}$直接的关系，并且在离散概率分布情况下，如果我们知道隐变量$\boldsymbol{x}$,它是数据$\boldsymbol{y}$的概率。注意，似然不是$\boldsymbol{x}$的分布，而是$\boldsymbol{y}$。<strong>我们称${p(\boldsymbol{y} \mid \boldsymbol{x})}$为给定$\boldsymbol{y}$的$\boldsymbol{x}$的似然，或给定$\boldsymbol{x}$的$\boldsymbol{y}$的概率， 但绝对不能称$\boldsymbol{y}$的似然</strong>。</p><p>后验${p(\boldsymbol{x} \mid \boldsymbol{y})}$得益于大量Bayes统计学，因为它准确表达了我们在观察$\boldsymbol{y}$后了解$\boldsymbol{x}$的兴趣。</p><blockquote><p>根据贝叶斯定理公式3，一个随机变量在给定另一随机变量值之后的后验概率分布可以通过先验概率分布与似然函数相乘并除以归一化常数求得<br>: </p><script type="math/tex; mode=display">f_{X\mid Y=y}(x)={f_X(x) L_{X\mid Y=y}(x) \over {\int_{-\infty}^\infty f_X(u) L_{X\mid Y=y}(u)\,du}} \tag{4}</script><p>上式为给出了随机变量$X$在给定数据$Y=y$后的后验概率分布函数，式中</p><ul><li><script type="math/tex">f_X(x)></script>  为  <script type="math/tex">X</script>  的先验密度函数，</li><li><script type="math/tex">L_{X\mid Y=y}(x) = f_{Y\mid X=x}(y)</script>   为   <script type="math/tex">x</script>   的似然函数，</li><li><script type="math/tex">\int_{-\infty}^\infty f_X(u) L_{X\mid Y=y}(u)du</script>   为归一化常数，</li><li><script type="math/tex">f_{X\mid Y=y}(x)</script> 为考虑了数据 <script type="math/tex">Y=y</script> 后 <script type="math/tex">X</script> 的后验密度函数。</li></ul></blockquote><p><a href="https://en.wikipedia.org/wiki/Posterior_probability">注:引用自维基/后验概率</a></p><h4 id="1-Likelihood-function"><a href="#1-Likelihood-function" class="headerlink" title="1. Likelihood function"></a>1. Likelihood function</h4><blockquote><p>In statistics, the likelihood function often simply called the likelihood <strong>measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters</strong>. It is formed from the joint probability distribution of the sample, but viewed and used as a function of the parameters only, thus treating the random variables as fixed at the observed values.</p><p>在数理统计学中，<strong>似然函数</strong>是一种关于统计模型中的<strong>参数</strong>的<strong>函数</strong>，表示模型参数中的<strong>似然性</strong>。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“概率”（或然性）又有明确的区分：<u>概率，用于在已知一些参数的情況下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值</u>。</p><p>在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数$B$时，事件$A$会发生的概率写作：</p><script type="math/tex; mode=display">P(A \mid B) = \frac{P(A , B)}{P(B)}</script><p>利用贝叶斯定理，</p><script type="math/tex; mode=display">P(B \mid A) = \frac{P(A \mid B)\;P(B)}{P(A)}</script><p>因此，我们可以反过来构造表示似然性的方法：已知有事件$A$发生，运用似然函数$\mathbb{L}(B \mid A)$，我们估计参数$B$的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：</p><script type="math/tex; mode=display">b\mapsto P(A \mid B=b)</script><p>注意到这里并不要求似然函数满足归一性：$\sum_{b \in \mathcal{B}}P(A \mid B=b) = 1$。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有$\alpha &gt; 0$，都可以有似然函数:</p><script type="math/tex; mode=display">L(b \mid A) = \alpha \; P(A \mid B=b)</script></blockquote><p><a href="[https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0](https://zh.wikipedia.org/wiki/似然函数">注:引用自维基/似然函数</a>)</p><h4 id="2-边缘似然-证据"><a href="#2-边缘似然-证据" class="headerlink" title="2.  边缘似然/证据"></a>2.  边缘似然/证据</h4><ul><li>the marginal likehood/evidence：</li></ul><script type="math/tex; mode=display">p(\boldsymbol{y}):=\int p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\mathbb{E}_{X}[p(\boldsymbol{y} \mid \boldsymbol{x})] \tag{5}</script><p>定义上，边缘似然是对应隐变量$x$数值积分。</p><blockquote><p>在统计学中， 边缘似然函数（marginal likelihood function），或积分似然（integrated likelihood），是一个某些参数变量边缘化的似然函数（likelihood function） 。在贝叶斯统计范畴，它也可以被称作为 证据 或者 模型证据 的。</p></blockquote><p><a href="https://zh.wikipedia.org/wiki/边缘似然">引用:维基/边缘似然</a></p><h3 id="2-均值和协方差"><a href="#2-均值和协方差" class="headerlink" title="2. 均值和协方差"></a>2. 均值和协方差</h3><ul><li>Means and Covariances</li></ul><p>均值和(协)方差经常用来描述概率分布的性质(期望值和范围)。</p><p>期望值的概念是机器学习的核心，并且概率本身的基础概念也源自于它。</p><p>​                                                                                                                        ——(Whittle, 2000).</p><h4 id="1-期望值"><a href="#1-期望值" class="headerlink" title="1. 期望值"></a>1. 期望值</h4><ul><li>Expected Value</li></ul><p>期望值函数$g: \mathbb{R} \rightarrow \mathbb{R}$单一连续随机变量$X \sim p(x)$给定如下：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(x)]=\int_{\mathcal{X}} g(x) p(x) \mathrm{d} x \tag{6}</script><p>对应离散随机变量$X \sim p(x)$:</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(x)]=\sum_{x \in \mathcal{X}} g(x) p(x) \tag{7}</script><p>$\mathcal{X}$：随机变量$X$可能结果的集合 target space</p><p><strong>注意：</strong></p><p>我们把多维随机变量$X$看作有限的单一随机变量的向量$[X_1, \cdots, X_D]^{T}$。对于多维随机变量,定义基于元素的期望值：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(\boldsymbol{x})]=\left[\begin{array}{c}\mathbb{E}_{X_{1}}\left[g\left(x_{1}\right)\right] \\\vdots \\\mathbb{E}_{X_{D}}\left[g\left(x_{D}\right)\right]\end{array}\right] \in \mathbb{R}^{D} \tag{8}</script><p>其中：  <script type="math/tex">{\mathbb{E}_{X}}_{d}</script>   表示对应向量  <script type="math/tex">x</script>  的第   <script type="math/tex">d</script>  个元素的期望值。</p><h4 id="2-均值"><a href="#2-均值" class="headerlink" title="2. 均值"></a>2. 均值</h4><p>对于状态量<script type="math/tex">x \in \mathbb{R}^{D}</script> 随机变量 <script type="math/tex">X</script> 的均值是一个平均值，其定义如下：</p><script type="math/tex; mode=display">\begin{align}&\mathbb{E}_{X}[\boldsymbol{x}]=\left[\begin{array}{c}\mathbb{E}_{X_{1}}\left[x_{1}\right] \\\vdots \\\mathbb{E}_{X_{D}}\left[x_{D}\right]\end{array}\right] \in \mathbb{R}^{D}, \\ &\text{where}\\\\ &\mathbb{E}_{x_{d}}\left[x_{d}\right]:=\left\{\begin{array}{ll}\int_{\mathcal{X}} x_{d} p\left(x_{d}\right) \mathrm{d} x_{d} & \text { if } X \text { is a continuous random variable } \\ \sum_{x_{i} \in \mathcal{X}} x_{i} p\left(x_{d}=x_{i}\right) & \text { if } X \text { is a discrete random variable }\end{array}\right. \tag{9}\end{align}</script><p>其中，$d = 1, \cdots, D$;$d$表示对应$x$的维数。遍历随机变量$x$目标空间的状态量$\mathcal{X}$积分或求和。</p><h4 id="3-协方差-Covariance-定义及推导"><a href="#3-协方差-Covariance-定义及推导" class="headerlink" title="3. 协方差(Covariance)定义及推导"></a>3. 协方差(Covariance)定义及推导</h4><p>两个多维随机变量$X, Y \in \mathbb{R}$之间的协方差，给定为它们与均值的偏差的积的期望：</p><script type="math/tex; mode=display">\operatorname{Cov}_{X, Y}[x, y]:=\mathbb{E}_{X, Y}\left[\left(x-\mathbb{E}_{X}[x]\right)\left(y-\mathbb{E}_{Y}[y]\right)\right] \tag{10}\</script><p>因为期望计算是线性的，对于一个实值函数$f(\boldsymbol{x}) = ag(\boldsymbol{x}) + bh(\boldsymbol{x})$， 当$a, b \in \mathbb{R} 且 \boldsymbol{x} \in \mathbb{R}^{D}$我们可以得到：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{X}[f(\boldsymbol{x})] &=\int f(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=\int[a g(\boldsymbol{x})+b h(\boldsymbol{x})] p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=a \int g(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} x+b \int h(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=a \mathbb{E}_{X}[g(\boldsymbol{x})]+b \mathbb{E}_{X}[h(\boldsymbol{x})]\end{aligned} \tag{11}</script><p>所以，公式10可以有如下推导：</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}_{X, Y}[x, y]&=\mathbb{E}_{X, Y}\left[\left(x-\mathbb{E}_{X}[x]\right)\left(y-\mathbb{E}_{Y}[y]\right)\right]\\ &=\mathbb{E}_{X, Y}\left[\left(xy - y\mathbb{E}_{X}[x] - x\mathbb{E}_{Y}[y] +  \mathbb{E}_{X}[x] \mathbb{E}_{Y}[y]\right)\right]\\ &= \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y] -\mathbb{E}[x]\mathbb{E}[y] + \mathbb{E}[x]\mathbb{E}[y]\\ &= \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y] \end{align}</script><p>即：</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}_{X, Y}[x, y] = \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y]  \tag{12}\end{align}</script><h4 id="4-多元随机变量的方差"><a href="#4-多元随机变量的方差" class="headerlink" title="4. 多元随机变量的方差"></a>4. 多元随机变量的方差</h4><ul><li>Covariance  (Multivariate)</li></ul><p>两元随机变量$X$和$Y$及其对应状态$\boldsymbol{x} \in \mathbb{R}^{D} 和 \boldsymbol{y} \in \mathbb{R}^{D}$，$X和 Y$之间的协方差为：</p><script type="math/tex; mode=display">\operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}] = \mathbb{E}[\boldsymbol{x}\boldsymbol{y}^{T}] - \mathbb{E}[\boldsymbol{x}]\mathbb{E}[\boldsymbol{y}]^{T} = \operatorname{Cov}[\boldsymbol{y}, \boldsymbol{x}]^{T} \in \mathbb{R}^{D \times E} \tag{13}</script><h4 id="5-方差"><a href="#5-方差" class="headerlink" title="5. 方差"></a>5. 方差</h4><ul><li>Variance</li></ul><p>带有状态$\boldsymbol{x} \in \mathbb{R}^{D}$和均值向量$\boldsymbol{\mu} \in \mathbb{R}^{D}$的随机变量$X$方差定义为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{V}_{X}[\boldsymbol{x}] &=\operatorname{Cov}_{X}[\boldsymbol{x}, \boldsymbol{x}] \\&=\mathbb{E}_{X}\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\right]=\mathbb{E}_{X}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right]-\mathbb{E}_{X}[\boldsymbol{x}] \mathbb{E}_{X}[\boldsymbol{x}]^{\top} \\&=\left[\begin{array}{cccc}\operatorname{Cov}\left[x_{1}, x_{1}\right] & \operatorname{Cov}\left[x_{1}, x_{2}\right] & \ldots & \operatorname{Cov}\left[x_{1}, x_{D}\right] \\\operatorname{Cov}\left[x_{2}, x_{1}\right] & \operatorname{Cov}\left[x_{2}, x_{2}\right] & \ldots & \operatorname{Cov}\left[x_{2}, x_{D}\right] \\\vdots & \vdots & \ddots & \vdots \\\operatorname{Cov}\left[x_{D}, x_{1}\right] & \ldots & \ldots & \operatorname{Cov}\left[x_{D}, x_{D}\right]\end{array}\right]\end{aligned} \tag{14}</script><p>$D \times D$的矩阵被称作多元随机变量的协方差矩阵 covariance matrix 。协方差矩阵是对称和半正定的，并且告诉我们数据范围的信息。在对角线上，协方差矩阵含有边缘方差：</p><script type="math/tex; mode=display">p\left(x_{i}\right)=\int p\left(x_{1}, \ldots, x_{D}\right) \mathrm{d} x \backslash_{i}p\left(x_{i}\right)=\int p\left(x_{1}, \ldots, x_{D}\right) \mathrm{d} x \backslash_{i} \tag{15}</script><p>$\backslash_{i} $表示”除了$i$之外的所有变量”。非对角线元素是互协方差项$\operatorname {Cov}[x_i, y_i]$,$i, j = 1, \cdots, D, i \ne j$。</p><h4 id="6-相关"><a href="#6-相关" class="headerlink" title="6. 相关"></a>6. 相关</h4><ul><li>Correlation</li></ul><p>两个随机变量$X,Y$之间的相关性定义为：</p><script type="math/tex; mode=display">\operatorname{corr}[x, y]=\frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x] \mathbb{V}[y]}} \in[-1,1] \tag{16}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/02/RTL5JkI9d1GzbQs.png?=raw" width=90% height=90%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">正负相关示意图 引用自https://mml-book.com</div> </center><p>上图中表明，正相关$corr[x, y]$意味着当$x$变大，$y$也随之变大。负相关$corr[x, y]$意味着当$x$变大，$y$也随之变小。</p><h3 id="3-经验均值和协方差"><a href="#3-经验均值和协方差" class="headerlink" title="3. 经验均值和协方差"></a>3. 经验均值和协方差</h3><ul><li>Empirical Means and Covariances</li></ul><p>在机器学习中，我们需要从数据中学习经验的观察结果。</p><p>我们用有限的数据集 size N 来构建经验统计，它是一个同一随机变量有限数值的函数。</p><p>我们观察数据，就是我们看每一个随机变量$x_1, x_2, \cdots, x_N$之间的联系和应用经验统计。</p><p>具体来说,对于均值，给定一个特殊的数据集我们获得均值的估计，这称作经验均值或者样本均值sample mean .经验协方差也一样适用。</p><h4 id="1-经验均值和协方差"><a href="#1-经验均值和协方差" class="headerlink" title="1. 经验均值和协方差"></a>1. 经验均值和协方差</h4><p>经验均值向量是每一个观察变量的算术平均,其定义如下:</p><script type="math/tex; mode=display">\begin{align}\overline{\boldsymbol{x}}:=\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}_{n} \tag{17} \\ \\ \text{where } {\boldsymbol{x}_{n} \in \mathbb{R}^{D}}\end{align}</script><p>跟经验均值一样,经验协方差矩阵$D \times D$定义为：</p><script type="math/tex; mode=display">\Sigma:=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\bar{x}\right){\left(x_{n}-\bar{x}\right)}\top \tag{18}</script><p>为了计算特定数据集的统计信息，我们用观察量 <script type="math/tex">\boldsymbol{x}_1, \cdots, \boldsymbol{x}_{N}</script> ，biased estimate 代入公式17.18来计算。经验协方差矩阵是对称半正定的。</p><h4 id="2-方差的3种表达式"><a href="#2-方差的3种表达式" class="headerlink" title="2. 方差的3种表达式"></a>2. 方差的3种表达式</h4><h5 id="1-标准方差定义"><a href="#1-标准方差定义" class="headerlink" title="1. 标准方差定义"></a>1. 标准方差定义</h5><p>对应协方差定义，它是随机变量$X$与其期望值$\mu$偏差平方的期望：</p><script type="math/tex; mode=display">\mathbb{V}_{X}[x]:=\mathbb{E}_{X}\left[(x-\mu)^{2}\right] \tag{19}</script><h5 id="2-方差的原始分数公式"><a href="#2-方差的原始分数公式" class="headerlink" title="2. 方差的原始分数公式"></a>2. 方差的原始分数公式</h5><script type="math/tex; mode=display">\mathbb{V}_{X}[x]=\mathbb{E}_{X}\left[x^{2}\right]-\left(\mathbb{E}_{X}[x]\right)^{2} \tag{20}</script><p>记作：“平方的均值减去均值的平方”。</p><h5 id="3-The-sum-of-N-2-pairwise-differences-is-the-empirical-variance-of-the-observations"><a href="#3-The-sum-of-N-2-pairwise-differences-is-the-empirical-variance-of-the-observations" class="headerlink" title="3. The sum of $N^{2}$ pairwise differences is the empirical variance of the observations"></a>3. The sum of $N^{2}$ pairwise differences is the empirical variance of the observations</h5><script type="math/tex; mode=display">\frac{1}{N^{2}} \sum_{i, j=1}^{N}\left(x_{i}-x_{j}\right)^{2}=2\left[\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-\left(\frac{1}{N} \sum_{i=1}^{N} x_{i}\right)^{2}\right] \tag{21}</script><p>当它用到随机变量的仿射变换时，均值和(协)方差表现一些有用的特性。考虑均值为$\boldsymbol{\mu}$、协方差矩阵$\boldsymbol{\Sigma}$随机变量$X$和关于一个$\boldsymbol{x}$(决定论的)仿射变换$\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}$。随机变量$\boldsymbol{y}$，其均值向量和协方差矩阵定义为:</p><script type="math/tex; mode=display">\begin{array}{l}\mathbb{E}_{Y}[\boldsymbol{y}]=\mathbb{E}_{X}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]=\boldsymbol{A} \mathbb{E}_{X}[\boldsymbol{x}]+\boldsymbol{b}=\boldsymbol{A} \boldsymbol{\mu}+\boldsymbol{b}  \\  \mathbb{V}_{Y}[\boldsymbol{y}]=\mathbb{V}_{X}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]=\mathbb{V}_{X}[\boldsymbol{A} \boldsymbol{x}]=\boldsymbol{A} \mathbb{V}_{X}[\boldsymbol{x}] \boldsymbol{A}^{\top}=\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top}  \tag{22}\end{array}</script><p>而且:</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}] &=\mathbb{E}\left[\boldsymbol{x}(\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b})^{\top}\right]-\mathbb{E}[\boldsymbol{x}] \mathbb{E}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]^{\top} \\&=\mathbb{E}[\boldsymbol{x}] \boldsymbol{b}^{\top}+\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right] \boldsymbol{A}^{\top}-\boldsymbol{\mu} \boldsymbol{b}^{\top}-\boldsymbol{\mu} \boldsymbol{\mu}^{\top} \boldsymbol{A}^{\top} \\&=\boldsymbol{\mu} \boldsymbol{b}^{\top}-\boldsymbol{\mu} \boldsymbol{b}^{\top}+\left(\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right]-\boldsymbol{\mu} \boldsymbol{\mu}^{\top}\right) \boldsymbol{A}^{\top} \\& = \boldsymbol{\Sigma} \boldsymbol{A}^{\top} \tag{23}\end{align}</script><p>这里，$\boldsymbol{\Sigma} = \mathbb{E}[\boldsymbol{x}\boldsymbol{x}\top] - \boldsymbol{\mu}\boldsymbol{\mu}\top$是$\boldsymbol{X}$的协方差。</p><h4 id="3-Statistical-Independence-独立"><a href="#3-Statistical-Independence-独立" class="headerlink" title="3. Statistical Independence(独立)"></a>3. Statistical Independence(独立)</h4><h5 id="1-独立"><a href="#1-独立" class="headerlink" title="1. 独立"></a>1. 独立</h5><p>两个随机变量$X, Y$在统计上独立当且仅当:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x})p(\boldsymbol{y}) \tag{24}</script><p>直觉地,如果$\boldsymbol{y}$的值不增加$\boldsymbol{x}$任何信息，两个随机变量$X, Y$独立。</p><p>如果$X, Y$统计上独立,有：</p><script type="math/tex; mode=display">\begin{array}{l}p(\boldsymbol{y} \mid \boldsymbol{x})=p(\boldsymbol{y}) \\p(\boldsymbol{x} \mid \boldsymbol{y})=p(\boldsymbol{x}) \\\mathbb{V}_{X, Y}[\boldsymbol{x}+\boldsymbol{y}]=\mathbb{V}_{X}[\boldsymbol{x}]+\mathbb{V}_{Y}[\boldsymbol{y}] \\\operatorname{Cov}_{X, Y}[\boldsymbol{x}, \boldsymbol{y}]=\mathbf{0}\end{array}</script><h5 id="2-条件独立-Conditional-Independence"><a href="#2-条件独立-Conditional-Independence" class="headerlink" title="2.条件独立 Conditional Independence"></a>2.条件独立 Conditional Independence</h5><p>给定$Z$，两个随机变量$X, Y$条件独立,当且仅当:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y} \mid \boldsymbol{z})=p(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{y} \mid \boldsymbol{z}) \quad \text { for all } \quad z \in \mathcal{Z} \tag{25}</script><p>$\mathcal{Z}$是随机变量$Z$的状态量集合.$X \perp Y \mid Z$表示给定$Z$情况下$X$条件独立$Y$,或者“我们已知$z$,关于$y$的信息不改变$x$的信息”。</p><p>公式25可以理解为“给定关于$z$的信息,  $x \ and \ y$  的分布的分解”。运用概率的积的法则,公式25左边展开可以得到:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y} \mid \boldsymbol{z})=p(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{z}) p(\boldsymbol{y} \mid \boldsymbol{z}) \tag{26}</script><p>比较公式25、26右边部分,我们得到：</p><script type="math/tex; mode=display">p(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{z}) = p(\boldsymbol{x} \mid \boldsymbol{z}) \tag{27}</script><h4 id="4-随机变量的内积"><a href="#4-随机变量的内积" class="headerlink" title="4. 随机变量的内积"></a>4. 随机变量的内积</h4><h5 id="1-两个随机变量的内积定义"><a href="#1-两个随机变量的内积定义" class="headerlink" title="1. 两个随机变量的内积定义"></a>1. 两个随机变量的内积定义</h5><p>如果我们有两个不相关随机变量$X，Y$,  uncorrelated random variable, covariance value is zero 有：</p><script type="math/tex; mode=display">\mathbb{V}[x+y]=\mathbb{V}[x]+\mathbb{V}[y] \tag{28}</script><p>如果随机变量$X，Y$是不相关的,它们在对应向量空间是正交向量,应用毕达哥拉斯定理可以有如下示意图</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/03/YWiBm5kDjK49uSq.png?=raw" width=40% height=40%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">随机变量X,Y内积示意图 引用自https://mml-book.com</div> </center><p>随机变量在向量空间可以被看着向量,并且我们定义内积来获得随机变量的几何性质。</p><script type="math/tex; mode=display">\langle X, Y\rangle:=\operatorname{Cov}[x, y] \tag{29}</script><p>对0均值随机变量$X,Y$，我们得到内积。我们看到协方差矩阵是对称正定的，对每个参数都是线性的。随机变量的长度为：</p><script type="math/tex; mode=display">\|X\|=\sqrt{\operatorname{Cov}[x, x]}=\sqrt{\mathbb{V}[x]}=\sigma[x] \tag{30}</script><p>其标准偏差。随机变量的”长”，是不确定的，但一个长为0的随机变量是确定的。其还有如下性质：</p><script type="math/tex; mode=display">\begin{array}{l}\operatorname{Cov}[x, x]=0 \Longleftrightarrow x=0 \\\operatorname{Cov}[\alpha x+z, y]= \alpha \operatorname{Cov}[x, y]+ \operatorname{Cov}[z, y] \text { for } \alpha \in \mathbb{R}\end{array}</script><p>如果我们观察两个随机变量$X，Y$之间的角度$\theta$,我们得到：</p><script type="math/tex; mode=display">\cos \theta=\frac{\langle X, Y\rangle}{\|X\|\|Y\|}=\frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x] \mathbb{V}[y]}} \tag{31}</script><p>随机变量$X,Y$是正交的当且仅当$\operatorname{Cov}[x, y] = 0$，意味着它们是无关的。</p><h3 id="4-高斯分布"><a href="#4-高斯分布" class="headerlink" title="4. 高斯分布"></a>4. 高斯分布</h3><h4 id="1-什么是高斯分布"><a href="#1-什么是高斯分布" class="headerlink" title="1. 什么是高斯分布?"></a>1. 什么是高斯分布?</h4><p>二元随机变量$x_1, x_2$的高斯分布如下：    </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/04/9hp2zKNoDm1HMXb.png?=raw" width=40% height=40%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">二元随机变量高斯分布示意图 引用自https://mml-book.com</div> </center><p>100个样本的高斯分布：</p><ul><li><p>a）一维实例</p></li><li><p>b）二维实例</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/04/5JyxHIbhATvfzZl.png?=raw" width=80% height=80%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">100个样本的高斯分布示意图 引用自https://mml-book.com</div> </center></li></ul><p>对于单一随机变量,高斯分布的概率密度函数定义为：</p><script type="math/tex; mode=display">p\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) \tag{32}</script><p>以均值向量$\boldsymbol{\mu}$和协方差矩阵$\boldsymbol{\Sigma}$完全地特征化的多元随机变量的高斯分布可以定义为：</p><script type="math/tex; mode=display">p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})=(2 \pi)^{-\frac{D}{2}}|\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right) \tag{33}</script><p>其中, <script type="math/tex">\boldsymbol{x} \in \mathbb{R}^{D}, \ \ p(\boldsymbol{x})=\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) \text { or } X \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</script>。</p><p>当<script type="math/tex">\boldsymbol{\mu}=0</script> 且 <script type="math/tex">\boldsymbol{\Sigma}=\boldsymbol{I}</script>时，称作标准正态分布。</p><h4 id="2-多重高斯分布-Gaussians-的边界-Marginal-和条件-Conditional"><a href="#2-多重高斯分布-Gaussians-的边界-Marginal-和条件-Conditional" class="headerlink" title="2. 多重高斯分布 Gaussians 的边界 Marginal 和条件 Conditional"></a>2. 多重高斯分布 Gaussians 的边界 Marginal 和条件 Conditional</h4><p>我们清晰地按照级联状态  <script type="math/tex">[\boldsymbol{x}, \boldsymbol{y}]^T</script> 写出高斯分布:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y})=\mathcal{N}\left(\left[\begin{array}{l}\boldsymbol{\mu}_{x} \\\boldsymbol{\mu}_{y}\end{array}\right],\left[\begin{array}{cc}\boldsymbol{\Sigma}_{x x} & \boldsymbol{\Sigma}_{x y} \\\boldsymbol{\Sigma}_{y x} & \boldsymbol{\Sigma}_{y y}\end{array}\right]\right) \tag{34}</script><p>其中，  <script type="math/tex">\boldsymbol{\Sigma }_{x x} = \operatorname{Cov}[\boldsymbol{x}, \boldsymbol{x}] \text{ and } \boldsymbol{\Sigma}_{y y}=\operatorname{Cov}[\boldsymbol{y}, \boldsymbol{y}]</script>   是  <script type="math/tex">\boldsymbol{x} \text{ and } \boldsymbol{y}</script>  边缘协方差矩阵。同时,   <script type="math/tex">\boldsymbol{\Sigma }_{x y} = \operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}]</script>  是  <script type="math/tex">\boldsymbol{x}</script> 和 <script type="math/tex">\boldsymbol{y}</script>  之间的互协方差矩阵。</p><p>条件分布conditional distribution  <script type="math/tex">p(\boldsymbol{x} \mid \boldsymbol{y})</script>  也是高斯分布,其定义为:</p><script type="math/tex; mode=display">\begin{aligned}p(\boldsymbol{x} \mid \boldsymbol{y}) &=\mathcal{N}\left(\boldsymbol{\mu}_{x \mid y}, \boldsymbol{\Sigma}_{x \mid y}\right) \\\boldsymbol{\mu}_{x \mid y} &=\boldsymbol{\mu}_{x}+\boldsymbol{\Sigma}_{x y} \boldsymbol{\Sigma}_{y y}^{-1}\left(\boldsymbol{y}-\boldsymbol{\mu}_{y}\right) \\ \boldsymbol{\Sigma}_{x \mid y} &=\boldsymbol{\Sigma}_{x x}-\boldsymbol{\Sigma}_{x y} \boldsymbol{\Sigma}_{y y}^{-1} \boldsymbol{\Sigma}_{y x}\end{aligned} \tag{35}</script><p>注意，公式35计算均值时,  <script type="math/tex">\boldsymbol{y}-value</script>  是一个观察量而不是变量。</p><p>一个联合高斯分布<script type="math/tex">p(\boldsymbol{x}, \boldsymbol{y})</script>的边缘分布<script type="math/tex">p(\boldsymbol{x})</script>，其高斯分布应用sum rule计算给定为：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\int p(\boldsymbol{x}, \boldsymbol{y}) d \boldsymbol{y}=\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{x}, \boldsymbol{\Sigma}_{x x}\right)  \tag{36}</script><p>对应的结果也适用于$p(\boldsymbol{y})$,这通过边缘化$\boldsymbol{x}$来获得。</p><h4 id="3-高斯分布密度的积"><a href="#3-高斯分布密度的积" class="headerlink" title="3.  高斯分布密度的积"></a>3.  高斯分布密度的积</h4><p>两个高斯分布<script type="math/tex">\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{a}, \boldsymbol{A}) \quad \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{b}, \boldsymbol{B})</script>的积,是按<script type="math/tex">c \in \mathbb{R}</script>成比例的高斯分布,给定为<script type="math/tex">c  \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{c}, \boldsymbol{C})</script>:</p><script type="math/tex; mode=display">\begin{align}\tag{37} \boldsymbol{C} &=\left(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1}\right)^{-1}  \\\tag{38} \boldsymbol{c} &=\boldsymbol{C}\left(\boldsymbol{A}^{-1} \boldsymbol{a}+\boldsymbol{B}^{-1} \boldsymbol{b}\right) \\\tag{39} c &=(2 \pi)^{-\frac{D}{2}}|\boldsymbol{A}+\boldsymbol{B}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{a}-\boldsymbol{b})^{\top}(\boldsymbol{A}+\boldsymbol{B})^{-1}(\boldsymbol{a}-\boldsymbol{b})\right)\end{align}</script><p>比例常数$c$，它可以按照高斯密度函数的形式，以$\boldsymbol{a}$或$\boldsymbol{b}$和一个”夸张的”协方差矩阵$\boldsymbol{A+B}$写作，如,$\mathcal{N}(\boldsymbol{a} \mid \boldsymbol{b}, \boldsymbol{A + B}) \text{ 或 } \mathcal{N}(\boldsymbol{b} \mid \boldsymbol{a}, \boldsymbol{A + B})$.</p><p>注意：为了记号简便,我们有时用$\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{m}, \boldsymbol{S})$来描述高斯密度函数的形式，即使$\boldsymbol{x}$不是一个随机变量。当我们写作如下公式时，仅需要做前面的证明。</p><script type="math/tex; mode=display">c=\mathcal{N}(\boldsymbol{a} \mid \boldsymbol{b}, \boldsymbol{A}+\boldsymbol{B})=\mathcal{N}(\boldsymbol{b} \mid \boldsymbol{a}, \boldsymbol{A}+\boldsymbol{B}) \tag{40}</script><h4 id="高斯分布的和、线性变换-Sums-and-Linear-Transformations"><a href="#高斯分布的和、线性变换-Sums-and-Linear-Transformations" class="headerlink" title="高斯分布的和、线性变换 Sums and Linear Transformations"></a>高斯分布的和、线性变换 Sums and Linear Transformations</h4><p>如果<script type="math/tex">X,\,Y</script>是独立的高斯随机变量，那么<script type="math/tex">\boldsymbol{x + y}</script>也是高斯分布，给定为：</p><script type="math/tex; mode=display">p(\boldsymbol{x}+\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{\mu}_{x}+\boldsymbol{\mu}_{y}, \boldsymbol{\Sigma}_{x}+\boldsymbol{\Sigma}_{y}\right) \tag{41}</script><h5 id="两个单一变量混合高斯分布密度"><a href="#两个单一变量混合高斯分布密度" class="headerlink" title="两个单一变量混合高斯分布密度"></a>两个单一变量混合高斯分布密度</h5><script type="math/tex; mode=display">p(x)=\alpha p_{1}(x)+(1-\alpha) p_{2}(x) \tag{42}</script><p>这里标量$0&lt; \alpha &lt; 1$是混合权重,  <script type="math/tex">p(x_1) \text{和} p(x_2)</script>  是有不同参数的单一高斯分布密度,如， <script type="math/tex">(\mu_{1}, \sigma_{1}) \, \neq \, (\mu_{2}, \sigma_{2})</script>  。</p><p>混合高斯分布密度  $p(x)$  用每个随机变量的均值的权重之和来定义：</p><script type="math/tex; mode=display">\mathbb{E}[x] = \alpha \mu_{1} + (1 - \alpha) \mu_{2} \tag{43}</script><p>混合高斯分布密度$p(x)$的方差给定为：</p><script type="math/tex; mode=display">\mathbb{V}(x) = \left[\alpha \sigma_{1}^{2} + (1-\alpha)\sigma_{2}^{2} \right] + \left( \left[\alpha \mu_{1}^{2} + (1-\alpha)\mu_{2}^{2}\right] - \left[ \alpha \mu_{1} + (1-\alpha)\mu_{2} \right] ^{2}    \right) \tag{44}</script><p>证明:$\mathbb{E}[x]$</p><script type="math/tex; mode=display">\begin{align}\mathbb{E}[x] &=\int_{-\infty}^{\infty} x p(x) \mathrm{d} x \\&=\int_{-\infty}^{\infty} \alpha x p_{1}(x)+(1-\alpha) x p_{2}(x) \mathrm{d} x \\&=\alpha \int_{-\infty}^{\infty} x p_{1}(x) \mathrm{d} x+(1-\alpha) \int_{-\infty}^{\infty} x p_{2}(x) \mathrm{d} x \\&=\alpha \mu_{1}+(1-\alpha) \mu_{2} \tag{45}\end{align}</script><p>而要证明$\mathbb{V}(x)$，得证明$\mathbb{E}(x^{2})$.</p><script type="math/tex; mode=display">\begin{align}\mathbb{E}(x^{2}) &= \int_{-\infty}^{\infty} x^{2}p(x) \mathrm{d}x \\ & = \int_{-\infty}^{\infty} \alpha x^{2} p_{1}(x) + (1 - \alpha)x^{2}p_{2}(x) \mathrm{d}x \\ & = \alpha \int_{-\infty}^{\infty}x^{2}p_{1}(x) \mathrm{d}x +  (1 - \alpha)\int_{-\infty}^{\infty}x^{2}p_{2}(x) \mathrm{d}x \\ & = \alpha(\mu_{1}^{2} + \sigma_{1}^{2}) + (1 - \alpha)(\mu_{2}^{2} + \sigma_{2}^{2}) \tag{46}\end{align}</script><p>公式46最后两步是因为:$\sigma^{2} = \mathbb{E}[x^2] - \mu^{2}$。重新整理就是随机变量平方的均值等于均值和方差平方和。因此，方差可以定义为：</p><script type="math/tex; mode=display">\begin{align}\mathbb{V}[x]=& \mathbb{E}\left[x^{2}\right]-(\mathbb{E}[x])^{2} \\=& \alpha\left(\mu_{1}^{2}+\sigma_{1}^{2}\right)+(1-\alpha)\left(\mu_{2}^{2}+\sigma_{2}^{2}\right)-\left(\alpha \mu_{1}+(1-\alpha) \mu_{2}\right)^{2} \\=&\left[\alpha \sigma_{1}^{2}+(1-\alpha) \sigma_{2}^{2}\right] \\&+\left(\left[\alpha \mu_{1}^{2}+(1-\alpha) \mu_{2}^{2}\right]-\left[\alpha \mu_{1}+(1-\alpha) \mu_{2}\right]^{2}\right) \tag{47}\end{align}</script><p>注意：前面的偏差可以应用到任何密度函数，但自从高斯分布完全由均值和方差决定后，混合密度函数也可以确定了。</p><h5 id="总方差定理-Law-of-total-variance"><a href="#总方差定理-Law-of-total-variance" class="headerlink" title="总方差定理 Law of total variance"></a>总方差定理 Law of total variance</h5><p>通常指对于两个随机变量$X \, 和 \, Y$，有：</p><script type="math/tex; mode=display">\mathbb{V}_{X}[x] = \mathbb{E}_Y \left[\mathbb{V}_{X} \left[x \mid y \right] \right] + \mathbb{V}_{Y} \left[\mathbb{E}_{X} \left[x \mid y \right] \right]</script><p>即条件方差的期望加上条件均值的方差。</p><p>我们认为二元高斯随机变量$X$，对其应用线性变换$\boldsymbol{Ax}$。结果是一个有均值zero协方差为$\boldsymbol{A}\boldsymbol{A}^{\top}$高斯分布。观察到，加上一个常数向量将会改变分布的均值，不影响其方差，那么，随机变量$\boldsymbol{x + \mu}$有均值$\boldsymbol{\mu}$和单位协方差矩阵。因此，高斯分布进行任何线性、仿射变换后都是高斯分布。</p><p>考虑高斯分布随机变量$X \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$,对于给定恰当形状的矩阵$\boldsymbol{A}$，让$Y$成为$\boldsymbol{x}$进行$\boldsymbol{y = Ax}$变换后的的随机变量。我们利用如下线性操作来计算$\boldsymbol{y}$</p><p>的均值：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{y}] = \mathbb{E}[\boldsymbol{Ax}]= \boldsymbol{A}\mathbb{E}[\boldsymbol{x}] = \boldsymbol{A\mu} \tag{48}</script><p>方差：</p><script type="math/tex; mode=display">\mathbb{V} \left[ \boldsymbol{y}\right] = \mathbb{V} \left[ \boldsymbol{Ax}\right] = \boldsymbol{A} \mathbb{V} \left[ \boldsymbol{x}\right]\boldsymbol{A}^{\top}  =\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top} \tag{49}</script><p>随机变量$\boldsymbol{y}$分布服从于：</p><script type="math/tex; mode=display">p(\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{A} \boldsymbol{\mu}, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top}\right) \tag{50}</script><p>现在，我们再考虑相反的变换：当一个随机变量有均值， 且是另一个随机变量的线性变换。对于给定一个满秩矩阵$\boldsymbol{A} \in \mathbb{R}^{M \times N}$,当 $M \ge N$, $\boldsymbol{y} \in \mathbb{R}^{M}$是一个均值为$\boldsymbol{Ax}$的高斯随机变量， 即：</p><script type="math/tex; mode=display">p(\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{Ax},\, \boldsymbol{\Sigma} \right) \tag{51}</script><p>其对应概率分布$p(\boldsymbol{x})$是什么？如果 $p(\boldsymbol{A})$是可逆的，我们可以写作$\boldsymbol{x} = \boldsymbol{A}^{-1}\boldsymbol{y}$,应用前面提到的变换。然而，通常$\boldsymbol{A}$是不可逆，我们使用相似的方法即伪逆。我们两边同乘$\boldsymbol{A}^{\top}$,然后逆变换$\boldsymbol{A}^{\top}\boldsymbol{A}$,它是对称正定的，给我们如下联系：</p><script type="math/tex; mode=display">\boldsymbol{y}=\boldsymbol{A} \boldsymbol{x} \Longleftrightarrow\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{y}=\boldsymbol{x} \tag{52}</script><p>因此，$\boldsymbol{y}$的逆变换是$\boldsymbol{x}$，我们可以得到：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{x} \mid\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{y},\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{\Sigma} \boldsymbol{A}\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1}\right) \tag{53}</script><h3 id="5-Conjugacy-and-the-Exponential-Family"><a href="#5-Conjugacy-and-the-Exponential-Family" class="headerlink" title="5. Conjugacy and the Exponential Family"></a>5. Conjugacy and the Exponential Family</h3>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Mathematic for machine learning 笔记 </tag>
            
            <tag> 高斯分布 </tag>
            
            <tag> Probability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter05:Vector calculus</title>
      <link href="2020/04/01/M4ML_Chapter05-Vector-Calculus/"/>
      <url>2020/04/01/M4ML_Chapter05-Vector-Calculus/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-05-Vector-calculus"><a href="#Chapter-05-Vector-calculus" class="headerlink" title="Chapter 05: Vector calculus"></a>Chapter 05: Vector calculus</h2><h3 id="1-单变量函数的微分"><a href="#1-单变量函数的微分" class="headerlink" title="1. 单变量函数的微分"></a>1. 单变量函数的微分</h3><h4 id="1-微分"><a href="#1-微分" class="headerlink" title="1. 微分"></a>1. 微分</h4><ul><li>derivative:</li></ul><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d} x}:=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \tag{1}</script><h4 id="2-Taylor-级数"><a href="#2-Taylor-级数" class="headerlink" title="2. Taylor 级数"></a>2. Taylor 级数</h4><script type="math/tex; mode=display">T_{\infty}(x)=\sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k} \tag{2}</script><h4 id="3-微分法则"><a href="#3-微分法则" class="headerlink" title="3. 微分法则"></a>3. 微分法则</h4><ul><li>Differentiation Rules:</li></ul><script type="math/tex; mode=display">\begin{align}\text{Product rule: }  &\quad(f(x) g(x))^{\prime}=f^{\prime}(x) g(x)+f(x) g^{\prime}(x)  \tag{3}\\ \text{Quotient rule: } &\left(\frac{f(x)}{g(x)}\right)^{\prime}=\frac{f^{\prime}(x) g(x)-f(x) g^{\prime}(x)}{(g(x))^{2}} \tag{4}\\ \text{Sum rule: } &(f(x)+g(x))^{\prime}=f^{\prime}(x)+g^{\prime}(x) \tag{5}\\ \text{Chain rule: } &(g(f(x)))^{\prime}=(g \circ f)^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x) \tag{6}\end{align}</script><p><strong>注：</strong>$g \circ f$ 记为复合函数$x \mapsto f(x) \mapsto g(f(x))$</p><script type="math/tex; mode=display">\begin{align}\\ \text{例子：}\\ &h(x) = (2x+1)^{4} = g(f(x))\\ \text{即}&f(x) = 2x + 1 \text{,}\qquad g(f) = f^{4}\\ &f^{\prime}(x) = 2 \text{,}\qquad g^{\prime}(f)=4f^{3}\\ \text{所以}&h^{\prime}(x) = g^{\prime}(f)f^{\prime}(x)=4f^{3}\cdot2=8(2x+1)^{3}\end{align}</script><h3 id="2-部分微分和梯度"><a href="#2-部分微分和梯度" class="headerlink" title="2.部分微分和梯度"></a>2.部分微分和梯度</h3><ul><li>Partial Differentiation andGradients</li></ul><h4 id="1-部分微分"><a href="#1-部分微分" class="headerlink" title="1.部分微分"></a>1.部分微分</h4><ul><li>Partial Derivative:</li></ul><script type="math/tex; mode=display">\nabla_{\boldsymbol{x}} f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}=\left[\begin{array}{llll}\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} & \frac{\partial f(\boldsymbol{x})}{\partial x_{2}} & \cdots & \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}\end{array}\right] \in \mathbb{R}^{1 \times n} \tag{7}</script><p>即把对$x$的一阶导的集合写在一起形成一行,也可以转置后形成一列。</p><h4 id="2-梯度"><a href="#2-梯度" class="headerlink" title="2. 梯度"></a>2. 梯度</h4><ul><li>Gradient as a Row Vector</li></ul><p>将<strong>Gradient</strong>写作行向量的原因：</p><ol><li><p>我们始终可以把梯度推广到向量值函数$f:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$;这能让梯度变成矩阵。</p></li><li><p>我们可以不考虑梯度维数来应用多变量链式法则。</p><script type="math/tex; mode=display">\begin{align}\\ &\text{函数} f(x_{1}, x_{2}) = x_1^{2}x_2 + x_1x_2{3}\quad\text{其梯度为：}\\&\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}} = \begin{bmatrix}\frac{\partial f(x_1, x_2)}{\partial x_1} & \frac{\partial f(x_1, x_2)}{\partial x_2}\end{bmatrix}= \begin{bmatrix}2x_1x_2^{3} & x_1^{2}+3x_1x_2^{2} \end{bmatrix}\in \mathbb{R}^{\color{red} {1 \times 2}}\end{align}</script></li></ol><h4 id="3-Chain-Rule"><a href="#3-Chain-Rule" class="headerlink" title="3. Chain Rule"></a>3. Chain Rule</h4><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d} t}=\left[\begin{array}{ll}\frac{\partial f}{\partial x_{1}} & \frac{\partial f}{\partial x_{2}}\end{array}\right]\left[\begin{array}{l}\frac{\partial x_{1}(t)}{\partial t} \\\frac{\partial x_{2}(t)}{\partial t}\end{array}\right]=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t} \tag{8}</script><p>用矩阵乘法获得梯度:</p><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d}(s, t)}=\frac{\partial f}{\partial \boldsymbol{x}} \frac{\partial \boldsymbol{x}}{\partial(s, t)}=\underbrace{\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}\right]}_{=\frac{\partial f}{\partial \boldsymbol{x}}}\underbrace{\left[\begin{array}{ll}\frac{\partial x_{1}}{\partial s} & \frac{\partial x_{1}}{\partial t} \\\frac{\partial x_{2}}{\partial s} &\frac{\partial x_{2}}{\partial t}\end{array}\right]}_{=\frac{\partial \boldsymbol{x}}{\partial(s, t)}}</script><h4 id="4-向量值函数的梯度-从实数域推广到vector-field"><a href="#4-向量值函数的梯度-从实数域推广到vector-field" class="headerlink" title="4. 向量值函数的梯度(从实数域推广到vector field)"></a>4. 向量值函数的梯度(从实数域推广到vector field)</h4><script type="math/tex; mode=display">\boldsymbol{f}(\boldsymbol{x})=\left[\begin{array}{c}f_{1}(\boldsymbol{x}) \\\vdots \\f_{m}(\boldsymbol{x})\end{array}\right] \in \mathbb{R}^{m}</script><h4 id="5-Jacobian"><a href="#5-Jacobian" class="headerlink" title="5. Jacobian"></a>5. Jacobian</h4><p>所有向量值函数$\boldsymbol{f}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$的一阶导数的集合称作$\mathit{Jacobian}$。见如下公式：</p><script type="math/tex; mode=display">\begin{align}J=\nabla_{x} f=\frac{\mathrm{d} f(x)}{\mathrm{d} x}&=\left[\frac{\partial f(x)}{\partial x_{1}} \quad \cdots \quad \frac{\partial f(x)}{\partial x_{n}}\right] \tag{9}\\&=\left[\begin{array}{ccc}\frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{1}} & \cdots & \frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{n}} \\\vdots & & \vdots \\\frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{1}} & \cdots & \frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{n}}\end{array}\right] \tag{10}\\ \boldsymbol{x}=\left[\begin{array}{c}x_{1} \\\vdots \\x_{n}\end{array}\right], \quad J(i, j)&=\frac{\partial f_{i}}{\partial x_{j}} \tag{11}\end{align}</script><p>确定$\boldsymbol{f}(x)$偏导数维度：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/26/bYTjzB1PcOZFhSN.png?=raw" width="30%" height="30%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">偏导数维度：引用自https://mml-book.com</div> </center><p>​    偏导数的维度：column由$\boldsymbol{f}(x)$决定；row由$\boldsymbol{x}$决定</p><p>详解：</p><ol><li>$f:\mathbb{R} \rightarrow \mathbb{R}$ 梯度是个标量</li><li>$f:\mathbb{R}^{N} \rightarrow \mathbb{R}$ 梯度是$1\times $N行向量(row)</li><li>$f:\mathbb{R} \rightarrow \mathbb{R}^{M}$ 梯度是$M \times 1$列向量(column)</li><li>$f:\mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ 梯度是$M \times N$矩阵(matrix)</li></ol><h5 id="可视化矩阵关于Vector的梯度计算（两种等价方法）"><a href="#可视化矩阵关于Vector的梯度计算（两种等价方法）" class="headerlink" title="可视化矩阵关于Vector的梯度计算（两种等价方法）"></a>可视化矩阵关于Vector的梯度计算（两种等价方法）</h5><ul><li>方法(a):先计算$\frac {\partial \boldsymbol{A}} {\partial {x_1}}, \frac {\partial \boldsymbol{A}} {\partial {x_2}}, \frac {\partial \boldsymbol{A}} {\partial {x_3}}$（都是$4 \times 2$的tensor;再整理成$4\times2\times3$的tensor。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/29/itgacHYjVuInxSR.png?=raw" width=40% height=40%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">方法(a) 引用自https://mml-book.com</div> </center><ul><li>方法(b):先把$\boldsymbol {A} \in \mathbb{R}^{4 \times 2}$展平成向量$\tilde{A} \in \mathbb{R}^{8}$，然后计算其梯度$\frac{\mathrm{d} \tilde{\boldsymbol{A}}}{\mathrm{d} \boldsymbol{x}} \in \mathbb{R}^{8 \times 3}$,我们把得到的梯度tensor再reshape成图例形状。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/29/AolB2qsUWfMdJnE.png?=raw" width="40%" height="40%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">方法(b) 引用自https://mml-book.com</div> </center>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Mathematic for machine learning 笔记 </tag>
            
            <tag> 微积分 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter04:Matrix Decompositions</title>
      <link href="2020/03/28/M4ML_Chapter04-Matrix-Decompositions/"/>
      <url>2020/03/28/M4ML_Chapter04-Matrix-Decompositions/</url>
      
        <content type="html"><![CDATA[<h2 id="Matrix-Decompositions"><a href="#Matrix-Decompositions" class="headerlink" title="Matrix Decompositions"></a>Matrix Decompositions</h2><h3 id="3-Cholesky-Decomposition"><a href="#3-Cholesky-Decomposition" class="headerlink" title="3. Cholesky Decomposition"></a>3. Cholesky Decomposition</h3><p><strong>Cholesky分解</strong> : 一个对称，正定的矩阵$A$</p><p>能被因式分解成一个积：$A = L L^{T}$,$L$是一个有正的对角元素的下三角矩阵：</p><script type="math/tex; mode=display">\begin{bmatrix}    a_{11} & \cdots & a_{1n} \\    \vdots & \ddots & \vdots \\    a_{n1} & \cdots & a_{nn}   \end{bmatrix}=\begin{bmatrix}    l_{11} & \cdots & 0 \\    \vdots & \ddots & \vdots \\   l_{n1} & \cdots & l_{nn}   \end{bmatrix} \begin{bmatrix}  l_{11} & \cdots & l_{n1} \\    \vdots & \ddots & \vdots \\    0 & \cdots & l_{nn}   \end{bmatrix}</script><p>$L$被称作$A$科斯基因子，并且$L$是唯一的。</p><h3 id="4-特征分解和对角化"><a href="#4-特征分解和对角化" class="headerlink" title="4. 特征分解和对角化"></a>4. 特征分解和对角化</h3><h4 id="1-对角阵（A-diagonal-matrix-is-a-matrix-that-has-value-zero-on-all-off-diagonal-elements-）"><a href="#1-对角阵（A-diagonal-matrix-is-a-matrix-that-has-value-zero-on-all-off-diagonal-elements-）" class="headerlink" title="1. 对角阵（A diagonal matrix is a matrix that has value zero on all off-diagonal elements ）"></a>1. 对角阵（A diagonal matrix is a matrix that has value zero on all off-diagonal elements ）</h4><script type="math/tex; mode=display">\begin{bmatrix}    c_{1} & \cdots & 0 \\    \vdots & \ddots & \vdots \\    0 & \cdots & c_{n}   \end{bmatrix}</script><h4 id="2-相似"><a href="#2-相似" class="headerlink" title="2. 相似"></a>2. 相似</h4><p>Marices $A, D$ are similar if there exists an invertible matrix P, such that :</p><script type="math/tex; mode=display">\boldsymbol{D} =\boldsymbol{P}^{-1} \boldsymbol{A} \boldsymbol{P}   \tag{1}</script><h4 id="3-对角化"><a href="#3-对角化" class="headerlink" title="3.对角化"></a>3.对角化</h4><ul><li>Diagonalizable</li></ul><p>矩阵$A \in \mathbb{R}^{n \times n}$如果相似于对角阵就能对角化。即：如果存在可逆矩阵$P \in \mathbb {R}^{n \times n}$如$D = P^{-1}AP$</p><p>​    <strong>注:</strong></p><p>​        若<script type="math/tex">A \in \mathbb{R}^{n \times n}, \quad \lambda_{1}, \cdots, \lambda_{n}</script>是标量的集合,   <script type="math/tex">p_{1}, \cdots, p_{n}</script>是在<script type="math/tex">\mathbb{R}^{n}</script>上的向量集合。定义： <script type="math/tex">P:=[p_{1}, \cdots, p_{n}]；\ D \in \mathbb{R}^{n \times n}\ 是含\lambda_{1}, \cdots,\lambda{n}</script> 对角元素的对角阵。</p><p>则由公式1有：</p><script type="math/tex; mode=display">\begin{align}\boldsymbol{A}\boldsymbol{P} &= \boldsymbol{P}\boldsymbol{D} \tag{2}\\ \boldsymbol{A}\boldsymbol{P} &= \boldsymbol{A}[\boldsymbol{p}_{1}, \cdots, \boldsymbol{p}_{n}] =\begin{bmatrix} \boldsymbol{A}\boldsymbol{p}_{1}, \cdots, {A}\boldsymbol{p}_{n} \tag{3} \end{bmatrix}\\ \boldsymbol{P}\boldsymbol{D}&= \begin{bmatrix} \boldsymbol{p}_1, \cdots, \boldsymbol{p}_{n}\end{bmatrix}\begin{bmatrix}   \lambda_{1} & \cdots & 0 \\   \vdots & \ddots & \vdots \\    0 & \cdots & \lambda_{n} \end{bmatrix} = \begin{bmatrix} \lambda_{1}\boldsymbol{p}_{1}, \cdots,  \lambda_{n}\boldsymbol{p}_{n}\tag{4}  \end{bmatrix} \end{align}</script><p>由（2）（3）（4）有：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{A} \boldsymbol{p}_{1} &=\lambda_{1} \boldsymbol{p}_{1} \\& \vdots \\\boldsymbol{A} \boldsymbol{p}_{n} &=\lambda_{n} \boldsymbol{p}_{n} \end{aligned}</script><p>因此：$\boldsymbol{P}$的列必须是$\boldsymbol{A}$的特征向量。</p><p>$\boldsymbol{P}$ 必须是可逆的，而且是满秩。</p><h4 id="4-特征分解"><a href="#4-特征分解" class="headerlink" title="4. 特征分解"></a>4. 特征分解</h4><ul><li>Eigendecomposition</li></ul><p>方阵$\boldsymbol{A} \in \mathbb{R}^{n \times n}$能分解成：</p><script type="math/tex; mode=display">\boldsymbol{A} =\boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}   \tag{6}</script><p><strong>注</strong>：$\boldsymbol{D}$ 是一个对角元素都是$\boldsymbol{A}$的特征值的对角阵，当且仅当$\boldsymbol{A}$的特征向量来自于$\mathbb{R}^{n}$的一组基。</p><h4 id="5-SVD"><a href="#5-SVD" class="headerlink" title="5. SVD"></a>5. SVD</h4><ul><li>singular  value  decomposition</li></ul><p>若$\boldsymbol{A}^{m\times n}$是一个秩为$r \in [0, min(m, n)]$的长方形矩阵， $\boldsymbol {A}$能分解成如下形式：</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261824042.png" style="zoom:40%;" /></p><p>注：<a href="https://mml-book.github.io/">引用自:Chapter4.5 singular value decomposition 4.64:</a></p><p>其中：正交阵$\boldsymbol{U} \in \mathbb{R}^{m \times m}$，其列向量$\boldsymbol{u}_{i}, i = 1, \cdots, m$</p><p>​            正交阵$\boldsymbol{V} \in \mathbb{R}^{n \times n}$，其列向量$\boldsymbol{v}_{j}, j = 1, \cdots, m$</p><p>​            并且，$\boldsymbol \sum$是$m \times n$的矩阵，其$\sum {ii} = \sigma_{i} \ge 0 \text{ and} \sum {ij} = 0, i \neq j$</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> Linear mapping </tag>
            
            <tag> Mathematic for machine learning 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7. 哈希表</title>
      <link href="2020/03/06/DS_7.%20%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
      <url>2020/03/06/DS_7.%20%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="7-哈希表"><a href="#7-哈希表" class="headerlink" title="7. 哈希表"></a>7. 哈希表</h3><h4 id="1-哈希表定义和简单应用"><a href="#1-哈希表定义和简单应用" class="headerlink" title="1. 哈希表定义和简单应用"></a>1. 哈希表定义和简单应用</h4><p><strong>哈希表定义:</strong> 哈希表，也叫散列表，是根据<strong>关键字(key)值</strong>直接进行访问的数据结构，它通过把关键字值<strong>映射</strong>到表中的一个位置(数组下标)来<strong>直接访问</strong>，以加快查找<strong>关键字值</strong>的速度。这个映射叫做<strong>哈希函数</strong>，存放记录的数组叫作<strong>哈希表</strong>.</p><p>给定表M，存在<script type="math/tex">f(key)</script>​, 对任意关键字值key，代入函数若能得到包含该关键字值的表中地址，就称表M为哈希表，函数<script type="math/tex">f(key)</script>​为哈希函数。</p><p><strong>示例1：统计字符串中，各个字符数量</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> char_map[<span class="number">128</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    std::string str = <span class="string">&quot;abcdefgaaxxy&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str.<span class="built_in">length</span>(); i++)&#123;</span><br><span class="line">        char_map[str[i]]++;<span class="comment">// char_map[&#x27;a&#x27;]++即char_map[97]++</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt; <span class="number">128</span>; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (char_map[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%c %d: %d\n&quot;</span>, i, i, char_map[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">=============================================================</span><br><span class="line">a <span class="number">97</span>: <span class="number">3</span></span><br><span class="line">b <span class="number">98</span>: <span class="number">1</span></span><br><span class="line">c <span class="number">99</span>: <span class="number">1</span></span><br><span class="line">d <span class="number">100</span>: <span class="number">1</span></span><br><span class="line">e <span class="number">101</span>: <span class="number">1</span></span><br><span class="line">f <span class="number">102</span>: <span class="number">1</span></span><br><span class="line">g <span class="number">103</span>: <span class="number">1</span></span><br><span class="line">x <span class="number">120</span>: <span class="number">2</span></span><br><span class="line">y <span class="number">121</span>: <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>示例2：哈希表排序整数</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">// **示例2：哈希表排序整数**</span></span><br><span class="line"><span class="comment">// 使用哈希表排序，数组的下标对正整数排序，这只适用正整数</span></span><br><span class="line"><span class="comment">//哈希表的长度，需要超过最大待排序数字</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> random[<span class="number">10</span>] = &#123;<span class="number">99</span>, <span class="number">1</span>, <span class="number">444</span>, <span class="number">7</span>, <span class="number">90</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">888</span>, <span class="number">5</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> hash_map[<span class="number">1000</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)&#123;</span><br><span class="line">        hash_map[random[i]]++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt; <span class="number">1000</span>; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j =<span class="number">0</span>; j&lt;hash_map[i]; j++)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, i);</span><br><span class="line">        &#125;<span class="comment">//时间复杂度为O(表厂+n)元素个数</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">======================================================================</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">90</span></span><br><span class="line"><span class="number">99</span></span><br><span class="line"><span class="number">444</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure><h4 id="2-任意元素的映射"><a href="#2-任意元素的映射" class="headerlink" title="2. 任意元素的映射"></a>2. 任意元素的映射</h4><ol><li>当遇到<strong>负数或非常大的整数</strong>，如何<strong>哈希</strong>?</li><li>当遇到<strong>字符串</strong>如何<strong>哈希</strong>？</li><li>当遇到无法直接映射的数据类型，如浮点数和数组、对象等如何进行哈希?</li></ol><p>利用<strong>哈希函数</strong>，将<strong>关键字值key</strong>(大整数、字符串、浮点数等)转换为整数在对<strong>表长取余</strong>，从而关键字值被转换为哈希表的<strong>表长范围内</strong>的整数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">int_func</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> table_len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> key % table_len;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//字符串字符ascii相加得到整数再对表长取余</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">string_func</span><span class="params">(std::string key, <span class="keyword">int</span> table_len)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; key.<span class="built_in">length</span>(); i++)&#123;</span><br><span class="line">        sum += key[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum % table_len;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//不同整数或字符串，用于哈希函数的选择，</span></span><br><span class="line"><span class="comment">//会映射到同一个下标出，产生冲突如4位置</span></span><br><span class="line"><span class="comment">//到底是abc出现2次还是bac出现两次还是各一次呢</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> TAB_LEN = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> hash_map[TAB_LEN] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    hash_map[<span class="built_in">int_func</span>(<span class="number">9995</span>, TAB_LEN)]++;</span><br><span class="line">    hash_map[<span class="built_in">int_func</span>(<span class="number">5</span>, TAB_LEN)]++;</span><br><span class="line">    hash_map[<span class="built_in">string_func</span>(<span class="string">&quot;abc&quot;</span>, TAB_LEN)]++;</span><br><span class="line">    hash_map[<span class="built_in">string_func</span>(<span class="string">&quot;bac&quot;</span>, TAB_LEN)]++;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i&lt; TAB_LEN; i++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;hash_map[%d] = %d\n&quot;</span>, i, hash_map[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">=============================================================</span><br><span class="line">hash_map[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">4</span>] = <span class="number">2</span></span><br><span class="line">hash_map[<span class="number">5</span>] = <span class="number">2</span></span><br><span class="line">hash_map[<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">7</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">8</span>] = <span class="number">0</span></span><br><span class="line">hash_map[<span class="number">9</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure><p><strong>哈希冲突</strong>，解决办法拉链法构造哈希表。</p><p>将所有哈希函数结果相同的节点连接在同一个单链表中。若选定哈希表长度为m，则可将哈希表定义为一个长度为m的指针数组t[0..m-1]，指针数组中每个指针指向哈希函数结果相同的单链表。</p><p><strong>插入<code>value</code>：</strong></p><p>将元素<code>value</code>插入哈希表，若元素<code>value</code>的<code>哈希函数值为hash_key</code>，将<code>value</code>对应的节点以<code>头插法</code>方式插入到<code>t[hash_key]</code>为<code>头指针的单链表</code>中。</p><p><strong>查找<code>value</code></strong>:</p><p>若元素<code>value</code>的<strong>哈希函数值</strong>为<code>hash_key</code>， 遍历以<code>t[hash_key]</code>为头指针的<strong>单链表</strong>，查找链表的各个节点的值域是否为<code>value</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/tensortimes/backup/202203261811547.png" alt="image-20210803012020263" style="zoom:40%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hash table </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. python实现单链表</title>
      <link href="2020/03/04/DS_601%E5%8D%95%E9%93%BE%E8%A1%A8/"/>
      <url>2020/03/04/DS_601%E5%8D%95%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="2-python实现单链表"><a href="#2-python实现单链表" class="headerlink" title="2. python实现单链表"></a>2. python实现单链表</h3><h4 id="1-创建Node节点"><a href="#1-创建Node节点" class="headerlink" title="1. 创建Node节点"></a>1. 创建Node节点</h4><ol><li>初始化节点属性 数据域和下一个节点的指针域</li><li>设置data属性，更加pythonic写法，类方法加<code>@property</code>转化为属性使用</li><li>修改属性</li><li>同样实现下一个节点指针的属性和修改</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建链表的Node节点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, next_node=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Node节点初始化方法</span></span><br><span class="line"><span class="string">        :param data: 存储的值</span></span><br><span class="line"><span class="string">        :param next_node: 下一个节点指针</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__data = data</span><br><span class="line">        self.__<span class="built_in">next</span> = next_node</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取Node存储值</span></span><br><span class="line"><span class="string">        :return: 当前节点存储的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.__data</span><br><span class="line"></span><br><span class="line"><span class="meta">    @data.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        设置Node节点值</span></span><br><span class="line"><span class="string">        :param data: 要设置的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__data = data</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_node</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取Node下一个节点指针</span></span><br><span class="line"><span class="string">        :return: 下一个节点指针</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.__<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @next_node.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_node</span>(<span class="params">self, next_node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Node下一个节点指针的修改方法</span></span><br><span class="line"><span class="string">        :param next_node: 新的下一个节点指针</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.__<span class="built_in">next</span> = next_node</span><br></pre></td></tr></table></figure><h4 id="2-创建单链表及方法"><a href="#2-创建单链表及方法" class="headerlink" title="2. 创建单链表及方法"></a>2. 创建单链表及方法</h4><p>这里实现的单链表操作方法有：</p><ol><li><p>查找</p><ul><li>按值查找</li><li>按索引查找节点</li></ul></li><li><p>插入</p><ul><li>头结点插入指定值</li><li>在链表指定节点后插入值为value的节点</li><li>在链表指定节点之前插入值为value的节点</li></ul></li><li><p>删除</p><ul><li>删除指定节点</li><li>删除指定值的节点</li><li>删除链表中倒数第n个节点</li></ul></li><li><p>一般方法</p><ul><li><p>创建一个节点</p></li><li><p>打印当前链表所有节点数据</p></li></ul></li><li><p>常见单链表考点</p><ul><li><p>查找链表中间节点</p></li><li><p>翻转相邻两个节点</p></li><li>翻转链表</li><li>检查链表是否有环</li></ul></li></ol><hr><ol><li><p>初始化单链表私有头结点属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleLinkList</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;单向链表类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__head = <span class="literal">None</span></span><br></pre></td></tr></table></figure></li></ol><ol><li><p>按值查找</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_by_value</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        按照指定值在单向链表中查找</span></span><br><span class="line"><span class="string">        :param value: 查找的指定值</span></span><br><span class="line"><span class="string">        :return: node</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self.__head</span><br><span class="line">        <span class="keyword">while</span> (node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (node.data != value):</span><br><span class="line">            <span class="comment"># 如果节点不为空且节点值不等于查找值就一直往后查找</span></span><br><span class="line">            node = node.next_node</span><br><span class="line">        <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>按索引查找节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_by_index</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        按照指定索引在单向链表中查找</span></span><br><span class="line"><span class="string">        :param index: 查找的指定索引</span></span><br><span class="line"><span class="string">        :return: node</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = self.__head</span><br><span class="line">        pos = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (pos != index):</span><br><span class="line">            <span class="comment"># 如果节点不空且索引不是查找的，节点网后移一个，索引也往后移一个</span></span><br><span class="line">            node = node.next_node</span><br><span class="line">            pos += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure></li><li><p>头结点插入指定值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_node_to_head</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        链表头部插入值为value的节点</span></span><br><span class="line"><span class="string">        :param value: 要插入的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        node = Node(value)</span><br><span class="line">        <span class="comment">#插入节点的下一个节点指向头, 头结点指向插入节点</span></span><br><span class="line">        node.next_node = self.__head</span><br><span class="line">        self.__head = node</span><br></pre></td></tr></table></figure></li><li><p>在链表指定节点后插入值为value的节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_after</span>(<span class="params">self, node, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在链表指定节点后插入一个值为value的node</span></span><br><span class="line"><span class="string">        :param node: 指定的节点</span></span><br><span class="line"><span class="string">        :param value: 要插入的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 指定节点为空就什么都不做</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        new_node = Node(value)</span><br><span class="line">        <span class="comment"># 将指定节点的下一个节点指针赋值给新节点的下一个指针</span></span><br><span class="line">        new_node.next_node = node.next_node</span><br><span class="line">        <span class="comment"># 新节点接到指定节点后</span></span><br><span class="line">        node.next_node = new_node</span><br></pre></td></tr></table></figure></li><li><p>在链表指定节点之前插入值为value的节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_before</span>(<span class="params">self, node, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在指定节点前插入值为value的节点</span></span><br><span class="line"><span class="string">        :param node: 指定节点</span></span><br><span class="line"><span class="string">        :param value: 插入的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> (node <span class="keyword">is</span> <span class="literal">None</span>) <span class="keyword">and</span> (self.__head <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">            <span class="comment"># 如果指定节点是空或者空链表之前插入数据，则什么都不做</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> node == self.__head:</span><br><span class="line">            <span class="comment"># 如果指定节点是头结点就直接用头结点插入</span></span><br><span class="line">            self.insert_to_head(value)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        new_node = Node(value)</span><br><span class="line">        pos = self.__head  <span class="comment"># 设置位置节点为头结点</span></span><br><span class="line">        not_found = <span class="literal">False</span>  <span class="comment"># 没找到标志位</span></span><br><span class="line">        <span class="keyword">while</span> pos.next_node != node:  <span class="comment"># 寻找节点直到等于指定节点       </span></span><br><span class="line">            <span class="keyword">if</span> pos.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 如果当前位置的下一个节点不为空，继续后移位置</span></span><br><span class="line">                pos = pos.next_node</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 直到链表最后节点,没找到指定节点，将没找到标志位置为True，并跳出循环</span></span><br><span class="line">                not_found = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> not_found:  <span class="comment"># 找到指定节点的前节点就插入node</span></span><br><span class="line">            pos.next_node = new_node</span><br><span class="line">            new_node.next_node = node</span><br></pre></td></tr></table></figure></li><li><p>删除指定节点，类似于7插入指定节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_by_node</span>(<span class="params">self, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        删除链表指定节点</span></span><br><span class="line"><span class="string">        :param node: 指定节点</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.__head <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment"># 如果头结点为空，就不用删除直接返回</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> node == self.__head:</span><br><span class="line">            <span class="comment"># 如果指定节点是头结点，直接将头结点接到节点下一个节点</span></span><br><span class="line">            self.__head = node.next_node</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        pos = self.__head</span><br><span class="line">        not_found = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> pos.next_node != node:</span><br><span class="line">            <span class="keyword">if</span> pos.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 如果当前位置的下一个节点不为空，继续后移到下一个位置</span></span><br><span class="line">                pos = pos.next_node</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 直到链表最后节点,没找到指定节点，将没找到标志位置为True，并跳出循环</span></span><br><span class="line">                not_found = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> not_found:</span><br><span class="line">            pos.next_node = node.next_node</span><br></pre></td></tr></table></figure></li><li><p>删除指定值的节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_by_value</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        删除链表中指定值节点</span></span><br><span class="line"><span class="string">        :param value: 删除的指定值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.__head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.__head.data == value:</span><br><span class="line">            <span class="comment"># 如果头节点值就等于删除值，直接将头结点指向下一个节点</span></span><br><span class="line">            self.__head = self.__head.next_node</span><br><span class="line"></span><br><span class="line">        pos = self.__head  <span class="comment"># 位置位</span></span><br><span class="line">        node = self.__head.next_node  <span class="comment"># 第一个节点</span></span><br><span class="line">        not_found = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> node.data != value:  <span class="comment"># 节点值不为删除值</span></span><br><span class="line">            <span class="keyword">if</span> node.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 如果当前节点的下一个节点不为空，继续后移位置节点</span></span><br><span class="line">                pos = node</span><br><span class="line">                node = node.next_node</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 直到链表最后节点,没找到指定节点，将没找到标志位置为True，并跳出循环</span></span><br><span class="line">                not_found = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> not_found:</span><br><span class="line">            pos.next_node = node.next_node</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>删除链表中倒数第n个节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_last_n_node</span>(<span class="params">self, n</span>):</span></span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       删除链表中倒数第n个节点</span></span><br><span class="line"><span class="string">       主体思路：</span></span><br><span class="line"><span class="string">       设置快慢指针，快指针先行n步，然后快慢指针同时往后移，</span></span><br><span class="line"><span class="string">       当快指针到链表尾部时，直接将这时慢指针的下一个节点接到第n个节点上</span></span><br><span class="line"><span class="string">       :param n: 删除倒数第n个节点</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       fast = self.__head</span><br><span class="line">       slow = self.__head</span><br><span class="line">       step = <span class="number">0</span></span><br><span class="line">       <span class="keyword">while</span> step &lt;= n:</span><br><span class="line">           fast = fast.next_node</span><br><span class="line">           step += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">       <span class="keyword">while</span> fast.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           tmp = slow</span><br><span class="line">           fast = fast.next_node</span><br><span class="line">           slow = slow.next_node</span><br><span class="line">       tmp.next_node = slow.next_node</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>查找链表中间节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_mid_node</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        查找链表中的中间节点</span></span><br><span class="line"><span class="string">        主体思路，依旧是快慢指针，快指针走2步，慢指针走1步，</span></span><br><span class="line"><span class="string">        当快指针到链表尾部时，慢指针刚好到中间</span></span><br><span class="line"><span class="string">        :return: 中间节点</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        fast = self.__head</span><br><span class="line">        slow = self.__head</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> fast.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            fast = fast.next_node.next_node</span><br><span class="line">            slow = slow.next_node</span><br><span class="line">        <span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure></li><li><p>创建一个节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_node</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        创建值为value的node</span></span><br><span class="line"><span class="string">        :param value: 创建节点的值</span></span><br><span class="line"><span class="string">        :return: 创建的节点</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> Node(value)</span><br></pre></td></tr></table></figure></li><li><p>打印当前链表所有节点数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_all</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        打印当前链表所有节点数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pos = self.__head</span><br><span class="line">        <span class="keyword">if</span> pos <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;当前链表为空&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> pos.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="built_in">str</span>(pos.data) + <span class="string">&quot;--&gt;&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            pos = pos.next_node</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(pos.data))</span><br></pre></td></tr></table></figure></li><li><p>翻转相邻两个节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__reversed_with_two_node</span>(<span class="params">self, pre, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        翻转相邻两个节点</span></span><br><span class="line"><span class="string">        :param pre: 前一个节点</span></span><br><span class="line"><span class="string">        :param node: 当前节点</span></span><br><span class="line"><span class="string">        :return: (pre， node)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        tmp = node.next_node</span><br><span class="line">        node.next_node = pre</span><br><span class="line">        pre = node  <span class="comment"># 啰嗦点但更容易理解</span></span><br><span class="line">        node = tmp</span><br><span class="line">        <span class="keyword">return</span> pre, node</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>翻转链表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversed_list</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        翻转链表</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> self.__head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> self.__head.next_node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 链表为空或只有一个节点直接返回</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    pre = self.__head</span><br><span class="line">    node = self.__head.next_node</span><br><span class="line">    <span class="comment"># 当下一个节点不为空，翻转两个节点</span></span><br><span class="line">    <span class="keyword">while</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        pre, node = self.__reversed_with_two_node(pre, node)</span><br><span class="line">        self.__head.next_node = <span class="literal">None</span></span><br><span class="line">        self.__head = pre</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>检查链表是否有环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">has_ring</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        检查链表是否有环, 快慢指针相遇为环</span></span><br><span class="line"><span class="string">        :return: True or False</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    fast = self.__head</span><br><span class="line">    slow = self.__head</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (fast <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (fast.next_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        <span class="comment"># 注意一定要有fast的下一个节点也不空</span></span><br><span class="line">        slow = slow.next_node</span><br><span class="line">        fast = fast.next_node.next_node</span><br><span class="line">        <span class="keyword">if</span> fast == slow:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ol><p>对上述代码进行测试如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    l = SingleLinkList()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        l.insert_node_to_head(i)</span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;5--&gt;4--&gt;3--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    node5 = l.find_by_value(<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(node5.data) <span class="comment">#5</span></span><br><span class="line">    node6 = l.find_by_index(<span class="number">6</span>)</span><br><span class="line">    <span class="built_in">print</span>(node6.data)<span class="comment">#2</span></span><br><span class="line">    l.insert_after(node5, <span class="number">31</span>)</span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;5--&gt;31--&gt;4--&gt;3--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    l.insert_before(node5, <span class="number">32</span>)</span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;32--&gt;5--&gt;31--&gt;4--&gt;3--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    l.delete_by_node(node5) <span class="comment">#删除指定节点5</span></span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;32--&gt;31--&gt;4--&gt;3--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    l.delete_by_value(<span class="number">31</span>) </span><br><span class="line">    l.print_all()<span class="comment">#8--&gt;7--&gt;6--&gt;32--&gt;4--&gt;3--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    l.delete_last_n_node(<span class="number">2</span>)<span class="comment">#删除倒数第2个节点</span></span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;32--&gt;4--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    node_mid = l.find_mid_node()</span><br><span class="line">    <span class="built_in">print</span>(node_mid.data) <span class="comment">#32</span></span><br><span class="line">    create_node = l.create_node(<span class="number">99</span>)</span><br><span class="line">    l.insert_after(node5, create_node.data)</span><br><span class="line">    l.print_all() <span class="comment">#8--&gt;7--&gt;6--&gt;32--&gt;4--&gt;2--&gt;1--&gt;0</span></span><br><span class="line">    l.reversed_list()</span><br><span class="line">    l.print_all() <span class="comment">#0--&gt;1--&gt;2--&gt;4--&gt;32--&gt;6--&gt;7--&gt;8</span></span><br><span class="line">    has_ring = l.has_ring()</span><br><span class="line">    <span class="built_in">print</span>(has_ring) <span class="comment">#False</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 单链表 </tag>
            
            <tag> 翻转链表 </tag>
            
            <tag> 链表存在环 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
